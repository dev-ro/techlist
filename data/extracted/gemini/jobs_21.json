[
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Milan, IL",
        "job_id": 3937088988,
        "company": "John Deere",
        "title": "2024067 Senior Software Engineer",
        "created_on": 1720635050.6882167,
        "description": "There are 7 billion people on this planet. And by 2050, there will be 2 billion more... many moving into urban centers at an unprecedented rate. Making sure there is enough food, fiber and infrastructure for our rapidly growing world is what we’re all about at John Deere. And it’s why we’re investing in our people and our technology like never before in our 175-year history. Here the world’s brightest minds are tackling the world’s biggest challenges. If you believe one person can make the world a better place, we’ll put you to work. RIGHT NOW. Deere & Company is seeking a Senior Software Engineer in Milan, IL with the following requirements: Bachelor’s degree in Electronics and Telematics or Computer Science or related field plus 5 years related experience. Required skills: Develops software solutions by studying information needs, conferring with users, and studying systems flow, data usage, and work processes (5 years); Documents and demonstrates solutions by developing documentation, flowcharts, layouts, diagrams, charts, code comments and clear code (5 years); Migrating existing on-premise applications and reporting environments to cloud-based application using Amazon web services (6 months); Connect and communicate with AWS environment to the backend SAP ERP components using Odata services (6 months); Front end developments using JAVA, REACT, CSS, HTML, JavaScript (5 years). 50% remote work allowed. Must live within normal commuting distance of the worksite. 10% domestic and international travel required. Please apply on-line at www.JohnDeere.com/USJobs and search for Job Posting Number 2024067. John Deere is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to, among other things, race, religion, color, national origin, sex, age, sexual orientation, gender identity or expression, status as a protected veteran, or status as a qualified individual with disability.",
        "url": "https://www.linkedin.com/jobs/view/3937088988",
        "summary": "John Deere seeks a Senior Software Engineer in Milan, IL to develop software solutions, migrate on-premise applications to AWS, connect AWS to SAP ERP using Odata services, and build front-end applications using Java, React, CSS, HTML, and JavaScript. 50% remote work is allowed, and 10% travel is required.",
        "industries": [
            "Agriculture",
            "Technology",
            "Software Development",
            "Cloud Computing",
            "Manufacturing"
        ],
        "soft_skills": [
            "Communication",
            "Problem Solving",
            "Teamwork",
            "Analytical Skills",
            "Documentation",
            "User Interaction"
        ],
        "hard_skills": [
            "Software Development",
            "Amazon Web Services (AWS)",
            "SAP ERP",
            "Odata",
            "Java",
            "React",
            "CSS",
            "HTML",
            "JavaScript"
        ],
        "tech_stack": [
            "AWS",
            "SAP ERP",
            "Odata",
            "Java",
            "React",
            "CSS",
            "HTML",
            "JavaScript"
        ],
        "programming_languages": [
            "Java",
            "JavaScript"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Electronics and Telematics",
                "Computer Science",
                "Related Fields"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Remote Work",
            "Travel"
        ]
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Chicago, IL",
        "job_id": 3938334379,
        "company": "JLL Technologies",
        "title": "Software Engineer II",
        "created_on": 1720635052.7124026,
        "description": "JLL is looking for a Full-stack Developer with strong Microsoft .Net, Node.js, ReactJS, NoSQL skills. In this role, the person will be responsible for application development and support on a day-to-day basis, maintaining, monitoring application performance, and applying patches to all tiers. Candidates must thrive in a fast-paced environment and have a passion for learning and working with cutting-edge technologies. The selected candidate will have the opportunity to work on GenAI Projects, pushing the boundaries of innovation and creating impactful solutions. Responsibilities: Work with product technical leads to design new products or enhancements.  Take part in system development including analysis, coding, and testing. Perform solution design and contribute to projects involving GenerativeAI Performs the unit testing, system integration testing and assist with user acceptance testing. Provide on-going support to application used within the organization. Sound like you? To apply you need to be / have: Experience & Education At least 4 years IT experience Reliable, self-motivated, and self-disciplined individual. Exceptional Debugging Skills And Strong Experience With Performance Tuning. Effective written and verbal communication skills. Excellent technical, analytical and organizational skills. Good range of hands-on technical experience Technical Skills & Competencies: Mandatory: Server side development - ASP.NET Core NoSQL Databases (Azure CosmosDB or MongoDB) Relational Databases (SQL Server) Rest API JavaScript frameworks ReactJS Conversant with agile methodologies Preferable: Experience using TypeScript Familiarity with Development of re-useable frameworks, libraries to be used by multiple development teams Familiarity with cloud native application architecture patterns Deploy Applications in Microsoft Azure API Design concepts and Development of RESTful web services Experience using DevOps Tools for CI/CD Experience with Tools and Best Practices for Secure Application Development Note: While prior experience with GenerativeAI technologies is not required, a strong interest in learning and working with GenerativeAI technologies is desired for this role. The selected candidate will have the opportunity to gain experience and contribute to GenAI Projects.",
        "url": "https://www.linkedin.com/jobs/view/3938334379",
        "summary": "JLL seeks a Full-stack Developer with 4+ years of experience in .NET, Node.js, ReactJS, and NoSQL.  The role involves application development, support, maintenance, performance monitoring, and patching. The ideal candidate is passionate about cutting-edge technologies, including GenAI, and has strong debugging, analytical, and communication skills.",
        "industries": [
            "Information Technology",
            "Real Estate"
        ],
        "soft_skills": [
            "Communication",
            "Teamwork",
            "Problem-Solving",
            "Analytical",
            "Self-Motivation",
            "Passion for Learning"
        ],
        "hard_skills": [
            "ASP.NET Core",
            "Node.js",
            "ReactJS",
            "NoSQL",
            "Azure CosmosDB",
            "MongoDB",
            "SQL Server",
            "REST API",
            "JavaScript",
            "TypeScript",
            "Agile Methodologies",
            "Cloud Native Architecture",
            "Microsoft Azure",
            "API Design",
            "RESTful Web Services",
            "DevOps",
            "CI/CD",
            "Secure Application Development"
        ],
        "tech_stack": [
            "Microsoft .NET",
            "Node.js",
            "ReactJS",
            "NoSQL",
            "Azure CosmosDB",
            "MongoDB",
            "SQL Server",
            "REST API",
            "JavaScript",
            "TypeScript",
            "Azure",
            "GenAI",
            "Generative AI"
        ],
        "programming_languages": [
            "C#",
            "JavaScript",
            "TypeScript"
        ],
        "experience": 4,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Chicago, IL",
        "job_id": 3919915010,
        "company": "Coinbase",
        "title": "Software Engineer, Frontend - Consumer Products",
        "created_on": 1720635054.4952068,
        "description": "At Coinbase, our mission is to increase economic freedom around the world, and we couldn’t do this without hiring the best people. We’re a group of hard-working overachievers who are deeply focused on building the future of finance and Web3 for our users across the globe, whether they’re trading, storing, staking or using crypto. Know those people who always lead the group project? That’s us. There are a few things we look for across all hires we make at Coinbase, regardless of role or team. First, we look for candidates who will thrive in a culture like ours, where we default to trust, embrace feedback, and disrupt ourselves. Second, we expect all employees to commit to our mission-focused approach to our work. Finally, we seek people who are excited to learn about and live crypto, because those are the folks who enjoy the intense moments in our sprint and recharge work culture. We’re a remote-first company looking to hire the absolute best talent all over the world. Ready to ? Who you are: You’ve got positive energy. You’re optimistic about the future and determined to get there. You’re never tired of learning. You want to be a pro in bleeding edge tech like DeFi, NFTs, DAOs, and Web 3.0. You appreciate direct communication. You’re both an active communicator and an eager listener - because let’s face it, you can’t have one without the other. You’re cool with candid feedback and see every setback as an opportunity to grow. You can pivot on the fly. Crypto is constantly evolving, so our priorities do, too. What you worked on last month may not be what you work on today, and that excites you. You’re not looking for a boring job. You have a “can do” attitude. Our teams create high-quality work on quick timelines. Owning a problem doesn’t scare you, but rather empowers you to take 100% responsibility for achieving our mission. You want to be part of a winning team. We’re stronger together, and you’re a person who embraces being pushed out of your comfort zone. Coinbase is seeking experienced frontend engineers to join our team to build out the next generation of crypto-forward products and features. You will help build the next generation of systems to make cryptocurrency accessible to everyone across the globe. The Consumer Product Group engineers strive to make the Coinbase retail app the easiest-to-use and fastest experience for accessing the crypto. In 2024, our goal is to be the #1 self custody wallet measured by MTU’s and download share. Wallet is the future of web3 and of Coinbase. Suite of products include: Coinbase app, advance trade, dapps, and staking. What You'll Be Doing Design secure and highly reliable services to integrate with blockchains and add new cryptocurrency assets to Coinbase, Pro, Custody among other business units. Build a mass market interface for digital currency apps by adding new functionality to existing cryptocurrencies (i.e. staking, voting, predicting). Write infrastructural services that provide secure storage, accounting and transactional services to help our customers transfer assets safely. Work with engineers, product managers and senior leadership to turn our vision into a tangible roadmap every quarter. Add positive energy in every meeting, and make your coworkers feel included in every interaction. What We Look For In You You have at least 2+ years of experience in developing web apps and shipping user-facing features with JavaScript and modern, component-based JS frameworks like React. You've developed and shipped user-facing features using component-based UI frameworks. You’re familiar with current trends and best practices in front-end architecture, including performance, security and usability. You’re familiar with product and design lifecycles, and collaborating closely with designers, engineers, and product managers. You write high quality, well tested code to meet the needs of your customers. Nice To Haves You've worked on front-end teams in the finance or security space You have at least 1 year of experience in developing React Native mobile apps or helped move existing native apps to React Native Job #: GCFE04US Pay Transparency Notice: Depending on your work location, the target annual salary for this position can range from $[Zone 3 Pay] to $[Zone 1 Pay] + target bonus + target equity + benefits (including medical, dental, vision and 401(k)). Pay Transparency Notice: Depending on your work location, the target annual salary for this position can range as detailed below. Full time offers from Coinbase also include target bonus + target equity + benefits (including medical, dental, vision and 401(k)). Pay Range $147,900—$174,000 USD Commitment to Equal Opportunity Coinbase is committed to diversity in its workforce and is proud to be an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, creed, gender, national origin, age, disability, veteran status, sex, gender expression or identity, sexual orientation or any other basis protected by applicable law. Coinbase will also consider for employment qualified applicants with criminal histories in a manner consistent with applicable federal, state and local law. For US applicants, you may view the Know Your Rights notice here. Additionally, Coinbase participates in the E-Verify program in certain locations, as required by law. Coinbase is also committed to providing reasonable accommodations to individuals with disabilities. If you need a reasonable accommodation because of a disability for any part of the employment process, please contact us at accommodations[at]coinbase.com to let us know the nature of your request and your contact information. For quick access to screen reading technology compatible with this site click here to download a free compatible screen reader (free step by step tutorial can be found here). Global Data Privacy Notice for Job Candidates and Applicants Depending on your location, the General Data Protection Regulation (GDPR) and California Consumer Privacy Act (CCPA) may regulate the way we manage the data of job applicants. Our full notice outlining how data will be processed as part of the application procedure for applicable locations is available here. By submitting your application, you are agreeing to our use and processing of your data as required. For US applicants only, by submitting your application you are agreeing to arbitration of disputes as outlined here.",
        "url": "https://www.linkedin.com/jobs/view/3919915010",
        "summary": "Coinbase is looking for experienced frontend engineers to join their team and build the next generation of crypto-forward products and features. The role involves designing secure and reliable services, building a mass market interface for digital currency apps, writing infrastructural services for secure storage, and collaborating with engineers, product managers, and senior leadership.",
        "industries": [
            "Finance",
            "Technology",
            "Cryptocurrency"
        ],
        "soft_skills": [
            "Positive Energy",
            "Optimistic",
            "Determined",
            "Continuous Learning",
            "Direct Communication",
            "Active Communicator",
            "Eager Listener",
            "Candid Feedback",
            "Adaptable",
            "Pivot on the Fly",
            "Can Do Attitude",
            "Problem-Solving",
            "Teamwork",
            "Embraces Challenges"
        ],
        "hard_skills": [
            "JavaScript",
            "React",
            "Component-Based UI Frameworks",
            "Front-End Architecture",
            "Performance Optimization",
            "Security",
            "Usability",
            "Product and Design Lifecycles",
            "Collaboration",
            "Code Quality",
            "Testing"
        ],
        "tech_stack": [
            "React",
            "React Native",
            "DeFi",
            "NFTs",
            "DAOs",
            "Web 3.0"
        ],
        "programming_languages": [
            "JavaScript"
        ],
        "experience": 2,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 174000,
            "min": 147900
        },
        "benefits": [
            "Medical",
            "Dental",
            "Vision",
            "401(k)",
            "Equity",
            "Bonus"
        ]
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Willowbrook, IL",
        "job_id": 3888423986,
        "company": "Stellent IT",
        "title": "Software Test Integration Engineer",
        "created_on": 1720635056.148701,
        "description": "Start asap! Must be local to Burr Ridge, IL area Here you go! CNH-BURR RIDGE IL (HYBRID) They are looking to bring on a Software Test Integration Engineer asap. This role will need to be based here locally in Burr Ridge to go on site and collaborate with local team. I have attached a few sample resumes of candidates that would be a match for this. This role will be to collaborate with systems engineers to review requirements and specifications. Help to write test cases for planer software tests Help execute tests and troubleshoot bugs. Test will be mostly manual. Interface with bench tests and simulators Help build test environments on test bench",
        "url": "https://www.linkedin.com/jobs/view/3888423986",
        "summary": "A Software Test Integration Engineer is needed in Burr Ridge, IL to collaborate with systems engineers, write test cases, execute tests, troubleshoot bugs, interface with bench tests and simulators, and build test environments.",
        "industries": [
            "Software",
            "Technology",
            "Engineering",
            "Manufacturing"
        ],
        "soft_skills": [
            "Collaboration",
            "Communication",
            "Problem-solving",
            "Troubleshooting"
        ],
        "hard_skills": [
            "Test Case Writing",
            "Test Execution",
            "Bug Troubleshooting",
            "Software Testing",
            "Test Environments",
            "Simulation"
        ],
        "tech_stack": [
            "Test Bench",
            "Simulators",
            "Planer Software"
        ],
        "programming_languages": [],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Arlington Heights, IL",
        "job_id": 3969344558,
        "company": "Energy Jobline",
        "title": "Sr. Software Development Engineer, AWS Commerce Platform",
        "created_on": 1720635059.407603,
        "description": "Job Description As a senior software engineer in AWS Commerce Platform, you will work on the hardest engineering problems in distributed systems at a massive scale and on multiple dimensions. You will exercise your intellectual curiosity, work with motivated teams of sharp individuals, partner with other senior and principal engineers, and provide guidance in defining your team’s engineering roadmap. AWS Commerce Platform provides the infrastructure and services that enable AWS teams to sell their services to AWS customers, as well as help AWS customers understand and manage their infrastructure costs. AWS Commerce Platform computes the bills for AWS customers based on their usage, invoices them and collects payments. Our team in AWS Billing (part of AWS Commerce Platform) owns one of the largest datastores at Amazon. We provide Git semantics like history, auditability, and branching for managing versioned datasets at scale. Today, we support clients with datasets that span across millions of repositories, comprising billions of files and petabytes of data, and handle upward of 1 million transactions per second. We enable myriad use cases in AWS that require low-latency, high throughput and are least error-tolerant workflows. 10017 Key job responsibilities As a Sr. Software Development Engineer, You Will Own design, development, test, deployment and operation of highly available, secure systems that handle confidential billing data at scale. Contribute directly to our growth by hiring smart and motivated engineers that can deliver swiftly and predictably, adjusting in an agile fashion to deliver what our customers need. Be responsible for ensuring sanity of architecture, operational excellence and quality, delivering results on time. Work closely with other teams across AWS and Amazon to deliver platform features that require cross-team leadership. Obsess over protecting customer information, maintaining single-digit millisecond latencies, and simplifying enterprise scale commerce complexity. Mentor other engineers and interns on the team. Regularly dive into architecture, code, test plans, project plans, deployments and operations to drive excellence throughout our organization. We are open to hiring candidates to work out of one of the following locations: Arlington, VA, USA BASIC QUALIFICATIONS- 5+ years of non- professional software development experience 5+ years of programming with at least one software programming experience 5+ years of leading design or architecture (design patterns, reliability and scaling) of new and existing systems experience Experience as a mentor, tech lead or leading an engineering team QUALIFICATIONS- 5+ years of full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations experience Bachelor's degree in computer science or equivalent Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of , origin, , , , protected veteran status, , , or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en//us.",
        "url": "https://www.linkedin.com/jobs/view/3969344558",
        "summary": "Senior Software Engineer position at AWS Commerce Platform responsible for designing, developing, testing, deploying, and operating highly available, secure systems that handle confidential billing data at scale.  The role requires experience with distributed systems, data management, and leading engineering teams. This role is also involved in hiring, mentoring, and driving operational excellence.",
        "industries": [
            "Software Development",
            "Cloud Computing",
            "E-commerce",
            "Data Management",
            "Information Technology",
            "Financial Technology (FinTech)"
        ],
        "soft_skills": [
            "Communication",
            "Leadership",
            "Problem Solving",
            "Collaboration",
            "Teamwork",
            "Mentorship",
            "Time Management",
            "Organization",
            "Analytical Skills",
            "Critical Thinking"
        ],
        "hard_skills": [
            "Distributed Systems",
            "Data Management",
            "System Design",
            "Architecture",
            "Scalability",
            "Reliability",
            "Security",
            "Agile Development",
            "Git",
            "Code Review",
            "Source Control Management",
            "Build Processes",
            "Testing",
            "Operations"
        ],
        "tech_stack": [
            "AWS",
            "AWS Commerce Platform",
            "AWS Billing",
            "Git",
            "Datastores",
            "Distributed Systems",
            "Large-Scale Data Management"
        ],
        "programming_languages": [],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Computer Science"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Illinois, United States",
        "job_id": 3952969784,
        "company": "Motorola Solutions",
        "title": "System Engineer",
        "created_on": 1720635061.212847,
        "description": "Company Overview At Motorola Solutions, we're guided by a shared purpose - helping people be their best in the moments that matter - and we live up to our purpose every day by solving for safer. Because people can only be their best when they not only feel safe, but are safe. We're solving for safer by building the best possible technologies across every part of our safety and security ecosystem. That's mission-critical communications devices and networks, AI-powered video security & access control and the ability to unite voice, video and data in a single command center view. We're solving for safer by connecting public safety agencies and enterprises, enabling the collaboration that's critical to connect those in need with those who can help. The work we do here matters. Department Overview At Motorola Solutions, we help people be their best in the moments that matter. We help firefighters see around buildings and police officers see around street corners. Our work is very meaningful, impactful, and cutting edge and we invite you to explore it. We are the Best! Motorola Solutions is a world leader in the field of professional mobile communications systems with an impressive heritage of technological innovations and a global base of installed systems. The group's main business is the implementation of mission critical digital mobile communication systems for Government & Public Safety Communications customers. Job Description Our Company has an outstanding track record of success and strives to build long term relationships with our customers. Customer satisfaction is given great emphasis and we strive to build a trusted advisor relationship with every customer. The customer team comprises Sales, Project Management, System Engineering and System Technologist personnel working together in a high performance culture, often with demanding cycle times and high customer expectations because we are working on mission critical communication systems. Many of our systems are designed around the APCO Project 25 communications standard. With the growth of data services some of our customer’s mobile wireless networks are being enhanced with LTE infrastructure for applications such as live mobile video transmission supplementing our traditional core voice and narrow-band data networks. The Senior System Engineer will take responsibility for and oversee all technical aspects of assigned project from award until system acceptance. This involves: Final Design and implementation of the radio network including infrastructure, subscribers, and mobile back-haul Customer presentations System staging System acceptance testing Radio coverage testing and interference mitigation Radio system licensing (FCC/Industry Canada/NTIA) Preparation of system fleet-map, encryption and radio programming templates Complete system documentation Live system upgrades FED: Technical Requirements tracking through the use of RTVMs Travel is required for all project phases, from design through final acceptance. Specific Knowledge/Skills: Experience/knowledge in radio communication systems, radio propagation, RF, wireless or broadband/LTE Experience, good understanding, and working knowledge in IP networking, network design methodologies & troubleshooting (Ethernet, TCP/IP, L2/L3, MPLS) Certifications through CompTIA Network+, Juniper JNCIA-Junos, Nokia NRS1, CCNA or similar Experience directly working with end-user customers is desirable Excellent communications skills and the ability to present complex topics to nontechnical audiences. Self motivated and able to maintain focus in a pressured environment Ready for business' trips up to 40% of working time Preferred Experience: Previous knowledge and experience working with Juniper, Aviat, Cisco, Nokia/Alcatel, Xtreme, Ceragon Experience with switching and routing protocols (OSPF, ISIS, iBGP, eBGP, M-BGP, MPLS RSVP-TE, and LDP) Experience/knowledge in line of sight microwave propagation Basic Requirements 3+ years of experience in one of the following; Engineering, Computer Science, Networking, IT, Cyber Security or Information Systems OR Bachelor's Degree in one if the following; Engineering, Computer Science, Networking, IT, Cyber Security or Information Systems Must be able to obtain background clearance as required by government customer Travel Requirements Under 25% Relocation Provided None Position Type Experienced Referral Payment Plan Yes Our U.S. Benefits include: Incentive Bonus Plans Medical, Dental, Vision benefits 401K with Company Match 9 Paid Holidays Generous Paid Time Off Packages Employee Stock Purchase Plan Paid Parental & Family Leave and more! EEO Statement Motorola Solutions is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion or belief, sex, sexual orientation, gender identity, national origin, disability, veteran status or any other legally-protected characteristic. We are proud of our people-first and community-focused culture, empowering every Motorolan to be their most authentic self and to do their best work to deliver on the promise of a safer world. If you’d like to join our team but feel that you don’t quite meet all of the preferred skills, we’d still love to hear why you think you’d be a great addition to our team. We’re committed to providing an inclusive and accessible recruiting experience for candidates with disabilities, or other physical or mental health conditions. To request an accommodation, please email ohr@motorolasolutions.com.",
        "url": "https://www.linkedin.com/jobs/view/3952969784",
        "summary": "Motorola Solutions is seeking a Senior System Engineer to oversee technical aspects of assigned projects in the field of professional mobile communication systems. The role involves final design and implementation of radio networks, customer presentations, system testing, and documentation. Experience with radio communication systems, IP networking, and project management is required, along with strong communication and presentation skills. Travel is expected for all project phases.",
        "industries": [
            "Telecommunications",
            "Technology",
            "Public Safety",
            "Government",
            "Engineering",
            "IT"
        ],
        "soft_skills": [
            "Communication",
            "Presentation",
            "Problem Solving",
            "Technical Skills",
            "Project Management",
            "Customer Service",
            "Teamwork",
            "Leadership",
            "Self-Motivation",
            "Attention to Detail",
            "Time Management",
            "Adaptability",
            "Problem Solving",
            "Decision Making"
        ],
        "hard_skills": [
            "Radio Communication Systems",
            "Radio Propagation",
            "RF",
            "Wireless",
            "Broadband",
            "LTE",
            "IP Networking",
            "Network Design",
            "Troubleshooting",
            "Ethernet",
            "TCP/IP",
            "L2/L3",
            "MPLS",
            "OSPF",
            "ISIS",
            "iBGP",
            "eBGP",
            "M-BGP",
            "MPLS RSVP-TE",
            "LDP",
            "Microwave Propagation",
            "Project 25",
            "FCC Licensing",
            "System Documentation",
            "RTVM",
            "System Staging",
            "System Acceptance Testing",
            "Radio Coverage Testing",
            "Interference Mitigation"
        ],
        "tech_stack": [
            "Juniper",
            "Aviat",
            "Cisco",
            "Nokia/Alcatel",
            "Xtreme",
            "Ceragon",
            "CompTIA Network+",
            "Juniper JNCIA-Junos",
            "Nokia NRS1",
            "CCNA"
        ],
        "programming_languages": [],
        "experience": 3,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Engineering",
                "Computer Science",
                "Networking",
                "IT",
                "Cyber Security",
                "Information Systems"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Incentive Bonus Plans",
            "Medical",
            "Dental",
            "Vision",
            "401K with Company Match",
            "9 Paid Holidays",
            "Generous Paid Time Off Packages",
            "Employee Stock Purchase Plan",
            "Paid Parental & Family Leave"
        ]
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Chicago, IL",
        "job_id": 3952750022,
        "company": "Flexport",
        "title": "Senior Software Engineer, Trucking",
        "created_on": 1720635062.8454666,
        "description": "About Flexport: At Flexport, we believe global trade can move the human race forward. That’s why it’s our mission to make global commerce so easy there will be more of it. We’re shaping the future of a $8.6T industry with solutions powered by innovative technology and exceptional people. Today, companies of all sizes - from emerging brands to Fortune 500s - use Flexport technology to move more than $19B of merchandise across 112 countries a year. The recent global supply chain crisis has put Flexport center stage as we continue to play a pivotal role in how goods move around the world. At a valuation of $8 billion, we’re experiencing record growth and are proud to have the support of the best investors in the game who believe in our mission, solutions and people. Ready to tackle global challenges that impact business, society, and the environment? Come join us. The opportunity: Flexport is a platform for global trade in an industry that comprises 12% of the global GDP. We are the first licensed freight forwarder and customs brokerage built around a modern tech stack. We help the world's fastest-growing brands manage their international supply chains, providing a fully-managed service for moving products between any two places on the planet. Exceptional software development is integral in allowing us to fulfill our mission of fixing the user experience in global trade. At Flexport you’ll develop products that are at the forefront of reshaping the entire logistics & supply chain industries. You’ll work alongside self-starters interested in solving real-world problems and streamlining the inefficiencies in the complex global trade industry. You’ll have the opportunity to reshape an industry by creating the new operating system for global trade. Flexport Trucking team is modernizing the trillion dollar trucking industry with a best in class digital freight network. We are building the platform to manage carrier contracts globally, plan the most efficient trucking execution and allocate carriers leveraging cutting edge recommendation and relevance models. Our engineers are top-notch software developers who love listening to the users, working as a team, and proactively taking the lead whenever necessary. ***This position requires attendance in the Chicago, IL, Atlanta, GA or New York office 3 days a week with flexibility to work remotely 2 days per week*** You will: Design, develop, test and maintain systems that represent the real-world intricacies of logistics entities. Collaborate with some of the best engineers in the industry to work on complex business and technical problems. Architect and drive major projects while coordinating with multiple stakeholders. Work closely with business partners and product managers to distill complex business problems into elegant technical solutions. Build and launch products rapidly and incrementally. We ship to production dozens of times per day. Lead by example, spending most of your time building performant and reliable services. Guide the team's technical roadmap, pushing for the technology choices that will make our systems performant, reliable, and easy to build in. Ensure our products maintain a high-quality bar. We believe in anticipating issues by obsessing over metrics and applying key learnings from post-mortems to all our services. You play a significant role in the career development of others, actively mentoring and educating the larger software engineering community on trends, technologies, and best practices. You should have: 5+ years of professional software development experience. 3+ years of design and architecture knowledge as well as familiarity with object-oriented analysis and design patterns (OOA/OOD) Experience of building large scale, high-performance systems in a complex, multi-tiered, distributed environment. Knowledge of at least one programming language such as Java, Kotlin, Ruby, Python or C++. Understanding of database tradeoffs and ability to pick and work with the right tool for the job (SQL, NoSQL, scaling characteristics, etc) Experience driving inter-team technical collaboration, communication and alignment Experience running code reviews and mentoring engineers in a technical capacity It’s a plus if you: Have experience with React Have experience with the AWS tech stack A bias for action. When you see a problem, you solve it. Understanding of performance tradeoffs, load balancing and operational issues. Demonstrated ability to mentor junior software engineers in all aspects of their engineering skill-sets. We know everyone's career looks a little bit different and encourage you to apply even if your experience doesn't precisely match the job description! Worried about not having any logistics experience? Don’t be! Our mission is to make global commerce so easy there will be more of it. That’s why it’s important to bring people from diverse backgrounds and experiences together with our industry veterans to help move the global logistics industry forward. We know this industry is complex. That’s why we invest in education starting day one with Flexport Academy, a one week intensive onboarding program designed specifically to set every new Flexport employee up for success. At Flexport, our ability to fulfill our mission of making global commerce easy and accessible relies on having a diverse, dedicated and engaged workforce. That is why Flexport is committed to creating and nurturing an environment where anyone can be their authentic self. All qualified applicants will receive consideration for employment regardless of race, color, religion, sex, national origin, age, physical and mental disability, health status, marital and family status, sexual orientation, gender identity and expression, military and veteran status, and any other characteristic protected by applicable law.",
        "url": "https://www.linkedin.com/jobs/view/3952750022",
        "summary": "Flexport is seeking a Senior Software Engineer to join their Trucking team in Chicago, Atlanta, or New York. The role involves designing, developing, and maintaining systems related to logistics entities, collaborating with engineers to solve complex problems, and architecting and driving major projects. Ideal candidates possess 5+ years of software development experience, 3+ years of design and architecture knowledge, and experience building large-scale, high-performance systems. Familiarity with programming languages like Java, Kotlin, Ruby, Python, or C++, understanding of database tradeoffs, and experience with inter-team technical collaboration are essential. Experience with React, AWS, and mentoring junior engineers is a plus. Flexport offers a competitive salary and benefits, including a one-week intensive onboarding program.",
        "industries": [
            "Logistics",
            "Supply Chain",
            "Transportation",
            "Freight Forwarding",
            "Technology",
            "Software Development",
            "E-commerce",
            "Global Trade"
        ],
        "soft_skills": [
            "Collaboration",
            "Communication",
            "Problem Solving",
            "Technical Leadership",
            "Mentorship",
            "Teamwork",
            "Self-Motivation",
            "Proactive",
            "Bias for Action"
        ],
        "hard_skills": [
            "Java",
            "Kotlin",
            "Ruby",
            "Python",
            "C++",
            "SQL",
            "NoSQL",
            "React",
            "AWS",
            "OOA/OOD",
            "Design Patterns",
            "Architecture",
            "Distributed Systems",
            "Performance Optimization",
            "Load Balancing"
        ],
        "tech_stack": [
            "React",
            "AWS"
        ],
        "programming_languages": [
            "Java",
            "Kotlin",
            "Ruby",
            "Python",
            "C++",
            "SQL"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Engineering",
                "Related Fields"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Competitive Salary",
            "Benefits Package",
            "Intensive Onboarding Program",
            "Mentorship",
            "Growth Opportunities",
            "Remote Work Flexibility"
        ]
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Schaumburg, IL",
        "job_id": 3852140739,
        "company": "Zurich North America",
        "title": "Senior Informatica MDM Data Engineer",
        "created_on": 1720635064.9631176,
        "description": "Zurich North America is seeking an experienced Senior MDM Data Engineer with Informatica SaaS expertise to join our Data & Analytics Engineering team. As a Senior MDM Developer, you will play a critical role in designing, implementing, and maintaining our Master Data Management (MDM) solutions using the Informatica SaaS platform. The Senior MDM Developer will work in a hybrid schedule out of our Schaumburg Headquarters. Join our dynamic Data & Analytics Engineering team and contribute to the development of cutting-edge MDM solutions using Informatica SaaS. Zurich North America offers a collaborative work environment, competitive compensation package, and opportunities for professional growth. Apply now to be part of our innovative team and make a difference in the world of data management. This is an exciting opportunity to contribute to the development of scalable MDM systems and drive data-driven decision-making across our organization. Responsibilities: Design, develop, and implement Master Data Management solutions using Informatica SaaS platform to meet business requirements and ensure data quality and integrity. Collaborate with cross-functional teams to gather MDM requirements and translate them into technical solutions leveraging Informatica MDM capabilities. Develop and maintain data integration interfaces with various source systems using IICS REST APIs, ensuring smooth and efficient data flow into the MDM system. Build and maintain MDM processes, workflows, and data models to support data governance and integration efforts. Perform data analysis and profiling tools to identify data quality issues and implement data cleansing and enrichment strategies. Collaborate with stakeholders to define and enforce data governance policies, standards, and best practices within the Informatica MDM environment. Provide technical guidance and mentorship to junior developers and team members on Informatica MDM best practices and development methodologies. Stay up to date with the latest Informatica MDM features and enhancements, evaluate their applicability to our MDM ecosystem, and recommend improvements to existing systems. Basic Qualifications: Bachelors Degree and 5 or more years of experience in the Data Management area OR High School Diploma or Equivalent and 7 or more years of experience in the Data Management area AND Experienced in data interrogation, data profiling or data modeling Preferred Qualifications: 5 or more years of experience in Master Data Management development, with a strong focus on Informatica IICS Hands-on experience with Informatica MDM SaaS platform, including configuration, customization, and development of MDM solutions. Strong understanding of IICS REST API and be able to configure requests, use the appropriate resources and methods, and work with applicable objects. Proficiency in Informatica Data Director, Informatica MDM Hub, and Informatica Cloud Data Integration. Solid understanding of MDM concepts, methodologies, and best practices. Strong SQL skills and experience with scripting languages (e.g., Python, PowerShell) for data manipulation and automation. Familiarity with data governance principles and their application within an Informatica MDM environment. Excellent analytical and problem-solving skills with the ability to analyze complex data-related issues. Effective communication and interpersonal skills to collaborate with cross-functional teams and stakeholders. Experience with Informatica IICS and its cloud-based components, including Master Data Management Cloud, Integration Cloud, Data Quality & Governance Cloud, and Data Security Cloud. Knowledge of cloud-based MDM solutions and integration with cloud platforms (e.g., Azure, AWS). Familiarity with data privacy regulations (e.g., GDPR, CCPA) and their impact on MDM. Previous experience in the insurance or financial services industry. As a condition of employment at Zurich, employees must adhere to any COVID-related health and safety protocols in place at that time ( https://www.zurichna.com/careers/faq ). A future with Zurich. What can go right when you apply at Zurich? Now is the time to move forward and make a difference. At Zurich, we want you to share your unique perspectives, experiences and ideas so we can grow and drive sustainable change together. As part of a leading global organization, Zurich North America has over 150 years of experience managing risk and supporting resilience. Today, Zurich North America is a leading provider of commercial property-casualty insurance solutions and a wide range of risk management products and services for businesses and individuals. We serve more than 25 industries, from agriculture to technology, and we insure 90% of the Fortune 500®. Our growth strategy is not limited to our business. As an employer, we strive to provide ongoing career development opportunities, and we foster an environment where voices are diverse, behaviors are inclusive, actions drive equity, and our people feel a sense of belonging. Be a part of the next evolution of the insurance industry. Join us in building a brighter future for our colleagues, our customers and the communities we serve. Zurich maintains a comprehensive employee benefits package for employees as well as eligible dependents and competitive compensation. Please click here to learn more. As a global company, Zurich recognizes the diversity of our workforce as an asset. We recruit talented people from a variety of backgrounds with unique perspectives that are truly welcome here. Taken together, diversity and inclusion bring us closer to our common goal: exceeding our customers’ expectations. Zurich does not discriminate on the basis of age, race, ethnicity, color, religion, sex, sexual orientation, gender expression, national origin, disability, protected veteran status or any other legally protected status. EOE disability/vet Zurich does not accept unsolicited resumes from search firms or employment agencies. Any unsolicited resume will become the property of Zurich American Insurance. If you are a preferred vendor, please use our Recruiting Agency Portal for resume submission. Location(s): AM - Schaumburg Remote Working: Hybrid Schedule: Full Time Employment Sponsorship Offered: Yes Linkedin Recruiter Tag:",
        "url": "https://www.linkedin.com/jobs/view/3852140739",
        "summary": "Zurich North America seeks an experienced Senior MDM Data Engineer with Informatica SaaS expertise to join their Data & Analytics Engineering team. The role involves designing, implementing, and maintaining MDM solutions using Informatica SaaS, collaborating with cross-functional teams, developing data integration interfaces, building and maintaining MDM processes, performing data analysis, and providing technical guidance. The position offers a hybrid work schedule out of Schaumburg Headquarters.",
        "industries": [
            "Insurance",
            "Financial Services",
            "Data Management",
            "Analytics"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Analytical",
            "Interpersonal",
            "Mentorship",
            "Leadership"
        ],
        "hard_skills": [
            "Informatica IICS",
            "Informatica MDM SaaS",
            "IICS REST API",
            "Informatica Data Director",
            "Informatica MDM Hub",
            "Informatica Cloud Data Integration",
            "SQL",
            "Python",
            "PowerShell",
            "Data Governance",
            "Data Privacy",
            "GDPR",
            "CCPA",
            "Azure",
            "AWS"
        ],
        "tech_stack": [
            "Informatica IICS",
            "Informatica MDM SaaS",
            "IICS REST API",
            "Informatica Data Director",
            "Informatica MDM Hub",
            "Informatica Cloud Data Integration",
            "SQL",
            "Python",
            "PowerShell",
            "Azure",
            "AWS"
        ],
        "programming_languages": [
            "SQL",
            "Python",
            "PowerShell"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelors Degree",
            "fields": [
                "Data Management"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Competitive Compensation",
            "Employee Benefits Package",
            "Career Development Opportunities"
        ]
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Chicago, IL",
        "job_id": 3927226418,
        "company": "Fabric Cryptography",
        "title": "Staff Software Engineer, LLVM",
        "created_on": 1720635066.698581,
        "description": "FABRIC CRYPTOGRAPHY WE ARE BUILDING THE WORLD'S FIRST CRYPTOGRAPHIC COMPUTER Fabric believes hardware determines the boundaries of humanity's collective creativity and imagination. We are building hardware for the next generation of cryptography because we believe in creating a more trustworthy world with secure, private computation at its core. Just as encryption and decryption enabled the Internet as we know it, this new paradigm of cryptographic algorithms, such as zero knowledge proofs, have even broader potential to revolutionize how trust, privacy, and identity work in our society. About the job We’re building the high-performance cryptography framework of the future, and we’re looking for someone to implement the fastest cryptography code on the planet. What you'll be doing... Use our framework to implement kernels in modern cryptography systems Optimize performance of kernels at an assembly level and codesign our ISA around performance opportunities Improve our framework and debug tools based on experience using them. Advise customers on using our system framework to build production systems Qualifications and experience requirements... Bachelor's degree in Computer Science, Software Engineering, Computer Engineering, or a related field 5+ years of industry experience Compiler development for hardware accelerators Experience in a startup or small company Excellent problem-solving skills and a strong attention to detail Effective communication and teamwork skills to collaborate with a diverse team Why Fabric of Cryptography? Our benefits are here to support you! Competitive salary and benefits Comprehensive medical / dental / vision, STD/LTD, AD+D and Life Insurance Flexible vacation and company paid holidays We’re remote friendly. We believe that your value to the team can be provided at one of our offices or from the comfort of your home. Depending on your function, enjoy the flexibility of remote work and join our growing community of remote employees in the US and around the world. We’re vested. You won’t just own your work here; you’ll have the potential to own equity in our company. We are competing in a hardware market that is projected to grow exponentially, which gives our company valuation room to grow at tremendous rates. Fabric Cryptography is committed to fostering a diverse work environment and proud to be an equal opportunity employer. As we highly value diversity in our current and future employees, we do not discriminate based on race, religion, color, national origin, gender, gender expression, sexual orientation, age, marital status, veteran status, disability status or any other characteristic protected by law.",
        "url": "https://www.linkedin.com/jobs/view/3927226418",
        "summary": "Fabric Cryptography is building the world's first cryptographic computer to revolutionize trust, privacy, and identity with secure, private computation. They are looking for a cryptography engineer to implement high-performance cryptography code and optimize its performance at an assembly level.",
        "industries": [
            "Computer Hardware",
            "Cybersecurity",
            "Cryptocurrency",
            "Software Development"
        ],
        "soft_skills": [
            "Problem-solving",
            "Attention to detail",
            "Communication",
            "Teamwork"
        ],
        "hard_skills": [
            "Compiler development",
            "Hardware acceleration",
            "Cryptography"
        ],
        "tech_stack": [
            "Modern cryptography systems",
            "Assembly level programming",
            "Hardware accelerator"
        ],
        "programming_languages": [],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Computer Science",
                "Software Engineering",
                "Computer Engineering"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Competitive salary",
            "Medical / dental / vision",
            "STD/LTD",
            "AD+D and Life Insurance",
            "Flexible vacation",
            "Paid holidays",
            "Remote work",
            "Equity"
        ]
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Chicago, IL",
        "job_id": 3938009989,
        "company": "Turnitin",
        "title": "Senior Software Engineer (C++ Angular) (USA & UK Remote)",
        "created_on": 1720635068.503167,
        "description": "Company Description When you join Turnitin, you'll be welcomed into a company that is a recognized innovator in the global education space. For more than 20 years, Turnitin has partnered with educational institutions to promote honesty, consistency, and fairness across all subject areas and assessment types. Over 16,000 academic institutions, publishers, and corporations use our services: Gradescope by Turnitin, iThenticate, Turnitin Feedback Studio, Turnitin Originality, Turnitin Similarity, ExamSoft, Ouriginal and ProctorExam. Turnitin has offices in Australia, India, Indonesia, Germany, Japan, Korea, Mexico, the Netherlands, the Philippines, Sweden, Ukraine, the United Kingdom, and the United States. Our diverse community of colleagues are all unified by a shared desire to make a difference in education. Come join us, and let's make change together. Job Description Turnitin is looking to grow our engineering team responsible for developing and supporting our desktop application used to deliver a software-based assessment experience to students worldwide. We are seeking C++ developers eager to solve challenging problems while maintaining quality, performance, and security. In this role, you’ll be working on various aspects of our codebase with support from top-notch colleagues. Our work requires a strong understanding of algorithms, experience with desktop application development, and working with multiple languages and technology stacks, primarily C++ and JavaScript. We use Kanban, an agile process, for our day-to-day work and project management. Our team is distributed across multiple time zones (US to UK), collaborates closely with our DevOps organization, and takes pride in the software we release. Unit and integration tests are essential, and code ownership means we are the team on call if issues arise. You’ll be working with people who love their jobs, relish challenges, and, most importantly, enjoy working together. You will gain experience working at a leading company using the latest technologies and have a direct impact on our users worldwide. Responsibilities: Hands-on position which requires 90-95% of writing code. Add new features to and maintain existing code. Design and build new services that add customer value Improve stability and monitoring of existing workflows Work with services running in concert at large scale Provide guidance in technical design and development activities. Ensure future success and stability through code reviews and automated testing. Work closely with software architects and database/operations engineers on system infrastructure and design. Qualifications Requirements: 5+ years experience of designing and developing applications using C++ 1+ year of cloud development w/AWS preferred Extensive experience with software development best practices (e.g. design patterns, test-driven development, code profiling, debugging). Effective technical and cross-functional communication (product, design, operations). Strong work ethic, “self-starter”, endless improvement, eager learner Preferred: Degree in Computer Science or related field 1+ years of experience with Angular/React/Javascript Experience developing and deploying applications utilizing Win32 APIs Experience in working with geographically distributed teams including the USA & European countries Additional Information The expected annual base salary range for this position is: $111,000/year to $185,000/year . This position is bonus eligible / commission-based. As a Remote-First company, actual compensation will be provided in writing at the time of offer, if extended, and is determined by work location and a range of other relevant factors, including but not limited to: experience, skills, degrees, licensures, certifications, and other job-related factors. Internal equity, market and organizational factors are also considered. Total Rewards @ Turnitin Turnitin maintains a Total Rewards package that is competitive within the local job market. People tend to think about their Total Rewards monetarily — solely as regular pay plus bonus or commission. This is what they earn in exchange for what they do. However, Turnitin delivers more than just these components. Beyond the intrinsic rewards of making a difference in the lives of educators, administrators, learners and researchers around the world, and thriving in an organization that is free of politics and full of humble, inclusive and collaborative teammates, the extrinsic rewards at Turnitin include generous time off and health and wellness programs that offer choice and flexibility and provide a safety net for the challenges that life presents from time to time. In our Remote-First approach to collaborating, you are also able to work the way that best fits your style and situation - whether that be remote, in one of our offices/rented spaces, or hybrid. Our Mission is to ensure the integrity of global education and meaningfully improve learning outcomes. Our Values underpin everything we do. Customer Centric - We realize our mission to ensure integrity and improve learning outcomes by  putting educators and learners at the center of everything we do. Passion for Learning - We seek out teammates that are constantly learning and growing and build a workplace which enables them to do so. Integrity - We believe integrity is the heartbeat of Turnitin. It shapes our products, the way we treat each other, and how we work with our customers and vendors. Action & Ownership - We have a bias toward action and empower teammates to make decisions. One Team - We strive to break down silos, collaborate effectively, and celebrate each other’s successes. Global Mindset - We respect local cultures and embrace diversity. We think globally and act locally to maximize our impact on education. Global Benefits Flexible/hybrid working Remote First Culture Health Care Coverage* Tuition Reimbursement* Competitive Paid Time Off 4 Self-Care Days per year National Holidays* 2 Founder Days + Juneteenth Observed Paid Volunteer Time* Charitable contribution match* Monthly Wellness Reimbursement/Home Office Equipment* Access to Modern Health (mental health platform) Parental Leave* Retirement Plan with match/contribution* * varies by country Seeing Beyond the Job Ad At Turnitin, we recognize it’s unrealistic for candidates to fulfill 100% of the criteria in a job ad.  We encourage you to apply if you meet the majority of the requirements because we know that skills evolve over time. If you’re willing to learn and evolve alongside us, join our team! Turnitin, LLC is committed to the policy that all persons have equal access to its programs, facilities and employment. All qualified applicants will receive consideration for employment without regard to age, race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.",
        "url": "https://www.linkedin.com/jobs/view/3938009989",
        "summary": "Turnitin is seeking a C++ developer to join its engineering team, responsible for developing and supporting its desktop application used for software-based assessment. The role requires 5+ years of C++ experience, 1+ year of cloud development with AWS preferred, and strong understanding of algorithms, desktop application development, and working with various tech stacks. The position is hands-on, with 90-95% of the work focused on writing code. Responsibilities include adding new features, designing and building new services, improving stability and monitoring workflows, working with large-scale services, providing technical guidance, ensuring code quality through reviews and testing, and collaborating with architects and engineers on system design. The expected salary range is $111,000 to $185,000 per year. Turnitin offers a competitive benefits package including health insurance, tuition reimbursement, generous time off, parental leave, and a retirement plan with a match.",
        "industries": [
            "Education Technology",
            "Software Development",
            "Online Education"
        ],
        "soft_skills": [
            "Communication",
            "Problem Solving",
            "Teamwork",
            "Collaboration",
            "Self-Motivation",
            "Time Management",
            "Leadership",
            "Continuous Learning",
            "Analytical Thinking"
        ],
        "hard_skills": [
            "C++",
            "JavaScript",
            "Angular",
            "React",
            "AWS",
            "Win32 APIs",
            "Kanban",
            "Agile",
            "DevOps",
            "Unit Testing",
            "Integration Testing",
            "Code Review",
            "Design Patterns",
            "Test-Driven Development",
            "Code Profiling",
            "Debugging",
            "Algorithms"
        ],
        "tech_stack": [
            "C++",
            "JavaScript",
            "Angular",
            "React",
            "AWS",
            "Win32 APIs",
            "Kanban",
            "Agile",
            "DevOps"
        ],
        "programming_languages": [
            "C++",
            "JavaScript",
            "Angular",
            "React"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Related Field"
            ]
        },
        "salary": {
            "max": 185000,
            "min": 111000
        },
        "benefits": [
            "Health Care Coverage",
            "Tuition Reimbursement",
            "Paid Time Off",
            "Self-Care Days",
            "National Holidays",
            "Founder Days",
            "Juneteenth Observed",
            "Paid Volunteer Time",
            "Charitable Contribution Match",
            "Wellness Reimbursement",
            "Home Office Equipment",
            "Access to Modern Health",
            "Parental Leave",
            "Retirement Plan with Match"
        ]
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Peoria, IL",
        "job_id": 3967172257,
        "company": "Falcon Placement Services",
        "title": "Embedded Software Engineer (Full Time)",
        "created_on": 1720635070.1235638,
        "description": "Job Responsibilities Design and implement software of embedded devices and systems from requirements to production and commercial deployment for product lines. Write and code logical and physical database descriptions and specify identifiers of database to management system or direct others in coding descriptions. Analyze and enhance efficiency, stability and scalability of system resources Review project requests describing database user needs to estimate time and cost required to accomplish project. Machine embedded software feature development Integration of software features taken from models and libraries to create software flash files Integration of configuration and calibration data Test plan development and software validation Identification and resolution of software defects- Software reviews Candidate Requirements Education & Experience Required BS in Computer Engineering, Computer Science, Electrical Engineering, or closely related field of study. Verbal and written communication skills, problem solving skills, customer service and interpersonal skills. Basic knowledge of the full software development lifecycle: from business/systems analysis, through requirements gathering and functional specification authoring, to development, testing and delivery. Basic ability to troubleshoot issues and make system changes as needed to resolve issue. Solid embedded programming experience in Matlab/Simulink. Solid embedded programming experience in embedded C Technical Skills Required Three or more years of experience with model-based development using Matlab/Simulink in an embedded software environment Two or more years of experience in embedded C software development Working knowledge of embedded real time operating systems Experience with embedded software development tools such as Git, debuggers, and compilers Strong communication skills, strong organizational and time management skills, strong analytical skills, and the ability to identify needed actions and act to resolve issues with minimal oversight Ability to work effectively with a culturally diverse work group Technical Skills Desired Experience with scripting (e.g. python, bash, ruby, etc.) Experience designing and developing embedded software based on the AUTOSAR framework Experience with FMEA processes and a basic knowledge of electronic system failure modes Experience developing embedded real-time software or systems for mobile, earthmoving, agricultural, industrial, or off-highway applications Soft Skills Strong communication skills, strong organizational and time management skills, strong analytical skills, and the ability to identify needed actions and act to resolve issues with minimal oversight Ability to work effectively with a culturally diverse work group.",
        "url": "https://www.linkedin.com/jobs/view/3967172257",
        "summary": "This job posting seeks an embedded software engineer with experience in model-based development using Matlab/Simulink, embedded C programming, and real-time operating systems. The role involves designing, implementing, and testing software for embedded devices and systems, collaborating with a diverse team, and adhering to the AUTOSAR framework.",
        "industries": [
            "Automotive",
            "Manufacturing",
            "Engineering",
            "Software Development"
        ],
        "soft_skills": [
            "Communication",
            "Organizational",
            "Time Management",
            "Analytical",
            "Problem Solving",
            "Customer Service",
            "Interpersonal",
            "Teamwork"
        ],
        "hard_skills": [
            "Matlab",
            "Simulink",
            "Embedded C",
            "Real-Time Operating Systems",
            "Git",
            "Debuggers",
            "Compilers",
            "Scripting",
            "Python",
            "Bash",
            "Ruby",
            "AUTOSAR",
            "FMEA",
            "Software Development Lifecycle",
            "Database Design",
            "Troubleshooting"
        ],
        "tech_stack": [
            "Matlab",
            "Simulink",
            "Embedded C",
            "Real-Time Operating Systems",
            "Git",
            "Debuggers",
            "Compilers",
            "AUTOSAR",
            "Python",
            "Bash",
            "Ruby"
        ],
        "programming_languages": [
            "C",
            "Python",
            "Bash",
            "Ruby"
        ],
        "experience": 3,
        "education": {
            "min_degree": "BS",
            "fields": [
                "Computer Engineering",
                "Computer Science",
                "Electrical Engineering"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Chicago, IL",
        "job_id": 3966202752,
        "company": "RemoteWorker US",
        "title": "Software Engineer Remote or Chicago (Blockchain DeFi)",
        "created_on": 1720635071.763766,
        "description": "Job Description Experienced Software Engineer Our client is on crypto’s cutting edge technology, and we’re looking for the right engineers to join them. This is a remote or onsite (Chicago) position for any US candidate. Our client's Blockchain Distribution Network ( BDN ) allows DeFi (Decentralized Finance) traders to make better trades by being better connected. The key difference is that in the decentralized world of DeFi, you need to be connected to everyone . Responsibilities As a member of an agile engineering team, you will help design, implement, test and deploy new features in very short cycles to an always-on critical-path infrastructure. Requirements 4+ years of backend SW development Experience using Go (Golang) Creative, independent, and hardworking Bonus Qualifications Experience building arb-bots, liquidations-bots, wallets, and similar crypto products Experience running ETH nodes Experience modifying node codebase Experience in automated crypto trading and DeFi Great compensation package. If you think you fit the role please apply to join the blockchain journey.",
        "url": "https://www.linkedin.com/jobs/view/3966202752",
        "summary": "Experienced Software Engineer needed for a remote or onsite (Chicago) position at a company building a Blockchain Distribution Network (BDN) for DeFi. Responsibilities include designing, implementing, testing, and deploying new features in short cycles.  Experience with Go (Golang), building arb-bots, liquidations-bots, wallets, and similar crypto products is highly desired.",
        "industries": [
            "Blockchain",
            "DeFi",
            "Cryptocurrency",
            "FinTech"
        ],
        "soft_skills": [
            "Creative",
            "Independent",
            "Hardworking"
        ],
        "hard_skills": [
            "Go (Golang)",
            "Backend SW development",
            "Arb-bots",
            "Liquidations-bots",
            "Wallets",
            "ETH nodes",
            "Node codebase modification",
            "Automated crypto trading",
            "DeFi"
        ],
        "tech_stack": [
            "Blockchain",
            "BDN",
            "DeFi",
            "Go (Golang)",
            "ETH nodes"
        ],
        "programming_languages": [
            "Go (Golang)"
        ],
        "experience": 4,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Great compensation package"
        ]
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Mettawa, IL",
        "job_id": 3961620566,
        "company": "Dice",
        "title": "Lead Data Engineer",
        "created_on": 1720635073.456012,
        "description": "Dice is the leading career destination for tech experts at every stage of their careers. Our client, Motion Recruitment Partners, LLC, is seeking the following. Apply via Dice today! Job Description Are you an experienced AWS Technical Lead looking to make a remarkable impact in the field of pharmaceutical research? Join us our quest to revolutionize the healthcare industry through data-driven insights and cutting-edge technology! We are a looking for a full-time lead data engineer based out of Mettawa, IL. In this role, you will be at the forefront of engineering and operationalizing the AWS cloud infrastructure environment to support our high-performing R&D Convergence Hub platform. Leverage your expertise in AWS development, data engineering pipelines, CI/CD, containerization, Python, SQL, NoSQL, and more to drive the success of the ARCH platform. We are a leading global pharmaceutical company dedicated to discovering and delivering innovative treatments for serious diseases. As an industry leader, we strive to empower our R&D scientists with the data and knowledge they need to maximize the impact of their life-changing work. With flexible work arrangements and an agile, fast-paced environment, you'll have the freedom to excel both personally and professionally. Our commitment to work-life balance ensures that you can be at your best, bringing your brightest ideas to the table while still having time for the things that matter most to you. Required Skills & Experience Bachelor's Degree with 7+ years of experience; master's degree with 6+ years of experience; PhD with 2+ years of experience in application program development. Extensive AWS development experience, including core services such as EC2, Glue, S3, Redshift, and Athena. Proficiency in building data engineering pipelines using tools like Apache Airflow, Glue ETL, etc. Strong technical proficiency in Python, SQL, and NoSQL for effective data manipulation and analysis. Experience with both relational and non-relational databases from a data domain perspective. Ability to gather technical requirements, validate architecture, and lead cross-functional teams. Desired Skills & Experience Knowledge of API architecture development to enhance data accessibility and integration capabilities. Familiarity with Cloudera Data Platform (CDP) in an AWS cloud environment. Exposure to logging tools like Splunk, Elasticsearch, and AWS CloudWatch for performance monitoring. Hands-on experience with CI/CD and containerization, preferably using Jenkins. Passion for leveraging technology to unlock information, enable data-driven insights, and contribute to life-changing therapies for patients. What You Will Be Doing Tech Breakdown 50% Amazon Web Services (AWS) 30% Data Engineering Tools (Apache Airflow, Glue ETL, etc.) 20% Programming Languages (Python, SQL, NoSQL) Daily Responsibilities 60% Hands On 20% Management Duties 20% Team Collaboration The Offer 12% bonus $11,500 Stock You will receive the following benefits: Medical Insurance Dental Benefits Vision Benefits Paid Time Off (PTO) 401(k) {including match- if applicable} Applicants must be currently authorized to work in the US on a full-time basis now and in the future.",
        "url": "https://www.linkedin.com/jobs/view/3961620566",
        "summary": "A leading pharmaceutical company is seeking an experienced AWS Technical Lead to join their R&D Convergence Hub platform. The role involves leading the engineering and operationalization of their AWS cloud infrastructure, leveraging expertise in data engineering, CI/CD, containerization, and various programming languages to support their high-performing platform. The ideal candidate will have 7+ years of experience in application program development, strong AWS development skills, experience with data engineering pipelines, and proficiency in Python, SQL, and NoSQL. Additional desired skills include API architecture development, Cloudera Data Platform (CDP) experience, and knowledge of logging tools like Splunk and AWS CloudWatch.",
        "industries": [
            "Pharmaceutical",
            "Healthcare",
            "Technology",
            "Research and Development",
            "Data Science"
        ],
        "soft_skills": [
            "Leadership",
            "Collaboration",
            "Communication",
            "Problem Solving",
            "Analytical Thinking"
        ],
        "hard_skills": [
            "AWS",
            "EC2",
            "Glue",
            "S3",
            "Redshift",
            "Athena",
            "Apache Airflow",
            "Glue ETL",
            "Python",
            "SQL",
            "NoSQL",
            "Relational Databases",
            "Non-Relational Databases",
            "API Development",
            "Cloudera Data Platform (CDP)",
            "Splunk",
            "Elasticsearch",
            "AWS CloudWatch",
            "CI/CD",
            "Containerization",
            "Jenkins"
        ],
        "tech_stack": [
            "AWS",
            "EC2",
            "Glue",
            "S3",
            "Redshift",
            "Athena",
            "Apache Airflow",
            "Glue ETL",
            "Python",
            "SQL",
            "NoSQL",
            "Splunk",
            "Elasticsearch",
            "AWS CloudWatch",
            "Jenkins",
            "Cloudera Data Platform (CDP)"
        ],
        "programming_languages": [
            "Python",
            "SQL",
            "NoSQL"
        ],
        "experience": 7,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Application Program Development"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Medical Insurance",
            "Dental Benefits",
            "Vision Benefits",
            "Paid Time Off (PTO)",
            "401(k)"
        ]
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Chicago, IL",
        "job_id": 3818800174,
        "company": "Cybertec, Inc",
        "title": "System Engineer III",
        "created_on": 1720635075.2318034,
        "description": "Role: System Engineer III UALJP00013215 Hybrid (Day 1 Onsite) Location: Chicago, IL Client: United Airlines Duration: 12+ Months contract to hire Rate: $65/hr Note: This will be a hybrid with the candidate expected to come into either Willis or AHNOC depending on their location. I need local candidates only. No out of towners. If they do not have AirWatch on the resume the candidate will be rejected. This is a MUST. Prefernce at least 2 to 3 years of knowledge of Air Watch, but if the candidate has 1 year they have to have strong knowledge of it. Job Description Coordinate iOS, Android, and macOS deployments. Provide deployment reports as needed. Troubleshoot tickets regarding application installations, profile installs, device enrollment and DEP Profile assignment Maintain iOS levels per business requirement Report Console issues and Devices issues to Mobility Engineering in timely manner and if needed generate tickets with VMware Manage Admin ID for Console Users Work with application development teams on setting up their devices and work with them during their UAT testing Create Smart Groups and Assignment Groups and Organization Groups when needed Provide daily operational support of a global complex IT enterprise. Administer and/or oversee mobility systems for multiple projects and environments. Analyze system faults, review diagnostic tests and troubleshoot Workspace One (AirWatch) Systems and underlying virtual servers to identify problems. Analyze connectivity issues to identify causes of device mail flow, web traffic and device enrollment communications Participate in integrated project teams and provide feedback and/or recommendations for Enterprise IT projects. Qualifications Minimum Qualifications: Bachelor's degree with four (4) or more years of mobile device management (MDM) administration experience, with at least one (1) year of hands-on experience supporting Workspace One (AirWatch) Desired Qualifications Two (2) or more years of experience with Workspace One Experience working with DEP Prior experience with IOS, Android, and MAC, and Windows mobile devices. MCSE or Workspace One (AirWatch) certification are highly preferred. Understanding of network technologies and virtual server technologies. Must be able to work collaboratively and cooperatively as a member of a team, but also work independently and apply judgment as needed. Should be capable of taking the initiative to get involved in troubleshooting efforts and identify process improvements. Ability to juggle multiple tasks or projects at the same time and effectively prioritize what is most important is required. Well-developed Written And Oral Communication Skills Are Required. Effective time management skills with respect to deadlines, priority changes, and interruptions is required.",
        "url": "https://www.linkedin.com/jobs/view/3818800174",
        "summary": "This role involves managing and troubleshooting a global enterprise mobility system using Workspace One (AirWatch). Responsibilities include coordinating device deployments, troubleshooting application and profile installations, managing user accounts, working with development teams, and providing operational support. The ideal candidate has 4+ years of MDM experience, at least 1 year with AirWatch, and experience with iOS, Android, macOS, and Windows devices.",
        "industries": [
            "Information Technology",
            "Mobile Device Management",
            "Enterprise Software"
        ],
        "soft_skills": [
            "Problem Solving",
            "Troubleshooting",
            "Collaboration",
            "Communication",
            "Time Management",
            "Prioritization",
            "Initiative",
            "Teamwork",
            "Independence"
        ],
        "hard_skills": [
            "AirWatch",
            "Workspace One",
            "MDM",
            "iOS",
            "Android",
            "macOS",
            "Windows",
            "DEP",
            "Network Technologies",
            "Virtual Server Technologies",
            "Smart Groups",
            "Assignment Groups",
            "Organization Groups"
        ],
        "tech_stack": [
            "AirWatch",
            "Workspace One"
        ],
        "programming_languages": [],
        "experience": 4,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Computer Science",
                "Information Technology",
                "Engineering"
            ]
        },
        "salary": {
            "max": 65,
            "min": 65
        },
        "benefits": [
            "Contract to Hire"
        ]
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Peoria, IL",
        "job_id": 3963078773,
        "company": "HCLTech",
        "title": "Control System Engineer",
        "created_on": 1720635079.1466873,
        "description": "Strong Knowledge of Control System & Matlab Design Electrified Powertrain Control System Perform System level testing & Hil Testing",
        "url": "https://www.linkedin.com/jobs/view/3963078773",
        "summary": "This job requires strong control systems expertise and proficiency in MATLAB to design electrified powertrain control systems. The role involves system-level testing and HIL (Hardware-in-the-Loop) testing.",
        "industries": [
            "Automotive",
            "Engineering",
            "Technology"
        ],
        "soft_skills": [
            "Problem-solving",
            "Analytical skills",
            "Attention to detail",
            "Teamwork"
        ],
        "hard_skills": [
            "Control System Design",
            "MATLAB",
            "Electrified Powertrain Control",
            "System Level Testing",
            "HIL Testing"
        ],
        "tech_stack": [
            "MATLAB"
        ],
        "programming_languages": [
            "MATLAB"
        ],
        "experience": 0,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Electrical Engineering",
                "Mechanical Engineering",
                "Control Engineering"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Palatine, IL",
        "job_id": 3944122967,
        "company": "Weber LLC",
        "title": "SOFTWARE QUALITY ASSURANCE ENGINEER",
        "created_on": 1720635081.162869,
        "description": "At Weber, grilling is a passion that’s reflected in everything we do. Our goal is to share this passion and spark inspiration with the people who matter most – our grilling community. Weber has been the world’s premiere manufacturer of charcoal and gas grills and accessories since 1952. If you have the desire to work for a company that is recognized for exceptional quality products and high customer satisfaction, employment with Weber may be right for you. We provide a friendly working atmosphere with an environment of growth and opportunity through innovation, pride, and excellence. Weber is committed to inclusive, equitable and diverse Hiring practices. Our goal is to create a workforce which resembles the diverse rich communities we live, play, and support every day. Discover What’s Possible with a career, at Weber. Summary: SOFTWARE QUALITY ASSURANCE ENGINEER  - This is a remote position.  We are looking for a self-driven, curious, creative, highly organized Software Quality Assurance Engineer who loves improving product quality through their knowledge of industry best practices and extensive experience. This role requires collaboration with team members across disciplines, an eye for detail, and critical thinking skills. You will focus on designing and executing comprehensive test plans across multiple products lines involving Internet-connected outdoor cooking products, mobile apps, and cloud services. Your role will be crucial to preventing defects and enabling the team to deliver next-generation products, on-schedule, with a top-notch consumer experience. Primary Responsibilities: Develop and adapt approaches to writing and maintaining test suites. Develop and adapt validation strategies with combinations of manual testing, automated testing, and testing tools for all aspects of the Weber Connect product line, including user interfaces, embedded systems, and related software stacks. Assess and adapt to changing risk tolerances throughout the product development lifecycle. Triage and report product defects, as well as validate fixes in software and some embedded hardware. Provide constructive feedback and recommendations to the engineering team and leadership. Produce reports of QA activities and results, and communicate with cross-functional teams on progress. Enjoy cooking with our products! Requirements: 5+ years of experience in a Software QA role. Experience testing on iOS and Android (Web, Desktop, and backend service testing is a plus). Experience designing and implementing tests for mobile app UI and embedded systems (e.g. IoT devices). Excellent verbal and written communication skills. Experience analyzing, regressing, and helping debug software issues. Conceptual understanding of automated testing and the ability to write simple programs with assistance (familiarity with Python is preferred) Familiarity with ticket management tools like Jira and test case management tools like TestRail. Familiarity with Behavior-Driven Development methodology is a plus. Accessibility testing experience is a plus. Weber-Stephen Products LLC is an equal opportunity employer and considers qualified applicants for employment without regard to race, color, creed, religion, national origin, sex, sexual orientation, gender identity and expression, age, disability, or Vietnam era, or other eligible veteran status, or any other protected factor.",
        "url": "https://www.linkedin.com/jobs/view/3944122967",
        "summary": "Weber is seeking a remote Software Quality Assurance Engineer with 5+ years of experience to join their team. This role involves designing and executing test plans for Internet-connected outdoor cooking products, mobile apps, and cloud services. The ideal candidate will have experience testing on iOS and Android, designing tests for mobile app UI and embedded systems, and possesses strong communication and analytical skills. Familiarity with Python, Jira, TestRail, and BDD methodology is a plus.",
        "industries": [
            "Consumer Goods",
            "Manufacturing",
            "Technology",
            "Software Development"
        ],
        "soft_skills": [
            "Self-Driven",
            "Curious",
            "Creative",
            "Highly Organized",
            "Detail-Oriented",
            "Critical Thinking",
            "Collaboration",
            "Communication",
            "Analytical",
            "Problem-Solving",
            "Feedback",
            "Reporting",
            "Passionate"
        ],
        "hard_skills": [
            "Software Quality Assurance",
            "Testing",
            "iOS",
            "Android",
            "Web",
            "Desktop",
            "Backend Services",
            "Mobile App UI",
            "Embedded Systems",
            "IoT",
            "Python",
            "Jira",
            "TestRail",
            "Behavior-Driven Development",
            "Accessibility Testing"
        ],
        "tech_stack": [
            "iOS",
            "Android",
            "Python",
            "Jira",
            "TestRail"
        ],
        "programming_languages": [
            "Python"
        ],
        "experience": 5,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Chicago, IL",
        "job_id": 3914876186,
        "company": "Brooksource",
        "title": "Sr. Software Engineer &amp;#8211; API",
        "created_on": 1720635082.8343337,
        "description": "Apply Now << Return to Search Results Software Engineer Contract to Hire 100% Remote About The Role We’re hiring a Software Engineer who will help design performant, scalable, web-based software solutions in our agile environment. As a software engineer, you will join one of our feature-delivery teams, collaborating across product stacks and looking for constant improvement to our organization's ability to connect consumers with the right insurance products. We are an open-source environment and encourage learning new languages and technologies. Our current platform is written in Java on the backend and uses JavaScript React on the front-end. You will report to a Principal Engineer. What You Will Do Achieve high-quality, well-tested, reliable software by following best practices for an agile delivery method. Collaborate across Product Management, Quality Assurance, and your fellow Engineers to ensure that we are delivering the best product possible. Participate in learning and professional growth opportunities that are provided by working with a diverse set of accomplished engineers. What We Are Looking For Minimum of 2+ years of contributing to a codebase and putting code into production, including experience with databases and application servers Preferred technologies: Java, JavaScript, TypeScript, Golang, Spring, React, and Redux Test – implementing high-quality tests to ensure our code is top-notch. Familiarity with Linux or MacOS, Open Source tools, frameworks, and technologies. Bachelor’s Degree in Computer Science, Engineering or similar field Brooksource provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, national origin, age, sex, citizenship, disability, genetic information, gender, sexual orientation, gender identity, marital status, amnesty, or status as a covered veteran in accordance with applicable federal, state, and local laws. For over 16 years, Brooksource has established and maintained relationships that are designed to meet your IT staffing needs. Whether it’s contract, contract-to-hire, or permanent placement work, we customise our search based upon your company’s unique initiatives, culture and technologies. With our national team of recruiters placed at 21 major hubs around the nation Brooksource finds the people best-suited for your business. When you work with us, we work with you. That’s the Brooksource promise. Brooksource is an equal opportunity employer that does not discriminate on the basis of actual or perceived race, color, creed, religion, national origin, ancestry, citizenship status, age, sex or gender (including pregnancy, childbirth, lactation and related medical conditions), gender identity or gender expression, sexual orientation, marital status, military service and veteran status, physical or mental disability, protected medical condition as defined by applicable state or local law, genetic information, or any other characteristic protected by applicable federal, state, or local laws and ordinances. Benefits & Perks Brooksource offers competitive medical, dental, vision, Health Savings Account, Dependent Care FSA, and supplemental coverage with plans that can fit each employee’s needs. We offer a 401k plan that includes a company match and is fully vested after you become eligible, paid time off, sick time, and paid company holidays. We also offer an Employee Assistance Program (EAP) that provides services like virtual counseling, financial services, legal services, life coaching, etc. Pay Disclaimer The pay range for this job level is a general guideline only and not a guarantee of compensation or salary. Additional factors considered in extending an offer include (but are not limited to) responsibilities of the job, education, experience, knowledge, skills, and abilities, as well as internal equity, alignment with market data, applicable bargaining agreement (if any), or other law. JO-2404-145741 Apply Now Tagged as: Yes",
        "url": "https://www.linkedin.com/jobs/view/3914876186",
        "summary": "Brooksource is seeking a Software Engineer to design, develop, and deploy web-based software solutions in an agile environment. This remote position requires 2+ years of experience with Java, JavaScript, TypeScript, and other related technologies.",
        "industries": [
            "Software Development",
            "Technology",
            "Insurance"
        ],
        "soft_skills": [
            "Collaboration",
            "Communication",
            "Problem-Solving",
            "Teamwork",
            "Learning Agility"
        ],
        "hard_skills": [
            "Java",
            "JavaScript",
            "TypeScript",
            "Golang",
            "Spring",
            "React",
            "Redux",
            "Linux",
            "MacOS",
            "Open Source",
            "Databases",
            "Application Servers",
            "Testing"
        ],
        "tech_stack": [
            "Java",
            "JavaScript",
            "TypeScript",
            "Golang",
            "Spring",
            "React",
            "Redux",
            "Linux",
            "MacOS",
            "Open Source",
            "Databases",
            "Application Servers"
        ],
        "programming_languages": [
            "Java",
            "JavaScript",
            "TypeScript",
            "Golang"
        ],
        "experience": 2,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Engineering"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Medical",
            "Dental",
            "Vision",
            "Health Savings Account",
            "Dependent Care FSA",
            "Supplemental Coverage",
            "401k with Company Match",
            "Paid Time Off",
            "Sick Time",
            "Paid Holidays",
            "Employee Assistance Program"
        ]
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Illinois, United States",
        "job_id": 3964764724,
        "company": "Allstate",
        "title": "Full-Stack Software Engineer",
        "created_on": 1720635084.5579612,
        "description": "At Allstate, great things happen when our people work together to protect families and their belongings from life’s uncertainties. And for more than 90 years our innovative drive has kept us a step ahead of our customers’ evolving needs. From advocating for seat belts, air bags and graduated driving laws, to being an industry leader in pricing sophistication, telematics, and, more recently, device and identity protection. Job Description We are seeking a highly skilled and innovative Product Engineer to join our dynamic team. The Product Engineer will be responsible for the design, development, and enhancement of our products. The role is pivotal in ensuring product agility, quality, and user satisfaction. In this Agile environment, you'll own the end-to-end development of backend services, following test driven development and XP practices. Key Responsibilities: Participates in iteration planning meetings ensuring that the team has a common understanding of each story and chores in a team s backlog Leads and participates in daily site, cross-site, and product team standups Participates in retrospectives to gather feedback and derive actionable items to improve the team and the product Participates in executing the strategy, keeping the customer needs and wants in mind Serves as an expert on a specific business domain and demonstrates leadership and accountability on at least one technology stack Provides input into the architecture and design of the product; collaborating with the team in solving problems the right way Practices daily paired programming and test-driven development in writing software and building product Establishes continuous integration, continuous delivery, and continuous deployment pipelines and practices Provides support for software products in user environments Coaches more junior team members around different technologies and XP practices Functional Skills: Strong understanding of computer science concepts, object-oriented design principles Hands-on experience with developing software in Java and J2EE technologies such as Spring Boot Experience and knowledge in developing REST APIs, database (relational, NoSQL, caching), building complex queries, implementation and optimization Knowledge and experience utilizing continuous integration and DevOps methodologies, preferred tools such as GitHub, Jenkins, SonarQube, Gradle, Maven, etc. Experience working in an eXtreme Programming (XP); experienced working in a paired programming/engineering model Experience in Test Driven Development; including knowledge and experience in testing frameworks like junit, mockito and other like technologies Experience on API Testing via Postman and SoapUI Utilize quality and security scan tools such as Sonar, Fortify and Contrast, or similar Ensure code and processes adhere to clean code practices and industry/application best practices Experience with messaging systems such as Kafka or similar tools Cloud knowledge with technologies such as PCF, AWS or other cloud technologies Education and Experience: 4 year Bachelors Degree (Preferred) 3+ years of experience (Preferred) In lieu of the above education requirements, an equivalent combination of education and experience may be considered. Notes: The preceding description is not designed to be a complete list of all duties and responsibilities. May be required to perform other related duties as assigned. Regular, predictable attendance is an essential function of this job. Skills Customer Centricity, Digital Literacy, Inclusive Leadership, Java, Java APIs, Java Frameworks, JavaScript, JavaScript Object Notation (JSON), Java Web Services, Learning Agility, Object Oriented Programming in Java, Results-Oriented Compensation Compensation offered for this role is $74,240 - 134,060 annually and is based on experience and qualifications. The candidate(s) offered this position will be required to submit to a background investigation, which includes a drug screen. Joining our team isn’t just a job — it’s an opportunity. One that takes your skills and pushes them to the next level. One that encourages you to challenge the status quo. And one where you can impact the future for the greater good. You’ll do all this in a flexible environment that embraces connection and belonging. And with the recognition of several inclusivity and diversity awards, we’ve proven that Allstate empowers everyone to lead, drive change and give back where they work and live. Good Hands. Greater Together.® Allstate generally does not sponsor individuals for employment-based visas for this position. Effective July 1, 2014, under Indiana House Enrolled Act (HEA) 1242, it is against public policy of the State of Indiana and a discriminatory practice for an employer to discriminate against a prospective employee on the basis of status as a veteran by refusing to employ an applicant on the basis that they are a veteran of the armed forces of the United States, a member of the Indiana National Guard or a member of a reserve component. For jobs in San Francisco, please click “here” for information regarding the San Francisco Fair Chance Ordinance. For jobs in Los Angeles, please click “here” for information regarding the Los Angeles Fair Chance Initiative for Hiring Ordinance. To view the “EEO is the Law” poster click “here”. This poster provides information concerning the laws and procedures for filing complaints of violations of the laws with the Office of Federal Contract Compliance Programs To view the FMLA poster, click “here”. This poster summarizing the major provisions of the Family and Medical Leave Act (FMLA) and telling employees how to file a complaint. It is the Company’s policy to employ the best qualified individuals available for all jobs. Therefore, any discriminatory action taken on account of an employee’s ancestry, age, color, disability, genetic information, gender, gender identity, gender expression, sexual and reproductive health decision, marital status, medical condition, military or veteran status, national origin, race (include traits historically associated with race, including, but not limited to, hair texture and protective hairstyles), religion (including religious dress), sex, or sexual orientation that adversely affects an employee's terms or conditions of employment is prohibited. This policy applies to all aspects of the employment relationship, including, but not limited to, hiring, training, salary administration, promotion, job assignment, benefits, discipline, and separation of employment.",
        "url": "https://www.linkedin.com/jobs/view/3964764724",
        "summary": "Allstate is looking for a Product Engineer with experience in Java and J2EE technologies to develop and enhance their products. The role involves working in an Agile environment, owning the end-to-end development of backend services, and following test driven development and XP practices. Responsibilities include participating in iteration planning, daily standups, and retrospectives, contributing to product architecture and design, practicing paired programming and test-driven development, and establishing continuous integration and delivery pipelines. The ideal candidate has experience with REST APIs, databases, continuous integration tools, DevOps methodologies, eXtreme Programming (XP), paired programming, test driven development, and messaging systems. Cloud knowledge with technologies such as PCF, AWS or other cloud technologies is a plus.",
        "industries": [
            "Insurance",
            "Technology",
            "Software Development",
            "Financial Services"
        ],
        "soft_skills": [
            "Customer Centricity",
            "Digital Literacy",
            "Inclusive Leadership",
            "Learning Agility",
            "Results-Oriented"
        ],
        "hard_skills": [
            "Java",
            "J2EE",
            "Spring Boot",
            "REST APIs",
            "Database",
            "NoSQL",
            "Caching",
            "Continuous Integration",
            "DevOps",
            "GitHub",
            "Jenkins",
            "SonarQube",
            "Gradle",
            "Maven",
            "eXtreme Programming (XP)",
            "Paired Programming",
            "Test Driven Development",
            "Junit",
            "Mockito",
            "API Testing",
            "Postman",
            "SoapUI",
            "Sonar",
            "Fortify",
            "Contrast",
            "Clean Code Practices",
            "Messaging Systems",
            "Kafka",
            "PCF",
            "AWS",
            "Object Oriented Programming in Java"
        ],
        "tech_stack": [
            "Java",
            "J2EE",
            "Spring Boot",
            "REST APIs",
            "Database",
            "NoSQL",
            "Caching",
            "Continuous Integration",
            "DevOps",
            "GitHub",
            "Jenkins",
            "SonarQube",
            "Gradle",
            "Maven",
            "eXtreme Programming (XP)",
            "Paired Programming",
            "Test Driven Development",
            "Junit",
            "Mockito",
            "API Testing",
            "Postman",
            "SoapUI",
            "Sonar",
            "Fortify",
            "Contrast",
            "Clean Code Practices",
            "Messaging Systems",
            "Kafka",
            "PCF",
            "AWS"
        ],
        "programming_languages": [
            "Java",
            "JavaScript"
        ],
        "experience": 3,
        "education": {
            "min_degree": "Bachelors Degree",
            "fields": [
                "Computer Science"
            ]
        },
        "salary": {
            "max": 134060,
            "min": 74240
        },
        "benefits": []
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "O'Fallon, IL",
        "job_id": 3905172351,
        "company": "Keylent Inc",
        "title": "Sr. Software Engineer MAHIN-JOB-33800",
        "created_on": 1720635087.782844,
        "description": "Sr. Software Engineer MAHIN-JOB-33800 Location: OFALLON Roles And Responsibilities Responsible for designing, implementing, and maintaining Java based software and applications Can contribute to all stages of the software development lifecycle. They thoroughly analyze user requirements, envision system features, and define application functionality. Strong knowledge of Vulnerability management, Encryption, Cloud security, Network security Strong communication skills Ability to work in a fast-paced environment",
        "url": "https://www.linkedin.com/jobs/view/3905172351",
        "summary": "Senior Software Engineer responsible for designing, implementing, and maintaining Java-based software and applications.  This role involves all stages of the software development lifecycle, including analyzing user requirements, envisioning system features, and defining application functionality.  Strong emphasis on security with knowledge of Vulnerability management, Encryption, Cloud security, and Network security.",
        "industries": [
            "Software Development",
            "Information Technology"
        ],
        "soft_skills": [
            "Communication"
        ],
        "hard_skills": [
            "Java",
            "Vulnerability Management",
            "Encryption",
            "Cloud Security",
            "Network Security"
        ],
        "tech_stack": [
            "Java"
        ],
        "programming_languages": [
            "Java"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Chicago, IL",
        "job_id": 3959292813,
        "company": "Grainger",
        "title": "Senior Software Engineer - Java",
        "created_on": 1720635089.4458642,
        "description": "As a leading industrial distributor with operations primarily in North America, Japan and the United Kingdom, We Keep The World Working® by serving more than 4.5 million customers worldwide with products delivered through innovative technology and deep customer relationships. With 2023 sales of $16.5 billion, we’re dedicated to providing value for customers, fostering an engaging culture for team members and driving strong financial results. Our welcoming workplace enables you to learn, grow and make a difference by keeping businesses running and their people safe. As a 2024 Glassdoor Best Place to Work and a Great Place to Work-Certified™ company, we’re looking for passionate people to join our team as we continue leading the industry over our next 100 years. Position Details The Profile team is part of Grainger’s Customer Information Management group and is responsible for developing, supporting and enhancing a set of microservices that sit at the core of customer data and interact with a multitude of internal systems. Our vision: CIM Profile is a custom-built replacement for legacy systems, that more accurately represents Grainger’s MRO customers. CIM Profile will provide customers and Grainger team members a comprehensive and accurate view of customer data across Grainger systems enabling seamless purchasing experiences across channels. This team is a highly visible and important strategic team in our customer data management roadmap, intersecting with multiple teams and working on cloud native applications. You will have a well-grounded experience in software design patterns, developing microservices and databases. You Will Dive into different layers of our technology stack to enhance backend services and build out user interfaces for our customer service team using technologies like: Spring Boot Docker Kafka PostgreSQL Kubernetes React Next.js AWS cloud services (S3, EKS, etc.) Collaboratively build the front and back-end components of the systems Support the systems you build as part of our on-call rotation Support our CI/CD pipelines Be open to pair programming and continuously experimenting with ways of working to improve Develop functionality with speed at high quality Build products that delight our users and have empathy for the problems they are trying to solve You Have 3+ years of experience building mission critical systems as a full stack engineer. A deep understanding of REST and HTTP and how to build robust APIs. Understanding of distributed system design and experience building production grade distributed systems. Understanding of producer/consumer design patterns and messaging frameworks like Kafka Built software that runs in the cloud, such as AWS or Google cloud and understand cloud native design patterns Some experience with CI/CD pipelines and have opinions on what makes a good pipeline Experience with production support and observability A belief that quality is everyone’s responsibility and hold opinions on how to validate and harden software systems Have an ownership mindset. Our team members are expected to work through or around obstacles and drive a task to completion. Rewards And Benefits With benefits starting day one, our programs provide choice and flexibility to meet team members' individual needs. Check out the highlights below and review all our benefits at GraingerTotalRewards.com. Medical, dental, vision, life, and pet insurance plans and 6 free sessions each year with a licensed therapist to support your emotional wellbeing Paid time off (PTO) and 6 company holidays per year 6% company contribution to a 401(k) Retirement Savings Plan each pay period, no match required Employee discounts, tuition reimbursement, student loan refinancing and free access to financial counseling, education and tools Maternity support programs, nursing benefits, and up to 14 weeks paid leave for birth parents and up to 4 weeks paid leave for non-birth parents We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal opportunity workplace. We are committed to fostering an inclusive, accessible environment that includes both providing reasonable accommodations to individuals with disabilities during the application and hiring process as well as throughout the course of one’s employment. With this in mind, should you need a reasonable accommodation during the application and selection process, please advise us so that we can provide appropriate assistance.",
        "url": "https://www.linkedin.com/jobs/view/3959292813",
        "summary": "Grainger, a leading industrial distributor, is seeking a Full Stack Engineer to join their Customer Information Management (CIM) Profile team. This team is responsible for developing, supporting, and enhancing a set of microservices that sit at the core of customer data. The ideal candidate will have 3+ years of experience building mission-critical systems, a deep understanding of REST and HTTP, experience with distributed systems, messaging frameworks like Kafka, cloud platforms like AWS, CI/CD pipelines, and production support. The role involves building user interfaces, supporting systems, and collaborating on a cloud native application.",
        "industries": [
            "Industrial Distribution",
            "Technology",
            "Customer Relationship Management",
            "Data Management"
        ],
        "soft_skills": [
            "Collaboration",
            "Problem-solving",
            "Communication",
            "Ownership",
            "Empathy",
            "Quality Focus",
            "Continuous Improvement",
            "Teamwork"
        ],
        "hard_skills": [
            "Spring Boot",
            "Docker",
            "Kafka",
            "PostgreSQL",
            "Kubernetes",
            "React",
            "Next.js",
            "AWS Cloud Services",
            "REST",
            "HTTP",
            "API Development",
            "Distributed Systems",
            "Producer/Consumer Design Patterns",
            "CI/CD Pipelines",
            "Production Support",
            "Observability"
        ],
        "tech_stack": [
            "Spring Boot",
            "Docker",
            "Kafka",
            "PostgreSQL",
            "Kubernetes",
            "React",
            "Next.js",
            "AWS Cloud Services"
        ],
        "programming_languages": [
            "Java",
            "JavaScript"
        ],
        "experience": 3,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Engineering",
                "Information Technology"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Medical Insurance",
            "Dental Insurance",
            "Vision Insurance",
            "Life Insurance",
            "Pet Insurance",
            "Paid Time Off",
            "Company Holidays",
            "401(k) Retirement Savings Plan",
            "Employee Discounts",
            "Tuition Reimbursement",
            "Student Loan Refinancing",
            "Financial Counseling",
            "Maternity Support Programs",
            "Nursing Benefits",
            "Paid Parental Leave",
            "Mental Health Support",
            "Emotional Wellbeing"
        ]
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Chicago, IL",
        "job_id": 3921137495,
        "company": "Guggenheim Partners",
        "title": "Data Integrity",
        "created_on": 1720635091.7040584,
        "description": "Guggenheim Partners Investment Management is hiring a Data Integrity Team Leader within the Business Operations department. The Team Lead  will coach and manage the data integrity team responsible for ensuring that data accuracy, consistency, analytics, and NAIC designations are maintained across all systems and data sources. This involves working directly with the Trading Desks, Portfolio Management, Compliance, Client Service and other Operations groups. The role also provides an opportunity to train with and learn from other groups within Business Operations including Analytics, Pricing and Performance Attribution. This position is based in the Chicago office. Develop and manage a team responsible for ensuring that data accuracy, consistency, analytics, and NAIC designations are maintained across all systems and data sources Ensure data produced by functional areas is accurate, consistent, and timely and that appropriate service levels are met in all internal and external delivery requirements ; resolve an issues that may arise Assist in the design or revision of processes and procedures to ensure that the operational infrastructure is able to scale to accommodate strategic growth targets Ensure all operational processes meet standards established by internal and external auditors, including SOC1 procedures Produce global risk management and executive reporting that include diverse asset classes consolidated from multiple platforms Monitor and ensure the security data flow across various investment management software systems is consistent and accurate across the firm Assess daily the Data Integrity Group’s suite of quality control reports including security terms and conditions checks, security issuer validation, asset class sector checks, and security ticker checks Analyze and source data from vendors such as Bloomberg, Reuters, and MSCI. Perform daily ESG (Environmental, Social, Governance) ratings reviews and updates Consult with management and I.T. to develop continued incremental improvement and efficiencies across Operations and produce monthly data metrics Thorough understanding of fixed income securities and structured products Ability to work on a team to create processes and influence change Demonstrated commitment to teamwork and client service Effective project management experience Completion of the CFA Level 1 exam is a plus Experience using Blackrock Aladdin software is a plus",
        "url": "https://www.linkedin.com/jobs/view/3921137495",
        "summary": "Guggenheim Partners Investment Management seeks a Data Integrity Team Leader to manage a team responsible for ensuring data accuracy, consistency, and analytics across all systems and data sources. This role involves collaborating with various departments, including Trading Desks, Portfolio Management, Compliance, and Client Service, to maintain high data integrity standards. The candidate will also have the opportunity to train and learn from other Business Operations teams like Analytics, Pricing, and Performance Attribution.  Responsibilities include designing and revising processes, ensuring compliance with internal and external audits, producing risk management reports, monitoring data flow across systems, conducting quality control checks, analyzing data from vendors, performing ESG ratings reviews, and collaborating with management and IT for process improvement.",
        "industries": [
            "Investment Management",
            "Finance",
            "Financial Services"
        ],
        "soft_skills": [
            "Leadership",
            "Teamwork",
            "Communication",
            "Problem-Solving",
            "Analytical Skills",
            "Project Management",
            "Client Service",
            "Influence",
            "Collaboration"
        ],
        "hard_skills": [
            "Data Integrity",
            "Data Accuracy",
            "Data Consistency",
            "Data Analytics",
            "NAIC Designations",
            "Fixed Income Securities",
            "Structured Products",
            "ESG Ratings",
            "Risk Management Reporting",
            "Quality Control",
            "Process Design",
            "Process Improvement"
        ],
        "tech_stack": [
            "Blackrock Aladdin",
            "Bloomberg",
            "Reuters",
            "MSCI"
        ],
        "programming_languages": [],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Chicago, IL",
        "job_id": 3948962074,
        "company": "Motorola Solutions",
        "title": "Senior Software Engineer",
        "created_on": 1720635093.3693397,
        "description": "Company Overview At Motorola Solutions, we're guided by a shared purpose - helping people be their best in the moments that matter - and we live up to our purpose every day by solving for safer. Because people can only be their best when they not only feel safe, but are safe. We're solving for safer by building the best possible technologies across every part of our safety and security ecosystem. That's mission-critical communications devices and networks, AI-powered video security & access control and the ability to unite voice, video and data in a single command center view. We're solving for safer by connecting public safety agencies and enterprises, enabling the collaboration that's critical to connect those in need with those who can help. The work we do here matters. Department Overview Motorola Solutions, Inc. Job Description Perform design, development, coding, testing, research, programming and documentation for software systems, applications and/or operating systems in conjunction with equipment designers and/or hardware developers. Perform modeling, designing, and coding activities, employing structured methods. Prepare design documentation for all levels of the software development process. Create and execute unit, integration, system, regression, performance, load and acceptance test plans and scripts. Use software system testing procedures, and document results. Analyze software requirements to determine feasibility of design within quality assurance, time and cost constraints. $102,690 - $120,900 per year. Telecommuting permitted from anywhere within the U.S. Basic Requirements Bachelors degree + 3 years experience or Masters degree + 1 years experience. Travel Requirements None Relocation Provided None Position Type Experienced Referral Payment Plan No Our U.S. Benefits include: Incentive Bonus Plans Medical, Dental, Vision benefits 401K with Company Match 9 Paid Holidays Generous Paid Time Off Packages Employee Stock Purchase Plan Paid Parental & Family Leave and more! EEO Statement Motorola Solutions is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion or belief, sex, sexual orientation, gender identity, national origin, disability, veteran status or any other legally-protected characteristic. We are proud of our people-first and community-focused culture, empowering every Motorolan to be their most authentic self and to do their best work to deliver on the promise of a safer world. If you’d like to join our team but feel that you don’t quite meet all of the preferred skills, we’d still love to hear why you think you’d be a great addition to our team. We’re committed to providing an inclusive and accessible recruiting experience for candidates with disabilities, or other physical or mental health conditions. To request an accommodation, please email ohr@motorolasolutions.com.",
        "url": "https://www.linkedin.com/jobs/view/3948962074",
        "summary": "Software Engineer with 3+ years of experience (Bachelor's degree) or 1+ years of experience (Master's degree) to perform design, development, coding, testing, research, programming, and documentation for software systems, applications, and/or operating systems. The role will involve modeling, designing, coding, creating test plans and scripts, and analyzing software requirements.",
        "industries": [
            "Technology",
            "Software Development",
            "Security",
            "Public Safety",
            "Telecommunications"
        ],
        "soft_skills": [
            "Problem Solving",
            "Communication",
            "Collaboration",
            "Analytical Thinking",
            "Time Management",
            "Documentation"
        ],
        "hard_skills": [
            "Software Development",
            "Coding",
            "Testing",
            "Research",
            "Programming",
            "Documentation",
            "Modeling",
            "Design",
            "Unit Testing",
            "Integration Testing",
            "System Testing",
            "Regression Testing",
            "Performance Testing",
            "Load Testing",
            "Acceptance Testing",
            "Software Requirements Analysis",
            "Quality Assurance"
        ],
        "tech_stack": [],
        "programming_languages": [],
        "experience": 3,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Computer Science",
                "Software Engineering"
            ]
        },
        "salary": {
            "max": 120900,
            "min": 102690
        },
        "benefits": [
            "Incentive Bonus Plans",
            "Medical",
            "Dental",
            "Vision",
            "401K with Company Match",
            "Paid Holidays",
            "Paid Time Off",
            "Employee Stock Purchase Plan",
            "Paid Parental & Family Leave"
        ]
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Chicago, IL",
        "job_id": 3919373155,
        "company": "Coinbase",
        "title": "Software Engineer, Backend - Consumer Products",
        "created_on": 1720635096.0486948,
        "description": "At Coinbase, our mission is to increase economic freedom around the world, and we couldn’t do this without hiring the best people. We’re a group of hard-working overachievers who are deeply focused on building the future of finance and Web3 for our users across the globe, whether they’re trading, storing, staking or using crypto. Know those people who always lead the group project? That’s us. There are a few things we look for across all hires we make at Coinbase, regardless of role or team. First, we look for candidates who will thrive in a culture like ours, where we default to trust, embrace feedback, and disrupt ourselves. Second, we expect all employees to commit to our mission-focused approach to our work. Finally, we seek people who are excited to learn about and live crypto, because those are the folks who enjoy the intense moments in our sprint and recharge work culture. We’re a remote-first company looking to hire the absolute best talent all over the world. Ready to ? Who you are: You’ve got positive energy. You’re optimistic about the future and determined to get there. You’re never tired of learning. You want to be a pro in bleeding edge tech like DeFi, NFTs, DAOs, and Web 3.0. You appreciate direct communication. You’re both an active communicator and an eager listener - because let’s face it, you can’t have one without the other. You’re cool with candid feedback and see every setback as an opportunity to grow. You can pivot on the fly. Crypto is constantly evolving, so our priorities do, too. What you worked on last month may not be what you work on today, and that excites you. You’re not looking for a boring job. You have a “can do” attitude. Our teams create high-quality work on quick timelines. Owning a problem doesn’t scare you, but rather empowers you to take 100% responsibility for achieving our mission. You want to be part of a winning team. We’re stronger together, and you’re a person who embraces being pushed out of your comfort zone. Coinbase is seeking experienced backend engineers to join our team to build out the next generation of crypto-forward products and features. You will solve unique, large scale, highly complex technical problems, bridging the constraints posed by web-scale applications and blockchain technology. You will help build the next generation of systems to make cryptocurrency accessible to everyone across the globe, operating real-time applications with high frequency, low latency updates, and managing the most secure, dockerized infrastructure running in the cloud. The Consumer Product Group engineers strive to make the Coinbase retail app the easiest-to-use and fastest experience for accessing the crypto. In 2023, our goal is to be the #1 self custody wallet measured by MTU’s and download share. Wallet is the future of web3 and of Coinbase. Suite of products include: Coinbase app, advance trade, dapps, and staking. What You'll Be Doing Decompose our monolithic Rails app into microservices Articulate a long term vision for maintaining and scaling our backend systems. Work with engineers, designers, product managers and senior leadership to turn our product and technical vision into a tangible roadmap every quarter. Write high quality, well tested code to meet the needs of your customers What We Look For In You You have at least 2+ years of experience in software engineering. You’ve designed, built, scaled and maintained production services, and know how to compose a service oriented architecture. You write high quality, well tested code to meet the needs of your customers. You’re passionate about building an open financial system that brings the world together. Nice To Haves You have gone through a rapid growth in your company (from startup to mid-size). You’ve build growth experiments or A/B tests. You have experience with Blockchains (such as Bitcoin, Ethereum etc..) You have experience decomposing a large monolith into microservices. You’ve worked with Golang, Ruby, Docker, Sinatra, Rails, Postgres, MongoDB or Redshift. You’ve built financial, high reliability or security systems. Job #: GCBE04US Pay Transparency Notice: Depending on your work location, the target annual salary for this position can range as detailed below. Full time offers from Coinbase also include target bonus + target equity + benefits (including medical, dental, vision and 401(k)). Pay Range $147,000—$174,000 USD Commitment to Equal Opportunity Coinbase is committed to diversity in its workforce and is proud to be an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, creed, gender, national origin, age, disability, veteran status, sex, gender expression or identity, sexual orientation or any other basis protected by applicable law. Coinbase will also consider for employment qualified applicants with criminal histories in a manner consistent with applicable federal, state and local law. For US applicants, you may view the Know Your Rights notice here. Additionally, Coinbase participates in the E-Verify program in certain locations, as required by law. Coinbase is also committed to providing reasonable accommodations to individuals with disabilities. If you need a reasonable accommodation because of a disability for any part of the employment process, please contact us at accommodations[at]coinbase.com to let us know the nature of your request and your contact information. For quick access to screen reading technology compatible with this site click here to download a free compatible screen reader (free step by step tutorial can be found here). Global Data Privacy Notice for Job Candidates and Applicants Depending on your location, the General Data Protection Regulation (GDPR) and California Consumer Privacy Act (CCPA) may regulate the way we manage the data of job applicants. Our full notice outlining how data will be processed as part of the application procedure for applicable locations is available here. By submitting your application, you are agreeing to our use and processing of your data as required. For US applicants only, by submitting your application you are agreeing to arbitration of disputes as outlined here.",
        "url": "https://www.linkedin.com/jobs/view/3919373155",
        "summary": "Coinbase is seeking experienced backend engineers to build out the next generation of crypto-forward products and features. This role involves solving complex technical problems, building systems to make cryptocurrency accessible globally, and contributing to the development of the Coinbase retail app. The ideal candidate has 2+ years of experience in software engineering, is passionate about building an open financial system, and is familiar with blockchain technology and microservices architectures.",
        "industries": [
            "FinTech",
            "Technology",
            "Cryptocurrency",
            "Blockchain",
            "Finance"
        ],
        "soft_skills": [
            "Positive energy",
            "Optimistic",
            "Determined",
            "Open to feedback",
            "Direct communication",
            "Active communicator",
            "Eager listener",
            "Can-do attitude",
            "Team player",
            "Adaptable",
            "Growth mindset",
            "Ownership",
            "Problem-solving"
        ],
        "hard_skills": [
            "Software engineering",
            "Microservices architecture",
            "Service-oriented architecture",
            "Production services",
            "Code testing",
            "Blockchain technology",
            "Bitcoin",
            "Ethereum",
            "Golang",
            "Ruby",
            "Docker",
            "Sinatra",
            "Rails",
            "Postgres",
            "MongoDB",
            "Redshift",
            "Financial systems",
            "High reliability systems",
            "Security systems",
            "Growth experiments",
            "A/B testing"
        ],
        "tech_stack": [
            "Rails",
            "Docker",
            "Sinatra",
            "Postgres",
            "MongoDB",
            "Redshift",
            "Golang",
            "Ruby"
        ],
        "programming_languages": [
            "Ruby",
            "Golang"
        ],
        "experience": 2,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 174000,
            "min": 147000
        },
        "benefits": [
            "Medical",
            "Dental",
            "Vision",
            "401(k)",
            "Bonus",
            "Equity"
        ]
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Chicago, IL",
        "job_id": 3945671794,
        "company": "WorldQuant",
        "title": "Software Engineer - Trading Systems",
        "created_on": 1720635097.8896863,
        "description": "WorldQuant develops and deploys systematic financial strategies across a broad range of asset classes and global markets. We seek to produce high-quality predictive signals (alphas) through our proprietary research platform to employ financial strategies focused on market inefficiencies. Our teams work collaboratively to drive the production of alphas and financial strategies – the foundation of a balanced, global investment platform. WorldQuant is built on a culture that pairs academic sensibility with accountability for results. Employees are encouraged to think openly about problems, balancing intellectualism and practicality. Excellent ideas come from anyone, anywhere. Employees are encouraged to challenge conventional thinking and possess an attitude of continuous improvement. Our goal is to hire the best and the brightest. We value intellectual horsepower first and foremost, and people who demonstrate an outstanding talent. There is no roadmap to future success, so we need people who can help us build it. The Role: WorldQuant is seeking exceptional C++ developers to join our front office development team to further enhance and build our next generation trading platforms. What You'll Do Write low latency, high throughput C++ code Implement mission critical trading infrastructure Work with portfolio managers to implement new trading systems Adapt existing systems to our next generation trading platform What You'll Bring A minimum of 4 years of writing high performance C++ Modern C++ knowledge (C++17/20, etc) C++ template meta programming knowledge Possess in-depth knowledge of network programming and distributed computing Strong knowledge of Unix/Linux fundamentals Strong understanding of data structures and algorithms Finance experience is preferred but not required Locations: Chicago, NYC, CT, or Austin WorldQuant is a total compensation organization where you will be eligible for a base salary, discretionary performance bonus, and benefits. The estimated salary range for this position is $125,000 to $250,000 which is specific to New York and may change in the future. When finalizing an offer we will take into consideration an individual’s experience level and the qualifications they bring to the role to formulate a competitive total compensation package. By submitting this application, you acknowledge and consent to terms of the WorldQuant Privacy Policy. The privacy policy offers an explanation of how and why your data will be collected, how it will be used and disclosed, how it will be retained and secured, and what legal rights are associated with that data (including the rights of access, correction, and deletion). The policy also describes legal and contractual limitations on these rights. The specific rights and obligations of individuals living and working in different areas may vary by jurisdiction. Copyright © 2024 WorldQuant, LLC. All Rights Reserved. WorldQuant is an equal opportunity employer and does not discriminate in hiring on the basis of race, color, creed, religion, sex, sexual orientation or preference, age, marital status, citizenship, national origin, disability, military status, genetic predisposition or carrier status, or any other protected characteristic as established by applicable law.",
        "url": "https://www.linkedin.com/jobs/view/3945671794",
        "summary": "WorldQuant is seeking exceptional C++ developers to join their front office development team to enhance and build next-generation trading platforms. Responsibilities include writing low latency, high throughput C++ code, implementing mission critical trading infrastructure, working with portfolio managers to implement new trading systems, and adapting existing systems to their next generation trading platform.",
        "industries": [
            "Finance",
            "Financial Services",
            "Investment Management",
            "Technology",
            "Software Development"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Critical Thinking",
            "Continuous Improvement",
            "Teamwork",
            "Adaptability",
            "Intellectual Curiosity",
            "Accountability"
        ],
        "hard_skills": [
            "C++",
            "C++17",
            "C++20",
            "Template Metaprogramming",
            "Network Programming",
            "Distributed Computing",
            "Unix",
            "Linux",
            "Data Structures",
            "Algorithms"
        ],
        "tech_stack": [
            "C++",
            "Unix",
            "Linux"
        ],
        "programming_languages": [
            "C++"
        ],
        "experience": 4,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 250000,
            "min": 125000
        },
        "benefits": [
            "Base Salary",
            "Performance Bonus"
        ]
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Chicago, IL",
        "job_id": 3966171864,
        "company": "Google",
        "title": "Software Engineer III, Mobile (Android), Wear OS",
        "created_on": 1720635101.9084785,
        "description": "Minimum qualifications: Bachelor’s degree or equivalent practical experience. 2 years of experience with software development in one or more programming languages, or 1 year of experience with an advanced degree in an industry setting. 2 years of experience with data structures or algorithms in either an academic or industry setting. 2 years of experience Android application development. Preferred qualifications: Master's degree or PhD in Computer Science or related technical field. 2 years of experience with performance, large scale systems data analysis, visualization tools, and/or debugging. Experience developing accessible technologies. Proficiency in code and system health, diagnosis and resolution, and software test engineering. About The Job Google's software engineers develop the next-generation technologies that change how billions of users connect, explore, and interact with information and one another. Our products need to handle information at massive scale, and extend well beyond web search. We're looking for engineers who bring fresh ideas from all areas, including information retrieval, distributed computing, large-scale system design, networking and data storage, security, artificial intelligence, natural language processing, UI design and mobile; the list goes on and is growing every day. As a software engineer, you will work on a specific project critical to Google’s needs with opportunities to switch teams and projects as you and our fast-paced business grow and evolve. We need our engineers to be versatile, display leadership qualities and be enthusiastic to take on new problems across the full-stack as we continue to push technology forward. With your technical expertise you will manage project priorities, deadlines, and deliverables. You will design, develop, test, deploy, maintain, and enhance software solutions. Google is an engineering company at heart. We hire people with a broad set of technical skills who are ready to take on some of technology's greatest challenges and make an impact on users around the world. At Google, engineers not only revolutionize search, they routinely work on scalability and storage solutions, large-scale applications and entirely new platforms for developers around the world. From Google Ads to Chrome, Android to YouTube, social to local, Google engineers are changing the world one technological achievement after another. The US base salary range for this full-time position is $136,000-$200,000 + bonus + equity + benefits. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process. Please note that the compensation details listed in US role postings reflect the base salary only, and do not include bonus, equity, or benefits. Learn more about benefits at Google . Responsibilities Write product or system development code. Review code developed by other engineers and provide feedback to ensure best practices (e.g., style guidelines, checking code in, accuracy, testability, and efficiency). Contribute to existing documentation or educational content and adapt content based on product/program updates and user feedback. Triage product or system issues and debug/track/resolve by analyzing the sources of issues and the impact on hardware, network, or service operations and quality. Participate in, or lead design reviews with peers and stakeholders to decide amongst available technologies. Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing our Accommodations for Applicants form .",
        "url": "https://www.linkedin.com/jobs/view/3966171864",
        "summary": "Google is looking for a Software Engineer to develop next-generation technologies for their products. The role involves writing, reviewing, and debugging code, as well as contributing to documentation and design reviews. The ideal candidate will have experience with Android application development, data structures, and algorithms, and be proficient in various programming languages. They will also have a strong understanding of software development best practices and be able to work independently and as part of a team.",
        "industries": [
            "Technology",
            "Software Development",
            "Internet",
            "Mobile",
            "Search",
            "Artificial Intelligence",
            "Machine Learning"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Analytical Skills",
            "Leadership",
            "Teamwork",
            "Critical Thinking",
            "Time Management",
            "Organization",
            "Adaptability"
        ],
        "hard_skills": [
            "Software Development",
            "Android Application Development",
            "Data Structures",
            "Algorithms",
            "Programming Languages",
            "Performance",
            "Large Scale Systems",
            "Data Analysis",
            "Visualization Tools",
            "Debugging",
            "Accessibility",
            "Code Review",
            "Documentation",
            "Issue Triage",
            "Design Reviews"
        ],
        "tech_stack": [
            "Android",
            "Java",
            "Python",
            "C++",
            "C",
            "Kotlin",
            "Swift",
            "Go",
            "Rust",
            "JavaScript",
            "TypeScript",
            "HTML",
            "CSS",
            "SQL",
            "NoSQL",
            "Cloud Computing",
            "Machine Learning",
            "Artificial Intelligence",
            "Big Data",
            "Data Mining",
            "Data Analytics",
            "Data Visualization",
            "DevOps",
            "Agile"
        ],
        "programming_languages": [
            "Java",
            "Kotlin",
            "Swift",
            "C++",
            "C",
            "Python",
            "Go",
            "Rust",
            "JavaScript",
            "TypeScript"
        ],
        "experience": 2,
        "education": {
            "min_degree": "Bachelor’s degree",
            "fields": [
                "Computer Science",
                "Related Technical Field"
            ]
        },
        "salary": {
            "max": 200000,
            "min": 136000
        },
        "benefits": [
            "Bonus",
            "Equity",
            "Health Insurance",
            "Dental Insurance",
            "Vision Insurance",
            "Life Insurance",
            "Disability Insurance",
            "Paid Time Off",
            "Paid Sick Leave",
            "Parental Leave",
            "Retirement Plan",
            "Employee Assistance Program",
            "Tuition Reimbursement",
            "Gym Membership",
            "Free Food"
        ]
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Chicago, IL",
        "job_id": 3906234190,
        "company": "Chicago Trading Company",
        "title": "Java Software Engineer, Reference Data",
        "created_on": 1720635103.7402048,
        "description": "CTC is a cutting-edge proprietary trading firm with a long-term vision and a clear focus on helping the world price and manage risk. Our fun and trusting culture inspires us to solve the industry’s most challenging problems and take calculated risks in a collaborative environment. We strive to be the most innovative firm in the industry today, tomorrow, and long into the future while upholding ethical excellence. We believe that CTC makes a positive impact on the markets, the lives of our employees, and all the communities to which we belong. Started in 1995 by a team of forward-thinking Traders, we are proud to call ourselves an industry leader that keeps making markets and each other better. As a Software Engineer on the Reference Data team, you will be building our next-generation multi-asset, global security master platform providing critical functionality for the trading, research, middle and back office processes. You will help expand reference data coverage and contribute to the data quality controls definition and automation. You will work closely with quants, traders and development teams to onboard new datasets, unlock new business opportunities and scale as markets grow. Responsibilities: Develop simple resilient services, both in the cloud and on-prem, to read, process, and distribute security reference data within the firm Contribute to an asset class-agnostic security master that will provide scale to quickly add new asset types, enrichment, and data sets Develop and collaborate on client APIs written in C++, Java and Python, with quants and developers in research, pricing and valuation, trading, exchange access, inventory, compliance, risk management and operations Be an active member of a software engineering team whose role is to understand requirements, propose solutions, and deliver software into production in a timely and robust manner Use new technologies and architectures with continuous improvement in mind Contribute to the growth of our platform through in team code reviews and system reviews, standardizing methodologies and CI/CD tooling Requirements: Experience in security master/reference data is not required, however is preferred 3+ years of total work experience, preferably in the financial industry. 3+ years of professional development in Java / Spring Framework. Development experience in C++ or Python is not required, however is preferred Knowledge of database technologies e.g. SQL, Snowflake and ability to write complex queries. Familiarity to streaming technologies e.g. Kafka, Solace Experience with cloud environments e.g. Azure, AWS, GCP is not required, however is preferred What it takes to be successful: You are a highly motivated, humble, and collaborative team member You have strong problem-solving skills, able to quickly triage issues and drive resolution effort through completion You can optimally communicate sophisticated ideas both written and verbally Our Benefits We strongly believe in the well-being of our employees and their families so we offer outstanding benefits to support you both professionally and personally. These benefits include generous medical coverage, paid parental leave, free breakfast and lunch (plus healthy snacks, of course), wellness reimbursement, quarterly recharge days, and a variety of other benefits focused on providing the best employee experience. (Disclaimer: Interns and Contractors are not eligible for benefits at CTC) Our Commitment to Diversity, Equity and Inclusion At CTC, we aim to cultivate a workplace that celebrates diversity and each person feels included, engaged and empowered. Where each of us feels we belong. We are committed to having a diverse workforce and are proud to be an equal opportunity employer. CTC does not discriminate on the basis of race, religion, color, national origin, sex, gender, gender expression, sexual orientation, age, marital status, veteran status, or disability status. We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform crucial job functions, and to receive other benefits and privileges of employment. If you have a disability and believe you need a reasonable accommodation in order to search for a job opening or to apply for a position, please contact us at info@chicagotrading.com. Note that emails sent to this email account for non-disability related issues, such as following up on an application, will not receive a response.",
        "url": "https://www.linkedin.com/jobs/view/3906234190",
        "summary": "CTC, a proprietary trading firm, is seeking a Software Engineer to build their next-generation security master platform. This role involves developing resilient services, contributing to an asset class-agnostic security master, and collaborating with teams across the firm.  Experience in security master/reference data is preferred, along with 3+ years of professional development in Java/Spring Framework. Knowledge of database technologies (SQL, Snowflake), streaming technologies (Kafka, Solace), and cloud environments (Azure, AWS, GCP) is also desirable.",
        "industries": [
            "Finance",
            "Trading",
            "Financial Technology",
            "Software Development"
        ],
        "soft_skills": [
            "Problem-solving",
            "Communication",
            "Collaboration",
            "Motivation",
            "Teamwork",
            "Humble"
        ],
        "hard_skills": [
            "Java",
            "Spring Framework",
            "C++",
            "Python",
            "SQL",
            "Snowflake",
            "Kafka",
            "Solace",
            "Azure",
            "AWS",
            "GCP"
        ],
        "tech_stack": [
            "Java",
            "Spring Framework",
            "C++",
            "Python",
            "SQL",
            "Snowflake",
            "Kafka",
            "Solace",
            "Azure",
            "AWS",
            "GCP"
        ],
        "programming_languages": [
            "Java",
            "C++",
            "Python"
        ],
        "experience": 3,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Medical Coverage",
            "Paid Parental Leave",
            "Free Breakfast and Lunch",
            "Healthy Snacks",
            "Wellness Reimbursement",
            "Recharge Days"
        ]
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Chicago, IL",
        "job_id": 3906320623,
        "company": "Alter Domus",
        "title": "Senior System Engineer I",
        "created_on": 1720635108.1721573,
        "description": "About Us We are Alter Domus. Meaning “The Other House” in Latin, Alter Domus is proud to be home to 85% of the top 30 asset managers in the alternatives industry, and more than 5,000 professionals across 23 countries. With a deep understanding of what it takes to succeed in alternatives, we believe in being different. Invest yourself in the alternative, and join an organization where you progress on merit, where you can speak openly with whoever you are speaking to, and where you will be supported along whichever path you choose to take. Find out more about life at Alter Domus at careers.alterdomus.com JOB DESCRIPTION: The Senior System Engineer will be responsible for the following activities: Windows Server Administration and engineering experience in an enterprise IT environment, having performed operational and break/fix activity along with engineering and design VMware Virtual Server administration and engineering experience in an enterprise IT environment, having performed virtual machine provisioning, resource changes, performance troubleshooting, monitoring and break/fix Assist managing various server related resources (Active Directory, IIS, DFS Namespace, File Shares, Office 365, Azure, SharePoint) including patches and other related security tasks and responsibilities Work directly with Application support or Development team to deploy, troubleshoot and resolve issues Work directly with end users (in person, phone and help desk ticketing) to troubleshot and resolve issues Set up workstations, domain accounts, etc. in various hardware and software related applications Deploy and troubleshoot various types of network elements, infrastructure services and platforms Install and troubleshoot Microsoft and various other system applications and solutions Manage clear communication of progress/status with project stakeholders- Collaborate with team members to create and resolve support tickets Provide hands-on expertise on systems related projects and collaborate with the internal Operations Team Work with a Global IT team supporting users all over the world Maintain consistent, quality documentation for the administration and operation of underlying infrastructure, including topologies, configuration details, run-books, and recovery processes TECHNICAL COMPETENCIES: Strong technical knowledge of Microsoft technologies, including server and client OS, Active Directory, DFS General Knowledge of Citrix solutions and VMWare technology General knowledge in Linux Support and maintain the existing network and associated infrastructure Collaborate with team members in resolving support tickets Experience in managing Microsoft cloud solutions (Azure, Office 365, SharePoint) Managed Services and Cloud experience Manage various server related resources (Active Directory, Office 365, Azure) including patches and other related security tasks and responsibilities Remote Connectivity troubleshooting (VPN, Citrix, Terminal Server) YOUR PROFILE: 2+ years of related IT industry experience 2+ years’ experience provisioning, operating, monitoring and maintaining system’s hardware, software and related infrastructure 2+ years’ experience in Office 365, Azure, SharePoint Excellent oral, written communication, and presentation skills Detail oriented and organized with an ability to meet deadlines Willingness to work with teams in multiple time zones Strong work ethic, responsiveness and passion for customer service and quality Excellent team player and good interpersonal skills Self-motivated, able to work within a project-based environment WHAT WE OFFER: We are committed to supporting your development, advancing your career, and providing benefits that matter to you. Our industry-leading Alter Domus Academy offers six learning zones for every stage of your career, with resources tailored to your ambitions and resources from LinkedIn Learning. Our global benefits also include: Support for professional accreditations such as ACCA and study leave Flexible arrangements, generous holidays, birthday leave and graduation leave Continuous mentoring along your career progression Active sports, events and social committees across our offices Support with mental, physical, emotional and financial support 24/7 from our Employee Assistance Program The opportunity to invest in our growth and success through our Employee Share Plan Plus additional local benefits depending on your location Equity in every sense of the word We are in the business of equity, in every sense of the word. For us, this means taking action to ensure every colleague has equal opportunity, valuing every voice and experience across our organisation, maintaining an inclusive culture where you can bring your whole self to work, and making Alter Domus a workplace where everyone feels they belong. We celebrate our differences, and understand that our success relies on diverse perspectives and experiences, working towards shared goals and a common purpose. Thanks to the work of our Group DE&I Committee and network of DE&I Champions, we empower all of our people to be truly invested in the alternative. We are committed to ensuring an inclusive recruiting and onboarding process. Please contact our hiring team if you require any accommodations to make our recruitment process more accessible for you. (Alter Domus Privacy notice can be reviewed via Alter Domus webpage: https://alterdomus.com/privacy-notice/ )",
        "url": "https://www.linkedin.com/jobs/view/3906320623",
        "summary": "Alter Domus is looking for a Senior System Engineer to join their team. This role is responsible for managing and maintaining Windows Server, VMware, and other IT infrastructure components. The ideal candidate will have a strong technical background in Microsoft technologies, including Active Directory, DFS, and cloud solutions like Azure and Office 365.  The role requires excellent communication skills, the ability to work with teams across multiple time zones, and a passion for customer service. The position offers competitive benefits including professional development opportunities, flexible work arrangements, and a global benefits package.",
        "industries": [
            "Technology",
            "Financial Services",
            "Asset Management"
        ],
        "soft_skills": [
            "Communication",
            "Presentation",
            "Detail-oriented",
            "Organized",
            "Teamwork",
            "Interpersonal",
            "Self-motivated",
            "Problem Solving",
            "Customer service",
            "Passionate"
        ],
        "hard_skills": [
            "Windows Server",
            "VMware",
            "Active Directory",
            "DFS",
            "Citrix",
            "Linux",
            "Office 365",
            "Azure",
            "SharePoint",
            "VPN",
            "Citrix",
            "Terminal Server",
            "Microsoft Technologies"
        ],
        "tech_stack": [
            "Windows Server",
            "VMware",
            "Active Directory",
            "DFS",
            "Citrix",
            "Linux",
            "Office 365",
            "Azure",
            "SharePoint"
        ],
        "programming_languages": [],
        "experience": 2,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Alter Domus Academy",
            "LinkedIn Learning",
            "Professional Accreditations (ACCA)",
            "Study Leave",
            "Flexible Arrangements",
            "Generous Holidays",
            "Birthday Leave",
            "Graduation Leave",
            "Mentoring",
            "Sports and Social Events",
            "Employee Assistance Program",
            "Employee Share Plan",
            "Local Benefits"
        ]
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Chicago, IL",
        "job_id": 3927225476,
        "company": "Fabric Cryptography",
        "title": "Sr. Software Engineer, Datacenter Infra",
        "created_on": 1720635114.3210275,
        "description": "FABRIC CRYPTOGRAPHY FABRIC CREATES NEXT-GENERATION CRYPTOGRAPHY TECHNOLOGY THAT REIMAGINES TRUST AND PRIVACY. Fabric believes hardware determines the boundaries of humanity's collective creativity and imagination. We are building hardware for the next generation of cryptography because we believe in creating a more trustworthy world with secure, private computation at its core. Just as encryption and decryption enabled the Internet as we know it, this new paradigm of cryptographic algorithms, such as zero knowledge proofs, have even broader potential to revolutionize how trust, privacy, and identity work in our society. About the job We’re building the high-performance cryptography framework of the future, and we’re looking for someone to make it run at datacenter scale. What you'll be doing... Building gRPC services to scale cryptography distributed systems. Building datacenter management, logging, monitoring, and container orchestration tools and integration. Deploying software to new installations and managing updates Implementing and optimizing cryptographic algorithms and protocols for performance and scalability. Collaborating with cross-functional teams to integrate cryptographic solutions into various software and hardware components. Designing and implementing secure key management systems for cryptographic operations. Contributing to the design and development of cryptographic hardware accelerators. Researching and evaluating emerging cryptographic technologies and standards to drive innovation within the organization. Troubleshooting and resolving complex issues related to cryptographic systems and protocols in production environments. Participating in code reviews, architectural discussions, and technical documentation efforts to ensure high-quality deliverables. Requirements & experience required... Bachelor's degree in Computer Science, Software Engineering, Computer Engineering, or a related field. 5+ years of industry experience Experience deploying large scale containerized applications Experience developing datacenter infrastructure Experience with C++ or Rust and gRPC/protobuf Fabric Cryptography is committed to fostering a diverse work environment and proud to be an equal opportunity employer. As we highly value diversity in our current and future employees, we do not discriminate based on race, religion, color, national origin, gender, gender expression, sexual orientation, age, marital status, veteran status, disability status or any other characteristic protected by law.",
        "url": "https://www.linkedin.com/jobs/view/3927225476",
        "summary": "Fabric Cryptography is looking for a software engineer to build high-performance cryptography frameworks that run at datacenter scale. Responsibilities include building gRPC services, datacenter management tools, deploying software, optimizing algorithms, and contributing to hardware accelerator development. Ideal candidates have 5+ years of experience with large scale containerized applications, datacenter infrastructure, C++ or Rust, and gRPC/protobuf.",
        "industries": [
            "Computer Hardware",
            "Software Development",
            "Cryptography",
            "Security",
            "Cybersecurity"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Troubleshooting",
            "Research",
            "Teamwork"
        ],
        "hard_skills": [
            "gRPC",
            "Containerization",
            "Datacenter Management",
            "C++",
            "Rust",
            "Protobuf",
            "Cryptography",
            "Key Management",
            "Algorithm Optimization",
            "Hardware Acceleration",
            "Security"
        ],
        "tech_stack": [
            "gRPC",
            "Kubernetes",
            "Docker",
            "C++",
            "Rust",
            "Protobuf",
            "Zero Knowledge Proofs"
        ],
        "programming_languages": [
            "C++",
            "Rust"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Computer Science",
                "Software Engineering",
                "Computer Engineering"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Chicago, IL",
        "job_id": 3948895793,
        "company": "Apex Fintech Solutions",
        "title": "Software Engineer II",
        "created_on": 1720635116.1228807,
        "description": "Who We Are Apex Fintech Solutions (AFS) powers innovation and the future of digital wealth management by processing millions of transactions daily, to simplify, automate, and facilitate access to financial markets for all. Our robust suite of fintech solutions enables us to support clients such as Stash, Betterment, SoFi, and Webull, and more than 20 million of our clients' customers. Collectively, AFS creates an environment in which companies with the biggest ideas in fintech are empowered to change the world. We are based in Dallas, TX and also have offices in Austin, New York, Chicago, Portland, and Belfast. If you are seeking a fast-paced and entrepreneurial environment where you'll have the opportunity to make an immediate impact, and you have the guts to change everything, this is the place for you. AFS has received a number of prestigious industry awards, including: 2021, 2020, 2019, and 2018 Best Wealth Management Company - presented by Fintech Breakthrough Awards 2021 Most Innovative Companies - presented by Fast Company 2021 Best API & Best Trading Technology - presented by Global Fintech Awards About This Role Apex Fintech Solutions is seeking a Software Engineer II to join our growing engineering team. They will be pivotal in developing robust backend systems that enhance our financial services software solutions. This role taps into optimizing and innovating on existing applications and designing new technological frameworks to meet the heightened demands of the marketplace. As part of our engineering team, you will contribute directly to projects that process millions of daily transactions, helping to streamline, automate, and democratize access to financial markets. Duties/Responsibilities Design and develop scalable, high-performance backend components and services. Collaborate with team members to convert business needs into technical specifications and system designs. Manage the full software development lifecycle including testing, integration, and deployment using Continuous Integration/Continuous Deployment (CI/CD) practices. Maintain and upgrade the code quality via best practices, including conducting code reviews, and implementing unit and automated tests. Write comprehensive integration and unit tests to ensure reliability and functionality. Analyze and enhance application performance, efficiency, and scalability. Support the production systems that you build. This includes tasks related to building, deployment (CI/CD), and monitoring using tools like DataDog and PagerDuty. Offer technical guidance and mentorship to junior developers, promoting knowledge sharing and expertise on software development tactics and new technologies. Education And/or Experience Bachelor’s degree in Computer Science, Information Technology, or a closely related field. 3+ years of experience as a Backend Developer or in a similar role. Proven track record in developing and managing scalable backend systems, ideally within a financial services or fintech environment. Experience with relational database management systems; particular experience with Postgres AlloyDB and familiarity with BigQuery is highly preferred. Required Skills/Abilities Proficiency in backend programming languages such as Java, Python, or Go. Strong knowledge of RESTful API design and microservices architecture. Familiarity with containerization and orchestration technologies like Docker and Kubernetes. Familiar with Google Cloud and understanding of infrastructure as code. Excellent analytical and problem-solving skills, capable of tackling complex algorithmic challenges. Work Environment This job operates in an office environment. Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice. #engineering #associate #full-time #APEX Our Rewards We offer a robust package of employee perks and benefits, including healthcare benefits (medical, dental and vision, EAP), competitive PTO, 401k match, parental leave, and HSA contribution match. We also provide our employees with a paid subscription to the Calm app and offer generous external learning and tuition reimbursement benefits. At AFS, we offer a hybrid work schedule for most roles that allows employees to have the flexibility of working from home and one of our primary offices. Diversity, Equity, Inclusion, and Belonging (DEIB) Commitment We're looking for all kinds of people. At Apex, we believe that wealth management and investing should be accessible to everyone, and we strive to create spaces to democratize investing for folks of all walks of life. Internally, we embrace diversity and are dedicated to creating an inclusive and equitable workplace, which reflects our company vision and mission. We value every team member's unique perspective and are committed to fostering a culture where everyone belongs. Join us in our mission to empower and celebrate individual differences. Apex is committed to being an equal opportunity employer. We ensure that qualified applicants receive fair consideration for employment without discrimination based on sex, gender identity, gender expression, sexual orientation, race, color, natural or protective hairstyle, genetics, religion, national origin, disability, protected veteran status, age, or any other characteristic protected by law. Know your rights: workplace discrimination is illegal. We stand by this commitment to promote a diverse, equitable, and inclusive workforce.",
        "url": "https://www.linkedin.com/jobs/view/3948895793",
        "summary": "Apex Fintech Solutions (AFS) is seeking a Software Engineer II to develop and manage backend systems for their financial services software solutions. The role involves designing and developing scalable backend components, collaborating with team members, managing the software development lifecycle, maintaining code quality, writing tests, and supporting production systems. The ideal candidate will have 3+ years of experience as a Backend Developer, proficiency in backend programming languages (Java, Python, or Go), strong knowledge of RESTful APIs and microservices architecture, and familiarity with containerization, orchestration technologies, Google Cloud, and infrastructure as code. The role offers a competitive salary, benefits, and a hybrid work schedule.",
        "industries": [
            "FinTech",
            "Financial Services",
            "Software Development",
            "Technology"
        ],
        "soft_skills": [
            "Collaboration",
            "Communication",
            "Problem-solving",
            "Analytical",
            "Technical Guidance",
            "Mentorship",
            "Knowledge Sharing"
        ],
        "hard_skills": [
            "Java",
            "Python",
            "Go",
            "RESTful APIs",
            "Microservices Architecture",
            "Docker",
            "Kubernetes",
            "Google Cloud",
            "Infrastructure as Code",
            "Postgres AlloyDB",
            "BigQuery",
            "CI/CD",
            "DataDog",
            "PagerDuty"
        ],
        "tech_stack": [
            "Java",
            "Python",
            "Go",
            "RESTful APIs",
            "Microservices Architecture",
            "Docker",
            "Kubernetes",
            "Google Cloud",
            "Infrastructure as Code",
            "Postgres AlloyDB",
            "BigQuery",
            "CI/CD",
            "DataDog",
            "PagerDuty"
        ],
        "programming_languages": [
            "Java",
            "Python",
            "Go"
        ],
        "experience": 3,
        "education": {
            "min_degree": "Bachelor’s degree",
            "fields": [
                "Computer Science",
                "Information Technology"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Healthcare Benefits (Medical, Dental, Vision, EAP)",
            "Competitive PTO",
            "401k match",
            "Parental leave",
            "HSA contribution match",
            "Paid subscription to Calm app",
            "External learning and tuition reimbursement",
            "Hybrid work schedule"
        ]
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Downers Grove, IL",
        "job_id": 3920826064,
        "company": "Xoriant",
        "title": "Azure Data Architect",
        "created_on": 1720635117.9942508,
        "description": "Position Title: Data Architect (Azure) Location: Downers Grove, IL Duration: 12 Months contract with possibility of extension Job Description: We are seeking a skilled Azure Data & BI Architect to join our dynamic team. As an Azure Data & BI Architect, you will be responsible for designing and implementing scalable, reliable, and high-performance data solutions on the Azure platform along with administrating the Power BI/Fabric tenant. You will collaborate with cross-functional teams to gather requirements, architect data solutions, and deliver business intelligence solutions that empower data-driven decision-making. Responsibilities: Design and implement end-to-end data solutions on the Azure platform, including data ingestion, storage, processing, and visualization. Develop data models, data pipelines, and ETL processes to support business requirements. Design and optimize data warehouses, data lakes, and analytical databases for performance and scalability. Implement data security, privacy, and compliance measures in accordance with industry standards and regulations. Collaborate with business stakeholders to understand requirements and translate them into technical solutions. Lead the evaluation and selection of Azure services, tools, and technologies to meet business needs. Provide technical leadership and mentorship to junior team members. Stay current with industry trends, best practices, and emerging technologies in data management and analytics. Qualifications: Bachelor’s degree in computer science, Information Systems, or related field. 12+ years of experience designing and implementing data solutions on the Azure platform. Strong proficiency in Azure services such as Azure SQL Database, Azure Data Lake Storage, Azure Synapse Analytics, Azure Data Factory, Azure Databricks, Microsoft Fabric etc. Experience with data modeling, data warehousing, and ETL/ELT processes. Proficiency in SQL, Python, or other programming languages for data manipulation and analysis. Solid understanding of data governance, data security, and regulatory compliance. Excellent problem-solving and analytical skills with attention to detail. Strong communication and interpersonal skills with the ability to collaborate effectively with cross-functional teams. Azure certifications such as Azure Data Engineer Associate or Azure Solutions Architect Expert are a plus.",
        "url": "https://www.linkedin.com/jobs/view/3920826064",
        "summary": "This position requires an experienced Azure Data & BI Architect to design and implement scalable data solutions on the Azure platform, including data ingestion, storage, processing, and visualization. The role involves collaborating with cross-functional teams to gather requirements, architect data solutions, and deliver business intelligence solutions. Responsibilities include designing data models, pipelines, and ETL processes, optimizing data warehouses and lakes, implementing data security, and providing technical leadership.",
        "industries": [
            "Information Technology",
            "Data Analytics",
            "Business Intelligence"
        ],
        "soft_skills": [
            "Problem Solving",
            "Analytical Skills",
            "Communication",
            "Interpersonal Skills",
            "Collaboration",
            "Leadership"
        ],
        "hard_skills": [
            "Data Modeling",
            "Data Warehousing",
            "ETL/ELT",
            "SQL",
            "Python",
            "Azure SQL Database",
            "Azure Data Lake Storage",
            "Azure Synapse Analytics",
            "Azure Data Factory",
            "Azure Databricks",
            "Microsoft Fabric"
        ],
        "tech_stack": [
            "Azure",
            "Azure SQL Database",
            "Azure Data Lake Storage",
            "Azure Synapse Analytics",
            "Azure Data Factory",
            "Azure Databricks",
            "Microsoft Fabric",
            "Power BI"
        ],
        "programming_languages": [
            "SQL",
            "Python"
        ],
        "experience": 12,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Information Systems"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Chicago, IL",
        "job_id": 3922066074,
        "company": "WiseTech Global",
        "title": "System Support Engineer (Windows / Linux) - Chicago",
        "created_on": 1720635119.5041404,
        "description": "WiseTech Global is a world-leading software company building software for the global logistics industry. We are united in our mission to create breakthrough products that enable and empower those who own and operate the supply chains of the world. Our software products are relied upon by the top 25 freight-forwarding companies globally as well as over 18,000 other logistics businesses. At WiseTech, we operate a hybrid environment, and for this role, we are seeking a Systems Support Engineer, experienced across Windows & Linux infrastructure, both physical and virtual. The suitable candidate will support our Chicago data centre On-site involving weekend support between the hours of 8.30 am to 5 pm from Thursday to Monday. Key Responsibilities: Available to work to a roster or schedule to administer, maintain and service IT infrastructure through out-of-business hours and weekends, and to handle emergency issues. Ensure smooth running of IT infrastructure and work alongside the Systems Engineering team Perform on-site and remote technical support, patching, maintenance and system administration of the infrastructure Assist global systems engineers and business units related to the infrastructure Monitor health and handle alerts related to the IT infrastructure The ideal candidate will have the following essential skills: Minimum 4 years experience in a similar role Experience working with server and storage hardware (rack mount servers, SAN, direct attached storage and fibre channel fabric solutions) Experience with enterprise level patching and systems administration platforms Windows Server administration Linux operating system administration Experience with virtualised environments (Hyper-V, VMware, KVM) Desired Skills: Hardware skills across multiple vendors related to servers, storage and related data centre technologies Software skills across multiple vendors related to enterprise management such as Microsoft System Center suite, etc. Proven experience with enterprise wide patching and maintenance, and ensure continuity of services Windows Scripting and Unix Scripting (PowerShell, Perl, Python, Bash) highly regarded Cloud based technology experience highly regarded, but not essential Before you Apply From time to time, WiseTech Global may use external service providers to assist us with assessing applications, including background checks, on our behalf. Accordingly, by applying for this role and providing your personal information to WiseTech Global, you consent to WiseTech Global providing this information to our external service providers who are required to treat such information with strict confidentiality in line with privacy and data protection laws and regulations.",
        "url": "https://www.linkedin.com/jobs/view/3922066074",
        "summary": "WiseTech Global is seeking a Systems Support Engineer to work in their Chicago data center, providing on-site support for their Windows & Linux infrastructure. The role involves weekend support, managing server and storage hardware, virtualization, and enterprise patching/maintenance. Experience with scripting languages and cloud technologies is desired.",
        "industries": [
            "Software",
            "Logistics",
            "Technology",
            "Data Center"
        ],
        "soft_skills": [
            "Problem Solving",
            "Communication",
            "Teamwork",
            "Attention to Detail"
        ],
        "hard_skills": [
            "Server Administration",
            "Storage Hardware",
            "Rack Mount Servers",
            "SAN",
            "Direct Attached Storage",
            "Fibre Channel Fabric",
            "Enterprise Patching",
            "Windows Server",
            "Linux Operating System",
            "Virtualization",
            "Hyper-V",
            "VMware",
            "KVM",
            "Microsoft System Center Suite",
            "Windows Scripting",
            "Unix Scripting",
            "PowerShell",
            "Perl",
            "Python",
            "Bash",
            "Cloud Technologies"
        ],
        "tech_stack": [
            "Windows Server",
            "Linux",
            "Hyper-V",
            "VMware",
            "KVM",
            "Microsoft System Center Suite",
            "PowerShell",
            "Perl",
            "Python",
            "Bash",
            "Cloud Technologies"
        ],
        "programming_languages": [
            "PowerShell",
            "Perl",
            "Python",
            "Bash"
        ],
        "experience": 4,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Chicago, IL",
        "job_id": 3816616738,
        "company": "Tempus AI",
        "title": "Senior Software Engineer - EHR Integrations",
        "created_on": 1720635121.2429101,
        "description": "Passionate about precision medicine and advancing the healthcare industry? Recent advancements in underlying technology have finally made it possible for AI to impact clinical care in a meaningful way. Tempus' proprietary platform connects an entire ecosystem of real-world evidence to deliver real-time, actionable insights to physicians, providing critical information about the right treatments for the right patients, at the right time. About Our Teams At Tempus, products are owned and developed by small, autonomous teams composed of developers, designers, scientists, and product managers. You and your team set the goals, build the software, deploy the code, and contribute to a growing software platform that will make a lasting impact in the field of cancer research and treatment. Tempus builds software as nimble as our teams. Our modern tech stack - React, NodeJS, Typescript and Python on GCP - allows our teams to iterate rapidly and lead our industry in innovation. Our decentralized, microservice architecture and emphasis on automation allow us to deliver advanced solutions with confidence, and at scale. As a Senior Software Engineer specializing in EHR Integrations at Tempus, you will play a crucial role in bridging Tempus' exceptional point-of-care insights with the EHR systems utilized by our partner practices for patient management. Your responsibilities will involve constructing cutting-edge full-stack applications, leveraging interoperability standards such as FHIR and HL7. This includes developing Chromium project overlay apps that are compatible with browser-based EHR systems, as well as applications tailored for Windows desktop-based EHR systems. Why We’re Looking For You You have several years of hands-on software development using any of the following languages: JavaScript/TypeScript/Java/Python or equivalent You have HealthCare development experience including EHR APIs (Epic, Cerner or OncoEMR's FHIR Resource, or proprietary APIs) and or HL7 integrations You love exploring hard problems and solving them with data-driven, iterative software development You understand the challenges and appreciate the rewards of a distributed and event-based architecture You've worked in fast-paced, agile environments and helped keep projects on track, troubleshoot roadblocks, write documentation, and coordinate cross-team collaboration You enjoy learning from and collaborating with a diverse, talented team and mentoring junior team members Excellent communication skills Bonus Points For Experience with Docker, AWS, GCP Experience a relational DB such as MySQL or PostgreSQL Experience in building pluggable UIs (overlays) via chrome plugins / chromium apps in browsers / windows desktop. Experience with API design and development Experience in biotech, genomics, and precision medicine Responsibilities For The Position Work on a Scrum team, create and maintain APIs, Services, and Components using JavaScript/Typescript and Node.js in a cloud-first environment Use forward-leaning software engineering approaches in your work, e.g. clean code. Contribute to a team culture that achieves a well designed system and a culture of learning and continuous improvement. Mentor other software engineers The expected salary range below is applicable if the role is performed from [ New York ] and may vary for other locations. Actual salary may vary based on qualifications and experience. Tempus offers a full range of benefits, which may include incentive compensation, restricted stock units, medical and other benefits, depending on the position. New York Pay Range $130,000—$175,000 USD The expected salary range below is applicable if the role is performed from [ California ] and may vary for other locations. Actual salary may vary based on qualifications and experience. Tempus offers a full range of benefits, which may include incentive compensation, restricted stock units, medical and other benefits, depending on the position. California Pay Range $130,000—$175,000 USD We are an equal opportunity employer. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",
        "url": "https://www.linkedin.com/jobs/view/3816616738",
        "summary": "Tempus is seeking a Senior Software Engineer specializing in EHR Integrations to build full-stack applications using FHIR and HL7 standards for browser-based and desktop EHR systems. The role involves connecting Tempus' insights with partner practices' EHR systems, developing Chromium project overlay apps, and collaborating with a team of developers, designers, scientists, and product managers.",
        "industries": [
            "Healthcare",
            "Biotechnology",
            "Genomics",
            "Precision Medicine",
            "Software Development",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Passionate",
            "Problem-Solving",
            "Data-Driven",
            "Communication",
            "Collaboration",
            "Mentoring",
            "Continuous Improvement"
        ],
        "hard_skills": [
            "JavaScript",
            "TypeScript",
            "Java",
            "Python",
            "EHR APIs",
            "FHIR",
            "HL7",
            "Epic",
            "Cerner",
            "OncoEMR",
            "Docker",
            "AWS",
            "GCP",
            "MySQL",
            "PostgreSQL",
            "Chrome Plugins",
            "Chromium Apps",
            "API Design",
            "Scrum",
            "Clean Code"
        ],
        "tech_stack": [
            "React",
            "NodeJS",
            "Typescript",
            "Python",
            "GCP",
            "FHIR",
            "HL7",
            "Chromium",
            "Docker",
            "AWS",
            "MySQL",
            "PostgreSQL"
        ],
        "programming_languages": [
            "JavaScript",
            "TypeScript",
            "Java",
            "Python"
        ],
        "experience": 3,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 175000,
            "min": 130000
        },
        "benefits": [
            "Incentive Compensation",
            "Restricted Stock Units",
            "Medical Benefits"
        ]
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Chicago, IL",
        "job_id": 3958458196,
        "company": "Jump Trading Group",
        "title": "Full Stack Engineer",
        "created_on": 1720635125.1494145,
        "description": "Jump Trading Group is committed to world class research. We empower exceptional talents in Mathematics, Physics, and Computer Science to seek scientific boundaries, push through them, and apply cutting edge research to global financial markets. Our culture is unique. Constant innovation requires fearlessness, creativity, intellectual honesty, and a relentless competitive streak. We believe in winning together and unlocking unique individual talent by incentivizing collaboration and mutual respect. At Jump, research outcomes drive more than superior risk adjusted returns. We design, develop, and deploy technologies that change our world, fund start-ups across industries, and partner with leading global research organizations and universities to solve problems. Jump Trading has a global team of technologists who architect, build and maintain our world-class trading platform. From optimizing our core trading engine to building custom hardware, we leverage software & hardware engineering, data science and research, to deliver the infrastructure and tools that drive our trading and business needs. As a Software Engineer with focus on Full Stack GUI development on the Trading Tools team, you’ll be responsible for developing and maintaining critical GUIs and applications for the trading community. What You'll Do In this role, you will work with internal users to build performant, informative, and user-friendly GUI applications. These applications will be the eyes of the trading community—monitoring trading activities in real-time with data-dense displays—and its hands—controlling our complex automated systems. The ideal candidate will be someone who is excited about application software development, continually stays up to date with the TypeScript/React ecosystem, and helps to elevate the productivity of the team via knowledge sharing and mentoring. Skills You'll Need At least 5+ years of experience programming in modern frontend frameworks At least 2+ years of experience with backend languages like Python, Rust, and C++ Passion for software development, strong work ethic, and drive to continually learn and improve Desire to make awesome GUIs which users enjoy using on a daily basis Understanding of how to build high-performance TypeScript applications capable of visualizing large amounts of data Ability to write reliable and readable code, and provide code review feedback to others Appreciation of the balance between beautiful abstractions vs. actually shipping code Reliable and predictable availability Bonus Points An eye for quality design Experience with Kubernetes, Helm, containers, CI/CD pipelines Experience working in trading / finance. Benefits Discretionary bonus eligibility Medical, dental, and vision insurance HSA, FSA, and Dependent Care options Employer Paid Group Term Life and AD&D Insurance Voluntary Life & AD&D insurance Paid vacation plus paid holidays Retirement plan with employer match Paid parental leave Wellness Programs Annual Base Salary Range $150,000—$200,000 USD",
        "url": "https://www.linkedin.com/jobs/view/3958458196",
        "summary": "Jump Trading Group is seeking a Software Engineer with a focus on full-stack GUI development to build and maintain critical GUIs and applications for their trading community. The role involves working with internal users to create high-performance, informative, and user-friendly applications that monitor real-time trading activities and control automated systems.  The ideal candidate will have strong experience in frontend frameworks, backend languages, and be passionate about software development, user experience, and knowledge sharing.",
        "industries": [
            "Finance",
            "Trading",
            "Technology",
            "Software Development",
            "Data Science",
            "Research"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Teamwork",
            "Mentorship",
            "Knowledge Sharing",
            "Passion for Learning",
            "Attention to Detail",
            "User Focus",
            "Creativity"
        ],
        "hard_skills": [
            "TypeScript",
            "React",
            "Python",
            "Rust",
            "C++",
            "GUI Development",
            "Data Visualization",
            "Performance Optimization",
            "Code Review",
            "Kubernetes",
            "Helm",
            "Containers",
            "CI/CD Pipelines"
        ],
        "tech_stack": [
            "TypeScript",
            "React",
            "Python",
            "Rust",
            "C++",
            "Kubernetes",
            "Helm",
            "Containers",
            "CI/CD Pipelines"
        ],
        "programming_languages": [
            "TypeScript",
            "Python",
            "Rust",
            "C++"
        ],
        "experience": 5,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 200000,
            "min": 150000
        },
        "benefits": [
            "Discretionary Bonus",
            "Medical Insurance",
            "Dental Insurance",
            "Vision Insurance",
            "HSA",
            "FSA",
            "Dependent Care",
            "Group Term Life Insurance",
            "AD&D Insurance",
            "Voluntary Life Insurance",
            "Paid Vacation",
            "Paid Holidays",
            "Retirement Plan with Employer Match",
            "Paid Parental Leave",
            "Wellness Programs"
        ]
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Moline, IL",
        "job_id": 3517288474,
        "company": "ATC",
        "title": "Infrastructure Engineer",
        "created_on": 1720635126.7185628,
        "description": "Title: Infrastructure Analyst III Location: 400 19th St Moline IL USA 61265-1373 Interview: Webex Description Major Purpose: Works on a delivery team and supports systems and Infrastructure deployment and/or maintenance by planning its work, coordinating with others to deliver functionality on complex systems. This level works on medium complexity infrastructure problems; applies technology to meet specific needs, focusing on the most difficult tasks and works on complex/significant parts of a system/sub-system or Infrastructure package and solves welldefined problems with little guidance. Delivers complex Infrastructure use-cases. This role relies on advancedlevel knowledge and experience on up to two Infrastructure technologies (collaboration, desktop, network, server, storage, etc.) and a solid understanding of relevant components and systems; recognized as an expert on a system(s) or Infrastructure package(s). Selection, planning, and implementation of changes to a broad subset of the information technology infrastructure; requires contacts with multiple functional and business partners on tactical issues and some contact for planning issues. Works independently; receives only occassional supervision from the Infrastructure Manager, Scrum Master or other senior roles. May act as a work lead, providing technical supervision to other analysts and support personnel. Major Duties Tests deliverables against a user story's acceptance tests. Creates and maintains automated test scripts and executes all other testing-related tasks. Works closely with other team members to ensure that features meet business needs. Follows industry-standard agile Infrastructure design methodology for development and documentation. Builds the work needed to implement features from the product backlog (managed by the product owner); Determines the size of user stories. Creates, maintains, and executes against product or service documentation. Works collaboratively with infrastructure, security, software delivery teams and other stakeholders. Mentors junior analysts in specific Infrastructure technologies such as collaboration, desktop, network, server, storage, etc. Skills, Abilities, Knowledge Understanding of relevant infrastructure components and operating systems and the ability to troubleshoot in those environments. Analytical skills. Ability to analyze, translate, and define business requirements into technical solutions. Excellence in verbal and written communication forms with emphasis on persuasive communication, tact and negotiation. Business knowledge, e.g., processes, relationships, etc. Education Degree in an Engineering/Technology discipline or equivalent experience. - University Degree (4 years or equivalent) Degree in an Information Technology discipline or equivalent experience. - University Degree (4 years or equivalent) Work Experience Infrastructure exp in 1 or more components, i.e., communications, private/public network, network access security, database and information access security, middleware, computing hardware, operating system. (4 - 7 years) Previous experience in programming/software development/data analyst. (4 - 7 years) Experience working in an Agile Environment (1 - 3 years) Position Title Infrastructure Engineer Specific Position Requirements Position Title: Infrastructure Engineer This team is currently working a hybrid schedule (Onsite every Tues/ Wed/ Thur, additional onsite days as needed). Fully remote candidates will not be considered for this position. Position Description Monitors, applies changes to, and/or manages current information technology infrastructure to ensure high availability and optimal performance in an integrated environment. Performs and applies information analysis to resolve infrastructure problems. Delivers infrastructure technology for business requirements. Identifies, analyzes, selects, and applies appropriate new information technology infrastructure components within a diverse environment as required to support strategic business plans and processes. Required Skill Set – Expert experience in configuring, deploying and managing end user computing systems including both hardware and operating systems for enterprise use – MUST HAVE Proven experience working with operating system scripting, builds, deployments and utilization of tools including: Intune, Microsoft Endpoint Manager (MEM), MECM, SCCM, and JAMF – MUST HAVE Experience with software packaging, scripting, deployment, management and reclamation through software management tools including Intune, MECM and Flexera. - MUST HAVE Experience with programming/scripting tools such as PowerShell used in end user computing environments. - MUST HAVE Excellence in verbal and written communication forms. - MUST HAVE Ability to work with a team in an agile environment, including a product manager and scrum master, to manage many competing priorities/requests in a fast-paced environment.- MUST HAVE Desired Skill Set – Ability to analyze, translate, and define business requirements into technical solutions. – Nice to have Performs and applies information analysis to resolve infrastructure problems. Delivers infrastructure technology for business requirements. – Nice to have Monitors, applies changes to, and/or manages current information technology infrastructure to ensure high availability and optimal performance in an integrated environment. – Nice to have General network and network services knowledge including TCP/IP, DNS, DHCP and Active Directory. – Nice to have Completes changes and improvements to processes. This may include modifying/configuring existing operating systems, programming/scripting, or designing new processes. Responsible for communicating and recording change documentation. – Nice to have Understanding of desktop operating system environments, infrastructure capabilities, and constraints. – Nice to have",
        "url": "https://www.linkedin.com/jobs/view/3517288474",
        "summary": "The Infrastructure Analyst III works on a delivery team and supports systems and Infrastructure deployment and/or maintenance. The role requires advanced-level knowledge and experience on up to two Infrastructure technologies (collaboration, desktop, network, server, storage, etc.) and a solid understanding of relevant components and systems. The Infrastructure Engineer monitors, applies changes to, and/or manages current information technology infrastructure to ensure high availability and optimal performance in an integrated environment. They also perform and apply information analysis to resolve infrastructure problems. The role requires expertise in configuring, deploying, and managing end-user computing systems, including both hardware and operating systems for enterprise use.",
        "industries": [
            "Information Technology",
            "Technology",
            "Engineering",
            "Infrastructure"
        ],
        "soft_skills": [
            "Analytical",
            "Communication",
            "Persuasion",
            "Negotiation",
            "Teamwork",
            "Problem Solving"
        ],
        "hard_skills": [
            "Troubleshooting",
            "Operating Systems",
            "Agile Methodology",
            "Automated Testing",
            "Test Scripting",
            "User Story Development",
            "Documentation",
            "Mentoring",
            "PowerShell",
            "Intune",
            "Microsoft Endpoint Manager (MEM)",
            "MECM",
            "SCCM",
            "JAMF",
            "Software Packaging",
            "Scripting",
            "Deployment",
            "Management",
            "Reclamation",
            "TCP/IP",
            "DNS",
            "DHCP",
            "Active Directory",
            "Change Management",
            "Desktop Operating Systems",
            "Infrastructure Capabilities",
            "Constraints"
        ],
        "tech_stack": [
            "Intune",
            "Microsoft Endpoint Manager (MEM)",
            "MECM",
            "SCCM",
            "JAMF",
            "PowerShell",
            "Flexera",
            "Active Directory",
            "TCP/IP",
            "DNS",
            "DHCP"
        ],
        "programming_languages": [
            "PowerShell"
        ],
        "experience": 4,
        "education": {
            "min_degree": "University Degree",
            "fields": [
                "Engineering",
                "Technology",
                "Information Technology"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Elgin, IL",
        "job_id": 3970179773,
        "company": "ZipRecruiter",
        "title": "Staff Software Release Engineer",
        "created_on": 1720635128.4006884,
        "description": "Job Description Arm has built the world’s most pervasive compute architecture, and we’ve led many of the technology revolutions that impact the day-to-day lives of people everywhere. The Future of Infrastructure is Built on Arm. Now we are building new software teams to take us to the next level. Technology built on Arm is all around us, from industrial and automotive applications, to the IoT, to the desktop and data center. ‘Wherever Computing Happens’, we need to enable Arm by providing software solutions that interface higher-level software stacks with the hardware itself. Job Overview Arm is seeking skilled, experienced, and highly motivated release engineering expert to join our Software Engineering group. As a member of the System Solutions team, you will have the opportunity to enable the evolution of Computing Infrastructure using Neoverse CSS. You will be working with a distributed team spread across Arm’s worldwide engineering centers. Your primary responsibilities will include release engineering and version control of production quality firmware releases. Responsibilities Release engineering and branch management for the production firmware releases. Providing tested releases of production quality downstream firmware to partners. Development of release plans in collaboration with various stakeholders. Documentation of release notes and effective communication to internal and external collaborators Management of entire release process, including branch management and all aspects of software version control. Development of CI pipelines for downstream firmware release processes. Are you are looking for a unique opportunity to be part of a Firmware release team transforming computing infrastructure landscape? We would like to hear from you! Required Skills And Experience Proven experience in Release management and Release automation of product quality system software. Experience in software building and build management – for official downstream software release candidates and formal releases. Hands on experience with Yocto SDK. “Nice To Have” Skills And Experience Familiarity with QA of platform software for server platforms. Defect triaging and defect management experience of system software releases. Familiarity with open-source projects such as Linux Kernel, TF-A, EDK II and OpenBMC. Line management experience of diverse team. In Return Arm Neoverse is the foundation for the next era of digital infrastructure. This role provides an outstanding opportunity to develop and contribute to the success of Arm Neoverse CSS based solutions. Arm is an equal opportunity employer, committed to providing an environment of mutual respect where equal opportunities are available to all applicants and colleagues. We are a diverse organization of resolute and innovative individuals, and do not discriminate based on any characteristic.",
        "url": "https://www.linkedin.com/jobs/view/3970179773",
        "summary": "Arm is seeking a release engineering expert to join their Software Engineering group. The role will involve release engineering and version control of production quality firmware releases for the Neoverse CSS platform. Responsibilities include branch management, release planning, documentation, and CI pipeline development.",
        "industries": [
            "Computer Hardware",
            "Software",
            "Technology"
        ],
        "soft_skills": [
            "Highly motivated",
            "Strong communication",
            "Collaboration",
            "Problem-solving",
            "Time management",
            "Organizational skills",
            "Detail-oriented",
            "Teamwork"
        ],
        "hard_skills": [
            "Release management",
            "Release automation",
            "Software building",
            "Build management",
            "Yocto SDK",
            "QA",
            "Defect triaging",
            "Defect management",
            "Linux Kernel",
            "TF-A",
            "EDK II",
            "OpenBMC",
            "CI/CD",
            "Branch management",
            "Version control"
        ],
        "tech_stack": [
            "Yocto SDK",
            "Neoverse CSS",
            "Linux Kernel",
            "TF-A",
            "EDK II",
            "OpenBMC"
        ],
        "programming_languages": [],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "O'Fallon, IL",
        "job_id": 3902894257,
        "company": "Keylent Inc",
        "title": "Sr. Software Engineer MAHIN-JOB-33578",
        "created_on": 1720635130.2877586,
        "description": "Sr. Software Engineer MAHIN-JOB-33578 Location: OFALLON Roles And Responsibilities JAVA, API, AzuresecOPS Work closely with software engineers, product managers, and other stakeholders to identify and understand the API requirements. Design and develop the API interface and documentation using programming languages like Java, Python, or Ruby on Rails2. Implement RESTful APIs to enable seamless communication between front end and back end systems1. Conduct thorough code reviews to maintain code quality, adherence to coding standards, and software design1. Identify and resolve performance bottlenecks and issues to optimize application speed and efficiency",
        "url": "https://www.linkedin.com/jobs/view/3902894257",
        "summary": "Senior Software Engineer with experience in Java, API development, and Azure security operations. Responsibilities include designing and implementing RESTful APIs, collaborating with other stakeholders, ensuring code quality, and optimizing application performance.",
        "industries": [
            "Software Development",
            "Information Technology",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Analytical Skills",
            "Code Review",
            "Performance Optimization"
        ],
        "hard_skills": [
            "Java",
            "API",
            "RESTful APIs",
            "Python",
            "Ruby on Rails",
            "Azure",
            "Security Operations"
        ],
        "tech_stack": [
            "Java",
            "Python",
            "Ruby on Rails",
            "Azure",
            "RESTful APIs"
        ],
        "programming_languages": [
            "Java",
            "Python",
            "Ruby"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Chicago, IL",
        "job_id": 3967788664,
        "company": "LHH",
        "title": "Lead Software Engineer",
        "created_on": 1720635131.9952834,
        "description": "Lead Software Engineer – Direct Hire – Chicago, IL - partial remote This is a direct hire, full time, permanent position to be a Lead Software Engineer within an extremely stable, rapidly growing, well-respected technology company. This role will be in the office most days but does have at least one day of remote work each week. On top of a very competitive base salary you will also receive unlimited PTO, excellent benefits, an annual bonus of around 20 percent and even some equity options! Responsibilities: Leading the effort in the design, development, and maintenance of our data production ETL process Collaborating with product managers, architects, and other stakeholders to define and prioritize requirements and translate them into actionable technical tasks Ensuring the timely delivery of high-quality, scalable, and maintainable ETL solutions that adhere to best practices and meet business requirements Reviewing code, providing constructive feedback, and ensuring adherence to coding standards and guidelines. Participating in architecture and design discussions, contributing to the overall technical vision and direction of the tea Build and maintain unit tests and integration tests to ensure code quality and reliability Continuously improving the performance, reliability, and security of our web services and ETL jobs Addressing technical debt, refactoring code as necessary, and driving improvements in the team's software development processes and practices Assisting in the recruitment, onboarding, and training of new team members Staying up to date with industry trends, emerging technologies, and best practices in ETL job and API development, sharing knowledge and insights with the team Requirements: Ideally this person should speak decent Mandarin as they will be interacting with an offshore development team in China when needed Bachelor’s degree in computer science, engineering, or a related field. A master’s degree is a plus. At least 5 years of software development experience with a focus on ETL job design and development. API development experience. Expertise in Java. (Spring Boot is a plus). Experience in Python and Pandas for backend development. (experience with AWS is a plus). Demonstrated experience in building and maintaining unit tests and integration tests. Familiarity with test automation tools and frameworks. Experience with version control systems (e.g., Git) and CI/CD pipelines. Knowledge of micro-services, event-driven architecture, and containerization technologies (e.g., Docker, Kubernetes) is preferred. Strong analytical, detail-oriented problem-solving abilities and excellent communication, capable of both independent work and effective collaboration in a team environment. Perks and Benefits: Unlimited PTO Good BCBS benefits 4% 401K match Shut down at 2:00 on Fridays (happy hours and social events follow if you want to join) No red tape, complete autonomy and you will wear multiple hats You will work with extremely smart driven people (lots of fun challenges and opportunity to learn) Excellent annual bonus and potentially equity as well!",
        "url": "https://www.linkedin.com/jobs/view/3967788664",
        "summary": "Lead Software Engineer to design, develop, and maintain data production ETL processes. Collaborate with product managers and architects. Ensure timely delivery of high-quality ETL solutions.  Review code and provide feedback. Participate in architecture and design discussions. Build and maintain unit tests and integration tests. Improve the performance, reliability, and security of web services and ETL jobs. Address technical debt and refactor code. Assist in team recruitment, onboarding, and training. Stay up-to-date with industry trends and best practices. ",
        "industries": [
            "Technology",
            "Software Development",
            "Data Engineering"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem-Solving",
            "Analytical",
            "Detail-Oriented",
            "Leadership",
            "Teamwork",
            "Independent Work",
            "Time Management"
        ],
        "hard_skills": [
            "ETL",
            "API Development",
            "Java",
            "Spring Boot",
            "Python",
            "Pandas",
            "Unit Testing",
            "Integration Testing",
            "Test Automation",
            "Git",
            "CI/CD",
            "Microservices",
            "Event-Driven Architecture",
            "Docker",
            "Kubernetes"
        ],
        "tech_stack": [
            "Java",
            "Spring Boot",
            "Python",
            "Pandas",
            "Git",
            "Docker",
            "Kubernetes"
        ],
        "programming_languages": [
            "Java",
            "Python"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Engineering"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Unlimited PTO",
            "BCBS Benefits",
            "401K Match",
            "Annual Bonus",
            "Equity Options",
            "Happy Hours",
            "Social Events",
            "Autonomy",
            "Learning Opportunities"
        ]
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Chicago, IL",
        "job_id": 3959947837,
        "company": "Surge Technology Solutions Inc",
        "title": "Senior Java Software Engineer",
        "created_on": 1720635133.6008167,
        "description": "Emp Type: W2 or 1099........ (No C2C) Visa: H1B, OPT, H4EAD, GCEAD, L2, Green Card, US Citizens (Only USA Applicants) Workplace Type: Onsite / Chicago – IL Experience: 8+ Yrs Position’s Contributions to Work Group: - Team member contributing to programming, project management, and development assignments. Working on backend services. Typical task breakdown: 1. Competent to perform all programming, project management, and development assignments without close supervision 2.Works directly on complex application/technical problem identification and resolution, including responding to off-shift and weekend support calls 3.Works independently on complex systems or infrastructure components that may be used by one or more applications or systems. 4. Drives application development focused around delivering business value 5. Mentors and assists software engineers, providing technical assistance and direction as needed 6. Maintains high standards of software quality within the team by establishing good practices and habits 7. Proactively reaches out for help when stuck on an issue after doing reasonable independent research 8. Maintains a sense of urgency when working on tasks and actively follow-up on any dependencies or blockers Work environment: - Hybrid Work model; will need to go into office 2x a week (Chicago office) Education & Experience Required: - Bachelor’s degree in Computer science or Electrical engineering or related field is required w/ 8+ years’ experience needed - Masters degree with 6+ years’ experience Technical Skills (Required) - •8+ years or more of experience in designing and developing software applications in Java. *Proven experience in many of the following, O Designing, developing, deploying and maintaining software at scale. O Developing software applications using relational and NoSQL databases. O Application architectural patterns, such as MVC, Microservices, Event-driven, etc. o Deploying software using CI/CD tools, such as Jenkins, Azure DevOps, GoCD, etc O Deploying and maintaining software using public clouds such as AWS or Azure. O Working within an Agile framework (ideally Scrum) -------- - • Strong understanding and/or experience in many of the following, O Message streaming solutions, such as AWS Kinesis, AWS SQS, AWS SNS, Apache Kafka, RabbitMQ, Apache ActiveMQ. O Experience designing well-defined Restful APIs o Hands on experience with API tools such as Swagger, Postman and Assertible O Test Driven Development and Behavior Driven Development. O Hands on experience with testing tools such as Cucumber and Selenium and their integration into CI/CD pipelines. o Datastores such as AWS Aurora, AWS RDS, AWS DynamoDB, MongoDB, Elasticsearch, Cassandra, Redis, MySQL, Oracle. O Other AWS technologies, such as API Gateway, ALB, NLB, Fargate, Lambda, S3, CloudWatch, etc O Debugging and maintaining software in Linux or Unix platforms. Soft Skills (Required) - • Must demonstrate solid knowledge of Computer Science fundamentals, such as data structures and algorithms. • Ability to work under pressure and within time constraints • Passion for technology and an eagerness to contribute to a team-oriented environment • Demonstrated leadership on small to medium-scale projects impacting strategic priorities Disqualifiers/Red Flags: - If candidate is not in local area, please do not submit - If the candidate’s current place of residence is not on resume, they will be DQ - If candidate currently resides outside of local area, but is open to relocate on their own dime, please make sure to list that clearly on the resume. Interview Process: - 3 rounds ( 1 hour each typically) ; coding exercises, theoretical questions to answer, etc. Please share your resume and contact details to sahithi_s@surgetechinc.com or can call on 832-990-6448",
        "url": "https://www.linkedin.com/jobs/view/3959947837",
        "summary": "SurgeTech Inc. is seeking a Senior Software Engineer with 8+ years of experience in designing and developing software applications in Java.  The role involves working on backend services, complex problem identification and resolution, driving application development, mentoring junior engineers, and maintaining high software quality standards. The candidate will work in a hybrid work model and must have strong knowledge of Computer Science fundamentals, data structures, and algorithms. ",
        "industries": [
            "Software Development",
            "Information Technology",
            "Technology"
        ],
        "soft_skills": [
            "Problem Solving",
            "Communication",
            "Teamwork",
            "Leadership",
            "Mentorship",
            "Technical Expertise",
            "Time Management",
            "Pressure Handling",
            "Passion for Technology",
            "Proactive",
            "Independent",
            "Research",
            "Urgency",
            "Follow-Up"
        ],
        "hard_skills": [
            "Java",
            "Software Design",
            "Software Development",
            "Software Architecture",
            "Relational Databases",
            "NoSQL Databases",
            "MVC",
            "Microservices",
            "Event-Driven Architecture",
            "CI/CD",
            "Jenkins",
            "Azure DevOps",
            "GoCD",
            "AWS",
            "Azure",
            "Agile",
            "Scrum",
            "AWS Kinesis",
            "AWS SQS",
            "AWS SNS",
            "Apache Kafka",
            "RabbitMQ",
            "Apache ActiveMQ",
            "RESTful APIs",
            "Swagger",
            "Postman",
            "Assertible",
            "Test Driven Development",
            "Behavior Driven Development",
            "Cucumber",
            "Selenium",
            "AWS Aurora",
            "AWS RDS",
            "AWS DynamoDB",
            "MongoDB",
            "Elasticsearch",
            "Cassandra",
            "Redis",
            "MySQL",
            "Oracle",
            "API Gateway",
            "ALB",
            "NLB",
            "Fargate",
            "Lambda",
            "S3",
            "CloudWatch",
            "Linux",
            "Unix",
            "Data Structures",
            "Algorithms"
        ],
        "tech_stack": [
            "Java",
            "AWS",
            "Azure",
            "Jenkins",
            "Azure DevOps",
            "GoCD",
            "AWS Kinesis",
            "AWS SQS",
            "AWS SNS",
            "Apache Kafka",
            "RabbitMQ",
            "Apache ActiveMQ",
            "Swagger",
            "Postman",
            "Assertible",
            "Cucumber",
            "Selenium",
            "AWS Aurora",
            "AWS RDS",
            "AWS DynamoDB",
            "MongoDB",
            "Elasticsearch",
            "Cassandra",
            "Redis",
            "MySQL",
            "Oracle",
            "API Gateway",
            "ALB",
            "NLB",
            "Fargate",
            "Lambda",
            "S3",
            "CloudWatch",
            "Linux",
            "Unix"
        ],
        "programming_languages": [
            "Java"
        ],
        "experience": 8,
        "education": {
            "min_degree": "Bachelor’s degree",
            "fields": [
                "Computer Science",
                "Electrical Engineering"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Lincolnshire, IL",
        "job_id": 3934965792,
        "company": "Snap-on",
        "title": "C# Software Engineer",
        "created_on": 1720635135.5199218,
        "description": "Snap-on Diagnostics is seeking an outstanding C# software engineer/developer to work on enhancing and maintaining applications which are critical for business. In this role, you will be working onsite directly with our team to help build new features, fix bugs, and address technical debt. This position requires an excellent team player with great communication skills who demonstrates a passion for delivering software of uncompromising quality to customers. The candidate will frequently be required to work on a cross-functional level in partnership with other Snap-on Diagnostics’ Software Development sites across the globe. Essential skills required (5 to 10 years solid experience of the following): C# Experience with architecture patterns like MVC, MVVM Experience with software design and UML Experience with Web Services and Web APIs Desktop Application Development Experience in writing unit tests Successful Track Record as a Team Player Excellent written and verbal communication in English Language Self-Motivated and self-organizing Experience with Agile and Scrum development The Following Skills Are Not Essential, But Desirable Test automation Experience working with SQL server Snap-on is an Equal Opportunity Employer, Minority/Female/Disabled/Veteran Snap-on is an Equal Opportunity Employer, Minority/Female/Disabled/Veteran",
        "url": "https://www.linkedin.com/jobs/view/3934965792",
        "summary": "Snap-on Diagnostics is looking for a C# software engineer/developer to work on enhancing and maintaining critical business applications. The ideal candidate will have 5-10 years of experience in C#, MVC, MVVM, software design, UML, web services, APIs, desktop application development, unit testing, and Agile/Scrum methodologies. They will work onsite with the team to build new features, fix bugs, and address technical debt. Strong communication skills and a passion for quality software delivery are essential.",
        "industries": [
            "Software Development",
            "Automotive"
        ],
        "soft_skills": [
            "Teamwork",
            "Communication",
            "Passion for Quality",
            "Self-Motivation",
            "Self-Organization"
        ],
        "hard_skills": [
            "C#",
            "MVC",
            "MVVM",
            "Software Design",
            "UML",
            "Web Services",
            "Web APIs",
            "Desktop Application Development",
            "Unit Testing",
            "Agile",
            "Scrum",
            "Test Automation",
            "SQL Server"
        ],
        "tech_stack": [
            "C#",
            "MVC",
            "MVVM",
            "UML",
            "Web Services",
            "Web APIs",
            "SQL Server"
        ],
        "programming_languages": [
            "C#"
        ],
        "experience": 5,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Chicago, IL",
        "job_id": 3961933225,
        "company": "PayPal",
        "title": "Senior Software Engineer",
        "created_on": 1720635137.251439,
        "description": "At PayPal (NASDAQ: PYPL), we believe that every person has the right to participate fully in the global economy. Our mission is to democratize financial services to ensure that everyone, regardless of background or economic standing, has access to affordable, convenient, and secure products and services to take control of their financial lives. Job Description Summary: At PayPal, we believe that now is the time to democratize financial services so that moving and managing money is a right for all citizens, not just the affluent.  We are driven by this purpose, and we uphold our cultural values of collaboration, innovation, wellness and inclusion as our guide for making decisions and conducting business every day.  It is our duty and privilege to be customer champions and put those we server at the center of everything we do.    We are one team that respects and values diversity of thought for everyone, everywhere, and we actively seek to create an energizing workplace that brings out the best in all of us.  If you are ready to shape the future of money, join the team at PayPal.  We are proud to work here.  You will be too.  The Customer Success Platform team is responsible for developing innovative solutions through applications, integrations, and automations which are used by thousands of teammates around the world.  As a member of this team, you will assist us in developing the solutions that will power the next generations of Customer Care Teammate products.   What is the opportunity?  Our team is looking for a Software Engineer with experience developing across the full application stack and is passionate about code craftsmanship, continuous improvement, and agile development.  You will work on projects which leverage technology to drive solutions, empowerment, and execution across the workforce through an effortless experience.  We are looking for you to bring a creative and motivated work ethic to tackle new problems and innovating solutions.  What you will do? Write code! Build enterprise applications to help customers Collaborate with the team in an agile environment to deliver innovative, quality code iteratively Work independently and with other engineers to develop tactical processes to create solutions that provide an effortless experience to the business.  Develop multi-tiered software applications with detailed documentation and effective monitoring.  Implement both front-end and back-end infrastructure with security, performance, and reliability at the forefront.  Use PayPal tools, operational procedures, site, and solution architecture to enhance the ability for monitoring of operations.  Take responsibility for the operability of the component being developed and proactively take ownership for related defects.  Learn new technologies quickly to keep up with the changing industry.  Demonstrate understanding of technical difficulty of delivering product ideas and of the technology stack.  Drive resolution of technical problems. Job Description: Passionate about developing amazing user experiences with a strong attention to detail Strong analytical and logical acumen Strong desire to learn, collaborate, succeed through collective team efforts Strong understanding of the Software Development Life Cycle (SDLC) & Agile/Scrum process and fundamentals Proficient written and verbal communication skills with the ability to present technical information in a clear and concise manner to a variety of audiences. Qualifications 0-2 years’ experience in software development or a related computer science field BS in computer science or equivalent Experience with React & Java Knowledge of REST and best practices in designing and developing APIs Understanding of HTTP, SSL, TCP/IP protocols. Experience with a version control system – preferably GitHub Experience with cross-browser, cross-platform, and design constraints Strong knowledge on design patterns, Strong written and communication skills Strong desire to learn, push the envelope, and share knowledge with others. Nice to Have Technology and Tools - Node.js, RDBMS like Oracle, SQL Server, Java or C#, Experience with automated test environments and CI/CD like Browserstack, leap works, Jenkins, JS test frameworks like cypress, Junit, postman Experience with HTML, JavaScript and web development frameworks (ReactJS, Electron, Bootstrap, jQuery) Experience with IDEs Our Benefits: At PayPal, we’re committed to building an equitable and inclusive global economy. And we can’t do this without our most important asset—you. That’s why we offer benefits to help you thrive in every stage of life. We champion your financial, physical, and mental health by offering valuable benefits and resources to help you care for the whole you. We have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https://www.paypalbenefits.com Who We Are: To learn more about our culture and community visit https://about.pypl.com/who-we-are/default.aspx PayPal has remained at the forefront of the digital payment revolution for more than 20 years. By leveraging technology to make financial services and commerce more convenient, affordable, and secure, the PayPal platform is empowering more than 400 million consumers and merchants in more than 200 markets to join and thrive in the global economy. For more information, visit paypal.com. PayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at paypalglobaltalentacquisition@paypal.com. As part of PayPal’s commitment to employees’ health and safety, we have established in-office Covid-19 protocols and requirements, based on expert guidance. Depending on location, this might include a Covid-19 vaccination requirement for any employee whose role requires them to work onsite. Employees may request reasonable accommodation based on a medical condition or religious belief that prevents them from being vaccinated. REQ ID R0112502",
        "url": "https://www.linkedin.com/jobs/view/3961933225",
        "summary": "PayPal seeks a Software Engineer to develop customer-facing applications within their Customer Success Platform team. This role requires experience with React & Java, REST APIs, and Agile development. You will work collaboratively to deliver high-quality code, leverage technology to solve business problems, and ensure secure, performant, and reliable infrastructure.",
        "industries": [
            "Financial Services",
            "Technology",
            "Software Development",
            "E-commerce",
            "Payments"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Teamwork",
            "Attention to Detail",
            "Analytical Skills",
            "Logical Thinking",
            "Desire to Learn",
            "Creative",
            "Motivated"
        ],
        "hard_skills": [
            "React",
            "Java",
            "REST",
            "API Design",
            "HTTP",
            "SSL",
            "TCP/IP",
            "GitHub",
            "Cross-Browser Development",
            "Cross-Platform Development",
            "Design Patterns",
            "Node.js",
            "Oracle",
            "SQL Server",
            "C#",
            "Automated Testing",
            "CI/CD",
            "Browserstack",
            "Leap Works",
            "Jenkins",
            "Cypress",
            "Junit",
            "Postman",
            "HTML",
            "JavaScript",
            "ReactJS",
            "Electron",
            "Bootstrap",
            "jQuery",
            "IDEs"
        ],
        "tech_stack": [
            "React",
            "Java",
            "REST APIs",
            "Node.js",
            "Oracle",
            "SQL Server",
            "C#",
            "Browserstack",
            "Leap Works",
            "Jenkins",
            "Cypress",
            "Junit",
            "Postman",
            "HTML",
            "JavaScript",
            "ReactJS",
            "Electron",
            "Bootstrap",
            "jQuery"
        ],
        "programming_languages": [
            "React",
            "Java",
            "JavaScript",
            "HTML",
            "C#",
            "Node.js"
        ],
        "experience": 2,
        "education": {
            "min_degree": "BS",
            "fields": [
                "Computer Science"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Flexible work environment",
            "Employee stock options",
            "Health and life insurance"
        ]
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Chicago, IL",
        "job_id": 3938852560,
        "company": "Coinbase",
        "title": "Software Engineer, Backend - Institutional Products",
        "created_on": 1720635138.9553616,
        "description": "At Coinbase, our mission is to increase economic freedom around the world, and we couldn’t do this without hiring the best people. We’re a group of hard-working overachievers who are deeply focused on building the future of finance and Web3 for our users across the globe, whether they’re trading, storing, staking or using crypto. Know those people who always lead the group project? That’s us. There are a few things we look for across all hires we make at Coinbase, regardless of role or team. First, we look for candidates who will thrive in a culture like ours, where we default to trust, embrace feedback, and disrupt ourselves. Second, we expect all employees to commit to our mission-focused approach to our work. Finally, we seek people who are excited to learn about and live crypto, because those are the folks who enjoy the intense moments in our sprint and recharge work culture. We’re a remote-first company looking to hire the absolute best talent all over the world. Ready to ? Who you are: You’ve got positive energy. You’re optimistic about the future and determined to get there. You’re never tired of learning. You want to be a pro in bleeding edge tech like DeFi, NFTs, DAOs, and Web 3.0. You appreciate direct communication. You’re both an active communicator and an eager listener - because let’s face it, you can’t have one without the other. You’re cool with candid feedback and see every setback as an opportunity to grow. You can pivot on the fly. Crypto is constantly evolving, so our priorities do, too. What you worked on last month may not be what you work on today, and that excites you. You’re not looking for a boring job. You have a “can do” attitude. Our teams create high-quality work on quick timelines. Owning a problem doesn’t scare you, but rather empowers you to take 100% responsibility for achieving our mission. You want to be part of a winning team. We’re stronger together, and you’re a person who embraces being pushed out of your comfort zone. Coinbase is seeking experienced backend engineers to join our team to build out the next generation of crypto-forward products and features. You will solve unique, large scale, highly complex technical problems, bridging the constraints posed by web-scale applications and blockchain technology. You will help build the next generation of systems to make cryptocurrency accessible to everyone across the globe, operating real-time applications with high frequency, low latency updates, and managing the most secure, dockerized infrastructure running in the cloud. The Institutional product group enables the foundations of Coinbase's institutional business supporting thousands of large financial institutions. We enable institutions to custody their digital assets and participate in the crypto economy in a secure and safe way. We take pride in building a scalable & secure crypto platform supporting hundreds of assets and enabling crypto participation with staking, governance and web3 gateways. The team has interesting work on building cutting-edge crypto technologies, scalable infrastructure and UI surfaces across web, mobile and browser extensions in a fast-paced industry with an opportunity to build several new initiatives What You'll Be Doing Decompose our monolithic Rails app into microservices Articulate a long term vision for maintaining and scaling our backend systems. Work with engineers, designers, product managers and senior leadership to turn our product and technical vision into a tangible roadmap every quarter. Write high quality, well tested code to meet the needs of your customers What We Look For In You You have at least 2+ years of experience in software engineering. You’ve designed, built, scaled and maintained production services, and know how to compose a service oriented architecture. You write high quality, well tested code to meet the needs of your customers. You’re passionate about building an open financial system that brings the world together. Nice To Haves You have gone through a rapid growth in your company (from startup to mid-size). You’ve build growth experiments or A/B tests. You have experience with Blockchains (such as Bitcoin, Ethereum etc..) You have experience decomposing a large monolith into microservices. You’ve worked with Golang, Ruby, Docker, Sinatra, Rails, Postgres, MongoDB or Redshift. You’ve built financial, high reliability or security systems. Job #: GIBE04NA Pay Transparency Notice: Depending on your work location, the target annual salary for this position can range as detailed below. Full time offers from Coinbase also include target bonus + benefits (including medical, dental, vision and 401(k)). Pay Range $147,900—$174,000 USD Commitment to Equal Opportunity Coinbase is committed to diversity in its workforce and is proud to be an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, creed, gender, national origin, age, disability, veteran status, sex, gender expression or identity, sexual orientation or any other basis protected by applicable law. Coinbase will also consider for employment qualified applicants with criminal histories in a manner consistent with applicable federal, state and local law. For US applicants, you may view the Know Your Rights notice here. Additionally, Coinbase participates in the E-Verify program in certain locations, as required by law. Coinbase is also committed to providing reasonable accommodations to individuals with disabilities. If you need a reasonable accommodation because of a disability for any part of the employment process, please contact us at accommodations[at]coinbase.com to let us know the nature of your request and your contact information. For quick access to screen reading technology compatible with this site click here to download a free compatible screen reader (free step by step tutorial can be found here). Global Data Privacy Notice for Job Candidates and Applicants Depending on your location, the General Data Protection Regulation (GDPR) and California Consumer Privacy Act (CCPA) may regulate the way we manage the data of job applicants. Our full notice outlining how data will be processed as part of the application procedure for applicable locations is available here. By submitting your application, you are agreeing to our use and processing of your data as required. For US applicants only, by submitting your application you are agreeing to arbitration of disputes as outlined here.",
        "url": "https://www.linkedin.com/jobs/view/3938852560",
        "summary": "Coinbase is looking for experienced backend engineers to join their team and build the next generation of crypto-forward products and features. You will be responsible for decomposing their monolithic Rails app into microservices, articulating a long-term vision for maintaining and scaling backend systems, and working with engineers, designers, product managers, and senior leadership to turn their product and technical vision into a tangible roadmap.",
        "industries": [
            "Financial Services",
            "Technology",
            "Fintech",
            "Cryptocurrency"
        ],
        "soft_skills": [
            "Positive Energy",
            "Optimistic",
            "Determined",
            "Learning",
            "Communication",
            "Active Listener",
            "Candid Feedback",
            "Adaptable",
            "Pivot",
            "Can Do Attitude",
            "Responsibility",
            "Teamwork",
            "Collaboration"
        ],
        "hard_skills": [
            "Software Engineering",
            "Service Oriented Architecture",
            "Microservices",
            "Rails",
            "Golang",
            "Ruby",
            "Docker",
            "Sinatra",
            "Postgres",
            "MongoDB",
            "Redshift",
            "A/B Testing",
            "Blockchains",
            "Bitcoin",
            "Ethereum",
            "Financial Systems",
            "High Reliability Systems",
            "Security Systems"
        ],
        "tech_stack": [
            "Rails",
            "Golang",
            "Ruby",
            "Docker",
            "Sinatra",
            "Postgres",
            "MongoDB",
            "Redshift",
            "Bitcoin",
            "Ethereum"
        ],
        "programming_languages": [
            "Ruby",
            "Golang"
        ],
        "experience": 2,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 174000,
            "min": 147900
        },
        "benefits": [
            "Medical",
            "Dental",
            "Vision",
            "401(k)"
        ]
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Chicago, IL",
        "job_id": 3961898987,
        "company": "FOSSA",
        "title": "Senior Software Engineer",
        "created_on": 1720635145.0228193,
        "description": "If you’re like us, you agree that open-source software is a wonderful and unique gift that the community of software developers across the world have given to each other. At FOSSA, we want to proliferate its usage with our software supply chain management platform that helps our customers know what licenses, vulnerabilities, and dependencies go into their software as they utilize open source software packages. And we won’t rest until all companies, big and small, love and feel as comfortable with using open source as we are. FOSSA is a rapidly growing series-B company with a truly distributed team. While our roots are in San Francisco, our growing team has members in several countries across many time zones. We’re always working to find the perfect balance of effective approaches to distributed work, while maintaining the direct personal connections that help us work better together and love our jobs. As a senior software engineer on our team, you'll be working closely with the whole company to bring features from ideation to implementation. Your participation will have a significant impact on our company roadmap, and you'll be able to see the outsized impact that your work has on our revenue. Your day-to-day will be largely determined by what you want to work on: in a team of our size, our engineering team also helps own our product, customer success, and growth functions. In this role you will: Work closely with the whole company to plan, design, build, and refine core product features. Own user-impacting features from conception to completion. Build and scale our core product and technical infrastructure. Have significant ownership in our technical architecture and product roadmap. Primarily work on NodeJS web application development in Typescript, but will also be exposed to many other projects in a variety of languages like Haskell, Rust, and Golang. About you: Experience building and supporting scalable SaaS products and features. Experience as a full-stack software engineer owning and building features end-to-end. Experience with relational databases and writing performant SQL queries. Ability to break down complex problems, drive towards a solution, and communicate it with the team and other stakeholders (both verbally and written). You're comfortable navigating complex domains and building intuitive software for them. You thrive in an environment that prefers prototypes over proposals. Proactive in sharing ideas and informed opinions with others Attention to detail and quality to anticipate edge cases ahead of time A growth mindset and willingness to learn new things You have 5+ years of experience working as a backend engineer. Willingness to take part in an on-call rotation. Ability to mentor and be mentored by others. Experience with observability tooling Why you'll love working at FOSSA: Amazing team culture and environment Named by Forbes as Best Start-up to work for 2022 and Best Remote Start-up 2024 by BuiltIn Competitive salary and equity packages Unlimited PTO Comprehensive benefits: Medical, dental, and vision insurance are covered for employees; flexible spending account; 401k Paid maternity and paternity leave FOSSA is an equal-opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status. The annual cash compensation range for this position is $160,000 - $200,000 USD. Final compensation will be determined based on experience and skills and may vary from the range listed.",
        "url": "https://www.linkedin.com/jobs/view/3961898987",
        "summary": "FOSSA is hiring a Senior Software Engineer to work on their open-source software supply chain management platform. You will be responsible for building and scaling the core product and technical infrastructure, working with NodeJS, Typescript, Haskell, Rust, and Golang. You will have significant ownership in the technical architecture and product roadmap. The ideal candidate has experience building and supporting scalable SaaS products and features, experience as a full-stack software engineer, experience with relational databases and writing performant SQL queries, and 5+ years of experience working as a backend engineer.",
        "industries": [
            "Software",
            "Technology",
            "Open-Source"
        ],
        "soft_skills": [
            "Communication",
            "Problem Solving",
            "Collaboration",
            "Teamwork",
            "Proactive",
            "Growth Mindset",
            "Attention to detail",
            "Mentoring",
            "Ownership"
        ],
        "hard_skills": [
            "NodeJS",
            "Typescript",
            "Haskell",
            "Rust",
            "Golang",
            "SQL",
            "SaaS",
            "Full-stack Development",
            "Relational Databases",
            "Observability Tooling"
        ],
        "tech_stack": [
            "NodeJS",
            "Typescript",
            "Haskell",
            "Rust",
            "Golang",
            "SQL",
            "SaaS"
        ],
        "programming_languages": [
            "NodeJS",
            "Typescript",
            "Haskell",
            "Rust",
            "Golang",
            "SQL"
        ],
        "experience": 5,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 200000,
            "min": 160000
        },
        "benefits": [
            "Competitive salary and equity packages",
            "Unlimited PTO",
            "Medical, dental, and vision insurance",
            "Flexible spending account",
            "401k",
            "Paid maternity and paternity leave"
        ]
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Chicago, IL",
        "job_id": 3963489830,
        "company": "Baxter Planning",
        "title": "Java - Software Engineer ( India )",
        "created_on": 1720635146.8631742,
        "description": "Responsibilities Design, develop, test, and document GraphQL APIs serving a new frontend application but making use of and transforming data in an existing database schema. Participate in peer code reviews. Qualifications 8 + years of experience with Java/J2EE, Spring, Spring Boot, Hibernate. Experience with GraphQL API design and backend development . Experience developing automated test units using Junit and testing technologies (e.g. DbUnit, Mockito). Strong exposure & experience in Product development practices such as Agile methodologies, tools, and standards. Strong communication skills Fluency in English Attributes of an Ideal Candidate. Experience with supply chain management. Experience developing with and deploying to AWS. Experience building customer ready, enterprise, SaaS applications. Exposure to building for modern web frameworks such as Angular or React. Client-side performance and optimization experience. Highly curious and always seek to understand new parts of a complex domain. Pragmatic and business-oriented approach to problem solving. #J-18808-Ljbffr",
        "url": "https://www.linkedin.com/jobs/view/3963489830",
        "summary": "This role involves designing, developing, testing, and documenting GraphQL APIs that interface with an existing database schema for a new frontend application. The ideal candidate will have 8+ years of experience with Java, Spring, Spring Boot, Hibernate, GraphQL, automated testing (Junit, DbUnit, Mockito), Agile methodologies, and AWS deployment. Experience with supply chain management, building SaaS applications, and client-side performance optimization is also desired. ",
        "industries": [
            "Software Development",
            "Technology",
            "SaaS",
            "Supply Chain Management"
        ],
        "soft_skills": [
            "Communication",
            "Problem Solving",
            "Agile",
            "Curious",
            "Pragmatic",
            "Business-Oriented"
        ],
        "hard_skills": [
            "Java",
            "J2EE",
            "Spring",
            "Spring Boot",
            "Hibernate",
            "GraphQL",
            "API Design",
            "Backend Development",
            "Junit",
            "DbUnit",
            "Mockito",
            "Automated Testing",
            "Agile Methodologies",
            "AWS"
        ],
        "tech_stack": [
            "Java",
            "Spring",
            "Spring Boot",
            "Hibernate",
            "GraphQL",
            "Junit",
            "DbUnit",
            "Mockito",
            "AWS"
        ],
        "programming_languages": [
            "Java"
        ],
        "experience": 8,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Mossville, IL",
        "job_id": 3941227722,
        "company": "PSRTEK",
        "title": "Embedded Software Engineer",
        "created_on": 1720635148.5064776,
        "description": "Job Responsibilities Concept, design, test, document, and promote new software features and processes. Perform requirements analysis and decomposition. Evaluate new languages, development tools, or processes. Perform product maintenance. Manage the software lifecycle through version control and configuration management. Keep up to date on new technologies or develop new technologies. Communicate technical information to customers, team members, suppliers, and other units. Support and advise other engineers, managers, marketing personnel, customers, and suppliers. Candidate Requirements: Education & Experience Required Associate's Degree acceptable with a minimum of 8 years' relevant work experience. Minimum BS In Computer Engineering / Electrical Engineering Required. 5-7 yrs industry experience. Will accept Master's or higher relevant internship/project experience toward the minimum. Technical Skills Required Real-world real-time embedded device driver experience is a key requirement. Proficiency In Git Version Mgmt Required. (Clearcase Experience Preferred.) Significant experience with Jenkins build platforms required. Experience troubleshooting/debugging using tools such as oscilloscopes. Experience designing embedded electronics controls, display, or telematics software, including development of device drivers, SPI/I2C peripheral interfacing, hardware diagnostics, operating system configurations, non-volatile memory interfacing, board initialization, and other chip-level interfacing. Experience with RTOS, control software design patterns and anti-patterns, ANSI C, Python, and other programming languages. Experience with software development processes such as Agile. Technical Skills Desired [Add any additional technical skills desired based on the input.] Soft Skills Verbal and written communication skills. Problem-solving skills, customer service, and interpersonal skills. Knowledge of reading schematics and data sheets for components. Ability to troubleshoot issues and make system changes as needed to resolve issues.",
        "url": "https://www.linkedin.com/jobs/view/3941227722",
        "summary": "This position requires a software engineer with extensive experience in embedded device driver development and proficiency in tools like Git, Jenkins, and oscilloscopes. They will be responsible for designing, testing, and documenting new software features, managing the software lifecycle, and staying current with new technologies.  Experience with RTOS, ANSI C, Python, and Agile methodologies are also desired.",
        "industries": [
            "Software Development",
            "Engineering",
            "Technology"
        ],
        "soft_skills": [
            "Communication",
            "Problem-solving",
            "Customer Service",
            "Interpersonal Skills",
            "Troubleshooting"
        ],
        "hard_skills": [
            "Embedded Device Drivers",
            "Git",
            "Jenkins",
            "Oscilloscope",
            "RTOS",
            "ANSI C",
            "Python",
            "Agile",
            "Schematics",
            "Data Sheets"
        ],
        "tech_stack": [
            "Git",
            "Jenkins",
            "RTOS",
            "ANSI C",
            "Python",
            "Clearcase"
        ],
        "programming_languages": [
            "ANSI C",
            "Python"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Engineering",
                "Electrical Engineering"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Northbrook, IL",
        "job_id": 3628461068,
        "company": "Mondo",
        "title": "Senior Software Engineer",
        "created_on": 1720635150.0847378,
        "description": "Apply now: Sr. Software Engineer - this is a hybrid position local to Northbrook, IL. The start date is ASAP for this long-term contract position. Job Title: Sr Software Engineer Location: Hybrid local to Northbrook, IL Duration: 6-12+ months Start Date: ASAP Rate: $55-$70 Per hour W2 As a member of the IT team, the Software Developer will be instrumental in building and supporting key products in our product portfolio. Additionally, this position may support development or integration work on non-product software. We are looking for an experienced full-stack developer who has a substantial amount of UI experience as well. Essential Functions Design and develop new applications (predominantly web-based) while helping to support existing products, offering process improvements, and platform reengineering Work as part of a team to build next-generation products and platforms, using pioneering technologies and processes to enrich the product portfolio Collaborate with UI/UX team in optimizing the user experience Work with the product team, business analysts, infrastructure team, and external vendors as appropriate to gather requirements and ensure products are successfully developed and launched Consistently follow best security practices in the development of products to secure client data Stay up to date in current industry trends and technologies Requirements Key Technologies and Competencies Microservices Framework or experience with large-scale web applications Languages: YAML, JSON, Azure Functions in C#, JWT, Bash Script Frontend: Node.js, Angular, RxJS, JavaScript, HTML, CSS, jQuery, SPA frameworks Pipeline/Environment: Azure Cloud, Azure DevOps, Azure Event Hub & Event Grid, Reverse Proxying, VNets, Containerization, OIDC Middle/Backend: Graph QL, Other related REST APIs, Kafka, Equinox, Event-sourced Domain Models, Azure Event Hub Identity Access Management (IAM): Azure AD B2C, SSO, SAML, External Authorization, Claims Based Authorization, OpenID Connect, OAuth or other related IAM tools Experience with Cosmos DB/NoSQL (DocumentDB) or other related database models Security concepts/frameworks implementing OWASP recommendations Experience with responsive and adaptive design Experience with graphic design applications such as Adobe Illustrator Ability to complete all phases of the software development life cycle including analysis, design, functionality, testing, and support The proactive, team-focused, conscientious, and detail-oriented Experience And Education Requirements 5-7 years of application development experience Bachelor's Degree in a related field preferred Professional Scrum Developer (desired, not required) Microsoft Certifications (desired, not required)",
        "url": "https://www.linkedin.com/jobs/view/3628461068",
        "summary": "A Sr. Software Engineer is needed for a 6-12+ month contract position at a company in Northbrook, IL. The position requires 5-7 years of application development experience and a bachelor's degree is preferred. The role will involve developing and supporting web-based applications, working with UI/UX teams, collaborating with other teams, and staying up-to-date on industry trends.  This is a hybrid role that is local to Northbrook, IL, and the start date is ASAP.",
        "industries": [
            "Software Development",
            "Technology",
            "IT",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Teamwork",
            "Communication",
            "Problem Solving",
            "Detail-Oriented",
            "Proactive",
            "Conscientious",
            "Collaboration"
        ],
        "hard_skills": [
            "Microservices",
            "YAML",
            "JSON",
            "C#",
            "JWT",
            "Bash Script",
            "Node.js",
            "Angular",
            "RxJS",
            "JavaScript",
            "HTML",
            "CSS",
            "jQuery",
            "SPA Frameworks",
            "Azure Cloud",
            "Azure DevOps",
            "Azure Event Hub",
            "Event Grid",
            "Reverse Proxying",
            "VNets",
            "Containerization",
            "OIDC",
            "Graph QL",
            "REST APIs",
            "Kafka",
            "Equinox",
            "Event-sourced Domain Models",
            "Azure Event Hub",
            "Azure AD B2C",
            "SSO",
            "SAML",
            "External Authorization",
            "Claims Based Authorization",
            "OpenID Connect",
            "OAuth",
            "Cosmos DB",
            "NoSQL",
            "DocumentDB",
            "OWASP",
            "Responsive Design",
            "Adaptive Design",
            "Adobe Illustrator",
            "Software Development Life Cycle"
        ],
        "tech_stack": [
            "Microservices",
            "YAML",
            "JSON",
            "C#",
            "JWT",
            "Bash Script",
            "Node.js",
            "Angular",
            "RxJS",
            "JavaScript",
            "HTML",
            "CSS",
            "jQuery",
            "SPA Frameworks",
            "Azure Cloud",
            "Azure DevOps",
            "Azure Event Hub",
            "Event Grid",
            "Reverse Proxying",
            "VNets",
            "Containerization",
            "OIDC",
            "Graph QL",
            "REST APIs",
            "Kafka",
            "Equinox",
            "Event-sourced Domain Models",
            "Azure Event Hub",
            "Azure AD B2C",
            "SSO",
            "SAML",
            "External Authorization",
            "Claims Based Authorization",
            "OpenID Connect",
            "OAuth",
            "Cosmos DB",
            "NoSQL",
            "DocumentDB",
            "Adobe Illustrator"
        ],
        "programming_languages": [
            "YAML",
            "JSON",
            "C#",
            "JavaScript",
            "Node.js",
            "Angular",
            "RxJS",
            "HTML",
            "CSS",
            "jQuery",
            "Bash Script"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Information Technology",
                "Software Engineering",
                "Related Fields"
            ]
        },
        "salary": {
            "max": 70,
            "min": 55
        },
        "benefits": []
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Chicago, IL",
        "job_id": 3948959531,
        "company": "Motorola Solutions",
        "title": "Senior Software Engineer",
        "created_on": 1720635152.0592332,
        "description": "Company Overview At Motorola Solutions, we're guided by a shared purpose - helping people be their best in the moments that matter - and we live up to our purpose every day by solving for safer. Because people can only be their best when they not only feel safe, but are safe. We're solving for safer by building the best possible technologies across every part of our safety and security ecosystem. That's mission-critical communications devices and networks, AI-powered video security & access control and the ability to unite voice, video and data in a single command center view. We're solving for safer by connecting public safety agencies and enterprises, enabling the collaboration that's critical to connect those in need with those who can help. The work we do here matters. Department Overview Motorola Solutions, Inc. Job Description Plan, develop and coordinate activities focused on design, development, coding, testing, research, programming and documentation for software systems, applications and/or operating systems in conjunction with equipment designers and/or hardware developers. Perform modeling, designing, and coding activities, employing structured methods. Review design documentation for all levels of the software development process. Create and execute unit, integration, system, regression, performance, load and acceptance test plans and scripts. Software system testing procedures, and the documentation of results. Analysis of software requirements to determine feasibility of design within quality assurance, time and cost constraints. Participate in formal design and implementation activities. Develop requirements, code and create unit and other automated testing solutions. Provide engineering management with accurate effort estimates for projected work. $144,560 - 166,200 per year. Telecommuting permitted anywhere within the U.S. Basic Requirements Bachelor's degree + 5 years experience or Masters degree + 2 years experience. Travel Requirements None Relocation Provided None Position Type Experienced Referral Payment Plan No Our U.S. Benefits include: Incentive Bonus Plans Medical, Dental, Vision benefits 401K with Company Match 9 Paid Holidays Generous Paid Time Off Packages Employee Stock Purchase Plan Paid Parental & Family Leave and more! EEO Statement Motorola Solutions is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion or belief, sex, sexual orientation, gender identity, national origin, disability, veteran status or any other legally-protected characteristic. We are proud of our people-first and community-focused culture, empowering every Motorolan to be their most authentic self and to do their best work to deliver on the promise of a safer world. If you’d like to join our team but feel that you don’t quite meet all of the preferred skills, we’d still love to hear why you think you’d be a great addition to our team. We’re committed to providing an inclusive and accessible recruiting experience for candidates with disabilities, or other physical or mental health conditions. To request an accommodation, please email ohr@motorolasolutions.com.",
        "url": "https://www.linkedin.com/jobs/view/3948959531",
        "summary": "This role focuses on software development and testing for safety and security systems. Responsibilities include design, coding, testing, documentation, and collaboration with hardware developers.  The position offers remote work opportunities within the U.S.",
        "industries": [
            "Technology",
            "Security",
            "Public Safety",
            "Telecommunications"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Analytical Skills",
            "Time Management"
        ],
        "hard_skills": [
            "Software Development",
            "Coding",
            "Testing",
            "Documentation",
            "Design",
            "Research",
            "Modeling",
            "Structured Methods",
            "Unit Testing",
            "Integration Testing",
            "System Testing",
            "Regression Testing",
            "Performance Testing",
            "Load Testing",
            "Acceptance Testing",
            "Requirements Analysis",
            "Automated Testing",
            "Effort Estimation"
        ],
        "tech_stack": [
            "AI-powered video security & access control",
            "Command center view"
        ],
        "programming_languages": [],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Computer Science",
                "Software Engineering",
                "Related Fields"
            ]
        },
        "salary": {
            "max": 166200,
            "min": 144560
        },
        "benefits": [
            "Incentive Bonus Plans",
            "Medical",
            "Dental",
            "Vision benefits",
            "401K with Company Match",
            "Paid Holidays",
            "Paid Time Off",
            "Employee Stock Purchase Plan",
            "Paid Parental & Family Leave"
        ]
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Chicago, IL",
        "job_id": 3950394012,
        "company": "Wolters Kluwer",
        "title": "Senior Software Engineer - Frontend Developer",
        "created_on": 1720635153.775539,
        "description": "This is an exciting opportunity to work on a greenfield software product that will be used directly by customers and generate new revenue opportunities for our organization. Wolters Kluwer Health’s Clinical Effectiveness (CE) organization is a fast-growing and innovation-driven healthcare information technology (HIT) provider working on the front lines of clinical care. Our talented team of physician editors, technologists, and product visionaries collaborate to provide advanced clinical decision support solutions that measurably improve clinical effectiveness by helping healthcare professionals provide optimal care for their patients. Its flagship product, UpToDate, is trusted by over 1 million clinicians in over 170 countries to make the best decisions at the point of patient care. Net Promoter Scores consistently fall in the high-70s, largely due to the unrivaled breadth and depth of its evidence-based content. More than 60 research studies confirm UpToDate’s widespread usage and association with improved patient care and hospital performance, including reduced lengths of stay, adverse complications, and mortality. As a mission-based organization committed to getting things right, we’re investing in cutting edge healthcare technology solutions to meet the strategic needs of our customers. We are seeking a talented, collaborative, high-energy individual who has deep and diverse technology expertise along with the ability to drive this transformation at multiple levels within the organization. The ideal candidate has significant experience building large web frontend products that are high-quality, reliable, accessible, and delightful to use. Responsibilities: Build, maintain, enhance, and extend Wolters Kluwer’s customer facing web applications built using MS Power Apps Collaborate across teams within Clinical Effectiveness to deliver a consistent user experience across our suite of products. Work with the product owners and user experience team to understand, design, and size new product features and enhancements. Use Responsive Design techniques to create an optimal user experience from a single code base for all devices from smartphones to tablets to laptops. Use Vue and/or other modern tools to implement highly functional and fast user interfaces. Collaborate with software test engineers to develop deep and thorough test coverage. Collaborate with user experience team to determine and implement the best solutions for the customer. Plan and execute tasks within an Agile/Scrum environment. Follow existing (and develop new) best practices and standards for coding and frontend development. Collaborate with and mentor other engineers to ensure the team fosters a culture of engineering excellence. Remain current with evolving industry trends and technologies related to frontend development and user experience. Advocate for adopting those of which would provide lasting benefit to the product by effectively communicating the costs and benefits to other engineers and product managers. Experience : Bachelors degree in Information technology or equivalent work experience 5+ years of frontend development experience. 3+ years working with frontend build tools such as Vite or Webpack. 3+ years writing JavaScript unit tests. Expert knowledge in HTML, CSS, and JavaScript. Expert knowledge working with at least one modern JavaScript framework such as VueJS, React, or Angular. Knowledge of user experience design philosophies, best practices, and methods. Knowledge web accessibility best-practices or standards – practical experience a plus. Other Knowledge, Skills, Abilities or Certifications: Experience using MS Power Apps is a plus. Software engineering experience in the healthcare industry a plus. Experience with Microsoft Power Pages (Power Apps Portals) a plus Experience with cloud platforms such as Amazon AWS or Microsoft Azure a plus. Familiarity with authentication technologies such as SAML or OAuth/OpenID Connect a plus. Benefits: A comprehensive benefits package that begins your first day of employment. Additional Information: Wolters Kluwer offers great benefits and programs to help meet your needs and balance your work and personal life, including Medical, Dental, & Vision Plans, 401(k), FSA/HSA, Commuter Benefits, Tuition Assistance Plan, Vacation and Sick Time, and Paid Parental Leave . Full details of our benefits are available - https://www.mywolterskluwerbenefits.com/index.html Diversity Matters Wolters Kluwer strives for an inclusive company culture in which we attract, develop, and retain diverse talent to achieve our strategy. As a global company, having a diverse workforce is of the utmost importance. We've been recognized by employees as a European Diversity Leader in the Financial Times, as one of Forbes America’s Best Employers for Diversity in 2022, 2021 and 2020 and as one of Forbes America’s Best Employers for Women in 2021, 2020, 2019 and 2018. In 2020, we placed third in the Female Board Index, and were recognized by the European Women on Boards Gender Diversity Index. Wolters Kluwer and all of our subsidiaries, divisions and customer/departments is an Equal Opportunity / Affirmative Action employer. Remote",
        "url": "https://www.linkedin.com/jobs/view/3950394012",
        "summary": "Wolters Kluwer Health is seeking a talented frontend developer with extensive experience in building large web applications. The ideal candidate will have a strong understanding of modern JavaScript frameworks (VueJS, React, Angular), responsive design, and user experience best practices. The role involves developing, maintaining, and enhancing customer-facing web applications built using MS Power Apps, collaborating with product owners and user experience teams, and staying up-to-date with industry trends.",
        "industries": [
            "Healthcare",
            "Information Technology",
            "Software Development"
        ],
        "soft_skills": [
            "Collaborative",
            "High-Energy",
            "Problem-Solving",
            "Communication",
            "Teamwork",
            "Mentoring",
            "Leadership",
            "Analytical",
            "Detail-Oriented"
        ],
        "hard_skills": [
            "HTML",
            "CSS",
            "JavaScript",
            "VueJS",
            "React",
            "Angular",
            "Vite",
            "Webpack",
            "JavaScript Unit Testing",
            "Responsive Design",
            "User Experience Design",
            "Web Accessibility",
            "MS Power Apps",
            "Microsoft Power Pages",
            "Amazon AWS",
            "Microsoft Azure",
            "SAML",
            "OAuth",
            "OpenID Connect"
        ],
        "tech_stack": [
            "MS Power Apps",
            "VueJS",
            "React",
            "Angular",
            "Vite",
            "Webpack",
            "Amazon AWS",
            "Microsoft Azure",
            "SAML",
            "OAuth",
            "OpenID Connect"
        ],
        "programming_languages": [
            "JavaScript"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Information Technology"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Medical",
            "Dental",
            "Vision",
            "401(k)",
            "FSA/HSA",
            "Commuter Benefits",
            "Tuition Assistance",
            "Vacation",
            "Sick Time",
            "Paid Parental Leave"
        ]
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Chicago, IL",
        "job_id": 3966743087,
        "company": "New Relic",
        "title": "Software Engineer (Multi-Threaded Java) - Remote",
        "created_on": 1720635155.578538,
        "description": "Your opportunity The Data, Identity, & API Platform group at New Relic builds the foundation for all of our products: data ingest, storage, and query. As an engineer working on NRDB, you’ll be contributing directly to the proprietary telemetry database technology at the core of our business. We own our software from top to bottom and are directly responsible for its quality and reliability. Each member of the team shares our pager rotation and will occasionally be on-call to respond to system failures; so we prioritize work that keeps the lights on and the pager quiet, in addition to the work that powers all of our new products and streams of data. If the idea of working on systems that process millions of messages per second and handle petabytes of data excites you, then you may be an excellent fit! What you'll do Working in an agile environment with a DevOps approach — the team builds and maintains their own software, including taking turns with being on-call. Investigate and resolve critical performance, availability, and resiliency issues and risks in a multi-region, multi-cloud, multi-tenant distributed database. Build and maintain scalable distributed Java services and Petabyte-scale Infrastructure. Proactively participate in cross-functional committees to move the query language forward, ranging from collaborations with AI, Visualizations, and Data Processing teams. Own the New Relic query language stack including grammar parsing, compilation, and processing. This role requires Experience working in Java, or equivalent experience in an alternate OOP language (C++, C#, etc). Ability to go deep on the command-line and fix things when they’re broken. Self-starter with a strong sense of ownership over your code and commitments. Firm grasp of Computer Science fundamentals, including data structures, algorithms, and modern software design and development methodologies. 3+ years of experience in collaborative software development. Bonus points if you have Familiarity with networks and load-balancing and how to use them well in low-latency, high-throughput distributed systems. Familiarity with modern observability and alerting patterns. Ability to work well asynchronously and effectively communicate your thoughts in writing. Fostering a diverse, welcoming and inclusive environment is important to us. We work hard to make everyone feel comfortable bringing their best, most authentic selves to work every day. We celebrate our talented Relics’ different backgrounds and abilities, and recognize the different paths they took to reach us – including nontraditional ones. Their experiences and perspectives inspire us to make our products and company the best they can be. We’re looking for people who feel connected to our mission and values, not just candidates who check off all the boxes. If you require a reasonable accommodation to complete any part of the application or recruiting process, please visit https://newrelic.avature.net/accommodations to submit your request. We believe in empowering all Relics to achieve professional and business success through a flexible workforce model. This model allows us to work in a variety of workplaces that best support our success, including fully office-based, fully remote, or hybrid. Our hiring process Please note that visa sponsorship is not available for this position. In compliance with applicable law, all persons hired will be required to verify identity and eligibility to work and to complete employment eligibility verification. Note: Our stewardship of the data of thousands of customers’ means that a criminal background check is required to join New Relic. We will consider qualified applicants with arrest and conviction records based on individual circumstances and in accordance with applicable law including, but not limited to, the San Francisco Fair Chance Ordinance. Headhunters and recruitment agencies may not submit resumes/CVs through this website or directly to managers. New Relic does not accept unsolicited headhunter and agency resumes, and will not pay fees to any third-party agency or company that does not have a signed agreement with New Relic. New Relic is proud to be an equal opportunity employer. We foster a diverse, equitable, and inclusive environment, free from all types of discrimination, so our Relics can thrive. We hire people with different backgrounds, experiences, abilities and perspectives. Candidates are evaluated based on qualifications, regardless of race, religion, ethnicity, national origin, sex, sexual orientation, gender expression or identity, age, disability, neurodiversity, veteran or marital status, political viewpoint, or other legally protected characteristics. Review our Applicant Privacy Notice at https://newrelic.com/termsandconditions/applicant-privacy-policy. Estimated Base Pay Range: $ 118,000 - $ 147,000 The pay range above represents a reasonable estimate of the salary for the listed position. This role is eligible for a corporate bonus plan. Pay within this range varies by work location and may also depend on job-related factors such as an applicant’s skills, qualifications, and experience. New Relic provides a variety of benefits for this role, including healthcare, dental, vision, parental leave and planning, mental health benefits, a 401(k) plan and match, flex time-off, 11 paid holidays, volunteer time off, and other competitive benefits designed to improve the lives of our employees.",
        "url": "https://www.linkedin.com/jobs/view/3966743087",
        "summary": "New Relic is seeking a Software Engineer to join their Data, Identity, & API Platform group. The role focuses on building and maintaining the NRDB, a proprietary telemetry database technology. The engineer will work on critical performance, availability, and resiliency issues within a distributed database, build scalable Java services, and own the New Relic query language stack.",
        "industries": [
            "Software",
            "Technology",
            "Database",
            "Big Data"
        ],
        "soft_skills": [
            "Self-starter",
            "Strong sense of ownership",
            "Strong communication skills",
            "Ability to work asynchronously",
            "Team player",
            "Problem-solving",
            "Analytical",
            "Critical thinking",
            "Collaborative"
        ],
        "hard_skills": [
            "Java",
            "C++",
            "C#",
            "Command-line",
            "Data structures",
            "Algorithms",
            "Software design",
            "Software development methodologies",
            "Networks",
            "Load-balancing",
            "Observability",
            "Alerting"
        ],
        "tech_stack": [
            "Java",
            "NRDB",
            "Telemetry database",
            "Distributed database",
            "Multi-region",
            "Multi-cloud",
            "Multi-tenant",
            "Scalable services",
            "Petabyte-scale infrastructure",
            "Query language",
            "Grammar parsing",
            "Compilation",
            "Processing",
            "DevOps"
        ],
        "programming_languages": [
            "Java",
            "C++",
            "C#"
        ],
        "experience": 3,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Engineering"
            ]
        },
        "salary": {
            "max": 147000,
            "min": 118000
        },
        "benefits": [
            "Healthcare",
            "Dental",
            "Vision",
            "Parental leave",
            "Mental health benefits",
            "401(k) plan",
            "Flex time-off",
            "Paid holidays",
            "Volunteer time off"
        ]
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Chicago, IL",
        "job_id": 3960249866,
        "company": "Epsilon",
        "title": "Senior Software Engineer",
        "created_on": 1720635157.281661,
        "description": "Job Description As a Full Stack Developer on the MIS team, you will be working in a highly collaborative environment developing web-based tools supporting financial applications that serve the Epsilon Account Management and Finance Teams. Your passion for writing well-structured and scalable code will help drive long-term product vision with key technology and architecture decisions. You will be surrounded by passionate, high-performing individuals collaborating to solve complex problems and making a meaningful impact on our business. Duties And Responsibilities Main responsibility will be the development and maintenance of software components of varying complexity for use in finance and billing applications. Collaborate with developers, product managers, business analysts and business users in conceptualizing, estimating, and developing new software applications and enhancements. Assist in the development, and documentation of software’s objectives, deliverables, and specifications in collaboration with internal users and departments. Collaborate with QA to define test cases, metrics, and resolve questions about test results. Advise, and mentor team members in specialized technical areas, decision making and methodologies. Assisting in the definition, development, and documentation of software’s objectives, business requirements, deliverables, and specifications in collaboration with internal users and departments. Minimum Qualifications Bachelor’s degree in computer science (or similar field), or equivalent experience, working experience in FinTech environments is a plus. 3+ years of experience in relevant technologies. Strong knowledge of JavaScript packages, package managers, bundlers, and their use in building complex interactive experiences. Proficiency in building APIs. Experience with relational databases (e.g., PostgreSQL, MySQL) with demonstrated proficiency in querying and tuning for performance. Strong understanding of Python development and scripting - Flask, FastAPI a plus. Understanding of CI/CD processes/tools and experience with Docker/Kubernetes is a plus. Experience with Source Control Systems preferably GIT. Experience with application performance monitoring and logging. Experience with writing detailed technical specifications for developers; design, validate and execute software test plans. Familiarity with Software Development Life Cycle, AGILE development methodologies, and able to independently participate in each phase. Ability to define and implement migration strategies from legacy systems to new architecture and technologies. Experience in performance tuning, application monitoring and support of production applications with distributed teams. Ability to work well within a team environment that includes geographically distributed team members. Strong communication skills (verbal and written) and an ability to communicate with internal and external customers and all levels of management, including communicating technical information to nontechnical audiences. Additional Information About Epsilon Epsilon is a global advertising and marketing technology company positioned at the center of Publicis Groupe. Epsilon accelerates clients’ ability to harness the power of their first-party data to activate campaigns across channels and devices, with an unparalleled ability to prove outcomes. The company’s industry-leading technology connects advertisers with consumers to drive performance while respecting and protecting consumer privacy. Epsilon’s people-based identity graph allows brands, agencies and publishers to reach real people, not cookies or devices, across the open web. For more information, visit epsilon.com. When you’re one of us, you get to run with the best. For decades, we’ve been helping marketers from the world’s top brands personalize experiences for millions of people with our cutting-edge technology, solutions and services. Epsilon’s best-in-class identity gives brands a clear, privacy-safe view of their customers, which they can use across our suite of digital media, messaging and loyalty solutions. We process 400+ billion consumer actions each day and hold many patents of proprietary technology, including real-time modeling languages and consumer privacy advancements. Thanks to the work of every employee, Epsilon has been consistently recognized as industry-leading by Forrester, Adweek and the MRC. Positioned at the core of Publicis Groupe, Epsilon is a global company with more than 8,000 employees around the world. Check out a few of these resources to learn more about what makes Epsilon so EPIC Our Culture https //www.epsilon.com/us/about-us/our-culture-epsilon Life at Epsilon https //www.epsilon.com/us/about-us/epic-blog DE&I https //www.epsilon.com/us/about-us/diversity-equity-inclusion CSR https //www.epsilon.com/us/about-us/corporate-social-responsibility Great People Deserve Great Benefits We know that we have some of the brightest and most talented associates in the world, and we believe in rewarding them accordingly. If you work here, expect competitive pay, comprehensive health coverage, and endless opportunities to advance your career. Epsilon is an Equal Opportunity Employer. Epsilon’s policy is not to discriminate against any applicant or employee based on actual or perceived race, age, sex or gender (including pregnancy), marital status, national origin, ancestry, citizenship status, mental or physical disability, religion, creed, color, sexual orientation, gender identity or expression (including transgender status), veteran status, genetic information, or any other characteristic protected by applicable federal, state or local law. Epsilon also prohibits harassment of applicants and employees based on any of these protected categories. Epsilon will provide accommodations to applicants needing accommodations to complete the application process. REF238475F",
        "url": "https://www.linkedin.com/jobs/view/3960249866",
        "summary": "Full Stack Developer role for the MIS team developing web-based tools supporting financial applications. Responsibilities include developing and maintaining software components, collaborating with teams, defining test cases, mentoring team members, and defining migration strategies. Requires 3+ years of relevant experience with strong knowledge of JavaScript, APIs, relational databases, Python development, CI/CD processes, Docker/Kubernetes, Git, application performance monitoring, and agile methodologies.",
        "industries": [
            "FinTech",
            "Finance",
            "Technology",
            "Advertising",
            "Marketing"
        ],
        "soft_skills": [
            "Collaboration",
            "Communication",
            "Problem Solving",
            "Decision Making",
            "Mentorship",
            "Teamwork",
            "Communication",
            "Technical Writing"
        ],
        "hard_skills": [
            "JavaScript",
            "API Development",
            "PostgreSQL",
            "MySQL",
            "Python",
            "Flask",
            "FastAPI",
            "CI/CD",
            "Docker",
            "Kubernetes",
            "Git",
            "Application Performance Monitoring",
            "Agile Methodologies"
        ],
        "tech_stack": [
            "JavaScript",
            "APIs",
            "PostgreSQL",
            "MySQL",
            "Python",
            "Flask",
            "FastAPI",
            "CI/CD",
            "Docker",
            "Kubernetes",
            "Git",
            "Application Performance Monitoring",
            "Agile Methodologies"
        ],
        "programming_languages": [
            "JavaScript",
            "Python"
        ],
        "experience": 3,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Similar Fields"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Competitive Pay",
            "Comprehensive Health Coverage",
            "Career Advancement Opportunities"
        ]
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Mossville, IL",
        "job_id": 3919107599,
        "company": "GCTECHINFO",
        "title": "Embedded Software Engineer",
        "created_on": 1720635158.8926477,
        "description": "Job Title: Embedded Software Engineer Duration: 12 Months Location: Mossville, IL, USA 61552 Industry: Manufacturing - Automotive Job Description: This is a position for a software application engineer to develop Onboard Core Information software specifications and verification documentation for Displays and Telematics used on machine & engine applications. Development of software specifications, development of verification documentation, and testing of wireless software and applications. This position includes working in close collaboration with cross-functional teams consisting of hardware engineers, internal and external software vendors, and systems engineers to architect and integrate the products. Typical Task Breakdown: Software release cadence every two weeks. Provides requirements and acceptance criteria Education & Experience Required: BSEE, BSCE, or BSCS. College graduates with applicable 2-4 years'' experience in projects/internships will be considered but not at max bill rate. (Required) Technical Skills BSEE, BSCE, or BSCS 2 years of experience with developing requirements and/or specifications for software features. 1 year of experience in wireless communications such as BLE, Cellular, and NB IoT in machine or industrial applications. 1 year of experience developing scripts using scripting languages such as Python & Linux Bash. 1 year of experience with Ethernet, TCP/IP, Wi-Fi, and analysis tools such as Wireshark. 1 year of experience with CAN, J1939 and other data link protocols. (Desired) Desire to work in a fast-paced Agile team environment Experience using a wide variety of testing tools Experience using CAN based tools like CANoe and CANalyzer Experience in Embedded Cyber Security Ability to read electrical schematics Experience with GIT configuration management tool (Required) Soft Skills Team Player, Good verbal and written communication skills, including the use of Microsoft Office when producing requirements, specifications, reports, and test plans. JOB CODE: 1000046",
        "url": "https://www.linkedin.com/jobs/view/3919107599",
        "summary": "This role involves developing software specifications and verification documentation for Displays and Telematics used in machine & engine applications for the automotive industry. The engineer will work in close collaboration with cross-functional teams to architect and integrate the products. This is a fast-paced Agile environment with a two-week software release cadence.",
        "industries": [
            "Manufacturing",
            "Automotive",
            "Telematics"
        ],
        "soft_skills": [
            "Team Player",
            "Communication",
            "Collaboration"
        ],
        "hard_skills": [
            "Software Development",
            "Requirements Engineering",
            "Specifications",
            "Verification Documentation",
            "Testing",
            "Wireless Communications",
            "BLE",
            "Cellular",
            "NB-IoT",
            "Python",
            "Linux Bash",
            "Scripting",
            "Ethernet",
            "TCP/IP",
            "Wi-Fi",
            "Wireshark",
            "CAN",
            "J1939",
            "Data Link Protocols",
            "Agile",
            "Testing Tools",
            "CANoe",
            "CANalyzer",
            "Embedded Cyber Security",
            "Electrical Schematics",
            "GIT"
        ],
        "tech_stack": [
            "BLE",
            "Cellular",
            "NB-IoT",
            "Python",
            "Linux Bash",
            "Ethernet",
            "TCP/IP",
            "Wi-Fi",
            "Wireshark",
            "CAN",
            "J1939",
            "CANoe",
            "CANalyzer",
            "GIT"
        ],
        "programming_languages": [
            "Python",
            "Bash"
        ],
        "experience": 2,
        "education": {
            "min_degree": "BSEE",
            "fields": [
                "Computer Science",
                "Electrical Engineering",
                "Computer Engineering"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Chicago, IL",
        "job_id": 3953281991,
        "company": "DV Trading LLC",
        "title": "Senior C++ Software Engineer",
        "created_on": 1720635162.401454,
        "description": "About Us: Founded more than 15 years ago and headquartered in Chicago, the DV Group of financial services firms has grown to more than 350 people operating throughout North America and in Europe. Since spinning out of a large brokerage firm in 2016, DV Trading has rapidly scaled as an independent proprietary trading firm utilizing its own capital, trading strategies, and risk management methodologies to provide liquidity to worldwide financial markets and hedging opportunities to commodity producers and users. Now, DV group affiliates include two broker dealers, a cryptocurrency market making firm, and a bourgeoning investment adviser. Job Responsibilities: Lead in the design, implementation, and deployment of DV’s core trading platform C++ 17 Bring deep technical knowledge such as parallel programming, trading systems, networking, or performance analysis Work on cross-functional teams across trading, quant, and development to troubleshoot and solve complex problems Work directly with trading desks on new feature requests Requirements: 2+ years of experience building performant, scalable applications in C++ Experience working on Linux Solid understanding of multi-threaded/multi-core programming paradigms Object-oriented design and programming experience Highly collaborative in nature, with excellent written and verbal communication skills Familiarity with options trading and trading systems is a plus. Bachelor’s degree in Computer Science, Engineering or a related field from an accredited institution DV is not accepting unsolicited resumes from search firms. Only search firms with valid, written agreements with DV should submit resumes in response to DV’s posted positions. All resumes submitted by search firms to DV via e-mail, the Internet, personal delivery, facsimile, or any other method without a valid written agreement shall be deemed the sole property of DV, and no fee will be paid in the event the candidate is hired by DV. DV is proud to be an equal opportunity employer and committed to creating an inclusive environment for all employees.",
        "url": "https://www.linkedin.com/jobs/view/3953281991",
        "summary": "DV Trading, a proprietary trading firm, is seeking a C++ developer to lead the design, implementation, and deployment of their core trading platform. The ideal candidate will have 2+ years of experience building performant, scalable applications in C++ and experience working on Linux. They should also have a strong understanding of multi-threaded/multi-core programming paradigms, object-oriented design and programming, and excellent communication skills. Familiarity with options trading and trading systems is a plus.",
        "industries": [
            "Financial Services",
            "Trading",
            "Investment Banking"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration"
        ],
        "hard_skills": [
            "C++",
            "Parallel Programming",
            "Trading Systems",
            "Networking",
            "Performance Analysis",
            "Linux",
            "Multi-threading",
            "Multi-core Programming",
            "Object-Oriented Design",
            "Options Trading"
        ],
        "tech_stack": [
            "C++",
            "Linux"
        ],
        "programming_languages": [
            "C++"
        ],
        "experience": 2,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Engineering"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Chicago, IL",
        "job_id": 3896795332,
        "company": "Cube Hub Inc.",
        "title": "Software Engineer II - Software Engineering",
        "created_on": 1720635164.0370994,
        "description": "Description Position Title Software Engineer Shift 1 day shift (approx. 9-5) Fully onsite at Chicago location. Preferred Local Candidates Must have experience: 4+ years Next.js/React.js (must be fully proficient) 2+ years Node.js 2+ years AWS Would Be Nice To Have Java",
        "url": "https://www.linkedin.com/jobs/view/3896795332",
        "summary": "Software Engineer position at a Chicago-based company. Must have 4+ years of experience with Next.js/React.js, 2+ years with Node.js, and 2+ years with AWS. Java experience is a plus.",
        "industries": [
            "Software Development",
            "Technology"
        ],
        "soft_skills": [],
        "hard_skills": [
            "Next.js",
            "React.js",
            "Node.js",
            "AWS",
            "Java"
        ],
        "tech_stack": [
            "Next.js",
            "React.js",
            "Node.js",
            "AWS"
        ],
        "programming_languages": [
            "JavaScript",
            "Java"
        ],
        "experience": 4,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Chicago, IL",
        "job_id": 3946214187,
        "company": "CyberCoders",
        "title": "REMOTE Senior Software Engineer - Java",
        "created_on": 1720635165.8631759,
        "description": "Job Title: Senior Software Engineer (Backend) Location: 100% REMOTE Salary: $150,000 - $185,000/year plus EQUITY + GREAT BENEFITS Requirements: 5+ years Java Backed by top-tier venture capital firms, we're a hyper-growth stage start-up company on a mission to make the internet more reliable and prevent costly outages for enterprise organizations. Our SaaS applications are built by developers for developers - allowing our customers to produce stronger, more resilient systems. As the first ones to do what we're doing, we set the standard for this type of engineering... Which is why we're trusted by 100+ Fortune 2000 companies and the largest financial institutions in the U.S. to bring value to their organizations. We're looking for a Senior-level Software Engineer with at least 5 years of Java experience to join us in moving the needle. The ideal candidate thrives in a fast-paced environment and values speed over perfection. We want your voice and ideas represented in our business. After all, our goal is for us all to flourish and build a more reliable internet. If you want to work alongside industry veterans and people who are eager to learn from one another, apply now. What You Will Be Doing Work closely with engineers, product managers, and other stakeholders to design and build the latest chaos engineering tooling Leverage strong collaboration and communication skills to deliver new features Partner with product and other business units to understand business problems and present technical solutions and tradeoffs Actively mentor and grow your teammates Care deeply about the customer experience Must Have Skills 7+ years of professional Software Engineering experience 5+ years of Backend experience with Java Experience in cloud technologies (AWS, Lambda, Serverless, GCP, Oracle, etc) Kubernetes & Docker Nice To Have Skills Experience in a fast-paced enterprise environment (startup experience is a huge plus) React or Typescript DynamoDB, NoSQL, etc. Infrastructure & systems level technologies like Linux, OpenShift, etc. Architecting complex distributed systems and integrating with external systems Mentorship experience Benefits Competitive base salary ($150-185k/year) PLUS GENEROUS EQUITY! 100% employer paid Medical, Dental, & Vision insurance Flexible PTO 401k Match Budget for Home Office AND MORE! Email Your Resume In Word To Looking forward to receiving your resume through our website and going over the position with you. Clicking apply is the best way to apply, but you may also: abi.harper@cybercoders.com Please do NOT change the email subject line in any way. You must keep the JobID: linkedin : AH12-1806460L401 -- in the email subject line for your application to be considered.*** Abi Harper - Lead Recruiter Applicants must be authorized to work in the U.S. CyberCoders is proud to be an Equal Opportunity Employer All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, sexual orientation, gender identity or expression, national origin, ancestry, citizenship, genetic information, registered domestic partner status, marital status, status as a crime victim, disability, protected veteran status, or any other characteristic protected by law. CyberCoders will consider qualified applicants with criminal histories in a manner consistent with the requirements of applicable law. CyberCoders is committed to working with and providing reasonable accommodation to individuals with physical and mental disabilities. If you need special assistance or an accommodation while seeking employment, please contact a member of our Human Resources team to make arrangements.",
        "url": "https://www.linkedin.com/jobs/view/3946214187",
        "summary": "We are seeking a Senior Software Engineer (Backend) with 7+ years of experience and 5+ years in Java to join our hyper-growth stage startup. You will design and build chaos engineering tooling, collaborate with engineers and product managers, mentor team members, and contribute to a more reliable internet. We offer a competitive salary ($150k-$185k), equity, and excellent benefits.",
        "industries": [
            "Software",
            "SaaS",
            "Technology",
            "Internet",
            "Cloud Computing",
            "Chaos Engineering",
            "Financial Services"
        ],
        "soft_skills": [
            "Collaboration",
            "Communication",
            "Problem Solving",
            "Mentorship",
            "Customer Focus",
            "Teamwork",
            "Fast-Paced Environment"
        ],
        "hard_skills": [
            "Java",
            "AWS",
            "Lambda",
            "Serverless",
            "GCP",
            "Oracle",
            "Kubernetes",
            "Docker",
            "React",
            "Typescript",
            "DynamoDB",
            "NoSQL",
            "Linux",
            "OpenShift",
            "Distributed Systems"
        ],
        "tech_stack": [
            "Java",
            "AWS",
            "Lambda",
            "Serverless",
            "GCP",
            "Oracle",
            "Kubernetes",
            "Docker",
            "React",
            "Typescript",
            "DynamoDB",
            "NoSQL",
            "Linux",
            "OpenShift"
        ],
        "programming_languages": [
            "Java",
            "React",
            "Typescript"
        ],
        "experience": 7,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 185000,
            "min": 150000
        },
        "benefits": [
            "Competitive Salary",
            "Equity",
            "Medical Insurance",
            "Dental Insurance",
            "Vision Insurance",
            "Flexible PTO",
            "401k Match",
            "Home Office Budget"
        ]
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Arlington Heights, IL",
        "job_id": 3801549916,
        "company": "Endeavor Health",
        "title": "System Engineer II - NCH IT",
        "created_on": 1720635173.3946748,
        "description": "Job Description Position Highlights: Position: Systems Engineer II Location: Arlington Heights, IL Full Time Hours: Monday-Friday, normal business hours Required Travel: N/A Job Summary The Systems Engineer II is responsible for researching, designing, building, configuring, testing, deploying, analyzing, administering, and maintaining/supporting environments and hardware, infrastructure and software platforms, and software technology components to meet current and future business needs. The Systems Engineer II leads efforts to research, design, plan and maintain new or existing hardware, infrastructure and software platforms, and software technology components. The Systems Engineer II utilizes systems, scripting and developer skills to execute on tasks related to analysis, integration, and moderately to highly complex incidents and problems affecting production systems and multiple applications. This role also helps lead the definition and update of items in the configuration management plan. The Systems Engineer II designs information systems that are appropriate for users' needs and consistent with the overall design of the organization's information systems architecture. This includes monitoring and controlling the performance and status of technology components and providing technology component support and problem resolution. The Systems Engineer II leads aspects of the implementation and design of hardware and software platforms and solutions, developing architectures to address business requirements, ensuring system scalability, security, performance and reliability. The Systems Engineer II typically works on more complex, larger and higher importance/impact projects and is expected to design highly optimal systems and processes as a result of broader and deeper experience. The Systems Engineer II possesses the ability to analyze and troubleshoot issues independently as well as coach other team members. A wide degree of creativity and latitude is expected. What You Will Do Leads the completion of assignments related to the on-going maintenance, expansion or upgrade of Endeavor's software, platforms, and devices within NCH legacy environment. Collaborates to build consensus with other Systems Engineers to ensure that the modified hardware and software interacts appropriately, data conversion impacts are considered, environments are developed and maintained, and other areas of impact are addressed and meet business function and performance requirements. Evaluates solutions for long term benefits and impact, not settling for short term resolution only. Leads, delegates and directs large scale projects involving team members and potentially teams of other disciplines. Responsible for activities like analysis and development, implementation, modification and installation of complex systems in different operational environments. Leads the creation of manuals and updates them as well as technical specifications, testing plans, etc. Leads implementation of hardware and software technology components by analyzing the current system environment and infrastructure, using technical tools and utilities, and performing complex product customizations. Has a fundamental understanding of the design/operation of the software, platform or infrastructure, critically evaluating the implications of technological changes. Executes changes to the platform which may impact other organizational functions. Able to quickly and accurately discuss the impact of new technology, applications and design criteria on the platform. Knowledgeable of the major business units which use the platform(s). Able to communicate with users and customers regarding various business functions and how they fit into the platform in an easily understood manner. Responsible for all activities related to system and platform administration. Ensures long term requirements of systems operations and administration are included in the overall information systems planning of the organization. Establishes standards related to implementing hardware and software technology components by analyzing the current system environment and hardware and software technology components using technical tools and utilities and performing high complexity product customizations. Demonstrates a conceptual and practical understanding of the software/platform product and its applicability, as well as related products. Able to quickly and accurately discuss the inter-relational effects between the product or its usage and new technology, platforms and design criteria. Installs new software releases and system upgrades, evaluates and installs patches, and resolves software related problems. Patching is inclusive of assessing security patches based on criticality and enterprise impact. Leads the effort to develop and maintain tools for the use of maintaining, modifying and monitoring hardware and software technology components. Executes assigned tasks concerning production service requests, administration and hardware and software technology components. Leads complex, hardware and software technology component analysis, and evaluation on resource requirements necessary to maintain/expand service levels or tune hardware and software infrastructure components for optimum performance. Plays a key role in the drafting of policies, procedures and associated training plans for system and infrastructure administration, appropriate use, security controls and disaster recovery. Leads efforts to ensure plans integrate effectively with other aspects of the technical hardware and software components. Leads execution of testing, debugging, performance analysis, and documenting hardware and software technology components. Supports the software, platforms, devices and infrastructure within Endeavor by addressing inquiries, reported issues and problems. Resolves incidents and perform root cause analysis, escalating high complexity incidents and problems to provide future application, hardware and/or software resiliency. Performs On-call rotational duties by providing off hour support to supported platforms, software and infrastructure through response to inquiries, reported issues and problems from the Endeavor End User Community as well as items escalated through system monitoring tooling and other HIT Engineering and Application Teams. Decomposes highly complex issues into meaningful and manageable parts. Creates contingency plans and develops alternative solutions. Escalates difficult problems in a timely manner, only after exhausting all possibilities for resolution within scope of responsibilities. Responds to system hardware and application failures, reports and documents any malfunctions and/or observed deviations from standard performance; works with support teams or vendors to restore services or minimize impact to the business. Handles and solves medium to high alerts and tickets in accordance with detailed documentation. Identifies tickets/incidents that need to be escalated onto the next level and passes them on. Provides complex hardware break/fix while working with associated vendor where necessary. Assists with maintaining operational security through reporting and communication of security breaches. Identifies and assist in the resolution of complex incidents and/or problems and work to provide future resiliency of technology and software platform components. Identifies and recommends major and significant changes for purposes of incident avoidance. Address inquiries or reported issues and problems from the Endeavor Health End User community and HIT Engineering and Application teams received via RFS or incident ticket. Resolves moderately to highly complex incidents and perform root cause analysis and escalate more complex incidents and problems to provide future hardware and/or software resiliency. Develops plans to meet required service levels; design data collection and reporting mechanisms for technical component health reporting. Clearly and effectively communicates with users, customers, and management regarding various functional changes and enhancements related to the product. Demonstrates the ability to anticipate, identify, reproduce, eliminate, and resolve problems in area of specialty in a professional and consultative manner Informs clients and users of problem status on a regular basis until a solution is implemented. Resolves difficult problems and implement solutions that may go beyond the scope of initial client or user complaint. Recognizes key issues inherent in problems and addresses them in relation to their severity and importance to the organization. Responds to and resolves reported issues per department standards: understand, assess, identify and resolve. Possesses in-depth knowledge of the new technology and its impact on the systems infrastructure of Endeavor. Provides accurate and timely status updates in accordance with Incident Management policy in response to End User inquiries to outstanding tickets, requests or system-wide issues. Implements Requests for Service (RFS) as well as troubleshoot incidents. Conducts root cause analysis (RCA) while proactively resolves issues. Leads the effort to determine the originating factors for a given incident and develop recommendations for preventing its repetition. Appropriately escalates incident tickets to the most senior levels of HIT Engineering resources, where appropriate. Documents information within the approved database or tracking software, providing clarifying information from investigation, with the primary objective to resolve and in rare instances escalate where necessary. Refers complex problems to the most senior level team members. Assists with investigations into operational problems and contribute to proposals for improvement and automation. Keeps current on all systems, software and related technology for their position that is used at Endeavor Health. Possesses acute knowledge of industry trends, emerging technologies, competitors' products and services, regulations and legislation impacting the company. Completes required training to stay current with Endeavor software, hardware and systems. Work within Endeavor Health's technical procedures and/or methods. Understands and monitors end user compliance to corporate guidelines as they relate to hardware and software. Supports other Systems Engineers in project management, planning and estimating, reporting, scheduling, and workflow. Guides junior level Systems Engineers in these processes. General Health Information Technology Maintain department records, reports and files as required. Attend team and departmental meetings, as required. Enhance professional development through participation in educational programs, current literature reviews, in-services, meetings and workshops. Adhere to the established best practices, standards, procedures, guidelines and methodologies within the work area. Benefits Career Pathways to Promote Professional Growth and Development Various Medical, Dental, and Vision options, including Domestic Partner Coverage Tuition Reimbursement Free Parking at designated locations Wellness Program Savings Plan Health Savings Account Options Retirement Options with Company Match Paid Time Off and Holiday Pay Community Involvement Opportunities",
        "url": "https://www.linkedin.com/jobs/view/3801549916",
        "summary": "The Systems Engineer II role involves researching, designing, building, configuring, testing, deploying, analyzing, administering, and maintaining/supporting environments and hardware, infrastructure and software platforms, and software technology components. The position leads efforts to research, design, plan and maintain new or existing hardware, infrastructure and software platforms, and software technology components. This role also involves analyzing, integrating, and resolving incidents and problems affecting production systems and multiple applications. The Systems Engineer II designs information systems that are appropriate for users' needs and consistent with the organization's information systems architecture. The role also involves monitoring and controlling the performance and status of technology components and providing technology component support and problem resolution. This position leads aspects of the implementation and design of hardware and software platforms and solutions, developing architectures to address business requirements, ensuring system scalability, security, performance and reliability. This role typically works on complex, larger and higher importance/impact projects and is expected to design highly optimal systems and processes as a result of broader and deeper experience.",
        "industries": [
            "Healthcare",
            "Information Technology",
            "Software Development"
        ],
        "soft_skills": [
            "Leadership",
            "Problem Solving",
            "Analytical Skills",
            "Communication",
            "Teamwork",
            "Collaboration",
            "Project Management",
            "Time Management",
            "Decision Making",
            "Creativity",
            "Troubleshooting",
            "Customer Service",
            "Interpersonal Skills",
            "Coaching",
            "Strategic Thinking",
            "Detail Oriented",
            "Organization",
            "Critical Thinking",
            "Problem Solving",
            "Root Cause Analysis"
        ],
        "hard_skills": [
            "Systems Engineering",
            "Hardware",
            "Software",
            "Infrastructure",
            "Scripting",
            "Developer Skills",
            "Configuration Management",
            "System Administration",
            "Security",
            "Performance Tuning",
            "Troubleshooting",
            "Patching",
            "System Monitoring",
            "Incident Management",
            "Root Cause Analysis",
            "Network Security",
            "Data Collection",
            "Reporting",
            "Technical Documentation",
            "Project Management",
            "Planning",
            "Estimating",
            "Reporting",
            "Scheduling",
            "Workflow",
            "Industry Trends",
            "Emerging Technologies",
            "Regulations",
            "Legislation"
        ],
        "tech_stack": [
            "NCH Legacy Environment",
            "System Monitoring Tools",
            "RFS (Request for Service)",
            "Incident Ticket System",
            "Database Tracking Software",
            "Approved Database or Tracking Software"
        ],
        "programming_languages": [],
        "experience": 2,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Information Technology",
                "Engineering"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Career Pathways to Promote Professional Growth and Development",
            "Various Medical, Dental, and Vision options, including Domestic Partner Coverage",
            "Tuition Reimbursement",
            "Free Parking at designated locations",
            "Wellness Program Savings Plan",
            "Health Savings Account Options",
            "Retirement Options with Company Match",
            "Paid Time Off and Holiday Pay",
            "Community Involvement Opportunities"
        ]
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Chicago, IL",
        "job_id": 3926840677,
        "company": "JPMorganChase",
        "title": "Software Engineer III - SAP BW, ABAP",
        "created_on": 1720635175.0184903,
        "description": "Job Description We have an exciting and rewarding opportunity for you to take your software engineering career to the next level. As a Software Engineer III at JPMorgan Chase within the Corporate Technology, Global Finance Technology team you serve as a seasoned member of an agile team to design and deliver trusted market-leading technology products in a secure, stable, and scalable way. You are responsible for carrying out critical technology solutions across multiple technical areas within various business functions in support of the firm’s business objectives. Job Responsibilities Executes software solutions, design, development, and technical troubleshooting with ability to think beyond routine or conventional approaches to build solutions or break down technical problems Creates secure and high-quality production code and maintains algorithms that run synchronously with appropriate systems Produces architecture and design artifacts for complex applications while being accountable for ensuring design constraints are met by software code development Gathers, analyzes, synthesizes, and develops visualizations and reporting from large, diverse data sets in service of continuous improvement of software applications and systems Proactively identifies hidden problems and patterns in data and uses these insights to drive improvements to coding hygiene and system architecture Contributes to software engineering communities of practice and events that explore new and emerging technologies Adds to team culture of diversity, equity, inclusion, and respect Required Qualifications, Capabilities, And Skills Formal training or certification on software engineering concepts and 3+ years applied experience Strong Hands on work experience in SAP BW, BW4HANA and Business Objects toolset (Webi, Universe design tool, Analysis for Office & Crystal Reports) Hands on work experience in cloud (AWS/AZURE) Strong hands on experience on SAP ABAP programming , Python & HANA SQL script and modeling knowledge Strong Software development experience in Operational Data Store / Data Warehouse environment Hands-on practical experience in system design, application development, testing, and operational stability Experience in developing, debugging, and maintaining code in a large corporate environment with one or more modern programming languages and database querying languages Solid understanding of agile methodologies such as CI/CD, Applicant Resiliency, and Security Analyze and identify efficiency improvements within SAP BW testing process, and design and implement automated solutions using chose tools, to ensure data integrity and functionality of SAP BW environment. Preferred Qualifications, Capabilities, And Skills Exposure to other visualization tool set such as QlikView, tableau is a plus Exposure to latest SAP HANA Database version, Experience in creating HANA views and Stored Procedures using HANA studio and/or Eclipse Knowledge of software applications and technical processes within a technical discipline (e.g., cloud, artificial intelligence, etc.) Experience with to S/4 HANA concepts and Fiori apps is a plus Experience in ETL/SAPBI projects, preferably in the finance industry About Us JPMorgan Chase & Co., one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world’s most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management. We offer a competitive total rewards package including base salary determined based on the role, experience, skill set, and location. For those in eligible roles, we offer discretionary incentive compensation which may be awarded in recognition of firm performance and individual achievements and contributions. We also offer a range of benefits and programs to meet employee needs, based on eligibility. These benefits include comprehensive health care coverage, on-site health and wellness centers, a retirement savings plan, backup childcare, tuition reimbursement, mental health support, financial coaching and more. Additional details about total compensation and benefits will be provided during the hiring process. We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. We also make reasonable accommodations for applicants’ and employees’ religious practices and beliefs, as well as mental health or physical disability needs. Visit our FAQs for more information about requesting an accommodation. JPMorgan Chase is an Equal Opportunity Employer, including Disability/Veterans About The Team Our professionals in our Corporate Functions cover a diverse range of areas from finance and risk to human resources and marketing. Our corporate teams are an essential part of our company, ensuring that we’re setting our businesses, clients, customers and employees up for success.",
        "url": "https://www.linkedin.com/jobs/view/3926840677",
        "summary": "JPMorgan Chase is seeking a Software Engineer III to join their Corporate Technology, Global Finance Technology team. The role involves designing and delivering technology solutions in a secure and scalable way, working with various business functions to support the firm's business objectives. The ideal candidate will have strong hands-on experience in SAP BW, BW4HANA, Business Objects, cloud (AWS/AZURE), SAP ABAP programming, Python, HANA SQL script, and data warehouse environments.  They will be responsible for executing software solutions, designing and developing applications, troubleshooting technical issues, creating secure and high-quality code, gathering and analyzing data for continuous improvement, and contributing to the software engineering community.  The role requires experience with agile methodologies such as CI/CD, application resilience, and security. Preferred qualifications include exposure to QlikView, Tableau, and experience with S/4HANA concepts and Fiori apps. ",
        "industries": [
            "Finance",
            "Technology",
            "Software Engineering"
        ],
        "soft_skills": [
            "Problem-solving",
            "Analytical thinking",
            "Communication",
            "Collaboration",
            "Teamwork",
            "Continuous improvement",
            "Proactive",
            "Data visualization",
            "Problem identification",
            "Technical troubleshooting",
            "Data analysis",
            "Design thinking",
            "Agile methodology"
        ],
        "hard_skills": [
            "SAP BW",
            "BW4HANA",
            "Business Objects",
            "Webi",
            "Universe design tool",
            "Analysis for Office",
            "Crystal Reports",
            "AWS",
            "Azure",
            "SAP ABAP",
            "Python",
            "HANA SQL",
            "Data warehousing",
            "System design",
            "Application development",
            "Testing",
            "Operational stability",
            "CI/CD",
            "Application Resilience",
            "Security",
            "QlikView",
            "Tableau",
            "SAP HANA",
            "HANA Views",
            "Stored Procedures",
            "HANA Studio",
            "Eclipse",
            "ETL",
            "SAPBI",
            "S/4HANA",
            "Fiori apps"
        ],
        "tech_stack": [
            "SAP BW",
            "BW4HANA",
            "Business Objects",
            "AWS",
            "Azure",
            "SAP ABAP",
            "Python",
            "HANA SQL",
            "QlikView",
            "Tableau",
            "S/4HANA",
            "Fiori apps",
            "HANA Studio",
            "Eclipse"
        ],
        "programming_languages": [
            "SAP ABAP",
            "Python",
            "HANA SQL"
        ],
        "experience": 3,
        "education": {
            "min_degree": "Formal training or certification",
            "fields": [
                "Software Engineering"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Competitive total rewards package",
            "Base salary",
            "Discretionary incentive compensation",
            "Comprehensive health care coverage",
            "On-site health and wellness centers",
            "Retirement savings plan",
            "Backup childcare",
            "Tuition reimbursement",
            "Mental health support",
            "Financial coaching"
        ]
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Chicago, IL",
        "job_id": 3948962073,
        "company": "Motorola Solutions",
        "title": "Senior Software Engineer",
        "created_on": 1720635178.3769894,
        "description": "Company Overview At Motorola Solutions, we're guided by a shared purpose - helping people be their best in the moments that matter - and we live up to our purpose every day by solving for safer. Because people can only be their best when they not only feel safe, but are safe. We're solving for safer by building the best possible technologies across every part of our safety and security ecosystem. That's mission-critical communications devices and networks, AI-powered video security & access control and the ability to unite voice, video and data in a single command center view. We're solving for safer by connecting public safety agencies and enterprises, enabling the collaboration that's critical to connect those in need with those who can help. The work we do here matters. Department Overview Motorola Solutions, Inc. Job Description Perform design, development, coding, testing, research, programming and documentation for software systems, applications and/or operating systems in conjunction with equipment designers and/or hardware developers. Perform modeling, designing, and coding activities, employing structured methods. Prepare design documentation for all levels of the software development process. Create and execute unit, integration, system, regression, performance, load and acceptance test plans and scripts. Use software system testing procedures, and document results. Analyze software requirements to determine feasibility of design within quality assurance, time and cost constraints. Basic Requirements Masters degree or Bachelors degree + 2 years experience. Travel Requirements None Relocation Provided None Position Type Experienced Referral Payment Plan No Our U.S. Benefits include: Incentive Bonus Plans Medical, Dental, Vision benefits 401K with Company Match 9 Paid Holidays Generous Paid Time Off Packages Employee Stock Purchase Plan Paid Parental & Family Leave and more! EEO Statement Motorola Solutions is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion or belief, sex, sexual orientation, gender identity, national origin, disability, veteran status or any other legally-protected characteristic. We are proud of our people-first and community-focused culture, empowering every Motorolan to be their most authentic self and to do their best work to deliver on the promise of a safer world. If you’d like to join our team but feel that you don’t quite meet all of the preferred skills, we’d still love to hear why you think you’d be a great addition to our team. We’re committed to providing an inclusive and accessible recruiting experience for candidates with disabilities, or other physical or mental health conditions. To request an accommodation, please email ohr@motorolasolutions.com.",
        "url": "https://www.linkedin.com/jobs/view/3948962073",
        "summary": "This role involves software development for safety and security systems. Responsibilities include design, coding, testing, documentation, and working with equipment designers and hardware developers. Experience with structured methods, unit testing, integration testing, and system testing is required. The position requires a Masters degree or a Bachelors degree with 2 years of experience.",
        "industries": [
            "Technology",
            "Software Development",
            "Security",
            "Safety",
            "Telecommunications"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Analytical Thinking",
            "Documentation",
            "Time Management",
            "Attention to Detail"
        ],
        "hard_skills": [
            "Software Development",
            "Coding",
            "Testing",
            "Design",
            "Documentation",
            "Structured Methods",
            "Unit Testing",
            "Integration Testing",
            "System Testing",
            "Performance Testing",
            "Load Testing",
            "Acceptance Testing",
            "Software Requirements Analysis"
        ],
        "tech_stack": [],
        "programming_languages": [],
        "experience": 2,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Computer Science",
                "Software Engineering",
                "Electrical Engineering",
                "Related Fields"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Incentive Bonus Plans",
            "Medical, Dental, Vision benefits",
            "401K with Company Match",
            "9 Paid Holidays",
            "Generous Paid Time Off Packages",
            "Employee Stock Purchase Plan",
            "Paid Parental & Family Leave"
        ]
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Carol Stream, IL",
        "job_id": 3963299809,
        "company": "Actalent",
        "title": "Software Engineer",
        "created_on": 1720635180.0621753,
        "description": "Job Title: Software Engineer Job Description As a Software Engineer, you will be instrumental in the design, development, and implementation of digital applications. You will take charge of designing, developing, and maintaining IoT connected Linux applications using Python, C++, and Qt/QML frameworks. Your role will involve integrating applications with the Linux operating system, leveraging system libraries, APIs, and services. Additionally, you will develop and maintain technical documentation, including design specs, software requirements, and user manuals. Collaborating with cross-functional teams, participating in code reviews, and contributing to quality assurance activities are also key aspects of this role. Hard Skills Python C++ Qt/QML frameworks Linux applications development APIs System libraries Soft Skills Technical documentation Cross-functional collaboration Code review participation Quality assurance contributions About Actalent Actalent is a global leader in engineering and sciences services and talent solutions. We help visionary companies advance their engineering and science initiatives through access to specialized experts who drive scale, innovation and speed to market. With a network of almost 30,000 consultants and more than 4,500 clients across the U.S., Canada, Asia and Europe, Actalent serves many of the Fortune 500. Diversity, Equity & Inclusion At Actalent, Diversity And Inclusion Are a Bridge Towards The Equity And Success Of Our People. DE&I Are Embedded Into Our Culture Through Hiring diverse talent Maintaining an inclusive environment through persistent self-reflection Building a culture of care, engagement, and recognition with clear outcomes Ensuring growth opportunities for our people The company is an equal opportunity employer and will consider all applications without regard to race, sex, age, color, religion, national origin, veteran status, disability, sexual orientation, gender identity, genetic information or any characteristic protected by law. If you would like to request a reasonable accommodation, such as the modification or adjustment of the job application process or interviewing process due to a disability, please email actalentaccommodation@actalentservices.com for other accommodation options.",
        "url": "https://www.linkedin.com/jobs/view/3963299809",
        "summary": "Software Engineer responsible for designing, developing, and maintaining IoT connected Linux applications using Python, C++, and Qt/QML frameworks. This role involves integrating applications with the Linux operating system, leveraging system libraries, APIs, and services, as well as developing technical documentation and collaborating with cross-functional teams.",
        "industries": [
            "Software Development",
            "Internet of Things (IoT)",
            "Engineering Services",
            "Technology"
        ],
        "soft_skills": [
            "Technical Documentation",
            "Cross-functional Collaboration",
            "Code Review",
            "Quality Assurance"
        ],
        "hard_skills": [
            "Python",
            "C++",
            "Qt/QML",
            "Linux Applications Development",
            "APIs",
            "System Libraries"
        ],
        "tech_stack": [
            "Python",
            "C++",
            "Qt/QML",
            "Linux"
        ],
        "programming_languages": [
            "Python",
            "C++"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Chicago, IL",
        "job_id": 3959393249,
        "company": "Genesis10",
        "title": "Network / System Engineer V",
        "created_on": 1720635183.8669224,
        "description": "Genesis10 is currently seeking an Network / System Engineer with our client in the financial industry located in Addison, TX, Chicago, IL, and Richmond, VA. This is a 12 + month contract position. Responsibilities: Assist in the development and execution of best-in-class designs & procedures to support Microsoft 365 delivery Assist members of the architecture, engineering & operations teams with duties as assigned by management Architect and design cloud-based solutions in a secure and compliant framework with focus on automation, scalability, and resiliency Provide architectural support for initiatives ranging from Proof of Concepts with new technologies and integration paradigms to complex multi-year implementations Bring strategic & deep understanding of productivity concepts and practices with alignment to financial services standards Provide strategic technical guidance to leadership and development teams to implement best practices and execution of industry leading designs Help create and execute best-in-class engineering processes in support of solution delivery Focus on process improvements contributing towards increasing operational excellence and an improved user experience Generate and maintain architectural design artifacts in support of governance standards, requirements, and operational excellence Perform analysis, identify gaps or improvements, and develop solutions based on current business needs Provide oversight on daily architectural activities, function as mentor, help with problem escalation, and communicate significant problems or issues to management Requirements: College degree in Computer Science, Technology, Engineering, or equivalent hands-on experience 10+ years' experience with architecture and engineering across email services Expert-level knowledge of relevant collaboration services both cloud and on-premise Expert-level knowledge of cross platform Directory solutions, Conditional Access, Identity & Access Management, Security/Compliance, Data Loss Prevention, Data Classification High-level knowledge of foundational components such as Active Directory, network infrastructure/architecture(DNS/DHCP, routing, etc.), & PowerShell scripting Expert level design and solution engineering experience in various collaboration technologies Expert knowledge of data loss prevention services including deployment, configuration and administration Self-starter, critical thinker with proven ability to deal with granular requirements yet see things holistically and strategically Ability to shift and pivot with changing responsibilities Ability to work effectively both independently Ability to work with business and technical teams to build requirements and technical artifacts Ability to prioritize and organize effectively Ability to learn new technology and business process as required Ability to create architecture documents including network/data flows and test scope Strong interpersonal skills with excellent verbal and written communication Comfortable sharing ideas and concepts in a highly collaborative team Have experience with ITIL foundational and Agile process methodologies Exceptional organizational and analytical skills with high attention to detail, effective time management skills Possess a personal sense of urgency and the ability to handle a fast-paced environment Strong experience architecting and building applications to support workstreams across end-to-end solutions Desired skills: Experience with enterprise security including TLS including SMTP over TLS, certificates, HSM, NIST standards Experience with Microsoft Windows operating system environment Experience with M365, including OneDrive, SharePoint Online, Exchange Online & Teams Authentication mechanisms including domain authentication, OAuth/Modern Authentication, Kerberos Experience with network & security components including load balancers, proxy, load balancers and firewalls Understanding of Financial Regulatory, Legal and Compliance requirements Experience with ITIL methodologies and best practices MCP or higher certification Pay Range: $63.68 - $71.68 Only candidates available and ready to work directly as Genesis10 employees will be considered for this position. If you have the described qualifications and are interested in this exciting opportunity, please apply! About Genesis10: Ranked a Top Staffing Firm in the U.S. by Staffing Industry Analysts for six consecutive years, Genesis10 puts thousands of consultants and employees to work across the United States every year in contract, contract-for-hire, and permanent placement roles. With more than 300 active clients, Genesis10 provides access to many of the Fortune 100 firms and a variety of mid-market organizations across the full spectrum of industry verticals. For contract roles, Genesis10 offers the benefits listed below. If this is a perm-placement opportunity, our recruiter can talk you through the unique benefits offered for that particular client. Benefits of Working with Genesis10: Access to hundreds of clients, most who have been working with Genesis10 for 5-20+ years. The opportunity to have a career-home in Genesis10; many of our consultants have been working exclusively with Genesis10 for years. Access to an experienced, caring recruiting team (more than 7 years of experience, on average.) Behavioral Health Platform Medical, Dental, Vision Health Savings Account Voluntary Hospital Indemnity (Critical Illness & Accident) Voluntary Term Life Insurance 401K Sick Pay (for applicable states/municipalities) Commuter Benefits (Dallas, NYC, SF) Remote opportunities available For multiple years running, Genesis10 has been recognized as a Top Staffing Firm in the U.S., as a Best Company for Work-Life Balance, as a Best Company for Career Growth, for Diversity, and for Leadership, amongst others. To learn more and to view all our available career opportunities, please visit us at our website. Genesis10 is an Equal Opportunity Employer. Candidates will receive consideration without regard to their race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.",
        "url": "https://www.linkedin.com/jobs/view/3959393249",
        "summary": "Genesis10 is seeking a Network/System Engineer with 10+ years of experience in email services architecture and engineering. This 12+ month contract role requires expertise in Microsoft 365, cloud-based solutions, and collaboration services, with a focus on automation, scalability, and security.  The ideal candidate will possess strong communication skills, a collaborative mindset, and experience with ITIL and Agile methodologies.",
        "industries": [
            "Financial Services",
            "Information Technology"
        ],
        "soft_skills": [
            "Self-starter",
            "Critical thinker",
            "Problem-solving",
            "Communication",
            "Collaboration",
            "Organization",
            "Time management",
            "Attention to detail",
            "Adaptability",
            "Teamwork",
            "Leadership",
            "Mentorship"
        ],
        "hard_skills": [
            "Microsoft 365",
            "Cloud-based Solutions",
            "Automation",
            "Scalability",
            "Resiliency",
            "Collaboration Services",
            "Email Services",
            "Active Directory",
            "Network Infrastructure",
            "DNS/DHCP",
            "Routing",
            "PowerShell Scripting",
            "Data Loss Prevention",
            "Data Classification",
            "Identity & Access Management",
            "Security",
            "Compliance",
            "ITIL",
            "Agile",
            "TLS",
            "SMTP",
            "Certificates",
            "HSM",
            "NIST Standards",
            "Microsoft Windows",
            "OneDrive",
            "SharePoint Online",
            "Exchange Online",
            "Teams",
            "Domain Authentication",
            "OAuth",
            "Modern Authentication",
            "Kerberos",
            "Load Balancers",
            "Proxy",
            "Firewalls",
            "Financial Regulatory",
            "Legal",
            "Compliance",
            "MCP"
        ],
        "tech_stack": [
            "Microsoft 365",
            "OneDrive",
            "SharePoint Online",
            "Exchange Online",
            "Teams",
            "Active Directory",
            "DNS",
            "DHCP",
            "PowerShell",
            "TLS",
            "SMTP",
            "HSM",
            "NIST Standards",
            "Windows",
            "Load Balancers",
            "Proxy",
            "Firewalls"
        ],
        "programming_languages": [
            "PowerShell"
        ],
        "experience": 10,
        "education": {
            "min_degree": "College Degree",
            "fields": [
                "Computer Science",
                "Technology",
                "Engineering"
            ]
        },
        "salary": {
            "max": 7168,
            "min": 6368
        },
        "benefits": [
            "Medical",
            "Dental",
            "Vision",
            "Health Savings Account",
            "Hospital Indemnity",
            "Life Insurance",
            "401K",
            "Sick Pay",
            "Commuter Benefits",
            "Remote Opportunities"
        ]
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Chicago, IL",
        "job_id": 3952033683,
        "company": "Genesis10",
        "title": "Network / System Engineer",
        "created_on": 1720635185.5729587,
        "description": "Genesis10 is currently seeking an Network / System Engineer with our client in the financial industry located in Jacksonville, FL, Chicago, IL, and Richmond, VA. This is a 12 + month contract position. Responsibilities: Design, engineering, and implementation of firewall service requests aligned to the delivery of infrastructure services Understand network-based technologies and protocols; demonstrates competency in the ability to translate business requirements into business solutions and successfully position with customers Accountable and can hold others accountable Interface directly with internal stakeholders and external suppliers/providers, architecture, product engineering, product management, and business and senior management Adhere to and use standards, product sets, templates, systems, and artifacts Requirements: 3-5 years of experience working in Network Services Possess the ability to analyze and understand a variety of existing, and emerging, business/technical requirements Experience with Network Technologies Firewall (Fortinet and Checkpoint) Add, modify, delete Firewall rules, NAT rules and Object groups Industry Certifications in Fortinet NSE (1-8), Checkpoint CCTA, CCTE, CCSA, CCSE or equivalent experience Industry Certification in Cisco CCNA, CCNP, and CCIE or equivalent experience Skills in scripting; Python, Ansible, PowerShell Automation Skills - Expect, Rancid, REST/SOAP Experience working in an Agile environment Experience in Networking-related disciplines within a design, implementation, or operations role Relevant Industry certifications in Network Technologies Experience of working within Financial services (Insurance, Banking, Investment banking) Experience with other network technologies (Splunk, ECSL, WAN, MAN, LAN, Optical, Routing, Switching, Proxy/Threat Prevention, DDI, Load Balancing, and AAA) Organized and detail oriented Strong technical acumen Strong analytical skills Ability to manage short and long-term engagements with multiple project tracks and teams Ability to interact with clients at all levels with capacity to manage and set client expectations Firm understanding of IT Service Management processes and project management (i.e., ITIL and Agile) Ability to manage client escalations and negotiate resolution Pay Range: $60.24 - $68.24 Only candidates available and ready to work directly as Genesis10 employees will be considered for this position. If you have the described qualifications and are interested in this exciting opportunity, please apply! About Genesis10: Ranked a Top Staffing Firm in the U.S. by Staffing Industry Analysts for six consecutive years, Genesis10 puts thousands of consultants and employees to work across the United States every year in contract, contract-for-hire, and permanent placement roles. With more than 300 active clients, Genesis10 provides access to many of the Fortune 100 firms and a variety of mid-market organizations across the full spectrum of industry verticals. For contract roles, Genesis10 offers the benefits listed below. If this is a perm-placement opportunity, our recruiter can talk you through the unique benefits offered for that particular client. Benefits of Working with Genesis10: Access to hundreds of clients, most who have been working with Genesis10 for 5-20+ years. The opportunity to have a career-home in Genesis10; many of our consultants have been working exclusively with Genesis10 for years. Access to an experienced, caring recruiting team (more than 7 years of experience, on average.) Behavioral Health Platform Medical, Dental, Vision Health Savings Account Voluntary Hospital Indemnity (Critical Illness & Accident) Voluntary Term Life Insurance 401K Sick Pay (for applicable states/municipalities) Commuter Benefits (Dallas, NYC, SF) Remote opportunities available For multiple years running, Genesis10 has been recognized as a Top Staffing Firm in the U.S., as a Best Company for Work-Life Balance, as a Best Company for Career Growth, for Diversity, and for Leadership, amongst others. To learn more and to view all our available career opportunities, please visit us at our website. Genesis10 is an Equal Opportunity Employer. Candidates will receive consideration without regard to their race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.",
        "url": "https://www.linkedin.com/jobs/view/3952033683",
        "summary": "Genesis10 seeks a Network/System Engineer with 3-5 years of experience in Network Services, specializing in firewall technologies (Fortinet and Checkpoint). This 12+ month contract position involves designing, engineering, and implementing firewall service requests, translating business requirements into solutions, and interfacing with stakeholders and vendors. Experience with Agile environments, scripting (Python, Ansible, PowerShell), and various network technologies (Splunk, ECSL, WAN, MAN, LAN, Optical, Routing, Switching, Proxy/Threat Prevention, DDI, Load Balancing, and AAA) is required. Industry certifications in Fortinet NSE, Checkpoint CCTA, CCTE, CCSA, CCSE, Cisco CCNA, CCNP, and CCIE are preferred. This role offers a competitive salary range of $60.24 - $68.24 per hour and various benefits such as medical, dental, vision, health savings account, 401K, sick pay, commuter benefits, and remote work opportunities.",
        "industries": [
            "Financial Services",
            "Insurance",
            "Banking",
            "Investment Banking",
            "Technology"
        ],
        "soft_skills": [
            "Communication",
            "Problem Solving",
            "Teamwork",
            "Collaboration",
            "Analytical Skills",
            "Decision Making",
            "Time Management",
            "Organization",
            "Detail-Oriented",
            "Client Management",
            "Negotiation",
            "Accountability",
            "Interpersonal Skills",
            "Presentation Skills"
        ],
        "hard_skills": [
            "Firewall",
            "Fortinet",
            "Checkpoint",
            "Network Technologies",
            "Scripting",
            "Python",
            "Ansible",
            "PowerShell",
            "Automation",
            "Expect",
            "Rancid",
            "REST/SOAP",
            "Agile",
            "Networking",
            "Splunk",
            "ECSL",
            "WAN",
            "MAN",
            "LAN",
            "Optical",
            "Routing",
            "Switching",
            "Proxy",
            "Threat Prevention",
            "DDI",
            "Load Balancing",
            "AAA",
            "ITIL",
            "Project Management",
            "IT Service Management"
        ],
        "tech_stack": [
            "Firewall",
            "Fortinet",
            "Checkpoint",
            "Python",
            "Ansible",
            "PowerShell",
            "Expect",
            "Rancid",
            "REST/SOAP",
            "Splunk",
            "ECSL",
            "WAN",
            "MAN",
            "LAN",
            "Optical",
            "Routing",
            "Switching",
            "Proxy",
            "Threat Prevention",
            "DDI",
            "Load Balancing",
            "AAA"
        ],
        "programming_languages": [
            "Python"
        ],
        "experience": 3,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 6824,
            "min": 6024
        },
        "benefits": [
            "Medical",
            "Dental",
            "Vision",
            "Health Savings Account",
            "Voluntary Hospital Indemnity",
            "Voluntary Term Life Insurance",
            "401K",
            "Sick Pay",
            "Commuter Benefits",
            "Remote opportunities"
        ]
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Chicago, IL",
        "job_id": 3942360271,
        "company": "Docusign",
        "title": "Software Engineer, Backend (Agreement Template Authoring)",
        "created_on": 1720635187.397085,
        "description": "Company Overview Docusign brings agreements to life. Over 1.5 million customers and more than a billion people in over 180 countries use Docusign solutions to accelerate the process of doing business and simplify people’s lives. With intelligent agreement management, Docusign unleashes business-critical data that is trapped inside of documents. Until now, these were disconnected from business systems of record, costing businesses time, money, and opportunity. Using Docusign’s Intelligent Agreement Management platform, companies can create, commit, and manage agreements with solutions created by the #1 company in e-signature and contract lifecycle management (CLM). What you'll do The Template Authoring team is a diverse group of engineers tasked to build the foundation for great template building experience and delightful user experiences. We're focused on making template authoring no code automation solution accessible to all DocuSign customers and our scope continues to grow and evolve. We’re looking for a Backend Engineer with the knowledge and skills to deliver highly scalable web solutions to our customers. The ideal candidate will have worked on large-scale SaaS projects and is highly skilled in designing and implementing backend services that deliver functionality to different mobile and web form factors and clients. Your role will have the opportunity to lead projects and work collaboratively with our product management teams. This position is an individual contributor role reporting to the Senior Engineering Manager. Responsibility Design and develop Microservices that continue our commitment to putting world class SaaS solutions into the hands of our customers Work across multiple teams to deliver solutions that can be leveraged throughout the organization Work with Product Management and other developers to understand and translate product requirements into engineering requirements and development estimates Be data focused, everything we do is measured and used to ensure 5-9’s availability and that we are solving the right problems Participate in, or lead design reviews with peers and stakeholders to decide among available technologies Guide and mentor other engineers through design and code reviews Job Designation Hybrid: Employee divides their time between in-office and remote work. Access to an office location is required. (Frequency: Minimum 2 days per week; may vary by team but will be weekly in-office expectation) Positions at DocuSign are assigned a job designation of either In Office, Hybrid or Remote and are specific to the role/job. Preferred job designations are not guaranteed when changing positions within DocuSign. DocuSign reserves the right to change a position's job designation depending on business needs and as permitted by local law. What you bring Basic 5+ years software engineering experience in a SaaS organization focused on building backend 5+ years experience building resilient platforms, software as a service, architecture and integration Experience documenting architectural standards and decisions Professional experience with client-side frameworks Experience building REST APIs or shared JavaScript libraries B.S. in Computer Science or equivalent experiences Preferred Experience full stack development preferably with languages like C# and Java Experience with telemetry software Experience with Git, continuous integration and deployment tools Experience working in an agile development environment Demonstrate a strong focus on instrumenting features being developed to gain insights into usage patterns and feature success Drive strategic code sharing and architecture for one or more functional area Wage Transparency Based on applicable legislation, the below details pay ranges in the following locations: California: $134,900.00 - $216,975.00 base salary Illinois and Colorado: $129,000.00 - $182,250.00 base salary Washington and New York (including NYC metro area): $129,000.00 - $190,550.00 base salary This role is also eligible for bonus, equity and benefits. Global Benefits Provide Options For The Following Paid Time Off: earned time off, as well as paid company holidays based on region Paid Parental Leave: take up to six months off with your child after birth, adoption or foster care placement Full Health Benefits Plans: options for 100% employer paid and minimum employee contribution health plans from day one of employment Retirement Plans: select retirement and pension programs with potential for employer contributions Learning and Development: options for coaching, online courses and education reimbursements Compassionate Care Leave: paid time off following the loss of a loved one and other life-changing events Life at Docusign Working here Docusign is committed to building trust and making the world more agreeable for our employees, customers and the communities in which we live and work. You can count on us to listen, be honest, and try our best to do what’s right, every day. At Docusign, everything is equal. We each have a responsibility to ensure every team member has an equal opportunity to succeed, to be heard, to exchange ideas openly, to build lasting relationships, and to do the work of their life. Best of all, you will be able to feel deep pride in the work you do, because your contribution helps us make the world better than we found it. And for that, you’ll be loved by us, our customers, and the world in which we live. Accommodation Docusign is committed to providing reasonable accommodations for qualified individuals with disabilities in our job application procedures. If you need such an accommodation, or a religious accommodation, during the application process, please contact us at accommodations@docusign.com. If you experience any issues, concerns, or technical difficulties during the application process please get in touch with our Talent organization at taops@docusign.com for assistance. Applicant and Candidate Privacy Notice States Not Eligible for Employment This position is not eligible for employment in the following states: Alaska, Hawaii, Maine, Mississippi, North Dakota, South Dakota, Vermont, West Virginia and Wyoming. Equal Opportunity Employer It's important to us that we build a talented team that is as diverse as our customers and where all employees feel a deep sense of belonging and thrive. We encourage great talent who bring a range of perspectives to apply for our open positions. Docusign is an Equal Opportunity Employer and makes hiring decisions based on experience, skill, aptitude and a can-do approach. We will not discriminate based on race, ethnicity, color, age, sex, religion, national origin, ancestry, pregnancy, sexual orientation, gender identity, gender expression, genetic information, physical or mental disability, registered domestic partner status, caregiver status, marital status, veteran or military status, or any other legally protected category. EEO Know Your Rights poster",
        "url": "https://www.linkedin.com/jobs/view/3942360271",
        "summary": "Docusign seeks a Backend Engineer to build scalable web solutions for its template authoring platform. This role involves designing and developing microservices, collaborating with product management, and ensuring high availability. The ideal candidate has 5+ years of experience in SaaS backend development, building resilient platforms, and working with REST APIs. Additional experience in full-stack development, telemetry software, Git, continuous integration/deployment, and agile environments is preferred. The position offers a competitive salary, bonus, equity, and comprehensive benefits.",
        "industries": [
            "Software",
            "Technology",
            "SaaS",
            "E-commerce",
            "Cloud Computing",
            "Contract Lifecycle Management"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Data-Driven",
            "Analytical",
            "Leadership",
            "Mentorship",
            "Strategic Thinking",
            "Time Management"
        ],
        "hard_skills": [
            "Microservices",
            "REST APIs",
            "SaaS",
            "Backend Development",
            "Resilient Platforms",
            "Architecture",
            "Integration",
            "Client-Side Frameworks",
            "JavaScript Libraries",
            "C#",
            "Java",
            "Telemetry Software",
            "Git",
            "Continuous Integration",
            "Continuous Deployment",
            "Agile Development"
        ],
        "tech_stack": [
            "Microservices",
            "REST APIs",
            "SaaS",
            "C#",
            "Java",
            "Git",
            "Telemetry Software"
        ],
        "programming_languages": [
            "C#",
            "Java",
            "JavaScript"
        ],
        "experience": 5,
        "education": {
            "min_degree": "B.S.",
            "fields": [
                "Computer Science"
            ]
        },
        "salary": {
            "max": 216975,
            "min": 129000
        },
        "benefits": [
            "Paid Time Off",
            "Paid Parental Leave",
            "Health Benefits",
            "Retirement Plans",
            "Learning and Development",
            "Compassionate Care Leave"
        ]
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Lisle, IL",
        "job_id": 3864629286,
        "company": "Broadcom",
        "title": "Software Development Engineer",
        "created_on": 1720635188.912061,
        "description": "Please Note: If you are a first time user, please create your candidate login account before you apply for a job. (Click Sign In > Create Account) If you already have a Candidate Account, please Sign-In before you apply. Job Description: We are looking for experienced Software Engineers to work on continuous improvement efforts in a small development team environment. Efforts will be focused on implementation of functional software solutions, as well as test automation and further evolution of robust CI/CD pipelines. You will work with product management to help define software requirements and implement the solutions in a small development team environment. In this role, you should be able to work independently with little supervision while also collaborating with team members. You should have excellent organization and problem-solving skills, experience in agile methodologies is a plus. The individual and team goal will be to develop high-quality software that is aligned with user needs and business objectives. Responsibilities Develop high-quality software design and architecture Producing, testing and debugging software solutions Providing peer leadership in an Agile team environment Technical interaction with product management and customers Coordination with management and internal teams Automate tasks through appropriate tools and scripting Perform validation and verification testing Document development phases and monitor systems Ensure software is up-to-date with latest technologies Requirements Proven Mainframe knowledge and skills IBM Assembler language proficiency C/C++ and Java language proficiency DB2 and relational database and internals skills Database design and SQL skills Source control systems and interactive debugging tool experience Experience developing/utilizing test automation Experience with DevOps and Continuous integration tools Strong Database Application design and implementation skills including schema design, queries, procedures, triggers etc. Familiarity with various operating systems, particularly z/OS Analytical mind with problem-solving aptitude Experience : Bachelor's and 5+ years of related experience Additional Job Description: Compensation And Benefits The annual base salary range for this position is $78,000 - $130,000 This position is also eligible for a discretionary annual bonus in accordance with relevant plan documents, and equity in accordance with equity plan documents and equity award agreements. Broadcom offers a competitive and comprehensive benefits package: Medical, dental and vision plans, 401(K) participation including company matching, Employee Stock Purchase Program (ESPP), Employee Assistance Program (EAP), company paid holidays, paid sick leave and vacation time. The company follows all applicable laws for Paid Family Leave and other leaves of absence. Broadcom is proud to be an equal opportunity employer. We will consider qualified applicants without regard to race, color, creed, religion, sex, sexual orientation, gender identity, national origin, citizenship, disability status, medical condition, pregnancy, protected veteran status or any other characteristic protected by federal, state, or local law. We will also consider qualified applicants with arrest and conviction records consistent with local law. If you are located outside USA, please be sure to fill out a home address as this will be used for future correspondence.",
        "url": "https://www.linkedin.com/jobs/view/3864629286",
        "summary": "Software Engineer needed to implement functional software solutions, test automation, and robust CI/CD pipelines.  Work with product management to define requirements.  Should be able to work independently and collaboratively, have strong organization and problem-solving skills, and experience with agile methodologies.",
        "industries": [
            "Software Development",
            "Information Technology",
            "Technology"
        ],
        "soft_skills": [
            "Problem-solving",
            "Organization",
            "Communication",
            "Collaboration",
            "Leadership"
        ],
        "hard_skills": [
            "Mainframe",
            "IBM Assembler",
            "C/C++",
            "Java",
            "DB2",
            "SQL",
            "Source Control Systems",
            "Interactive Debugging",
            "Test Automation",
            "DevOps",
            "Continuous Integration",
            "Database Design",
            "Schema Design",
            "Queries",
            "Procedures",
            "Triggers",
            "z/OS",
            "Agile Methodologies"
        ],
        "tech_stack": [
            "Mainframe",
            "IBM Assembler",
            "C/C++",
            "Java",
            "DB2",
            "SQL",
            "Source Control Systems",
            "Interactive Debugging",
            "Test Automation",
            "DevOps",
            "Continuous Integration",
            "z/OS"
        ],
        "programming_languages": [
            "Assembler",
            "C",
            "C++",
            "Java"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Computer Science",
                "Engineering",
                "Information Technology"
            ]
        },
        "salary": {
            "max": 130000,
            "min": 78000
        },
        "benefits": [
            "Medical",
            "Dental",
            "Vision",
            "401(K)",
            "Company Matching",
            "Employee Stock Purchase Program",
            "Employee Assistance Program",
            "Paid Holidays",
            "Paid Sick Leave",
            "Vacation Time",
            "Paid Family Leave",
            "Leaves of Absence"
        ]
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Illinois, United States",
        "job_id": 3952554450,
        "company": "CDW",
        "title": "SR Software Engineer II",
        "created_on": 1720635190.564021,
        "description": "Description Bring your IT career and talents to CDW, where you can have a greater impact, be inspired by our mission and excited about your career and future.A Fortune 200 leader, we’re the driven professionals and technology experts companies turn to most to solve their IT challenges. The Senior Software Engineer II – Data will play a pivotal role in building and operationalizing the minimally inclusive data necessary for the enterprise data and analytics initiatives following best practices The bulk of the data engineer’s work would be in building, managing and optimizing data pipelines and then moving these data pipelines effectively into production for key data and analytics consumers like business/data analysts, data scientists or any persona that needs curated data for data and analytics use cases across the enterprise What you will do: Lead a portfolio of diverse technology projects and a team of developers with deep experience in distributed microservices, and full stack systems to create solutions that help meet regulatory needs for the company. Interface with other technology teams to build Azure Data Pipelines to integrate with Data Lake and other. Collaborate with other technology teams to help engineer data sets that data science teams use to implement advanced analytics algorithms that exploit our rich datasets for statistical analysis, prediction, clustering, and machine learning. Collaborates with business teams to improve data models that feed business intelligence tools, increasing data accessibility, and fostering data-driven decision making across the organization. Share your passion for staying on top of tech trends, experimenting with, and learning new technologies, participating in internal & external technology communities, mentoring other members of the engineering community. Collaborate with digital product managers and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment. Utilize programming languages like C#.NET, VS2022, JavaScript, Asp.Net Core API, Cosmos DB and other. Responsible for using innovative and modern tools, techniques, and architectures to automate the most-common, repeatable and tedious data preparation and integration tasks partially or completely in order to minimize manual and error-prone processes and improve productivity. Be curious and knowledgeable about new data management techniques and how to apply them to solve business problems. Train counterparts such as data scientists, data analysts, data consumers in data pipelines and preparation techniques, which make it easier for them to integrate and consume the data they need for their own use cases. What we expect of you: BS or MS degree in Computer Science or a related technical field. 7+ years software development experience At least 4 years of experience working with Azure Integration Services Hands on experience in designing, Building and Maintaining cloud systems on Azure using all available services with focus on PaaS/iPaaS. Hands on experience with Azure Integration Services (iPaaS) – Azure Logic Apps, Azure Functions, Azure APIM, Azure Event Grid Hands on experience of DevOps toolset & practices (CI/CD pipeline, Jenkins, Jira, etc.) Experience with Azure Service Bus and Azure Storage Experience with the implementation of cloud solutions in an enterprise environment Strong knowledge of technology and cloud topics including network and application security, security baselines, web server, and database security Knowledge of Azure security strategies and tools Must have worked in a team and have led a team for technical delivery. Azure certification is a plus. Who we are: CDW is a leading technology solutions provider to business, government, education and healthcare organizations across the globe.Our fingerprints can be found on technology inworkplacesof more than 250,000companies;from fresh-faced start-ups to international conglomerates. With the breadth of products and services we offer, there is no request too big or too small. What you can expect from us: Culture, coworkers, careers. CDW is not only the People Who Get IT but the People who get People. Our relationships are fueled by our deep expertise and grounded in the CDW Way. Our empowering leadership makes things happen and inspires their teams to do the same. From the teammates beside us to the leaders who guide us, we move forward together. At CDW, you’ll work with people who inspire you. People with positive, success-driven attitudes who you will learn from and forge strong relationships with. Bring your best true self—and your best ideas—to CDW. Because diverse perspectives bring forth better problem solving—and better solutions for our customers on a rapidly evolving technology landscape. Equal Opportunity Employer, including disability and protected veteran status Benefits overview: https://cdw.benefit-info.com/",
        "url": "https://www.linkedin.com/jobs/view/3952554450",
        "summary": "CDW is seeking a Senior Software Engineer II – Data to build and operationalize data pipelines, integrate with Azure Data Lake, and collaborate with data science teams to implement advanced analytics. The ideal candidate will have 7+ years of software development experience, at least 4 years with Azure Integration Services, and hands-on experience with Azure PaaS/iPaaS services. ",
        "industries": [
            "Information Technology",
            "Data and Analytics",
            "Software Development",
            "Cloud Computing",
            "Financial Technology"
        ],
        "soft_skills": [
            "Leadership",
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Teamwork",
            "Mentorship",
            "Passion for Technology",
            "Curiosity",
            "Training",
            "Decision Making"
        ],
        "hard_skills": [
            "Azure Data Pipelines",
            "Azure Integration Services",
            "Azure Logic Apps",
            "Azure Functions",
            "Azure APIM",
            "Azure Event Grid",
            "Azure Service Bus",
            "Azure Storage",
            "DevOps",
            "CI/CD",
            "Jenkins",
            "Jira",
            "C#.NET",
            "VS2022",
            "JavaScript",
            "Asp.Net Core API",
            "Cosmos DB",
            "Network Security",
            "Application Security",
            "Security Baselines",
            "Web Server Security",
            "Database Security",
            "Azure Security Strategies",
            "Azure Security Tools"
        ],
        "tech_stack": [
            "Azure",
            "Azure Data Lake",
            "Azure Integration Services",
            "Azure Logic Apps",
            "Azure Functions",
            "Azure APIM",
            "Azure Event Grid",
            "Azure Service Bus",
            "Azure Storage",
            "DevOps",
            "CI/CD",
            "Jenkins",
            "Jira",
            "C#.NET",
            "VS2022",
            "JavaScript",
            "Asp.Net Core API",
            "Cosmos DB"
        ],
        "programming_languages": [
            "C#.NET",
            "JavaScript"
        ],
        "experience": 7,
        "education": {
            "min_degree": "BS",
            "fields": [
                "Computer Science",
                "Related Technical Field"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Culture",
            "Coworkers",
            "Careers",
            "Empowering Leadership",
            "Diverse Perspectives",
            "Equal Opportunity Employer",
            "Disability and Protected Veteran Status"
        ]
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Chicago, IL",
        "job_id": 3963085512,
        "company": "Motorola Solutions",
        "title": "Sr. Software Engineer",
        "created_on": 1720635193.9792657,
        "description": "Company Overview At Motorola Solutions, we're guided by a shared purpose - helping people be their best in the moments that matter - and we live up to our purpose every day by solving for safer. Because people can only be their best when they not only feel safe, but are safe. We're solving for safer by building the best possible technologies across every part of our safety and security ecosystem. That's mission-critical communications devices and networks, AI-powered video security & access control and the ability to unite voice, video and data in a single command center view. We're solving for safer by connecting public safety agencies and enterprises, enabling the collaboration that's critical to connect those in need with those who can help. The work we do here matters. Department Overview Motorola Solutions, Inc. Job Description Perform design, development, coding, testing, research, programming and documentation for software systems, applications and/or operating systems in conjunction with equipment designers and/or hardware developers. Perform modeling, designing, and coding activities, employing structured methods. Prepare design documentation for all levels of the software development process. Create and execute unit, integration, system, regression, performance, load and acceptance test plans and scripts. Use software system testing procedures, and document results. Analyze software requirements to determine feasibility of design within quality assurance, time and cost constraints. Telecommuting permitted anywhere within the U.S. $102,690 - $147,000/year. Basic Requirements Masters degree + 1 years of experience or Bachelors degree + 3 years experience. Travel Requirements None Relocation Provided None Position Type Experienced Referral Payment Plan No Our U.S. Benefits include: Incentive Bonus Plans Medical, Dental, Vision benefits 401K with Company Match 9 Paid Holidays Generous Paid Time Off Packages Employee Stock Purchase Plan Paid Parental & Family Leave and more! EEO Statement Motorola Solutions is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion or belief, sex, sexual orientation, gender identity, national origin, disability, veteran status or any other legally-protected characteristic. We are proud of our people-first and community-focused culture, empowering every Motorolan to be their most authentic self and to do their best work to deliver on the promise of a safer world. If you’d like to join our team but feel that you don’t quite meet all of the preferred skills, we’d still love to hear why you think you’d be a great addition to our team. We’re committed to providing an inclusive and accessible recruiting experience for candidates with disabilities, or other physical or mental health conditions. To request an accommodation, please email ohr@motorolasolutions.com.",
        "url": "https://www.linkedin.com/jobs/view/3963085512",
        "summary": "Motorola Solutions is looking for a Software Engineer to design, develop, code, test, research, program, and document software systems, applications, and operating systems. The role involves modeling, designing, and coding activities using structured methods. The candidate will prepare design documentation for all levels of the software development process, create and execute test plans and scripts, analyze software requirements, and document results. Telecommuting is permitted within the U.S.",
        "industries": [
            "Technology",
            "Software Development",
            "Telecommunications",
            "Security",
            "Safety"
        ],
        "soft_skills": [
            "Communication",
            "Problem Solving",
            "Collaboration",
            "Analytical Thinking",
            "Time Management",
            "Teamwork",
            "Documentation",
            "Research",
            "Creativity",
            "Detail-Oriented"
        ],
        "hard_skills": [
            "Software Development",
            "Coding",
            "Testing",
            "Research",
            "Programming",
            "Documentation",
            "Modeling",
            "Designing",
            "Structured Methods",
            "Design Documentation",
            "Unit Testing",
            "Integration Testing",
            "System Testing",
            "Regression Testing",
            "Performance Testing",
            "Load Testing",
            "Acceptance Testing",
            "Test Plans",
            "Test Scripts",
            "Software System Testing",
            "Requirements Analysis",
            "Feasibility Analysis",
            "Quality Assurance"
        ],
        "tech_stack": [],
        "programming_languages": [],
        "experience": 3,
        "education": {
            "min_degree": "Bachelors",
            "fields": [
                "Computer Science",
                "Software Engineering",
                "Information Technology"
            ]
        },
        "salary": {
            "max": 147000,
            "min": 102690
        },
        "benefits": [
            "Incentive Bonus Plans",
            "Medical",
            "Dental",
            "Vision",
            "401K with Company Match",
            "9 Paid Holidays",
            "Generous Paid Time Off Packages",
            "Employee Stock Purchase Plan",
            "Paid Parental & Family Leave"
        ]
    },
    {
        "task_id": "da06c365d04b4f01b8718c478e76defc",
        "keyword": "Data Engineer",
        "location": "Chicago, IL",
        "job_id": 3871172744,
        "company": "Optiver",
        "title": "Production Software Engineer",
        "created_on": 1720635195.5665996,
        "description": "Optiver is looking for an experienced Production Software Engineer with responsibilities for building our infrastructure and automating our trading production environment. This team occupies a wide intersection of stakeholders across the business from operations to development, specifically focusing on scaling up change, increasing robustness, and reducing overhead. Your contributions have the potential to directly enable a faster and safer innovation cycle for Optiver. Who we are: Optiver is a tech-driven trading firm and leading global market maker. As one of the oldest market making institutions, we are a trusted partner of 70+ exchanges across the globe. Our mission is to constantly improve the market by injecting liquidity, providing accurate pricing, increasing transparency and acting as a stabilising force no matter the market conditions. With a focus on continuous improvement, we participate in the safeguarding of healthy and efficient markets for everyone who participates. Optiver Chicago’s culture parallels the energetic city we’re immersed in. Home to the Chicago Board of Trade, Chicago Board Options Exchange, and Chicago Mercantile Exchange, Chicago has established itself as the trading capital of the US. Active on leading US exchanges, Optiver’s Chicago office trades a wide range of products from listed derivatives to cash equities, ETFs, bonds and foreign exchange. What you’ll do: Design pipelines to improve the way changes enter production, from validation to merging to deployment Build production environment monitoring infrastructure, focusing on incident management and operations visibility Build tooling or infrastructure to reduce overhead and increase reliability through the production management space Work with stakeholders to establish or improve feedback loops across all aspects of production management Work collaboratively with engineers in our global offices to align our production environments Mentor junior team members and help foster the growth of the next generation of engineers Who you are: Bachelor’s degree in CS or related technical field Strong experience with programming (Python would be beneficial) Broad understanding of the wider technology including networks, hardware, OS, databases, web servers and applications Experience with CI/CD tooling to assist with production change pipelines This team currently utilizes Ansible, Django, Flask and Jenkins so any previous experience with these technologies would be beneficial but not required Previous experience in production management would be beneficial Ability to bring an improvement mindset and critical eye to complex operational challenges and systems Specific knowledge of the trading/finance industry is not required What you’ll get: Work alongside best-in-class professionals from over 40 different countries. Performance based bonus structure that is unmatched anywhere in the industry. We combine our profits across desks, teams and offices into a global profit pool fostering a truly collaborative environment to work in. Ownership over initiatives that directly solve business problems. Alongside this you will get great other benefits such as 25 paid vacation days and market holidays, fully paid health insurance, daily breakfast and lunch, training opportunities, 401(k) match up to 50% and charitable match opportunities, regular social events and clubs, and many more. At Optiver, we are committed to creating a diverse and inclusive environment of mutual respect. Optiver recruits, employs, trains, compensates and promotes regardless of race, religion, color, sex, gender identity, sexual orientation, age, physical or mental disability, or other legally protected characteristics.",
        "url": "https://www.linkedin.com/jobs/view/3871172744",
        "summary": "Optiver is seeking an experienced Production Software Engineer to build and automate their trading production environment, focusing on scaling change, increasing robustness, and reducing overhead. The role involves designing pipelines for production changes, building monitoring infrastructure, developing tools for reliability, collaborating with global teams, and mentoring junior engineers. Prior experience with Python, CI/CD tooling, and technologies like Ansible, Django, Flask, and Jenkins is beneficial.  The position offers competitive compensation, benefits, and opportunities for professional growth.",
        "industries": [
            "Financial Services",
            "Technology",
            "Trading",
            "Market Making"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Critical Thinking",
            "Teamwork",
            "Mentorship",
            "Improvement Mindset"
        ],
        "hard_skills": [
            "Python",
            "Networks",
            "Hardware",
            "Operating Systems",
            "Databases",
            "Web Servers",
            "Applications",
            "CI/CD",
            "Ansible",
            "Django",
            "Flask",
            "Jenkins"
        ],
        "tech_stack": [
            "Python",
            "Ansible",
            "Django",
            "Flask",
            "Jenkins"
        ],
        "programming_languages": [
            "Python"
        ],
        "experience": 0,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Related Technical Field"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Performance-Based Bonus",
            "Profit Sharing",
            "Ownership of Initiatives",
            "25 Paid Vacation Days",
            "Market Holidays",
            "Fully Paid Health Insurance",
            "Daily Breakfast and Lunch",
            "Training Opportunities",
            "401(k) Match",
            "Charitable Match",
            "Social Events",
            "Clubs"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Los Gatos, CA",
        "job_id": 3942358726,
        "company": "Netflix",
        "title": "Data Engineer (L5) - Customer Service",
        "created_on": 1720635201.2843935,
        "description": "Netflix is revolutionizing entertainment and shaping the evolution of storytelling around the world with over 260 million members in 190 countries. We are focused on delivering an incredible customer experience, and when a Netflix member has a problem using Netflix, we want to quickly and thoroughly resolve their issue and get them back to streaming. Customer Service is one of the few areas where we hear directly from our members, and we engage with millions of them via phone and chat in 25 different languages. Within Data Science and Engineering (DSE), we use data to rigorously measure, optimize, and personalize our Customer Service investments to enable a delightful, efficient experience and to improve our product based on what our members tell us. In this role, you’ll collaborate with data engineers, software engineers, and CS business owners to build reliable, scalable data pipelines, transform raw data into core business metrics, and work with custom visualization tools that surface CS insights. The team is small, so the role requires a high degree of autonomy, strong technical judgment, and the ability to span across the data stack, from pipeline development to business metrics and tools. We’re Looking For At least 4 years of experience engineering data pipelines using big data technologies (Hive, Presto, Spark, Flink) on medium to large-scale data sets (10s of millions of records) Strong data modeling skills. You design structures that are adaptable to changes in the source data or business processes Strong proficiency in Python or Scala and passion for writing clean, supportable code Advanced SQL skills and advocacy for data quality Analytical aptitude to think through business problems (e.g. metrics development) and prioritize autonomously. Strong communication and productive partnership with analysts, engineers, business leaders. Netflix culture Netflix's culture is an integral part of what makes us successful, and we approach diversity and inclusion seriously and thoughtfully. We are an equal-opportunity employer and celebrate diversity, recognizing that bringing together different perspectives and backgrounds helps build stronger teams. Our compensation structure consists solely of an annual salary; we do not have bonuses. You choose each year how much of your compensation you want in salary versus stock options. To determine your personal top of market compensation, we rely on market indicators and consider your specific job family, background, skills, and experience to determine your compensation in the market range. The range for this role is $170,000 - $720,000. Netflix provides comprehensive benefits including Health Plans, Mental Health support, a 401(k) Retirement Plan with employer match, Stock Option Program, Disability Programs, Health Savings and Flexible Spending Accounts, Family-forming benefits, and Life and Serious Injury Benefits. We also offer paid leave of absence programs. Full-time hourly employees accrue 35 days annually for paid time off to be used for vacation, holidays, and sick paid time off. Full-time salaried employees are immediately entitled to flexible time off. See more details about our Benefits here. Netflix is a unique culture and environment. Learn more here. We are an equal-opportunity employer and celebrate diversity, recognizing that diversity of thought and background builds stronger teams. We approach diversity and inclusion seriously and thoughtfully. We do not discriminate on the basis of race, religion, color, ancestry, national origin, caste, sex, sexual orientation, gender, gender identity or expression, age, disability, medical condition, pregnancy, genetic makeup, marital status, or military service.",
        "url": "https://www.linkedin.com/jobs/view/3942358726",
        "summary": "Netflix is seeking a Data Engineer with 4+ years of experience building data pipelines using technologies like Hive, Presto, Spark, and Flink. This role involves collaborating with data engineers, software engineers, and customer service (CS) business owners to develop scalable data pipelines, transform raw data into key metrics, and create custom visualizations to gain CS insights. Strong data modeling skills, proficiency in Python or Scala, advanced SQL skills, analytical aptitude, and excellent communication are essential. The role offers a competitive salary range of $170,000 - $720,000 and a comprehensive benefits package.",
        "industries": [
            "Entertainment",
            "Media Streaming",
            "Technology",
            "Data Science",
            "Customer Service"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Autonomy",
            "Analytical Thinking",
            "Problem Solving",
            "Prioritization",
            "Partnership"
        ],
        "hard_skills": [
            "Hive",
            "Presto",
            "Spark",
            "Flink",
            "Data Modeling",
            "Python",
            "Scala",
            "SQL",
            "Data Quality",
            "Metrics Development"
        ],
        "tech_stack": [
            "Hive",
            "Presto",
            "Spark",
            "Flink",
            "Python",
            "Scala",
            "SQL"
        ],
        "programming_languages": [
            "Python",
            "Scala"
        ],
        "experience": 4,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 720000,
            "min": 170000
        },
        "benefits": [
            "Health Plans",
            "Mental Health Support",
            "401(k) Retirement Plan with employer match",
            "Stock Option Program",
            "Disability Programs",
            "Health Savings and Flexible Spending Accounts",
            "Family-forming benefits",
            "Life and Serious Injury Benefits",
            "Paid leave of absence programs",
            "Paid time off",
            "Flexible time off"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Sunnyvale, CA",
        "job_id": 3951532647,
        "company": "LinkedIn",
        "title": "Senior Data Engineer - Data Science",
        "created_on": 1720635202.9835436,
        "description": "LinkedIn is the world’s largest professional network, built to help members of all backgrounds and experiences achieve more in their careers. Our vision is to create economic opportunity for every member of the global workforce. Every day our members use our products to make connections, discover opportunities, build skills and gain insights. We believe amazing things happen when we work together in an environment where everyone feels a true sense of belonging, and that what matters most in a candidate is having the skills needed to succeed. It inspires us to invest in our talent and support career growth. Join us to challenge yourself with work that matters. This role will be based in Sunnyvale or San Francisco. At LinkedIn, we trust each other to do our best work where it works best for us and our teams. This role offers a hybrid work option, meaning you can work from home and commute to a LinkedIn office, depending on what’s best for you and when it is important for your team to be together. LinkedIn’s Data Science team leverages big data to empower business decisions and deliver data-driven insights, metrics, and tools in order to drive member engagement, business growth, and monetization efforts. With over 800 million members around the world, a focus on great user experience, and a mix of B2B and B2C programs, LinkedIn offers countless ways for an ambitious data engineer to have an impact and transform your career. We are now looking for a talented and driven individual to accelerate our efforts and be a major part of our data-centric culture. This person will work closely with various cross-functional teams such as product, marketing, sales, engineering, and operations to develop infrastructure and deliver tools or data structures that enable data-driven decision-making. Successful candidates will exhibit technical acumen and business savviness with a passion for making an impact by enabling both producers and consumers of data insight to work smarter. Responsibilities: ● Work with a team of high-performing data science professionals, and cross-functional teams to identify business opportunities and build scalable data solutions. ● Build data expertise, act like an owner for the company and manage complex data systems for a product or a group of products. ● Perform all of the necessary data transformations to serve products that empower data-driven decision making. ● Build and manage data pipelines, design and architect databases. ● Establish efficient design and programming patterns for engineers as well as for non-technical partners. ● Design, implement, integrate and document performant systems or components for data flows or applications that power analysis at a massive scale. ● Ensure best practices and standards in our data ecosystem are shared across teams. ● Understand the analytical objectives to make logical recommendations and drive informed actions. ● Engage with internal platform teams to prototype and validate tools developed in-house to derive insight from very large datasets or automate complex algorithms. ● Be a self-starter, Initiate and drive projects to completion with minimal guidance. ● Contribute to engineering innovations that fuel LinkedIn’s vision and mission. Basic Qualifications: ● Bachelor's Degree in a quantitative discipline: Computer science, Statistics, Operations Research, Informatics, Engineering, Applied Mathematics, Economics, etc. ● 3+ years of relevant industry or relevant academia experience working with large amounts of data ● Experience with SQL/Relational databases ● Background in at least one programming languages (e.g., R, Python, Java, Scala, PHP, JavaScript) Preferred Qualifications: ● BS and 5+ years of relevant work experience, MS and 3+ years of relevant work experience, or Ph.D. and 1+ years of relevant work/academia experience working with large amounts of data ● MS or PhD in a quantitative discipline: statistics, operations research, computer science, informatics, engineering, applied mathematics, economics, etc. ● Experience in developing data pipelines using Spark and Hive. ● Experience with data modeling, ETL (Extraction, Transformation & Load) concepts, and patterns for efficient data governance. Experience with manipulating massive-scale structured and unstructured data. ● Experience with using distributed data systems such as Spark and related technologies (Presto/Trino, Hive, etc.). ● Experience with either data workflows/modeling, front-end engineering, or back-end engineering. ● Deep understanding of technical and functional designs for relational and MPP Databases ● Experience in data visualization and dashboard design including tools such as Tableau, R visualization packages, streamlit, D3, and other libraries, etc. ● Knowledge of Unix and Unix-like systems, version control systems such as Git. Suggested Skills: ● Distributed Systems ● ETL ● Data Modeling You will Benefit from our Culture: We strongly believe in the well-being of our employees and their families. That is why we offer generous health and wellness programs and time away for employees of all levels LinkedIn is committed to fair and equitable compensation practices. The pay range for this role is $117,000.00 to $192,000.00 Actual compensation packages are based on several factors that are unique to each candidate, including but not limited to skill set, depth of experience, certifications, and specific work location. This may be different in other locations due to differences in the cost of labor. The total compensation package for this position may also include annual performance bonus, stock, benefits and/or other applicable incentive compensation plans. For more information, visit https://careers.linkedin.com/benefits. Equal Opportunity Statement LinkedIn is committed to diversity in its workforce and is proud to be an equal opportunity employer. LinkedIn considers qualified applicants without regard to race, color, religion, creed, gender, national origin, age, disability, veteran status, marital status, pregnancy, sex, gender expression or identity, sexual orientation, citizenship, or any other legally protected class. LinkedIn is an Affirmative Action and Equal Opportunity Employer as described in our equal opportunity statement here: https://microsoft.sharepoint.com/:b:/t/LinkedInGCI/EeE8sk7CTIdFmEp9ONzFOTEBM62TPrWLMHs4J1C_QxVTbg?e=5hfhpE. Please reference https://www.eeoc.gov/sites/default/files/2023-06/22-088_EEOC_KnowYourRights6.12ScreenRdr.pdf and https://www.dol.gov/ofccp/regs/compliance/posters/pdf/OFCCP_EEO_Supplement_Final_JRF_QA_508c.pdf for more information. LinkedIn is committed to offering an inclusive and accessible experience for all job seekers, including individuals with disabilities. Our goal is to foster an inclusive and accessible workplace where everyone has the opportunity to be successful. If you need a reasonable accommodation to search for a job opening, apply for a position, or participate in the interview process, connect with us at accommodations@linkedin.com and describe the specific accommodation requested for a disability-related limitation. Reasonable accommodations are modifications or adjustments to the application or hiring process that would enable you to fully participate in that process. Examples of reasonable accommodations include but are not limited to: -Documents in alternate formats or read aloud to you -Having interviews in an accessible location -Being accompanied by a service dog -Having a sign language interpreter present for the interview A request for an accommodation will be responded to within three business days. However, non-disability related requests, such as following up on an application, will not receive a response. LinkedIn will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by LinkedIn, or (c) consistent with LinkedIn's legal duty to furnish information. Pay Transparency Policy Statement As a federal contractor, LinkedIn follows the Pay Transparency and non-discrimination provisions described at this link: https://lnkd.in/paytransparency. Global Data Privacy Notice for Job Candidates This document provides transparency around the way in which LinkedIn handles personal data of employees and job applicants: https://lnkd.in/GlobalDataPrivacyNotice",
        "url": "https://www.linkedin.com/jobs/view/3951532647",
        "summary": "LinkedIn is looking for a Data Engineer to join their team in Sunnyvale or San Francisco. This role involves building and managing data pipelines, designing and architecting databases, and ensuring data-driven decision making. The ideal candidate will have experience with large amounts of data, SQL/Relational databases, and programming languages like Python, Java, Scala, or R. They will also have experience with data pipelines using Spark and Hive, data modeling, ETL, and distributed data systems. This is a hybrid role, offering a mix of in-office and remote work. ",
        "industries": [
            "Technology",
            "Data Science",
            "Engineering",
            "Software Development",
            "Social Media",
            "Recruitment",
            "Human Resources",
            "Business Intelligence"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Analytical Thinking",
            "Decision Making",
            "Leadership",
            "Self-Motivation",
            "Teamwork",
            "Time Management",
            "Organization",
            "Project Management",
            "Detail Oriented",
            "Results Oriented"
        ],
        "hard_skills": [
            "SQL",
            "Relational Databases",
            "Python",
            "Java",
            "Scala",
            "R",
            "Spark",
            "Hive",
            "ETL",
            "Data Modeling",
            "Data Pipelines",
            "Distributed Data Systems",
            "Presto/Trino",
            "Tableau",
            "R Visualization Packages",
            "Streamlit",
            "D3",
            "Unix",
            "Git",
            "Data Visualization",
            "Dashboard Design"
        ],
        "tech_stack": [
            "SQL",
            "Relational Databases",
            "Python",
            "Java",
            "Scala",
            "R",
            "Spark",
            "Hive",
            "ETL",
            "Data Modeling",
            "Data Pipelines",
            "Distributed Data Systems",
            "Presto/Trino",
            "Tableau",
            "R Visualization Packages",
            "Streamlit",
            "D3",
            "Unix",
            "Git"
        ],
        "programming_languages": [
            "SQL",
            "Python",
            "Java",
            "Scala",
            "R",
            "PHP",
            "JavaScript"
        ],
        "experience": 3,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Statistics",
                "Operations Research",
                "Informatics",
                "Engineering",
                "Applied Mathematics",
                "Economics"
            ]
        },
        "salary": {
            "max": 192000,
            "min": 117000
        },
        "benefits": [
            "Health and wellness programs",
            "Performance bonus",
            "Stock",
            "Other incentive compensation plans"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Santa Monica, CA",
        "job_id": 3959730905,
        "company": "The Walt Disney Company",
        "title": "Data Engineer II",
        "created_on": 1720635204.6449058,
        "description": "Disney Entertainment & ESPN Technology On any given day at Disney Entertainment & ESPN Technology, we’re reimagining ways to create magical viewing experiences for the world’s most beloved stories while also transforming Disney’s media business for the future. Whether that’s evolving our streaming and digital products in new and immersive ways, powering worldwide advertising and distribution to maximize flexibility and efficiency, or delivering Disney’s unmatched entertainment and sports content, every day is a moment to make a difference to partners and to hundreds of millions of people around the world. A few reasons why we think you’d love working for Disney Entertainment & ESPN Technology Building the future of Disney’s media business: DE&E Technologists are designing and building the infrastructure that will power Disney’s media, advertising, and distribution businesses for years to come. Reach & Scale: The products and platforms this group builds and operates delight millions of consumers every minute of every day – from Disney+ and Hulu, to ABC News and Entertainment, to ESPN and ESPN+, and much more. Innovation: We develop and execute groundbreaking products and techniques that shape industry norms and enhance how audiences experience sports, entertainment & news. Job Summary The Product & Data Engineering team is responsible for end to end development for Disney’s world-class consumer-facing products, including streaming platforms Disney+, Hulu, and ESPN+, and digital products & experiences across ESPN, Marvel, Disney Studios, NatGeo, and ABC News. The team drives innovation at scale for millions of consumers around the world across Apple, Android, Smart TVs, game consoles, and the web, with our platforms powering core experiences like personalization, search, messaging and data. The Data Engineer II will contribute to the Company’s success by partnering with business, analytics and infrastructure teams to design and build data pipelines to facilitate measuring subscriber movements and metrics. Collaborating across disciplines, they will identify internal/external data sources, design table structure, define ETL strategy & automated Data Quality checks. Responsibilities Partner with technical and non-technical colleagues to understand data and reporting requirements. Work with engineering teams to collect required data from internal and external systems. Contribute to the design of table structures and defining ETL pipelines to build performant Data solutions that are reliable and scalable in a fast growing data ecosystem. Develop Data Quality checks, write code, complete programming, write tests, perform testing and debug code. Develop and maintain ETL routines using ETL and orchestration tools such as Airflow Implement database deployments using tools like Schema Change Perform ad hoc analysis as necessary. Perform SQL and ETL tuning as necessary. Basic Qualifications 3+ years of relevant data engineering experience. Good understanding of data modeling principles including Dimensional modeling, data normalization principles. Good understanding of SQL Engines and able to conduct advanced performance tuning. Ability to think strategically, analyze and interpret market and consumer information. Strong communication skills – written and verbal presentations. Strong conceptual and analytical reasoning competencies. Comfortable working in a fast-paced and highly collaborative environment. Familiarity with Agile Scrum principles and ceremonies Preferred Qualifications 2+ years of work experience implementing and reporting on business key performance indicators in data warehousing environments, required. 2+ years of experience using analytic SQL, working with traditional relational databases and/or distributed systems (Snowflake or Redshift), required. 1+ years of experience programming languages (e.g. Python, Pyspark), preferred. 1+ years of experience with data orchestration/ETL tools (Airflow, Nifi), preferred. Experience with Snowflake, Databricks/EMR/Spark, and/or Airflow. Required Education Bachelor’s Degree in computer science, information systems, or related field or equivalent work experience. Additional Information #DISNEYTECH The hiring range for this position in Santa Monica, CA is $102,500.00 to $137,500.00 per year, in San Francisco, CA is $112,300.00 to $150,600.00 per year, in Seattle, WA is $107,400.00 to $144,000.00 per year, and in New York, NY is $107,400.00 to $144,000.00 per year. The base pay actually offered will take into account internal equity and also may vary depending on the candidate’s geographic region, job-related knowledge, skills, and experience among other factors. A bonus and/or long-term incentive units may be provided as part of the compensation package, in addition to the full range of medical, financial, and/or other benefits, dependent on the level and position offered.",
        "url": "https://www.linkedin.com/jobs/view/3959730905",
        "summary": "Disney Entertainment & ESPN Technology seeks a Data Engineer II to design and build data pipelines for measuring subscriber movements and metrics. This role involves collaborating with teams to collect data, designing table structures, defining ETL strategies, and developing data quality checks. The ideal candidate will have 3+ years of relevant experience, strong SQL skills, and experience with data orchestration tools like Airflow.",
        "industries": [
            "Media & Entertainment",
            "Streaming Services",
            "Technology",
            "Data Analytics"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Strategic Thinking",
            "Problem Solving",
            "Analytical Reasoning"
        ],
        "hard_skills": [
            "SQL",
            "ETL",
            "Data Modeling",
            "Data Warehousing",
            "Data Quality",
            "Performance Tuning",
            "Agile Scrum"
        ],
        "tech_stack": [
            "Airflow",
            "Snowflake",
            "Redshift",
            "Python",
            "Pyspark",
            "Databricks",
            "EMR",
            "Spark",
            "Schema Change"
        ],
        "programming_languages": [
            "Python",
            "Pyspark",
            "SQL"
        ],
        "experience": 3,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Information Systems"
            ]
        },
        "salary": {
            "max": 150600,
            "min": 102500
        },
        "benefits": [
            "Medical",
            "Financial",
            "Long-term Incentive Units"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3968575937,
        "company": "Ashby",
        "title": "Data Engineer",
        "created_on": 1720635208.8778722,
        "description": "About Ashby We’re building the next generation of recruiting software and we’re starting with a suite of products that helps talent leaders, recruiters, and hiring managers run an efficient and data-driven hiring process. We are well-funded and backed by great investors, including Y Combinator, Elad Gil and Lachy Groom. We have over 1,300 amazing customers including Ramp, Notion, and Zapier – yet we've only taken the first steps toward a much larger opportunity. In short, it's the perfect time to join! 🚀 About This Role As a data engineer, you will be responsible for building the foundations of all things data at Ashby. At the foundation, your work will include managing our Snowflake data warehouse, the associated data ingestion, and data modeling infrastructure. Upon that foundation, you will work closely with our product and go-to-market teams to help operationalize data, analysis, and reporting across the organization. Data Culture at Ashby At Ashby, we draw a clear distinction between when to look to data for \"the answer\" and when to use data to aid principled thinking for decision making. In general, a primary consideration is whether or not data is needed to help clarify points of uncertainty. When data is considered necessary, a secondary consideration is to what depth data will be required. Data questions lend themselves to ongoing what about... or what if... explorations, which can be important in some situations but are not always necessary. With these dynamics in mind, we do not foster an environment where every decision we make should be backed by data, but we do foster an environment where teammates ask whether or not data will fit the task at hand well. This, in turn, allows us to operate a lean team focused on projects where data provides true leverage to our overall business and product goals. As a related but distinct point, Ashby’s data culture as far as transparency and access goes is very open. We share company-wide access to our financial standings, progress, and goals and generally make any data and reporting we do consider valuable broadly available. Outside of constraints related to whether a teammate has access to a particular tool, there are close to no constraints on access the state of the company as told by data or reporting. In general, we would like to continue and extend open access to data throughout the company (with your help!), but with consideration and not as a substitute for principled thinking :) Requirements Expertise in data modeling and data warehousing. Ideally, deep experience and familiarity with dbt, Snowflake and AWS or similar cloud providers Proficiency with data orchestration and workflow management tooling (we use Prefect, but prior experience with that tool isn’t a hard requirement) Familiarity with standard data integration services (e.g. Fivetran) Strong proficiency in SQL and Python Experience with custom ETL and “reverse ETL” development using, e.g. REST APIs and webhooks Experience modeling and working with product analytics events Strong analytical thinking and familiarity with data visualization (i.e. visual communication of results) - relates to autonomy of role Excellent written and verbal communication. Ashby has a writing-centric culture, but you will also be responsible for direct collaboration with various teammates Interest in working directly with business partners, e.g. product or go-to-market teammates, to identify and plan related data efforts You could be a great fit if You have enjoyed experience as the primary or sole data engineer with full ownership of the data stack You have working experience in B2B SaaS You're product-minded and analytically curious, eager to explore business and product data You have an intuition for solving problems pragmatically and an eye for leverage You enjoy collaboration and find reward in enabling other teammates to succeed You have strong, experience-backed opinions about data products and data culture in a company setting Reasons not to apply You do not enjoy working directly with analysts or other business stakeholders. This will mean working closely and in earnest with teammates across the company as various data-related needs come up. You are looking for consumer-scale data infrastructure projects. We have a huge and interesting set of data to work with (check out our data trends reports!), but we are a B2B SaaS company. Relative to B2C services, we might best be considered small but mighty :) You are seeking an in-person work experience. Although there are hot spots of Ashby employees around the world, Ashby is a remote workplace. Rest assured, we do meet up in person for various events throughout the year! Ashby's Data Stack As it will undoubtedly be a point of interest, a quick summary of our data stack is provided here. Our production database is postgres, which we integrate into a Snowflake data warehouse via a custom data pipeline. All other business systems are integrated via Fivetran. We use Prefect for orchestration and dbt for data modeling. Frontend event tracking is done via Rudderstack. Data visualization and reporting varies, but the broader company is primarily served standardized analytics via Looker. For version control we use Git/Github (and Github Actions for CI/CD). Interview Process Our interview process is thorough — we aim to ensure each person that joins the team is the right fit for Ashby and will provide ample information for you to assess if Ashby is the right fit for you. The process for this role is as follows: Recruiter Screen - 30 min Hiring Manager Interview - 45 min Data Engineering Take Home Final Round - about 2.5 hours (can do multi-day) Benefits Competitive salary and equity. 10-year exercise window for stock options. You shouldn’t feel pressure to purchase stock options if you leave Ashby —do it when you feel financially comfortable. Unlimited PTO with four weeks recommended per year. Expect “Vacation?” in our one-on-one agenda until you start taking it 😅. Twelve weeks of fully paid family leave in the US. We plan to expand this to employees in other countries as situations arise. Generous equipment, software, and office furniture budget. Get what you need to be happy and productive! $100/month education budget with more expensive items (like conferences) covered with manager approval. If you’re in the US, top-notch health insurance for you and your dependents with all premiums covered by us. Ashby’s success hinges on hiring great people and creating an environment where we can be happy, feel challenged, and do our best work. We’re being deliberate about building that environment from the ground up. I hope that excites you enough to apply. Compensation Range: $160K - $185K",
        "url": "https://www.linkedin.com/jobs/view/3968575937",
        "summary": "Ashby is seeking a Data Engineer to build and manage their Snowflake data warehouse, data ingestion, and data modeling infrastructure. The role will involve collaborating with product and go-to-market teams to operationalize data, analysis, and reporting.  The ideal candidate has experience with dbt, Snowflake, AWS, Prefect, data integration services, SQL, Python, ETL/reverse ETL, product analytics, data visualization, and strong communication skills.  Ashby offers competitive salary and equity, unlimited PTO, family leave, equipment/software budget, education budget, and health insurance.",
        "industries": [
            "Software",
            "Technology",
            "SaaS",
            "Data"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Analytical thinking",
            "Problem-solving",
            "Product-minded",
            "Curious",
            "Pragmatic",
            "Autonomy",
            "Data-driven"
        ],
        "hard_skills": [
            "Data modeling",
            "Data warehousing",
            "dbt",
            "Snowflake",
            "AWS",
            "Data orchestration",
            "Workflow management",
            "Prefect",
            "Data integration",
            "Fivetran",
            "SQL",
            "Python",
            "ETL",
            "Reverse ETL",
            "REST APIs",
            "Webhooks",
            "Product analytics",
            "Data visualization",
            "Git",
            "Github",
            "Github Actions"
        ],
        "tech_stack": [
            "Snowflake",
            "dbt",
            "Prefect",
            "Fivetran",
            "Rudderstack",
            "Looker",
            "Git",
            "Github",
            "Github Actions",
            "Postgres",
            "AWS"
        ],
        "programming_languages": [
            "SQL",
            "Python"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 185000,
            "min": 160000
        },
        "benefits": [
            "Competitive salary",
            "Equity",
            "10-year exercise window for stock options",
            "Unlimited PTO",
            "Four weeks recommended PTO per year",
            "Twelve weeks of fully paid family leave (US)",
            "Generous equipment, software, and office furniture budget",
            "$100/month education budget",
            "Top-notch health insurance (US)"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Los Gatos, CA",
        "job_id": 3934964843,
        "company": "Netflix",
        "title": "Analytics Engineer (L5) - Regional Understanding",
        "created_on": 1720635210.8734126,
        "description": "Netflix is one of the world's leading entertainment services, with 270 million paid memberships in over 190 countries, enjoying TV series, films, and games across a wide variety of genres and languages. Members can play, pause, and resume watching as much as they want, anytime, anywhere, and can change their plans at any time. In this role, you will dive into different regions and countries to holistically answer questions like “Who are our members?”, “How satisfied are current members with our service?” and “How do the needs and expectations of non-members differ from those of members?” The questions you answer will help shape how senior leadership thinks about opportunities and challenges in these different regions, and enable us to evaluate and determine how our country-level strategies are performing. As a Senior Analytics Engineer, you will be responsible for developing and maintaining data pipelines, conducting in-depth analyses, and developing tools and products to create a single source of truth around Netflix's greatest consumer-facing opportunities and challenges—at both a company and country level. The ideal candidate will excel in data analytics, storytelling with data, cross-functional collaboration, clear communication with executive stakeholders, and share a passion for continuously improving the way we use data to make Netflix better. To learn more about analytics engineering at Netflix, read here. In This Role, You Will Become an expert in country-level performance by diving deep into our data to understand both our members and non-members. Spearheaded research to understand the consumer-facing opportunities and challenges facing Netflix. Scale these insights—Netflix is a global company, and we need to be able to efficiently zoom in on different regions throughout the world and ladder this up to a global view Share your learnings with high-level leaders in a way that is digestible, actionable, and clear. To Be Successful In This Role, You Have At least 5 years of experience as an analytics professional. Expertise in SQL, programming skills (e.g. Python, Scala), and some exposure to ETL and data warehousing concepts. A proven track record of data analysis, reporting and visualization (e.g. Tableau, D3). Strong communication with the ability to build meaningful stakeholder relationships. Enthusiasm for creative thinking and innovation in a fast-paced data and analytics space. Excitement to learn about new fields, with the ability to be scrappy as needed. Comfort with ambiguity; able to thrive with minimal oversight and process. Our compensation structure consists solely of an annual salary; we do not have bonuses. You choose each year how much of your compensation you want in salary versus stock options. To determine your personal top of market compensation, we rely on market indicators and consider your specific job family, background, skills, and experience to determine your compensation in the market range. The range for this role is $170,000 - $720,000. Netflix provides comprehensive benefits including Health Plans, Mental Health support, a 401(k) Retirement Plan with employer match, Stock Option Program, Disability Programs, Health Savings and Flexible Spending Accounts, Family-forming benefits, and Life and Serious Injury Benefits. We also offer paid leave of absence programs. Full-time hourly employees accrue 35 days annually for paid time off to be used for vacation, holidays, and sick paid time off. Full-time salaried employees are immediately entitled to flexible time off. See more detail about our Benefits here. Netflix is a unique culture and environment. Learn more here. We are an equal-opportunity employer and celebrate diversity, recognizing that diversity of thought and background builds stronger teams. We approach diversity and inclusion seriously and thoughtfully. We do not discriminate on the basis of race, religion, color, ancestry, national origin, caste, sex, sexual orientation, gender, gender identity or expression, age, disability, medical condition, pregnancy, genetic makeup, marital status, or military service.",
        "url": "https://www.linkedin.com/jobs/view/3934964843",
        "summary": "Netflix seeks a Senior Analytics Engineer to conduct in-depth analyses, build data pipelines, and develop tools for understanding member and non-member demographics, satisfaction, and needs across different regions.  This role involves translating insights into actionable strategies and communicating them to senior leadership. The ideal candidate is adept at data analytics, storytelling, and cross-functional collaboration, with a passion for continuous improvement.",
        "industries": [
            "Entertainment",
            "Media",
            "Streaming",
            "Technology",
            "Data Analytics"
        ],
        "soft_skills": [
            "Communication",
            "Data Storytelling",
            "Cross-Functional Collaboration",
            "Stakeholder Management",
            "Creative Thinking",
            "Innovation",
            "Ambiguity Tolerance",
            "Problem Solving"
        ],
        "hard_skills": [
            "SQL",
            "Python",
            "Scala",
            "ETL",
            "Data Warehousing",
            "Data Analysis",
            "Reporting",
            "Visualization",
            "Tableau",
            "D3"
        ],
        "tech_stack": [
            "SQL",
            "Python",
            "Scala",
            "ETL",
            "Data Warehousing",
            "Tableau",
            "D3"
        ],
        "programming_languages": [
            "SQL",
            "Python",
            "Scala"
        ],
        "experience": 5,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 720000,
            "min": 170000
        },
        "benefits": [
            "Health Plans",
            "Mental Health support",
            "401(k) Retirement Plan with employer match",
            "Stock Option Program",
            "Disability Programs",
            "Health Savings and Flexible Spending Accounts",
            "Family-forming benefits",
            "Life and Serious Injury Benefits",
            "Paid leave of absence",
            "Paid Time Off",
            "Flexible Time Off"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Santa Monica, CA",
        "job_id": 3886782670,
        "company": "Edmunds",
        "title": "Data Engineer",
        "created_on": 1720635212.6406994,
        "description": "Edmunds offers flexibility to work fully remote, from our Edquarters, or a combination of both At Edmunds we’re driven to make car buying easier. Ever since we began publishing printed car guides in the 60’s, the company has been in the business of trust, innovating ways to empower and support car shoppers. When Edmunds launched the car industry’s first Internet site in 1994, we established a leadership position online and have never looked back. Now, as one of the most trusted review sites on the Internet, millions of visitors use our research, shopping and buying tools every month to make an easy and informed decision on their next car. For consumers, we bring peace of mind. For dealers, we make tools to help them solve their problems and sell more cars. How do we do it, you ask? The key ingredients are our enthusiastic employees, progressive company culture and cutting-edge technology. Want to join the team? Read on to find out how! What You’re Applying For Edmunds is looking for a Data Engineer to help us manage the data explosion dilemma! You will be joining a strong, determined and results-oriented Data Engineering team that is transforming Edmunds from IT to real time data-driven. You will get hands-on experience solving complex data problems using Big Data frameworks and methodologies. You’ll be helping the company make real time decisions based on the torrent of data it receives, empowering analytics on existing products as well as providing insights on potential new opportunities for growth. What You’ll Do Create and maintain scalable, maintainable and reliable data pipelines that process very large quantities of structured and unstructured data in both batch and real time. Enhance and maintain the data lakehouse that powers the core of the company’s decision making process. Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. Collaborate with team members with a goal of improving personal knowledge of the systems, ensuring that code changes meet business goals and technology best practices. Regularly interact and collaborate with colleagues across functions and teams. Work with stakeholders including the Executive, Product, and Data teams to assist with data-related technical issues and support their data infrastructure needs. What You Need Excellent problem solving, troubleshooting, and communication skills especially in a hybrid and remote environment. Desire to learn new technologies. Demonstrated ability to design and write maintainable software. Understanding of software engineering best practices, object oriented analysis & design, and design patterns & algorithms. Experience enhancing and evolving existing systems. Almost all of our codebase is in Scala and Python, but we only require that you have a high proficiency in at least one object oriented or functional programming language. A strong candidate will also have: Experience writing ETL Jobs and working with data at scale. Experience writing and maintaining real time / streaming data pipelines. Familiarity with some of the following: Spark, Scala, Python, AWS, Databricks, Airflow. Fluency in SQL Other nice to haves: Kubernetes, Machine Learning. The compensation range for this position is $109,900 - $191,815 per year. The base pay will take into account internal equity as well as job-related knowledge, skills, and experience among other factors. In addition, Edmunds offers full-time employees a comprehensive total rewards package including the benefits listed below. Edmunds Perks Flexible time off 13 Paid Holidays Comprehensive Health Benefits (medical, dental, vision, life and disability) Flexible Spending Accounts (Employees) and Health Savings Accounts (Employee and Employer Contributions) 401K Plan with company matching at 100%, up to 6% of eligible salary with immediate vesting Stock purchase program CarMax vehicle discount Up to 4 months Paid Parental Leave HeartCash matches employee donations to the causes that are important to them 2 Days of Paid Time Off for time to dedicate to social impact causes FitCash covers a portion of gym or fitness activity fees Well being sessions and events such as yoga, meditation and walking challenges On-going career development sessions and an annual learning event Pet insurance Sabbatical leave Education Reimbursement Pre-tax spending accounts for qualified transportation expenses Plus a coffee bar, frozen yogurt and more! Working @ Edmunds.com: Employees think it’s a pretty great place to work and some pretty impressive publications think it is too: we have been recognized as one of the best places to work by the Fortune Magazine and Great Places to Work, LA Business Journal (for the last 6 years!), Computerworld, Built in LA and Inc. Magazine. We've also been identified as one of the best workplaces specifically in Technology and also for Diversity and Asian Americans. If you’re interested in learning more and joining our mission, we’d love to hear from you! E dmunds will consider for employment qualified candidates with criminal histories in a manner consistent with the requirements of all applicable laws.",
        "url": "https://www.linkedin.com/jobs/view/3886782670",
        "summary": "Edmunds is seeking a Data Engineer to manage their data explosion. The role involves creating and maintaining scalable data pipelines, enhancing the data lakehouse, automating processes, and collaborating with cross-functional teams to improve data infrastructure.  Candidates should have experience with Scala and Python, ETL Jobs, real-time data pipelines, and familiarity with Spark, AWS, Databricks, Airflow, and SQL. ",
        "industries": [
            "Automotive",
            "Technology",
            "Data",
            "Software",
            "E-commerce"
        ],
        "soft_skills": [
            "Problem Solving",
            "Troubleshooting",
            "Communication",
            "Collaboration",
            "Teamwork",
            "Organizational",
            "Analytical",
            "Decision Making"
        ],
        "hard_skills": [
            "Scala",
            "Python",
            "Spark",
            "AWS",
            "Databricks",
            "Airflow",
            "SQL",
            "ETL",
            "Data Pipelines",
            "Data Lakehouse",
            "Big Data",
            "Software Engineering",
            "Object-Oriented Programming",
            "Design Patterns",
            "Algorithms",
            "Kubernetes",
            "Machine Learning"
        ],
        "tech_stack": [
            "Scala",
            "Python",
            "Spark",
            "AWS",
            "Databricks",
            "Airflow",
            "SQL",
            "Kubernetes",
            "Machine Learning"
        ],
        "programming_languages": [
            "Scala",
            "Python"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 191815,
            "min": 109900
        },
        "benefits": [
            "Flexible Time Off",
            "Paid Holidays",
            "Health Benefits",
            "Flexible Spending Accounts",
            "Health Savings Accounts",
            "401K Plan",
            "Stock Purchase Program",
            "CarMax Vehicle Discount",
            "Paid Parental Leave",
            "Employee Donations Matching",
            "Paid Time Off for Social Impact",
            "Fitness Activity Fee Reimbursement",
            "Well Being Sessions",
            "Career Development Sessions",
            "Pet Insurance",
            "Sabbatical Leave",
            "Education Reimbursement",
            "Transportation Expense Reimbursement",
            "Coffee Bar",
            "Frozen Yogurt"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Fremont, CA",
        "job_id": 3882997984,
        "company": "Meta",
        "title": "Data Engineer",
        "created_on": 1720635220.029891,
        "description": "At Meta, we have many opportunities to work with data each and every day. In this role as a Data Engineer on the Meta Data Center’s Data Science team, your primary responsibility will be to partner with key stakeholders, data scientists and software engineers to support and enable the continued growth critical to Meta's Data Center organization. You will be responsible for creating the technology and data architecture that moves and translates data used to inform our most critical strategic and real-time decisions. You will also help translate business needs into requirements and identify efficiency opportunities. In addition to extracting and transforming data, you will be expected to use your expertise to build extensible data models, provide meaningful recommendations and actionable strategies to partnering data scientist for performance enhancements and development of best practices, including streamlining of data sources and related programmatic initiatives. The ideal candidate will have a passion for working in white space and creating impact from the ground up in a fast-paced environment. This position is part of the Infrastructure Data Center team. Data Engineer Responsibilities: Partner with leadership, engineers, program managers and data scientists to understand data needs Apply proven expertise and build high-performance scalable data warehouses Design, build and launch efficient & reliable data pipelines to move and transform data (both large and small amounts) Securely source external data from numerous partners Intelligently design data models for optimal storage and retrieval Deploy inclusive data quality checks to ensure high quality of data Optimize existing pipelines and maintain of all domain-related data pipelines Ownership of the end-to-end data engineering component of the solution Support on-call shift as needed to support the team Design and develop new systems in partnership with software engineers to enable quick and easy consumption of data Minimum Qualifications: BS/MS in Computer Science or a related technical field 5+ years of Python or other modern programming language development experience 5+ years of SQL and relational databases experience 5+ years experience in custom ETL design, implementation and maintenance 3+ years of experience with workflow management engines (i.e. Airflow, Luigi, Prefect, Dagster, Digdag, Google Cloud Composer, AWS Step Functions, Azure Data Factory, UC4, Control-M) 3+ years experience with Data Modeling Experience working with cloud or on-premises Big Data/MPP analytics platform (i.e. Netezza, Teradata, AWS Redshift, Google BigQuery, Azure Data Warehouse, or similar) 2+ years experience working with enterprise DE tools and experience learning in-house DE tools Bachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent practical experience. Preferred Qualifications: Experience with more than one coding language Experience designing and implementing real-time pipelines Experience with data quality and validation Experience with SQL performance tuning and end-to-end process optimization Experience with anomaly/outlier detection Experience with notebook-based Data Science workflow Experience with Airflow Experience querying massive datasets using Spark, Presto, Hive, Impala, etc. Experience building systems integrations, tooling interfaces, implementing integrations for ERP systems (Oracle, SAP, Saleforce etc). About Meta: Meta builds technologies that help people connect, find communities, and grow businesses. When Facebook launched in 2004, it changed the way people connect. Apps like Messenger, Instagram and WhatsApp further empowered billions around the world. Now, Meta is moving beyond 2D screens toward immersive experiences like augmented and virtual reality to help build the next evolution in social technology. People who choose to build their careers by building with us at Meta help shape a future that will take us beyond what digital connection makes possible today—beyond the constraints of screens, the limits of distance, and even the rules of physics. Meta is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Meta participates in the E-Verify program in certain locations, as required by law. Please note that Meta may leverage artificial intelligence and machine learning technologies in connection with applications for employment. Meta is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com. $134,000/year to $204,000/year + bonus + equity + benefits Individual compensation is determined by skills, qualifications, experience, and location. Compensation details listed in this posting reflect the base hourly rate, monthly rate, or annual salary only, and do not include bonus, equity or sales incentives, if applicable. In addition to base compensation, Meta offers benefits. Learn more about  benefits  at Meta.",
        "url": "https://www.linkedin.com/jobs/view/3882997984",
        "summary": "Meta is looking for a Data Engineer to join their Data Center’s Data Science team. The role involves collaborating with stakeholders, data scientists, and software engineers to build data architecture and pipelines for critical decision-making. This includes extracting, transforming, and modeling data, optimizing existing pipelines, and designing new systems for easy data consumption. The ideal candidate will have a passion for working in a fast-paced environment and creating impact.",
        "industries": [
            "Technology",
            "Data Science",
            "Software Engineering",
            "Data Warehousing"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Analytical Thinking",
            "Strategic Thinking",
            "Passion for Data",
            "Teamwork",
            "Leadership"
        ],
        "hard_skills": [
            "Python",
            "SQL",
            "ETL",
            "Airflow",
            "Data Modeling",
            "Big Data",
            "Data Warehousing",
            "Data Quality",
            "Performance Tuning",
            "Anomaly Detection",
            "Spark",
            "Presto",
            "Hive",
            "Impala",
            "System Integration",
            "ERP Systems"
        ],
        "tech_stack": [
            "Python",
            "SQL",
            "Airflow",
            "Spark",
            "Presto",
            "Hive",
            "Impala",
            "Netezza",
            "Teradata",
            "AWS Redshift",
            "Google BigQuery",
            "Azure Data Warehouse",
            "Oracle",
            "SAP",
            "Salesforce"
        ],
        "programming_languages": [
            "Python",
            "SQL"
        ],
        "experience": 5,
        "education": {
            "min_degree": "BS/MS",
            "fields": [
                "Computer Science",
                "Related Technical Field"
            ]
        },
        "salary": {
            "max": 204000,
            "min": 134000
        },
        "benefits": [
            "Bonus",
            "Equity",
            "Health Insurance",
            "Dental Insurance",
            "Vision Insurance",
            "Life Insurance",
            "Disability Insurance",
            "Paid Time Off",
            "Paid Parental Leave",
            "Employee Assistance Program",
            "Tuition Reimbursement",
            "Employee Stock Purchase Plan"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Foster City, CA",
        "job_id": 2938046886,
        "company": "Zoox",
        "title": "Data Engineer",
        "created_on": 1720635221.7337227,
        "description": "The Data team leverages data from our autonomous vehicles and operations to determine autonomy and service readiness. We provide the foundation for strategic decision-making at Zoox. You will develop and implement the next generation of our data pipeline to ensure visibility into our business as we scale toward the launch of an autonomous mobility service. You will define the system and build the pipeline to enable Zoox to develop and scale with a data-first culture. You will join a diverse, experienced team with rapidly growing scope and responsibility while also having access to one of the most unique data sets in the autonomous vehicle industry. Hence, we are seeking all skill levels to grow with the team. In This Role, You Will Design, build, and maintain the infrastructure that transforms autonomous vehicle data at scale to support analytics throughout the company Define and execute on how data from perception, prediction, planning and other parts of the autonomous stack is consumed to generate valuable insights by data scientists, engineers, and business users Establish robust data integrity monitoring so that company-wide metrics are based on accurate data Partner with engineering and product teams to define data consumption patterns and establish best practices Qualifications BS/MS degree in a technical field Experience designing and building complex data infrastructure at scale Advanced Structure Query Language (SQL) and data warehousing experience Experience operating a workflow manager such as Airflow Experience with large scale streaming platforms (e.g. Kafka, Kinesis), processing frameworks (e.g. Spark, Hadoop) and storage engines (e.g. HDFS, HBase) Bonus Qualifications Exceptional Python or Scala skills Basic fluency in C++ Familiarity with or exposure to experimentation platforms A strong DataOps mindset and opinions on next-generation warehousing tools Compensation There are three major components to compensation for this position: salary, Amazon Restricted Stock Units (RSUs), and Zoox Stock Appreciation Rights. The salary will range from $143,000 to $245,000. A sign-on bonus may be part of a compensation package. Compensation will vary based on geographic location, job-related knowledge, skills, and experience. Zoox also offers a comprehensive package of benefits including paid time off (e.g. sick leave, vacation, bereavement), unpaid time off, Zoox Stock Appreciation Rights, Amazon RSUs, health insurance, long-term care insurance, long-term and short-term disability insurance, and life insurance. About Zoox Zoox is developing the first ground-up, fully autonomous vehicle fleet and the supporting ecosystem required to bring this technology to market. Sitting at the intersection of artificial intelligence, robotics, and design, Zoox aims to provide the next generation of mobility-as-a-service in urban environments. We’re looking for top talent that shares our passion and wants to be part of a fast-moving and highly execution-oriented team. Follow us on LinkedIn About Zoox Zoox is developing the first ground-up, fully autonomous vehicle fleet and the supporting ecosystem required to bring this technology to market. Sitting at the intersection of robotics, machine learning, and design, Zoox aims to provide the next generation of mobility-as-a-service in urban environments. We’re looking for top talent that shares our passion and wants to be part of a fast-moving and highly execution-oriented team. Follow us on LinkedIn Accommodations If you need an accommodation to participate in the application or interview process please reach out to accommodations@zoox.com or your assigned recruiter. A Final Note You do not need to match every listed expectation to apply for this position. Here at Zoox, we know that diverse perspectives foster the innovation we need to be successful, and we are committed to building a team that encompasses a variety of backgrounds, experiences, and skills.",
        "url": "https://www.linkedin.com/jobs/view/2938046886",
        "summary": "Zoox is hiring a Data Engineer to design, build, and maintain their data pipeline for autonomous vehicle data. You will be responsible for ensuring visibility into the business, defining data consumption patterns, and establishing best practices. You will also partner with engineering and product teams to create a data-first culture.",
        "industries": [
            "Autonomous Vehicles",
            "Data Engineering",
            "Artificial Intelligence",
            "Robotics",
            "Technology",
            "Transportation"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Critical Thinking",
            "Decision Making",
            "Data-Driven",
            "Leadership",
            "Teamwork",
            "Organization",
            "Time Management"
        ],
        "hard_skills": [
            "SQL",
            "Data Warehousing",
            "Airflow",
            "Kafka",
            "Kinesis",
            "Spark",
            "Hadoop",
            "HDFS",
            "HBase",
            "Python",
            "Scala",
            "C++",
            "DataOps",
            "Data Integrity Monitoring"
        ],
        "tech_stack": [
            "SQL",
            "Airflow",
            "Kafka",
            "Kinesis",
            "Spark",
            "Hadoop",
            "HDFS",
            "HBase",
            "Python",
            "Scala",
            "C++"
        ],
        "programming_languages": [
            "SQL",
            "Python",
            "Scala",
            "C++"
        ],
        "experience": 0,
        "education": {
            "min_degree": "BS/MS",
            "fields": [
                "Technical Field"
            ]
        },
        "salary": {
            "max": 245000,
            "min": 143000
        },
        "benefits": [
            "Paid Time Off",
            "Unpaid Time Off",
            "Zoox Stock Appreciation Rights",
            "Amazon RSUs",
            "Health Insurance",
            "Long-Term Care Insurance",
            "Long-Term Disability Insurance",
            "Short-Term Disability Insurance",
            "Life Insurance"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Santa Clara, CA",
        "job_id": 3947593286,
        "company": "Natron Energy",
        "title": "Data Analytics Engineer",
        "created_on": 1720635223.3984787,
        "description": "The Data Team At Natron Is Responsible For The Collection, Management, Analysis, And Distribution Of All The Battery Manufacturing And Test Data. The Data Team Drives One Of The Core Principles Of Natron, Rational Decision Making, By Providing Data For Every Critical Decision. As a Data Analytics Engineer On The Data Team, You Will: Be part of a small team that owns and maintains data pipelines and infrastructure for the entire company. Work closely with many positions and teams across Natron such as Data Engineers, Analysts, Test Engineers, Process Engineers, Reliability Team, R&D Team etc. Own and develop robust and scalable analytics solutions. Contribute to the mission of building a company and a product to be proud of. Tech Stack: Python, SQL, DBT, Dagster, BigQuery, Tableau, JMP, MATLAB, Dash, Ignition This position is a hybrid with 3 days onsite and 2 days remote. Responsibilities: Design, develop and own data analysis pipelines and models. ­Apply principles of electrochemical theory and statistical analysis to develop new algorithms to calculate initial and lifetime battery performance metrics. ­Craft compelling visualizations to depict process parameters and product performance metrics. ­Work with operations, product, technology, and R&D teams on their analytical needs, including identifying KPIs and supporting root cause analysis. ­Drive and own data quality and documentation for areas of ownership. Experience And Skills: ­2+ years of industrial or academic experience developing data analysis and visualization tools. ­Working knowledge of electrical circuits and/or energy storage devices such batteries. ­Fluent in Python. ­Experience with SQL query writing. Familiar with version control software such as Git. ­Has a do-whatever-it-takes attitude, thrives in a fast-paced start up environment and can deliver project deliverables on time. ­A bachelor’s degree or higher in a science or engineering field, preferably Physics, Electrical Engineering, Chemical Engineering, Materials Engineering or similar. Nice To Have: ­ Strong understanding of electrochemistry and battery performance testing ­ Experience with JMP or other statistical analysis tools ­ Experience with MATLAB ­ Familiar with dbt (data build tool) ­ Familiar with Dagster, AirFlow or other similar data orchestration tools Compensation/Pay Transparency Disclaimer: The actual salary of a successful applicant may vary from posted ranges based on the candidate’s experience, knowledge, skills, and abilities, internal equity and alignment with market data, and other legitimate business reasons, including, but not limited to, compliance with applicable immigration law prevailing wages. In addition, Natron Energy has a strong benefits package including Medical, Dental, Vision, 401k Plan with Match, Life Insurance, Parental Leave Benefits, Discretionary Time Off (DTO) and Paid Time Off (PTO) for Exempt and Non-Exempt employees respectively, and 11 paid holidays. The salary range for this position is a minimum of $106,500 and a maximum of $155,500. About Us: Natron Energy (natron.energy) is the future of energy storage. Our battery products solve operations performance and reliability problems for the world’s biggest electricity customers. Our initial products target markets exceeding twenty-five billion dollars including data centers, oil & gas, EV fast charging, and commercial aviation. We have additional products in development for larger markets including commercial and residential grid storage. Our products are based on sodium-ion cells containing Prussian blue electrodes that deliver unique power, cycle life, and safety: full discharge and recharge in just minutes all from a nonflammable, fault-tolerant system. Unsolicited Resume Policy Natron Energy, Inc. (“Natron Energy” or the “Company”) does not accept unsolicited resumes from professional recruiters, third-party recruiting or staffing agencies, placement services, or any other source other than directly from a candidate. Any unsolicited resumes, including partial resumes, candidate profiles, and candidate details or information, sent to Natron Energy or its personnel will be treated as public information provided free of any charges or fees. Natron Energy will not pay a fee for any placement resulting from the receipt of an unsolicited resume, unless in connection with a written agreement with the Company then in effect. Such agreement must be pre-approved by Natron Energy and executed by an authorized representative of the Company. Natron Energy specifically rejects, and denies any liability under, any agreement purporting to be accepted based on negative consent, negotiation with a candidate, performance, or any means other than the signature of an authorized representative of the Company. Natron Energy is proud to be an equal-opportunity employer . We value diversity. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. If you need assistance or an accommodation due to a disability, you may contact us at: jobs@natron.energy",
        "url": "https://www.linkedin.com/jobs/view/3947593286",
        "summary": "Natron Energy is seeking a Data Analytics Engineer to join their Data Team.  This role will be responsible for designing, developing, and owning data analysis pipelines and models, applying principles of electrochemical theory and statistical analysis to develop new algorithms for calculating battery performance metrics, creating visualizations, working with various teams on their analytical needs, and driving data quality and documentation. This is a hybrid role with 3 days onsite and 2 days remote.",
        "industries": [
            "Energy",
            "Battery Manufacturing",
            "Data Analytics",
            "Technology"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Analytical",
            "Detail Oriented",
            "Project Management",
            "Time Management",
            "Teamwork"
        ],
        "hard_skills": [
            "Python",
            "SQL",
            "DBT",
            "Dagster",
            "BigQuery",
            "Tableau",
            "JMP",
            "MATLAB",
            "Dash",
            "Ignition",
            "Electrochemical Theory",
            "Statistical Analysis",
            "Data Visualization",
            "Data Quality",
            "Data Documentation",
            "Version Control",
            "Git"
        ],
        "tech_stack": [
            "Python",
            "SQL",
            "DBT",
            "Dagster",
            "BigQuery",
            "Tableau",
            "JMP",
            "MATLAB",
            "Dash",
            "Ignition"
        ],
        "programming_languages": [
            "Python",
            "SQL"
        ],
        "experience": 2,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Physics",
                "Electrical Engineering",
                "Chemical Engineering",
                "Materials Engineering"
            ]
        },
        "salary": {
            "max": 155500,
            "min": 106500
        },
        "benefits": [
            "Medical",
            "Dental",
            "Vision",
            "401k Plan with Match",
            "Life Insurance",
            "Parental Leave Benefits",
            "Discretionary Time Off",
            "Paid Time Off",
            "Paid Holidays"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Burbank, CA",
        "job_id": 3921542101,
        "company": "The Walt Disney Company",
        "title": "Sr. Data Engineer",
        "created_on": 1720635224.9447095,
        "description": "This role is based out of the Burbank are and will require relocation if not local. This is not a remote role. The Disney Decision Science and Integration (DDSI) analytics consulting team is responsible for supporting clients across The Walt Disney Company including Disney Parks, Experiences and Products (e.g., Walt Disney World, Disneyland, Disney Cruise Line), Disney Media & Entertainment Distribution (e.g., Disney+, ESPN+, Hulu), Studios Content (e.g., The Walt Disney Studios, Disney Theatrical Group), General Entertainment Content (e.g., 20th Television, ABC Entertainment, ABC News) and ESPN and Sports Content. DDSI leverages technology, data analytics, optimization, statistical and econometric modeling to explore opportunities, shape business decisions and drive business value. The team is involved in various activities ranging from data acquisition and validation, designing and implementing ETL/ELT data pipelines, designing and implementing databases, and evolving our next generation data platform to fulfill the needs of our applications, data services, ad-hoc analytics and self-service/POC initiatives. Responsibilities The Data Engineering team (within DDSI) is looking to fill a Senior Data Engineer role responsible for designing and implementing ETL/ELT data pipelines, designing and implementing database schema/tables/views, and building batch processes leveraging Airflow for several projects. This Senior Data Engineer will be responsible for partnering with our Studios or Enterprise Technology team members in various activities around data requirements gathering, data validation scripting and review, developing and monitoring ETL/ELT data pipelines, designing and implementing database schema/tables/views, plus deployment across multiple environments such as DEV, QA/UAT and PROD. The role will leverage a multitude of technologies to fulfill the work including, but not limited to SQL, Python, Docker, Gitlab, Airflow, Snowflake, Looker and PostgreSQL. Basic Qualifications: 3+ years experience with Python 4+ years experience with SQL 3+ years experience designing, building and maintaining ETL/ELT data pipelines 2+ years experience with cloud based technologies, preferably AWS and S3 Experience using Apache Airflow, Databricks 2+ years leveraging, designing and building relational databases 2+ years of experience using Gitlab/Github Experience working with large datasets and big data technologies, preferably cloud-based, such as Snowflake, Databricks, or similar Experience with SQL Query optimization based on runtime and cost Participate in driving best practices around data engineering software development processes Preferred Qualifications Experience with Snowflake, Airflow, Gitlab, , Tableau Master’s degree in Computer Science, Mathematics, Engineering, or a related field Experience in streaming, media, or digital marketing domain Minimum Qualifications Bachelor degree in Computer Science, Mathematics, Engineering, or a related field 5+ years experience in a Data Engineering role in the field not counting academic experience SQL, Python, Snowflake, Gitlab, Airflow, AWS S3 and Relational Databases 2+ years experience working on a cloud platform 2+ years experience working with data lakes, data warehouses and application databases The hiring range for this position in Burbank is $136,100.00 to $160,000.00 per year. The base pay actually offered will take into account internal equity and also may vary depending on the candidate’s geographic region, job-related knowledge, skills, and experience among other factors. A bonus and/or long-term incentive units may be provided as part of the compensation package, in addition to the full range of medical, financial, and/or other benefits, dependent on the level and position offered.",
        "url": "https://www.linkedin.com/jobs/view/3921542101",
        "summary": "Disney is looking for a Senior Data Engineer to join their Decision Science and Integration (DDSI) team in Burbank, California. The role involves designing and implementing ETL/ELT data pipelines, database schema, and batch processes using Airflow. The candidate will work with various technologies like SQL, Python, Docker, Gitlab, Airflow, Snowflake, Looker, and PostgreSQL. The team supports clients across various Disney divisions, including Parks, Media & Entertainment, Studios, and ESPN.",
        "industries": [
            "Media & Entertainment",
            "Technology",
            "Data Analytics",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Analytical Thinking",
            "Critical Thinking"
        ],
        "hard_skills": [
            "SQL",
            "Python",
            "Docker",
            "Gitlab",
            "Airflow",
            "Snowflake",
            "Looker",
            "PostgreSQL",
            "ETL/ELT",
            "Data Pipelines",
            "Database Design",
            "Batch Processing",
            "AWS",
            "S3",
            "Databricks",
            "Relational Databases",
            "Data Warehousing",
            "Data Lakes",
            "Data Validation",
            "Query Optimization"
        ],
        "tech_stack": [
            "SQL",
            "Python",
            "Docker",
            "Gitlab",
            "Airflow",
            "Snowflake",
            "Looker",
            "PostgreSQL",
            "AWS",
            "S3",
            "Databricks",
            "Tableau"
        ],
        "programming_languages": [
            "SQL",
            "Python"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Mathematics",
                "Engineering"
            ]
        },
        "salary": {
            "max": 160000,
            "min": 136100
        },
        "benefits": [
            "Medical",
            "Financial",
            "Bonus",
            "Long-Term Incentive Units"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3903879731,
        "company": "Strava",
        "title": "Data Engineer II",
        "created_on": 1720635226.7450452,
        "description": "About This Role Strava is the leading digital community for active people with more than 120 million athletes, in more than 190 countries. The platform offers a holistic view of your active lifestyle, no matter where you live, which sport you love and/or what device you use. Everyone belongs on Strava when they are pursuing an active life. We are seeking data engineers to join our Data Platform team. Our vision is that key decisions and product at Strava can be greatly enriched with data to benefit athletes and the business. Our mission is to build and support a platform that enables access to data through self-service and extensible tools. This means we listen to all corners of the company for opportunities where data can make a difference. We distill this down and use modern technologies to develop both generalized and special-purpose data solutions. For more information on compensation and benefits, please click here. This is a hybrid role based in our San Francisco office. You’re excited about this opportunity because you will: Collaborate with people across teams and functions that hold deep curiosity for data. Work with hefty data systems at the global scale of Strava. Deliver value more through software, leaning into tooling and automation rather than repetitive toil. Grow your expertise in the steadily evolving technologies and ecosystem of data. You will be successful here by: Holding empathy for the users of our platform to truly understand the challenges we tackle for them. Fostering an inclusive and motivating team culture to help everyone achieve their best. Caring about high quality and reliable code, but also the end user experience. Solving problems efficiently, creatively, and completely despite constraints in time or resources. Understanding how critical it is we maintain a high bar of data security and privacy. We’re excited about you because you: Have the ability to adapt and apply evolving data technologies to business needs (which means the list of bullets below will change over time!). Have developed software using programming languages like Python, Scala, Java, Go, Ruby, etc. Have sufficient familiarity to understand SQL queries in the context of data pipelines (i.e. dbt). Have experience with distributed data tools (i.e. Spark, Flink, Kafka) on large datasets. Have worked with cloud-data warehouses (i.e. Snowflake, BigQuery, Redshift) or other warehousing solutions. Have an understanding of underlying infrastructure needed to serve production services (i.e. Kubernetes, AWS, GCP, Azure). About Strava Strava is Swedish for “strive,” which epitomizes who we are and what we do. We’re a passionate and committed team, unified by our mission to connect athletes to what motivates them and help them find their personal best. And with billions of activity uploads from all over the world, we have a humbling and audacious vision: to be the record of the world’s athletic activities and the technology that makes every effort count. Strava builds software that makes the best part of our athletes’ days even better. And just as we’re deeply committed to unlocking their potential, we’re dedicated to providing a world-class, inclusive workplace where our employees can grow and thrive, too. We’re backed by Sequoia Capital, Madrone Partners and Jackson Square Ventures, and we’re expanding in order to exceed the needs of our growing community of global athletes. Our culture reflects our community – we are continuously striving to hire and engage diverse teammates from all backgrounds, experiences and perspectives because we know we are a stronger team together. Despite challenges in the world around us, we are continuing to grow camaraderie and positivity within our culture and we are unified in our commitment to becoming an antiracist company. We are differentiated by our truly people-first approach, our compassionate leadership, and our belief that we can bring joy and inspiration to athletes’ lives — now more than ever. All to say, it’s a great time to join Strava! Strava is an equal opportunity employer. In keeping with the values of Strava, we make all employment decisions including hiring, evaluation, termination, promotional and training opportunities, without regard to race, religion, color, sex, age, national origin, ancestry, sexual orientation, physical handicap, mental disability, medical condition, disability, gender or identity or expression, pregnancy or pregnancy-related condition, marital status, height and/or weight. We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation. California Consumer Protection Act Applicant Notice",
        "url": "https://www.linkedin.com/jobs/view/3903879731",
        "summary": "Strava, a leading digital community for active individuals, seeks data engineers to join its Data Platform team. The role involves collaborating with diverse teams, working with large-scale data systems, developing software using languages like Python, Scala, Java, Go, and Ruby, and leveraging technologies like Spark, Flink, Kafka, Snowflake, BigQuery, and Kubernetes.",
        "industries": [
            "Technology",
            "Sports",
            "Fitness",
            "Social Media",
            "Data Science"
        ],
        "soft_skills": [
            "Collaboration",
            "Empathy",
            "Teamwork",
            "Communication",
            "Problem Solving",
            "Creativity",
            "Adaptability",
            "Efficiency",
            "Data Security",
            "Privacy"
        ],
        "hard_skills": [
            "Python",
            "Scala",
            "Java",
            "Go",
            "Ruby",
            "SQL",
            "dbt",
            "Spark",
            "Flink",
            "Kafka",
            "Snowflake",
            "BigQuery",
            "Redshift",
            "Kubernetes",
            "AWS",
            "GCP",
            "Azure"
        ],
        "tech_stack": [
            "Python",
            "Scala",
            "Java",
            "Go",
            "Ruby",
            "SQL",
            "dbt",
            "Spark",
            "Flink",
            "Kafka",
            "Snowflake",
            "BigQuery",
            "Redshift",
            "Kubernetes",
            "AWS",
            "GCP",
            "Azure"
        ],
        "programming_languages": [
            "Python",
            "Scala",
            "Java",
            "Go",
            "Ruby"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Sunnyvale, CA",
        "job_id": 3854805931,
        "company": "23andMe",
        "title": "Sr. Data Engineer I",
        "created_on": 1720635230.8600063,
        "description": "We are looking for a Data Engineer who could be a technical powerhouse when scaling data infrastructure, dashboards, tools, and reporting forecasts. As a Data Engineer, you will be working cross-functionally with business domain experts, analytics partners, and engineering teams to design and implement our Data Warehouse model. This is a great role for problem solvers, who can anticipate problems and can also look beyond immediate issues. They will be a self-starter, detail and quality oriented, and passionate about having a huge impact at 23andMe. We look for people who are curious, inventive, and work to be a little better every single day. In our work together we aim to be smart, humble, hardworking and, above all, collaborative. An ideal candidate brings curiosity, a passion for data, and a deep understanding of the technologies behind data pipelines, warehousing, big data and analytics. Who We Are Since 2006, 23andMe’s mission has been to help people access, understand, and benefit from the human genome. We are a group of passionate individuals pushing the boundaries of what’s possible to help turn genetic insight into better health and personal understanding. What You'll Do We are looking for a Data Engineer with passion for data analytics and product. As a part of the team you will: Acquire deep business understanding on several domains and build scalable and optimized data solutions that impact many stakeholders. Be an advocate for data quality and excellence of our platform. Build close relationships with our partners to understand the value our platform can bring and how we can make it better. Build tools that help streamline the management and operation of our data ecosystem. Ensure best practices and standards in our data ecosystem are shared across teams. Improve data discovery by creating data exploration processes and promoting adoption of data sources across the company. Has a desire to write tools and applications to automate work rather than do everything by hand. Comfortable with weekly on-call rotation Passionate about CI/CD process. Design, develop and establish KPIs to monitor analysis and provide strategic insights to drive growth and performance. What You'll Bring Bachelor's Degree in a quantitative discipline: computer science, statistics, operations research, informatics, engineering, applied mathematics, economics, etc. 5+ years of relevant work experience in analytics, data engineering, business intelligence, research or related fields. Experience with SQL/Relational databases Experience with Amazon Web Services: Redshift, S3, Glue,EMR, or Athena Experience developing low latency data processing solutions like AWS Kinesis, Kafka, Spark Stream processing. Experience in at least one programming languages (e.g. Python, Java,) Strong experience with data scheduling tools like Stonebranch(Opswise) or Airflow. Experience with data visualization using Microstrategy,Tableau, Quicksight, or similar tools. Note: Ability to work from 23andMe's office in Sunnyvale, CA a minimum of 3 days per week Strongly Preferred Master's degree or equivalent in Computer Science, Engineering, Mathematics, Statistics, Economics, or a related field. Experience using analytics & reporting tools like Microstrategy,Tableau, Quicksight etc. Experience with Airflow and Kinesis is a plus. Advanced knowledge and experience building on AWS QuickSight. About Us 23andMe, headquartered in Sunnyvale, CA, is a leading consumer genetics and research company. Founded in 2006, the company’s mission is to help people access, understand, and benefit from the human genome. 23andMe has pioneered direct access to genetic information as the only company with multiple FDA authorizations for genetic health risk reports. The company has created the world’s largest crowdsourced platform for genetic research, with 80 percent of its customers electing to participate. The platform also powers the 23andMe Therapeutics group, currently pursuing drug discovery programs rooted in human genetics across a spectrum of diseases. More information is available at www.23andMe.com. At 23andMe, we value a diverse, inclusive workforce and we provide equal employment opportunity for all applicants and employees. All qualified applicants for employment will be considered without regard to an individual’s race, color, sex, gender identity, gender expression, religion, age, national origin or ancestry, citizenship, physical or mental disability, medical condition, family care status, marital status, domestic partner status, sexual orientation, genetic information, military or veteran status, or any other basis protected by federal, state or local laws. If you are unable to submit your application because of incompatible assistive technology or a disability, please contact us at accommodations-ext@23andme.com. 23andMe will reasonably accommodate qualified individuals with disabilities to the extent required by applicable law. Please note: 23andMe does not accept agency resumes and we are not responsible for any fees related to unsolicited resumes. Thank you. Pay Transparency 23andMe takes a market-based approach to pay, and amounts will vary depending on your geographic location. The salary range reflected here is for a candidate based in the San Francisco Bay Area. The successful candidate’s starting pay will be determined based on job-related skills, experience, qualifications, work location, and market conditions. These ranges may be modified in the future. San Francisco Bay Area Base Pay Range $153,600—$230,400 USD",
        "url": "https://www.linkedin.com/jobs/view/3854805931",
        "summary": "23andMe is seeking a Data Engineer to design and implement their Data Warehouse model, working with business domain experts, analytics partners, and engineering teams. This role requires strong technical skills in data infrastructure, dashboards, tools, and reporting forecasts, and a passion for data analytics and product. Responsibilities include building scalable data solutions, ensuring data quality, collaborating with partners, building data management tools, and improving data discovery.",
        "industries": [
            "Biotechnology",
            "Genetics",
            "Healthcare",
            "Technology",
            "Data Analytics",
            "Research"
        ],
        "soft_skills": [
            "Problem Solving",
            "Anticipating Problems",
            "Self-Starter",
            "Detail Oriented",
            "Quality Oriented",
            "Collaborative",
            "Curious",
            "Inventive",
            "Passion for Data",
            "Communication",
            "Advocacy",
            "Relationship Building",
            "Automation",
            "Strategic Thinking"
        ],
        "hard_skills": [
            "SQL",
            "Relational Databases",
            "Amazon Web Services",
            "Redshift",
            "S3",
            "Glue",
            "EMR",
            "Athena",
            "AWS Kinesis",
            "Kafka",
            "Spark Stream Processing",
            "Python",
            "Java",
            "Stonebranch",
            "Airflow",
            "Microstrategy",
            "Tableau",
            "Quicksight",
            "CI/CD"
        ],
        "tech_stack": [
            "AWS",
            "Redshift",
            "S3",
            "Glue",
            "EMR",
            "Athena",
            "Kinesis",
            "Kafka",
            "Spark",
            "Airflow",
            "Microstrategy",
            "Tableau",
            "Quicksight"
        ],
        "programming_languages": [
            "SQL",
            "Python",
            "Java"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Statistics",
                "Operations Research",
                "Informatics",
                "Engineering",
                "Applied Mathematics",
                "Economics"
            ]
        },
        "salary": {
            "max": 230400,
            "min": 153600
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Cupertino, CA",
        "job_id": 3967725685,
        "company": "Formula.Monks",
        "title": "Data Engineer",
        "created_on": 1720635232.4365013,
        "description": "Data Engineer Location: Cupertino, CA (hybrid-on site 3 days per week) Job Type: This is a Full-Time W2 position with us here at Formula.Monks where you will be working as a consultant at this top FAANG company. Since you’re a full time-employee, you will receive an annual salary, full benefits, PTO, etc. This position is on a high impact team working on a long-term engagement. You are also able to explore employment opportunities at this tech company while working via Formula.Monks! Description: You will develop, and test large scale data solutions, to provide efficient analytical and reporting capabilities across the clients global and regional sales and finance teams. You will help develop highly scalable data pipelines to load data from various source systems leveraging cloud tools to schedule and monitor the workflows. Build generic and reusable solutions that can scale and utilize various technologies and frameworks to solve our complex business requirements. Extend and build out existing data reporting warehouses and capabilities. You will be required to understand existing solutions, fine-tune them and support them as needed. Data quality is our goal and we expect you to meet our high standards on data and software quality. We are a rapidly growing team with plenty of interesting technical and business challenges to solve. We seek a self-starter, who is willing to learn fast, adapt well to changing requirements and work with cross functional teams. Key Qualifications: 5+ years of hands-on database engineering and data warehousing experience. Proficiency in advanced SQL including query performance tuning Understanding of materialized views, stored procedures, table design and data mart design 3+ years of experience in OOO programming skill sets Python a strong plus. Experience developing and working with custom ETL pipelines using SQL or other scripting languages (Python, Bash scripting) Development experience with SQL based cloud data warehouses like Snowflake, Redshift, BigQuery etc Experience with version control system Git and virtualization tools specifically Docker a must. Understanding of continuous integration and delivery (CI/CD) practice. Experience with cloud computing platforms like AWS, Google Cloud is a plus. Job workflow management with Airflow is also a plus. Ability to learn and adapt to new tools and technologies in a fast paced environment. An analytical and mathematical mind capable of evaluating and solving various complex data related problems. Ability to take directions as part of a team as well as deliver independent work. Excellent oral and written communication skills.",
        "url": "https://www.linkedin.com/jobs/view/3967725685",
        "summary": "Develop and test large-scale data solutions for sales and finance teams at a top FAANG company, build highly scalable data pipelines using cloud tools, and extend existing data reporting warehouses. Requires strong SQL, Python, and experience with cloud data warehouses like Snowflake, Redshift, and BigQuery.",
        "industries": [
            "Technology",
            "Data & Analytics",
            "Finance"
        ],
        "soft_skills": [
            "Self-starter",
            "Fast learner",
            "Adaptability",
            "Teamwork",
            "Communication",
            "Analytical",
            "Problem-solving"
        ],
        "hard_skills": [
            "SQL",
            "Query Performance Tuning",
            "Materialized Views",
            "Stored Procedures",
            "Table Design",
            "Data Mart Design",
            "Python",
            "ETL Pipelines",
            "Snowflake",
            "Redshift",
            "BigQuery",
            "Git",
            "Docker",
            "CI/CD",
            "AWS",
            "Google Cloud",
            "Airflow"
        ],
        "tech_stack": [
            "SQL",
            "Python",
            "Snowflake",
            "Redshift",
            "BigQuery",
            "Git",
            "Docker",
            "AWS",
            "Google Cloud",
            "Airflow"
        ],
        "programming_languages": [
            "SQL",
            "Python"
        ],
        "experience": 5,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Annual salary",
            "Full benefits",
            "PTO"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3931629349,
        "company": "HomeLight",
        "title": "Data Engineer",
        "created_on": 1720635234.1867433,
        "description": "This role is based in San Francisco, CA - Office days are Wed and Thur ***** Who We Are We’re building the future of real estate — today. HomeLight is the essential technology platform used by hundreds of thousands of homebuyers and sellers to partner with top real estate agents and win at any step of the real estate journey, whether that’s finding a top agent, securing a competitive mortgage, or ensuring on-time, easy close. HomeLight facilitates billions of dollars of real estate on its platform every year. Our vision is a world where every real estate transaction is simple, certain, and satisfying for all. Our team breaks barriers every day while staying committed to HomeLight's goals and core values, which is a crucial element to our shared success. Who You Are We are building our Data Engineering team to tackle HomeLight’s diverse data challenges. This position is an excellent opportunity for an engineer that wants to own the development, optimization, and operation of our data pipeline. We collect, processe, and distribute this data to a suite of HomeLight products and teams. You will provide mission-critical data to our algorithms and internal users. What You’ll Do Here Optimize and execute on requests to pull, analyze, interpret and visualize data Partner with team leaders across the organization to build out and iterate on team, and individual performance metrics Optimize our data release processes, and partner with team leads to iterate on and improve existing data pipelines. Design and develop systems that ingest and transform our data streams using the latest tools. Design, build, and integrate new cutting-edge databases and data warehouses, develop new data schemas and figure out new innovative ways of storing and representing our data. Research, architect, build, and test robust, highly available and massively scalable systems, software, and services. What You Bring 1 to 3 years of Python and ETL experience, preferably Airflow Experience writing and executing complex SQL queries Experience building data pipelines and ETL design (implementation and maintenance) Scrum/Agile software development process. Bonus points for Familiarity with chatgpt, transcription analysis Familiarity with AWS, Elasticsearch, Django Experience setting up and managing internal API services. Experience working on a small team, ideally at a startup. Familiarity with the Amazon AWS ecosystem Benefits And Perks Medical (Anthem or Kaiser), Dental & Vision (Guardian) Long-Term Disability & Short-Term Disability, Hospital Indemnity Insurance (Guardian) 401k (Guideline), Life Insurance (Guardian) & Pet Insurance (Nationwide) Commuter benefits are offered in certain locations PTO, including Volunteer Days to give back to your community Annual Anniversary Perks, including professional development and sabbaticals! HomeLight Services to help you with buying and selling your home The following compensation information is provided to comply with job posting disclosure requirements in Colorado, New York, Washington, and California. Base Pay Range: $100,000.00 - $150,000.00, b a se pay will vary depending on several factors, such as the position, location, qualifications of the individual, market conditions, and other operational business requirements. Let’s chat! HomeLight is an equal opportunity employer dedicated to building an inclusive and diverse workforce, providing employees with a work environment free of discrimination and harassment. All employment decisions are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. We will provide accommodations during the recruitment process upon request and any accommodation will be addressed confidentially.",
        "url": "https://www.linkedin.com/jobs/view/3931629349",
        "summary": "HomeLight is seeking a Data Engineer to optimize and execute data requests, build performance metrics, improve existing data pipelines, develop data ingestion and transformation systems, design and integrate databases and data warehouses, research and build scalable systems, and more. The ideal candidate will have 1-3 years of Python and ETL experience, specifically with Airflow, strong SQL skills, experience building data pipelines, and familiarity with Scrum/Agile methodology.",
        "industries": [
            "Real Estate",
            "Technology",
            "Data Engineering"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Analytical Thinking",
            "Time Management",
            "Organization"
        ],
        "hard_skills": [
            "Python",
            "ETL",
            "Airflow",
            "SQL",
            "Data Pipelines",
            "Scrum/Agile"
        ],
        "tech_stack": [
            "Airflow",
            "SQL",
            "AWS",
            "Elasticsearch",
            "Django"
        ],
        "programming_languages": [
            "Python",
            "SQL"
        ],
        "experience": 1,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 150000,
            "min": 100000
        },
        "benefits": [
            "Medical",
            "Dental",
            "Vision",
            "Long-Term Disability",
            "Short-Term Disability",
            "Hospital Indemnity Insurance",
            "401k",
            "Life Insurance",
            "Pet Insurance",
            "Commuter benefits",
            "PTO",
            "Volunteer Days",
            "Annual Anniversary Perks",
            "Professional development",
            "Sabbaticals",
            "HomeLight Services"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Pleasanton, CA",
        "job_id": 3825082679,
        "company": "UrBench, LLC",
        "title": "Data Engineer",
        "created_on": 1720635235.8809297,
        "description": "Pleasanton, CA (Onsite) Long Term Contract Design, develop, document, and design big data applications using Azure cloud services. Develop code using knowledge of SQL, NoSQL, data warehousing, Scala, Spark, Hive & Hadoop. Be an expert in newer concepts like Google BigQuery/BigTable,Apache Spark and Scala programming. Manage and monitor Hadoop log files. Develop MapReduce coding that works seamlessly on Hadoop clusters. Seamlessly convert hard-to-grasp technical requirements into outstanding designs. Design web services for swift data tracking and query data at high speeds. Test software prototypes, propose standards, and smoothly transfer them to operations. Experience with Google Data Platform, Big Query, Experience with Spark, Hadoop or Databricks but minimum 2 years GCP experience is needed.",
        "url": "https://www.linkedin.com/jobs/view/3825082679",
        "summary": "This is a long-term contract position for a Big Data Engineer in Pleasanton, CA. The role involves designing, developing, and documenting big data applications using Azure cloud services. The ideal candidate will have strong expertise in SQL, NoSQL, data warehousing, Scala, Spark, Hive, Hadoop, Google BigQuery/BigTable, Apache Spark, Scala programming, and MapReduce. Experience with Google Data Platform, BigQuery, Spark, Hadoop, or Databricks is required, with a minimum of 2 years of GCP experience.",
        "industries": [
            "Technology",
            "Data Science",
            "Software Development",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Communication",
            "Problem-Solving",
            "Analytical Skills",
            "Design",
            "Documentation",
            "Collaboration",
            "Teamwork",
            "Project Management"
        ],
        "hard_skills": [
            "SQL",
            "NoSQL",
            "Data Warehousing",
            "Scala",
            "Spark",
            "Hive",
            "Hadoop",
            "Google BigQuery",
            "Google BigTable",
            "Apache Spark",
            "MapReduce",
            "Web Services",
            "Software Testing",
            "Google Data Platform",
            "Databricks"
        ],
        "tech_stack": [
            "Azure",
            "Google Cloud Platform (GCP)",
            "BigQuery",
            "BigTable",
            "Spark",
            "Hadoop",
            "Hive",
            "Scala",
            "MapReduce"
        ],
        "programming_languages": [
            "SQL",
            "Scala"
        ],
        "experience": 2,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Data Science",
                "Engineering"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Jose, CA",
        "job_id": 3946906355,
        "company": "TSMC",
        "title": "Data Engineer (5175)",
        "created_on": 1720635239.3869727,
        "description": "Overview Of Role As a Data Engineer, you will collect and manage data, facilitating further interpretation of the model created by data scientists. If you have a passion for data engineering and a desire to work on complex data problems, we encourage you to apply for this exciting opportunity. This position will be based in our San Jose, CA office, where we are currently operating in a hybrid environment working 3 days in the office. Responsibilities Create, construct, and maintain the essential infrastructure for storing and processing vast amounts of data. Collaborate closely with data scientists to ensure that the data is correctly organized and available for analysis. Develop data pipelines for data scientists to extract insights from structured and unstructured data using AI-enabled analytics procedures. Work with Big Data technologies, NoSQL databases, machine learning and deep learning algorithms to capture profitable growth opportunities and future challenges. Minimum Qualifications Master’s degree or higher in computer science, information systems, information science; or related field . In-depth knowledge of Python (with Jupyter, PyCharm, or similar environment), GitHub, and Markdown Experience connecting to REST API endpoints, particularly in parsing data in JSON and XML formats Experience developing web crawlers to collect data from web pages using tools such as Scrapy and BeautifulSoup In-depth knowledge of relational data management, including extracting, transforming, loading, querying from MySQL databases Familiarity with server-based front-end/UI technologies including HTML/CSS/PHP Must be willing to relocate to Taiwan for a 3-month period for training Preferred Qualifications Implementation and operationalization of Big Data technologies, including Hadoop and Spark for unstructured, non-transactional data Experience working with NoSQL databases, including Elasticsearch for documents and Neo4j for graph structured data Knowledge on how to extract content (e.g., with regular expressions, entity extraction) from unstructured data and format the extractions into data structures such as lists, trees, graphs, grids, and/or sequences Experience deploying machine learning and deep learning pipelines with serverless front-end/UI technologies, including Streamlit and Netlify In-depth knowledge of conventional machine learning and deep learning algorithms and packages, including scikit-learn, Keras, and/or Pytorch Deep understanding in finance, accounting, and market analysis Company Description As a trusted technology and capacity provider, TSMC is driven by the desire to be: The world’s leading dedicated semiconductor foundry The technology leader with a strong reputation for manufacturing excellence Advancing semiconductor manufacturing innovations to enable the future of technology TSMC pioneered the pure-play foundry business model when it was founded in 1987 and has been the world’s leading dedicated semiconductor foundry ever since. The Company supports a thriving ecosystem of global customers and partners with the industry’s leading process technologies and a portfolio of design enablement solutions to unleash innovation for the global semiconductor industry. With global operations spanning Asia, Europe, and North America, TSMC serves as a committed corporate citizen around the world. In North America, TSMC has a strong sales and service organization that works with customers by helping them achieve silicon success with cutting-edge technologies and manufacturing excellence. The Company has continued to accelerate its R&D investment and staffing in recent years and is expanding its manufacturing footprint to support customer innovation with 3D IC technologies and optimal manufacturing capacity. Diversity statement TSMC Technology, Inc. is committed to employing a diverse workforce and provides Equal Employment Opportunity for all individuals regardless of race, color, religion, gender, age, national origin, marital status, sexual orientation, gender identity, status as a protected veteran, genetic information, or any other characteristic protected by applicable law. TSMC is an equal opportunity employer prizing diversity and inclusion. We are committed to treating all employees and applicants for employment with respect and dignity. If you require reasonable accommodation due to a disability during the application or the recruiting process, please feel free to notify us at G_Accomodations@tsmc.com . TSMC confirms to all applicants its commitment to meet TSMC’s obligations under applicable employment law. Reasonable accommodations will be determined on a case-by-case basis. Pay Transparency Statement At TSMC, your base pay is only part of your overall total compensation package. At the time of this posting, this role typically pays a base salary between $115,000 and $150,000 per year. The range displayed reflects the minimum and maximum target for new hires. Actual pay may be more or less than the posted range. Factors that influence pay include the individual's skills, qualifications, education, experience and the position level and location. TSMC’s total compensation package consists of market competitive pay, allowances, bonuses, and comprehensive benefits. We also offer extensive development opportunities and programs.",
        "url": "https://www.linkedin.com/jobs/view/3946906355",
        "summary": "Data Engineer responsible for building and maintaining data infrastructure, collaborating with data scientists, developing data pipelines, and working with Big Data technologies. The role requires strong Python skills, experience with databases (MySQL, NoSQL), web scraping, and machine learning. This position is based in San Jose, CA with a hybrid work environment. A 3-month relocation to Taiwan for training is required.",
        "industries": [
            "Semiconductor",
            "Technology",
            "Data Engineering",
            "Machine Learning",
            "Artificial Intelligence"
        ],
        "soft_skills": [
            "Collaboration",
            "Communication",
            "Problem Solving",
            "Analytical Thinking",
            "Time Management",
            "Organization"
        ],
        "hard_skills": [
            "Python",
            "Jupyter",
            "PyCharm",
            "GitHub",
            "Markdown",
            "REST API",
            "JSON",
            "XML",
            "Web Scraping",
            "Scrapy",
            "BeautifulSoup",
            "MySQL",
            "Relational Databases",
            "HTML",
            "CSS",
            "PHP",
            "Hadoop",
            "Spark",
            "Elasticsearch",
            "Neo4j",
            "Regular Expressions",
            "Entity Extraction",
            "Streamlit",
            "Netlify",
            "Scikit-learn",
            "Keras",
            "Pytorch",
            "Finance",
            "Accounting",
            "Market Analysis"
        ],
        "tech_stack": [
            "Python",
            "Jupyter",
            "PyCharm",
            "GitHub",
            "Markdown",
            "REST API",
            "JSON",
            "XML",
            "Scrapy",
            "BeautifulSoup",
            "MySQL",
            "Hadoop",
            "Spark",
            "Elasticsearch",
            "Neo4j",
            "Streamlit",
            "Netlify",
            "Scikit-learn",
            "Keras",
            "Pytorch"
        ],
        "programming_languages": [
            "Python",
            "HTML",
            "CSS",
            "PHP"
        ],
        "experience": 0,
        "education": {
            "min_degree": "Master’s degree",
            "fields": [
                "Computer Science",
                "Information Systems",
                "Information Science"
            ]
        },
        "salary": {
            "max": 150000,
            "min": 115000
        },
        "benefits": [
            "Market Competitive Pay",
            "Allowances",
            "Bonuses",
            "Comprehensive Benefits",
            "Development Opportunities",
            "Programs"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Torrance, CA",
        "job_id": 3914117516,
        "company": "LTIMindtree",
        "title": "Junior AWS Data Engineer",
        "created_on": 1720635241.0654318,
        "description": "About Us: LTIMindtree is a global technology consulting and digital solutions company that enables enterprises across industries to reimagine business models, accelerate innovation, and maximize growth by harnessing digital technologies. As a digital transformation partner to more than 700+ clients, LTIMindtree brings extensive domain and technology expertise to help drive superior competitive differentiation, customer experiences, and business outcomes in a converging world. Powered by nearly 90,000 talented and entrepreneurial professionals across more than 30 countries, LTIMindtree — a Larsen & Toubro Group company — combines the industry-acclaimed strengths of erstwhile Larsen and Toubro Infotech and Mindtree in solving the most complex business challenges and delivering transformation at scale. For more information, please visit www.ltimindtree.com. Job Title: Junior AWS Cloud Data Engineer Work Location Torrance, CA Job Description: We are seeking a highly skilled and motivated AWS Cloud Data Engineer to join our dynamic team. As a key member of our cloud engineering group, you will play a crucial role in designing, implementing, and maintaining data pipelines and workflows on AWS. Your expertise in S3, AWS Glue / EMR, AWS Redshift, Athena, SQL, and optionally Python/Java/Scala will be essential in ensuring the success of our data-driven projects. Responsibilities: Data Pipeline Development: Design, develop, and maintain ETL processes using AWS Glue/EMR to extract, transform, and load data from various sources (including S3, ORC/Parquet/Text files) into AWS Redshift. Optimize data pipelines for performance, scalability, and reliability. Implement data aggregation, consolidation, and enrichment. 3+ years of experience with AWS tools and technologies (S3, EMR, Glue, Athena, RedShift) Workflow Automation: Utilize AWS Managed Airflow (Apache Airflow) to orchestrate complex data workflows. Schedule, monitor, and manage data processing tasks efficiently. Database Management: Leverage your strong SQL skills to create and optimize database queries. Strong knowledge of data storage and processing technologies, including databases (SQL and NoSQL), data lakes, and distributed computing frameworks (e.g., Hadoop, Spark). 5+ years of experience in data engineering, database design, ETL processes, and data warehousing. Support Project Processes: Work closely with cross-functional teams to understand project requirements and deliverables. Provide technical support during project execution and troubleshoot issues as needed. Optional Programming Skills: Familiarity with Python/Java/Scala is a plus. Qualifications: Bachelor’s or Master’s degree in Computer Science, Information Technology, or related field. AWS certifications (e.g., AWS Certified Data Analytics - Specialty) are advantageous. Strong problem-solving skills and ability to work independently. Excellent communication and collaboration skills.\" Benefits/perks listed below may vary depending on the nature of your employment with LTIMindtree (“LTIM”): Benefits and Perks: Medical Plan Covering Medical, Dental, Vision Term and Long-Term Disability Coverage Plan with Company match Insurance Time, Sick Leave, Paid Holidays Paternity and Maternity Leave The range displayed on each job posting reflects the minimum and maximum salary target for the position across all US locations. Within the range, individual pay is determined by work location and job level and additional factors including job-related skills, experience, and relevant education or training. Depending on the position offered, other forms of compensation may be provided as part of overall compensation like an annual performance-based bonus, sales incentive pay and other forms of bonus or variable compensation. Disclaimer: The compensation and benefits information provided herein is accurate as of the date of this posting. LTIMindtree is an equal opportunity employer that is committed to diversity in the workplace. Our employment decisions are made without regard to race, color, creed, religion, sex (including pregnancy, childbirth or related medical conditions), gender identity or expression, national origin, ancestry, age, family-care status, veteran status, marital status, civil union status, domestic partnership status, military service, handicap or disability or history of handicap or disability, genetic information, atypical hereditary cellular or blood trait, union affiliation, affectional or sexual orientation or preference, or any other characteristic protected by applicable federal, state, or local law, except where such considerations are bona fide occupational qualifications permitted by law. Safe return to office : In order to comply with LTIMindtree’ s company COVID-19 vaccine mandate, candidates must be able to provide proof of full vaccination against COVID-19 before or by the date of hire. Alternatively, one may submit a request for reasonable accommodation from LTIMindtree’s COVID-19 vaccination mandate for approval, in accordance with applicable state and federal law, by the date of hire. Any request is subject to review through LTIMindtree’s applicable processes.",
        "url": "https://www.linkedin.com/jobs/view/3914117516",
        "summary": "LTIMindtree is seeking an AWS Cloud Data Engineer to design, implement, and maintain data pipelines on AWS. The role requires expertise in S3, AWS Glue/EMR, AWS Redshift, Athena, SQL, and optionally Python/Java/Scala. Responsibilities include developing ETL processes, optimizing data pipelines, implementing data aggregation, and automating workflows using AWS Managed Airflow. Strong SQL skills and experience with data storage and processing technologies are essential.",
        "industries": [
            "Technology",
            "Consulting",
            "Digital Solutions",
            "Data Engineering",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Problem-solving",
            "Communication",
            "Collaboration"
        ],
        "hard_skills": [
            "AWS Glue",
            "EMR",
            "S3",
            "Redshift",
            "Athena",
            "SQL",
            "Python",
            "Java",
            "Scala",
            "Apache Airflow",
            "Data Warehousing",
            "ETL",
            "Database Design",
            "Hadoop",
            "Spark"
        ],
        "tech_stack": [
            "AWS",
            "S3",
            "AWS Glue",
            "EMR",
            "AWS Redshift",
            "Athena",
            "SQL",
            "Python",
            "Java",
            "Scala",
            "Apache Airflow",
            "Hadoop",
            "Spark"
        ],
        "programming_languages": [
            "Python",
            "Java",
            "Scala"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor’s",
            "fields": [
                "Computer Science",
                "Information Technology"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Medical Plan",
            "Dental",
            "Vision",
            "Disability Coverage",
            "401k",
            "Life Insurance",
            "Paid Time Off",
            "Sick Leave",
            "Paid Holidays",
            "Paternity Leave",
            "Maternity Leave"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Sunnyvale, CA",
        "job_id": 3959133639,
        "company": "Vakula Technologies Inc",
        "title": "Big Data Engineer",
        "created_on": 1720635243.1811633,
        "description": "Job Role: Big Data Engineer Location: Sunnyvale, CA Contract: W2 Strong in real time & batch pipelines in big data technologies (i.e. Spark/Kafka/Cassandra/Hadoop/Hive/Elasticsearch) Proficient in RESTful Services, Java, Scala, Spring Boot/Play Framework, RDBMS, NoSql, Python, Trino, Airflow, Looker, Bit Query Proficient in development of scalable cloud native microservices Proficient with Designing and building APIs Azure and or GCP exposure and experience",
        "url": "https://www.linkedin.com/jobs/view/3959133639",
        "summary": "Big Data Engineer with experience in real-time and batch pipelines using technologies like Spark, Kafka, Cassandra, Hadoop, Hive, Elasticsearch, RESTful Services, Java, Scala, Spring Boot/Play Framework, RDBMS, NoSql, Python, Trino, Airflow, Looker, Bit Query, cloud native microservices, API design and development, and exposure to Azure and/or GCP.",
        "industries": [
            "Technology",
            "Data Science",
            "Software Engineering"
        ],
        "soft_skills": [
            "Problem Solving",
            "Communication",
            "Collaboration",
            "Analytical Thinking"
        ],
        "hard_skills": [
            "Spark",
            "Kafka",
            "Cassandra",
            "Hadoop",
            "Hive",
            "Elasticsearch",
            "RESTful Services",
            "Java",
            "Scala",
            "Spring Boot",
            "Play Framework",
            "RDBMS",
            "NoSql",
            "Python",
            "Trino",
            "Airflow",
            "Looker",
            "Bit Query",
            "Microservices",
            "API Design",
            "Azure",
            "GCP"
        ],
        "tech_stack": [
            "Spark",
            "Kafka",
            "Cassandra",
            "Hadoop",
            "Hive",
            "Elasticsearch",
            "RESTful Services",
            "Java",
            "Scala",
            "Spring Boot",
            "Play Framework",
            "RDBMS",
            "NoSql",
            "Python",
            "Trino",
            "Airflow",
            "Looker",
            "Bit Query",
            "Microservices",
            "API Design",
            "Azure",
            "GCP"
        ],
        "programming_languages": [
            "Java",
            "Scala",
            "Python"
        ],
        "experience": 0,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Data Science",
                "Engineering"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco Bay Area",
        "job_id": 3945916377,
        "company": "MasterClass",
        "title": "Sr. Data Engineer",
        "created_on": 1720635245.0872602,
        "description": "Who We Are MasterClass is the streaming platform where the world’s best come together so anyone, anywhere, can access and be inspired by their knowledge and stories. We put you in the room with the creators, thinkers, makers and leaders who have changed the world, so that you can change yours. Members get unprecedented access to 150+ instructors and classes across a wide variety of fields, including Arts & Entertainment, Business, Design & Style, Sports & Gaming, Writing and more. Step into Nas’ recording studio and Gordon Ramsay’s kitchen, and go behind the big screen with James Cameron. Design your career with Elaine Welteroth, get ready to win with Lewis Hamilton, perfect your pitch with Shonda Rhimes and discover your inner negotiator with Chris Voss. It's a pivotal time for MasterClass – and we want you to be a part of the journey. With offices in San Francisco (HQ) and Waterloo, Ontario plus a studio in Los Angeles, we are looking to expand our team to support the business. If you want to help impact our members' lives – we want to hear from you! Snapshot Of The Role At MasterClass, data is pivotal to our decision-making processes, influencing business strategies, product development, content creation, and operational efficiency. Our expanding Data teams are central to the company, collaborating across various departments to drive decisions and steer future growth at MasterClass. Our Data engineering team tackles challenging problems across many technical disciplines, including data warehouse, batch compute infrastructure, data orchestration systems, critical integrations (and more.) We seek an exceptional Senior Data Engineer to help design, build, and operate our data platform to scale the business and enable the Data organization and teams to solve challenges. This role is a remote position with periodic in-person touchpoints (HQ in San Francisco, CA) What You Will Do Design, build, and manage our data warehouse and data ingestion solutions. Understand and translate business needs into data models to support long-term, scalable, and reliable data pipelines. Enhance and maintain Data Infrastructure using best practices and latest features to ensure high data quality. Define and manage SLAs for data sets and processes running in production Continuously improve our data infrastructure and empower teams with the best data tooling and systems. Build strong cross-functional partnerships with Data Scientists, Analysts, Product Managers, and Software Engineers to understand data needs and deliver on those needs. Be part of a data engineering team that is also responsible for the reliability of the data systems that are built and be available to respond to critical incidents as needed Requirements 4+ years of experience in Data Engineering and Data Warehousing Bachelor's degree in a quantitative field, e.g., Computer Science, Math, Physics Experience scaling data environments with distributed processing technologies Advanced proficiency with SQL, Python, Postgres, and integrations via APIs Experience working in AWS cloud environment in designing and implementing a cloud data warehouse, other types of storage, and developing ETL/ELT pipelines Experience integrating and building data platforms in support of BI, Analytics, and Data Science Strong communication skills, with the ability to initiate and drive projects proactively and accurately Eligible to work in the United States legally At MasterClass, we believe we put our best work forward when our employees bring together ideas that are diverse in thought. We are proud to be an equal opportunity workplace and are committed to equal employment opportunity regardless of race, color, religion, national origin, age, sex, marital status, ancestry, physical or mental disability, genetic information, veteran status, gender identity or expression, sexual orientation, or any other characteristic protected by applicable federal, state or local law. In addition, MasterClass will provide reasonable accommodations for qualified individuals with disabilities. If you have a disability or special need, we would like to know how we can better accommodate you. The salary range listed is for NYC/CA/WA. As a company, we have a location based strategy, which means the disclosed range estimate has not been adjusted for any applicable geographic differential associated with the location where the position may be filled. MasterClass’s salary ranges are based on paying competitively for our size and industry. In addition to salary, we also offer equity and comprehensive benefits (medical, dental, vision, flexible PTO, and more). The range listed is for the expectations as laid out in the job description, however we are often open to a wide variety of profiles, and recognize that the person we hire may be less experienced (or more senior) than this job description as posted. If that ends up being the case, the updated salary range will be communicated with you as a candidate. Salary Range $182,000—$205,000 USD",
        "url": "https://www.linkedin.com/jobs/view/3945916377",
        "summary": "MasterClass is seeking a Senior Data Engineer to design, build, and manage their data warehouse and data ingestion solutions. The role involves understanding business needs, enhancing data infrastructure, defining SLAs, and collaborating with various teams to ensure data quality and reliability. The ideal candidate has 4+ years of experience in data engineering and warehousing, proficiency in SQL, Python, Postgres, AWS cloud environment, and strong communication skills.",
        "industries": [
            "Technology",
            "Streaming",
            "Education",
            "Entertainment",
            "Media"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Project Management",
            "Decision Making",
            "Analytical Thinking",
            "Problem Solving"
        ],
        "hard_skills": [
            "Data Engineering",
            "Data Warehousing",
            "SQL",
            "Python",
            "Postgres",
            "AWS",
            "ETL/ELT",
            "API Integration",
            "Data Modeling",
            "Data Pipelines",
            "Data Quality",
            "Data Infrastructure",
            "Distributed Processing"
        ],
        "tech_stack": [
            "AWS",
            "Postgres",
            "SQL",
            "Python"
        ],
        "programming_languages": [
            "SQL",
            "Python"
        ],
        "experience": 4,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Computer Science",
                "Math",
                "Physics"
            ]
        },
        "salary": {
            "max": 205000,
            "min": 182000
        },
        "benefits": [
            "Medical",
            "Dental",
            "Vision",
            "Flexible PTO",
            "Equity"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Los Angeles, CA",
        "job_id": 3925057980,
        "company": "Spruce Technology, Inc.",
        "title": "Data Engineer",
        "created_on": 1720635248.3634086,
        "description": "The Los Angeles Department of Building and Safety (LADBS) Technology Services Bureau (TSB) is requesting to solicit the services of a Data Engineer who will primarily focus on building data pipelines. They will be expected to leverage a variety of advanced tools and technologies such as Kafka/Kinesis for real-time data processing/streaming, Relational/No-SQL databases for robust data storage and management, and other Integration tools for seamless data flow across various cloud and on-premises platforms. The Data Engineer will also utilize ETL processes to extract data from various sources, transform the data to fit operational and business needs, and load it into an end target. In addition to these, they will be expected to have familiarity with Pub-Sub messaging patterns or similar data dissemination models to ensure efficient data distribution and consumption. One of the primary goals is to create a real-time bidirectional data pipeline from Oracle transactional databases to a data lake in the cloud. Work Products and Outcomes: The Data Engineer shall meet the following key high-level work products and outcomes as identified by LADBS: (Note: this list is not exhaustive.) Develop, construct, test, and maintain data architectures and pipelines. Create best-practice ETL frameworks; repeatable and reliable data pipelines that convert data into powerful signals and features. Handle raw data (structured, unstructured, and semi-structured) and align it into a more usable, structured format that is better suited for reporting and analytics. Work with the cloud solutions architect to ensure data solutions are aligned with company platform architecture and all aspects related to infrastructure. Collaborate with business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization. Ensure data pipeline architecture will support the requirements of the business. Document processes and perform periodic system reviews to ensure adherence to established standards and processes. Evaluate and advise on technical aspects of open work requests in the product backlog with the project lead. Define Cloud infrastructure Reference Architectures for common solution archetypes Real-time bidirectional data pipeline from Oracle transactional databases to a data lake in the cloud. Clear, comprehensive documentation related to the data pipeline. Regular reports on the status of data pipeline development. Other related tasks identified by LADBS Performance Specifications: The qualified candidate must possess the following skills and experience in the following areas: A bachelor's degree in Computer Science, Data Science, Software/Computer Engineering, or a related field. Proven experience as a data engineer or in a similar role, with a track record of manipulating, processing, and extracting value from large disconnected datasets. Demonstrated technical proficiency with data architecture, databases, and processing large data sets. Proficient in Oracle databases and comprehensive understanding of ETL processes, including creating and implementing custom ETL processes. Experience with cloud services (AWS, Azure), and understanding of distributed systems, such as Hadoop/MapReduce, Spark, or equivalent technologies. Knowledge of Kafka, Kinesis, OCI Data Integration, Azure Service Bus or similar technologies for real-time data processing and streaming. Experience designing, building, and maintaining data processing systems, as well as experience working with either a MapReduce or an MPP system. Strong organizational, critical thinking, and problem-solving skills, with clear understanding of high-performance algorithms and Python scripting. Experience with machine learning toolkits, data ingestion technologies, data preparation technologies, and data visualization tools is a plus. Excellent communication and collaboration abilities, with the ability to work in a dynamic, team-oriented environment and adapt to changes in a fast-paced work environment. Data-driven mindset, with the ability to translate business requirements into data solutions. Experience with version control systems e.g. Git, and with agile methodologies/scrum. Certifications in related field would be an added advantage (e.g. Google Certified Professional Data Engineer, AWS Certified Big Data, etc.). Work hours and location: Estimated Start Date: 5/8/2024 ** The candidate proposed must be available on the estimated start date. Estimated Completion Date: 3/22/2025 This is a Remote role Evaluation Criteria: LADBS will review the TOS Responses received and select up to 5 of the most qualified candidates to be interviewed based on a review of the resumes provided and the criteria below. Education Relevant degree in Computer Science, Engineering, Information Technology, or related field Advanced degrees or certifications related to data engineering Experience Previous work experience with data migration and engineering Hands-on experience with data warehouses Demonstrated experience in managing and optimizing data pipelines and architectures Technical Knowledge Strong understanding of streaming data platforms and pub-sub models In-depth knowledge of data warehousing concepts, including data storage, retrieval, and pipeline optimization Evaluation Criteria Points Education and experience30Technical Knowledge50Cost10Qualifications based on a review of the candidate's resume10Total100 Spruce Technology, Inc. is a mid-size, award-winning (Inc 5000, SmartCEO, Entrepreneur of the Year) technology services firm with a steadily growing portfolio of commercial and government clients. Spruce provides innovative technology solutions, specialized IT staff, and IT strategy consulting nationwide. Spruce maintains partnerships with major technology vendors and continually develops leading-edge offerings in service areas such as digital experience, data services, application development, infrastructure, cyber security, and IT staffing. Spruce Technology, Inc. is an affirmative action and equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, disability, age, sexual orientation, gender identity, national origin, veteran status, or genetic information. Consistent with the Americans with Disabilities Act, it is the policy of Spruce Technology, Inc. to provide reasonable accommodation when requested by a qualified applicant or employee with a disability, unless such accommodation would cause an undue hardship. The policy regarding requests for reasonable accommodation applies to all aspects of employment, including the application process.",
        "url": "https://www.linkedin.com/jobs/view/3925057980",
        "summary": "The Los Angeles Department of Building and Safety (LADBS) is seeking a Data Engineer to build and maintain real-time data pipelines. This role involves extracting, transforming, and loading data from Oracle databases into a data lake in the cloud, using technologies like Kafka/Kinesis, relational/NoSQL databases, and ETL processes. The ideal candidate will have experience with cloud services (AWS, Azure), distributed systems, and pub-sub messaging patterns. They will also possess strong organizational, problem-solving, and communication skills.",
        "industries": [
            "Government",
            "Technology",
            "Data Engineering"
        ],
        "soft_skills": [
            "Organizational",
            "Critical Thinking",
            "Problem-solving",
            "Communication",
            "Collaboration",
            "Adaptability",
            "Data-driven",
            "Teamwork"
        ],
        "hard_skills": [
            "Data Architecture",
            "Databases",
            "ETL Processes",
            "Oracle Databases",
            "Cloud Services (AWS, Azure)",
            "Distributed Systems",
            "Hadoop/MapReduce",
            "Spark",
            "Kafka",
            "Kinesis",
            "OCI Data Integration",
            "Azure Service Bus",
            "MapReduce",
            "MPP Systems",
            "Python Scripting",
            "Machine Learning",
            "Data Ingestion",
            "Data Preparation",
            "Data Visualization",
            "Git",
            "Agile Methodologies/Scrum"
        ],
        "tech_stack": [
            "Kafka",
            "Kinesis",
            "Oracle",
            "Hadoop",
            "MapReduce",
            "Spark",
            "AWS",
            "Azure",
            "Python",
            "Git"
        ],
        "programming_languages": [
            "Python"
        ],
        "experience": 3,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Data Science",
                "Software/Computer Engineering"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Los Angeles, CA",
        "job_id": 3959241630,
        "company": "LTIMindtree",
        "title": "Data Engineer",
        "created_on": 1720635250.1518958,
        "description": "Work closely with project managers to understand and maintain focus on their analytics needs including critical metrics and KPIs and deliver actionable insights to relevant decisionmakers Proactively analyze data to answer key questions for stakeholders or yourself with an eye on what drives business performance and investigate and communicate which areas need improvement in efficiency and productivity Create and maintain rich interactive visualizations through data interpretation and analysis with reporting components from multiple data sources Define and implement data acquisition and integration logic selecting an appropriate combination of methods and tools within the defined technology stack to ensure optimal scalability and performance of the solution Develop and maintain databases by acquiring data from primary and secondary sources and build scripts that will make our data evaluation process more flexible or scalable across datasets Required Skills And Qualifications Three or more years of experience mining data as a data analyst Proven analytics skills including mining evaluation and visualization Technical writing experience in relevant areas including queries reports and presentations Strong SQL or Excel skills with aptitude for learning other analytics tools",
        "url": "https://www.linkedin.com/jobs/view/3959241630",
        "summary": "Data Analyst to analyze data, create visualizations, and deliver actionable insights. Develop and maintain databases, ensuring data integrity and scalability. Requires 3+ years of experience, strong SQL or Excel skills, and proven analytics capabilities.",
        "industries": [
            "Data Analysis",
            "Business Intelligence",
            "Analytics",
            "Technology"
        ],
        "soft_skills": [
            "Communication",
            "Problem Solving",
            "Analytical Thinking",
            "Project Management",
            "Data Interpretation",
            "Collaboration",
            "Decision Making",
            "Time Management"
        ],
        "hard_skills": [
            "Data Mining",
            "Data Analysis",
            "Data Visualization",
            "Technical Writing",
            "SQL",
            "Excel",
            "Data Acquisition",
            "Data Integration",
            "Database Development",
            "Scripting"
        ],
        "tech_stack": [
            "SQL",
            "Excel"
        ],
        "programming_languages": [
            "SQL"
        ],
        "experience": 3,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3895817853,
        "company": "Unreal Staffing, Inc",
        "title": "Data Engineer",
        "created_on": 1720635254.3389375,
        "description": "Company Overview: Welcome to the forefront of data-driven innovation! At our company, we're revolutionizing how businesses leverage data to drive insights and make informed decisions. Our mission is to develop scalable and robust data infrastructure that empowers organizations to unlock the full potential of their data. Join us and be part of a dynamic team committed to shaping the future of data engineering. Position Overview: As a Data Engineer, you'll play a crucial role in designing, building, and maintaining the data infrastructure that powers our analytics and machine learning initiatives. Working closely with cross-functional teams of data scientists, analysts, and software engineers, you'll ensure the reliability, scalability, and efficiency of our data pipelines and systems. If you're passionate about data engineering and eager to drive innovation through scalable data solutions, we want you on our team. Requirements Key Responsibilities: Data Pipeline Development: Design, build, and maintain scalable and reliable data pipelines for ingesting, processing, and transforming large volumes of data from diverse sources Data Modeling: Design and implement data models and schemas to support analytics, reporting, and machine learning applications, ensuring data quality, consistency, and performance Data Integration: Integrate data from internal and external sources, including databases, APIs, and streaming platforms, ensuring data consistency and integrity across systems Data Transformation: Implement data transformation and cleansing processes to standardize and enrich data for analysis and modeling, using tools and frameworks such as Apache Spark or Apache Beam Data Storage and Management: Manage data storage solutions, including data lakes, data warehouses, and NoSQL databases, optimizing for performance, cost, and scalability Data Governance and Security: Implement data governance policies and security controls to ensure compliance with regulatory requirements and protect sensitive data Monitoring and Optimization: Monitor data pipelines and systems for performance, reliability, and efficiency, proactively identifying and addressing issues to minimize downtime and optimize resource utilization Documentation and Collaboration: Document data infrastructure and processes, including data dictionaries, data lineage, and workflow diagrams, and collaborate with cross-functional teams to ensure alignment and transparency Qualifications: Bachelor's degree or higher in Computer Science, Engineering, or related field Strong background in data engineering, with hands-on experience in designing, building, and maintaining data pipelines and systems Proficiency in programming languages such as Python, Java, or Scala, and experience with data processing frameworks such as Apache Spark or Apache Beam Experience with data modeling and database design, including relational databases (e.g., PostgreSQL, MySQL) and NoSQL databases (e.g., MongoDB, Cassandra) Familiarity with data integration tools and technologies, such as Apache Kafka, AWS Glue, or Google Dataflow Strong problem-solving abilities and analytical thinking, with a keen attention to detail and a passion for tackling complex technical challenges Excellent communication and collaboration skills, with the ability to work effectively in cross-functional teams and communicate technical concepts to non-technical stakeholders Benefits Competitive salary: The industry standard salary for Data Engineers typically ranges from $140,000 to $210,000 per year, depending on experience and qualifications. Exceptional candidates may be eligible for higher compensation packages Comprehensive health, dental, and vision insurance plans Flexible work hours and remote work options Generous vacation and paid time off Professional development opportunities, including access to training programs, conferences, and workshops State-of-the-art technology environment with access to cutting-edge tools and resources Vibrant and inclusive company culture with team-building activities and social events Opportunities for career growth and advancement within the company Exciting projects with real-world impact across diverse industries Chance to work alongside top talent and industry experts in the field of data engineering Join Us: Ready to shape the future of data engineering? Apply now to join our team and be part of an exciting journey of innovation and discovery!",
        "url": "https://www.linkedin.com/jobs/view/3895817853",
        "summary": "This is a job description for a Data Engineer at a company focused on data-driven innovation. The role involves designing, building, and maintaining data infrastructure for analytics and machine learning. Key responsibilities include data pipeline development, modeling, integration, transformation, storage and management, governance, monitoring, and collaboration. The ideal candidate will have a strong background in data engineering, proficiency in programming languages like Python, Java, or Scala, experience with data processing frameworks such as Apache Spark or Apache Beam, and knowledge of various data storage technologies.",
        "industries": [
            "Data Engineering",
            "Data Science",
            "Machine Learning",
            "Analytics",
            "Software Development",
            "Technology"
        ],
        "soft_skills": [
            "Problem-solving",
            "Analytical thinking",
            "Attention to detail",
            "Communication",
            "Collaboration",
            "Teamwork",
            "Technical communication",
            "Stakeholder management"
        ],
        "hard_skills": [
            "Python",
            "Java",
            "Scala",
            "Apache Spark",
            "Apache Beam",
            "Data Modeling",
            "Database Design",
            "PostgreSQL",
            "MySQL",
            "MongoDB",
            "Cassandra",
            "Apache Kafka",
            "AWS Glue",
            "Google Dataflow",
            "Data Governance",
            "Data Security"
        ],
        "tech_stack": [
            "Apache Spark",
            "Apache Beam",
            "PostgreSQL",
            "MySQL",
            "MongoDB",
            "Cassandra",
            "Apache Kafka",
            "AWS Glue",
            "Google Dataflow"
        ],
        "programming_languages": [
            "Python",
            "Java",
            "Scala"
        ],
        "experience": 0,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Computer Science",
                "Engineering"
            ]
        },
        "salary": {
            "max": 210000,
            "min": 140000
        },
        "benefits": [
            "Competitive salary",
            "Health insurance",
            "Dental insurance",
            "Vision insurance",
            "Flexible work hours",
            "Remote work",
            "Vacation",
            "Paid time off",
            "Professional development",
            "Training programs",
            "Conferences",
            "Workshops",
            "Cutting-edge technology",
            "Team building activities",
            "Social events",
            "Career growth",
            "Advancement opportunities"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Los Gatos, CA",
        "job_id": 3902136641,
        "company": "Netflix",
        "title": "Analytics Engineer (L5) - Finance",
        "created_on": 1720635256.0196376,
        "description": "Netflix is one of the world's leading entertainment services with over 260 million paid memberships in over 190 countries enjoying TV series, films and games across a wide variety of genres and languages. Members can play, pause and resume watching as much as they want, anytime, anywhere, and can change their plans at any time. As Netflix's business grows, financial data is becoming increasingly mission critical. Netflix’s financial footprint is expanding with complexity due to its global scale, product innovations and business model evolvements. The Finance Data Science and Engineering team is part of the centralized Data and Insights organization, supporting the office of our Chief Financial Officer. The team’s core mission is to enable our business partners to access key financial data efficiently and provide insights to drive revenue growth and expense optimization. The team is looking to add a Senior Analytics Engineer, who will work collaboratively with cross-functional partners including Finance, FP&A, Data Engineering, Product, and Engineering teams to build insights and tools that help the company make better data-driven decisions. To learn more about analytics engineering at Netflix, read here. Visit our culture deck and long-term view to learn more about the unique Netflix culture and the opportunity to be part of our team. In This Role, You Will Partner closely with Finance leads to identify strategic, high-impact analytical problems and innovative ways to solve them with data. Conduct statistical analyses and modeling, exploratory analysis, and metric development to uncover data insights and inform key decisions. Deliver data insights and drive for adoption through tools (e.g. dashboards, self-serve reporting), memos and presentations. Develop scalable data pipelines to collect, clean, and analyze data from various sources; prototype, and productionize metrics and models. Facilitate data ownership and accountability by closely partnering with data engineering, product, and engineering partners to improve data robustness. To Be Successful In This Role, You Are/have Highly effective in engaging with diverse stakeholders and adept at cultivating strong partnerships. Strategic-minded, impact-driven, and capable of incorporating larger business context into data questions and product development. A background in statistics, math, data science, or a similar quantitative field, with strong statistical skills and intuition. High proficiency in statistical programming, Python preferred. High proficiency in scripting with SQL, extracting large sets of data, and designing ETL flows. Experienced with developing data tools, memos, and presentations to deliver data and insights to stakeholders. Past experience in solving problems in Finance or other business-facing areas is a plus. A passionate learner who is eager to learn and apply a broad set of data techniques. A self-starter who thrives under a high level of ambiguity and autonomy. Our compensation structure consists solely of an annual salary; we do not have bonuses. You choose each year how much of your compensation you want in salary versus stock options. To determine your personal top of market compensation, we rely on market indicators and consider your specific job family, background, skills, and experience to determine your compensation in the market range. The range for this role is $170,000 - $720,000. Netflix provides comprehensive benefits including Health Plans, Mental Health support, a 401(k) Retirement Plan with employer match, Stock Option Program, Disability Programs, Health Savings and Flexible Spending Accounts, Family-forming benefits, and Life and Serious Injury Benefits. We also offer paid leave of absence programs. Full-time hourly employees accrue 35 days annually for paid time off to be used for vacation, holidays, and sick paid time off. Full-time salaried employees are immediately entitled to flexible time off. See more detail about our Benefits here. Netflix is a unique culture and environment. Learn more here. We are an equal-opportunity employer and celebrate diversity, recognizing that diversity of thought and background builds stronger teams. We approach diversity and inclusion seriously and thoughtfully. We do not discriminate on the basis of race, religion, color, ancestry, national origin, caste, sex, sexual orientation, gender, gender identity or expression, age, disability, medical condition, pregnancy, genetic makeup, marital status, or military service.",
        "url": "https://www.linkedin.com/jobs/view/3902136641",
        "summary": "Netflix is seeking a Senior Analytics Engineer to join their Finance Data Science and Engineering team. This role involves collaborating with various departments to build insights and tools for data-driven decision making. Responsibilities include identifying analytical problems, conducting statistical analyses, developing data pipelines, and delivering insights through dashboards, memos, and presentations. The ideal candidate possesses strong analytical skills, proficiency in Python, SQL, and data engineering, and experience in Finance or business-facing areas. This position offers a competitive salary range of $170,000 - $720,000 and comprehensive benefits.",
        "industries": [
            "Entertainment",
            "Media",
            "Finance",
            "Technology"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Strategic Thinking",
            "Data-Driven Decision Making",
            "Impact-Driven",
            "Analytical Thinking",
            "Critical Thinking",
            "Self-Starter",
            "Adaptability"
        ],
        "hard_skills": [
            "Python",
            "SQL",
            "Statistical Analysis",
            "Modeling",
            "Data Pipelines",
            "ETL",
            "Dashboards",
            "Reporting",
            "Presentations",
            "Data Engineering"
        ],
        "tech_stack": [
            "Python",
            "SQL",
            "Data Pipelines",
            "ETL",
            "Dashboards",
            "Reporting",
            "Presentations"
        ],
        "programming_languages": [
            "Python",
            "SQL"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": [
                "Statistics",
                "Math",
                "Data Science"
            ]
        },
        "salary": {
            "max": 720000,
            "min": 170000
        },
        "benefits": [
            "Health Plans",
            "Mental Health Support",
            "401(k) Retirement Plan with employer match",
            "Stock Option Program",
            "Disability Programs",
            "Health Savings and Flexible Spending Accounts",
            "Family-forming benefits",
            "Life and Serious Injury Benefits",
            "Paid leave of absence programs",
            "Paid Time Off",
            "Flexible Time Off"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Santa Clara, CA",
        "job_id": 3941213879,
        "company": "Amick Brown - SAP, Cloud Technologies and Business Intelligence Staffing & Consulting",
        "title": "Data Engineer",
        "created_on": 1720635258.1841273,
        "description": "Amick Brown is seeking an experienced Data Engineer for our direct client. Location : Santa Clara, CA Duration: 6 months Estimated pay range: $76.34 - 95.43 per client contract and candidate skills, experience and work location. Job Description Description Data Engineer: Designs, builds and oversees the deployment and operation of technology architecture, solutions and software to capture, manage, store and utilize structured and unstructured data from internal and external sources. Establishes and builds processes and structures based on business and technical requirements to channel data from multiple inputs, route appropriately and store using any combination of distributed (cloud) structures, local databases, and other applicable storage forms as required. Develops technical tools and programming that leverage artificial intelligence, machine learning and big-data techniques to cleanse, organize and transform data and to maintain, defend and update data structures and integrity on an automated basis. Creates and establishes design standards and assurance processes for software, systems and applications development to ensure compatibility and operability of data connections, flows and storage requirements. Establish product requirements with proper documentation for quality control, and support testing effort. BS degree: 7-9 years of hands-on experience MS degree- 4-5 years of hands-on experience In order to perform the responsibilities of this position, the individual must have: M.S. in Computer Science, Software/Computer Engineering, or Applied Math with minimum of 4 years industry experience or B.S. degree with minimum (7) years industry experience Demonstrated excellent communication skills both written and verbal Ability to independently work with services team to gather product requirements and manage development life cycle Demonstrated ability to work on large data sets Interested in early pipeline research and development/prototype efforts Proficient with relational SQL ( Microsoft SQL , MySQL, Snowflake Postgres, Mongo, NoSQL etc.) Proficient in two of Python, C# Solid knowledge of statistics Proficient in AWS (S3,CLI, Lambda, SNS, SQS) Good understanding of Event driven workflows and patterns Proficient in Dockers/Containers Any experience in the following would be ideal o EKS or Kubernetes o Elastic Search - ELK o Database design and management o CI/CD pipeline and Build tools such as Jenkins, CircleCI, GitLab etc. Following Skills Sets Is Must 5 years of hands-on experience working on data using SQL on multiple platforms (SQL server, my SQL, NoSQL, Snowflake, Mongo) (required) 4 years of hands-on experience with Python and C# programming (required) 4 years of hands-on AWS Stack (S3, CLI, Lamdba, SNS,SQS) 3-4 years of hands-on experience with Application Development Skills - web based or service based (required) 3 years of hands-on experience with ETL tools and automation (required) 2 years experience of using statistics for data modeling and predictions Amick Brown is an Information Technology consulting company specializing in ERP, Data Analytics, Information Security, Application Development, Networking, and Cloud Computing. The company was founded in 2010 and is headquartered in San Ramon, California. Regular full-time employees are eligible for the following Amick Brown provided benefits: Health Vision Dental 401k with company match Paid time off Sick Leave long-Term Disability Life Insurance Wellness & Discount Programs",
        "url": "https://www.linkedin.com/jobs/view/3941213879",
        "summary": "Amick Brown is seeking a Data Engineer with 4+ years of experience in AWS, Python, C#, SQL, and ETL tools. The ideal candidate will have experience with Kubernetes, Elastic Search, and CI/CD pipelines. This is a 6-month contract position in Santa Clara, CA, with a pay range of $76.34 to $95.43 per hour.",
        "industries": [
            "Information Technology",
            "Data Analytics",
            "Cloud Computing",
            "ERP"
        ],
        "soft_skills": [
            "Communication",
            "Problem-solving",
            "Teamwork",
            "Analytical",
            "Research",
            "Organization",
            "Documentation"
        ],
        "hard_skills": [
            "SQL",
            "Python",
            "C#",
            "AWS",
            "S3",
            "CLI",
            "Lambda",
            "SNS",
            "SQS",
            "ETL",
            "Data Modeling",
            "Statistics",
            "Docker",
            "Containers",
            "Kubernetes",
            "EKS",
            "Elasticsearch",
            "ELK",
            "Database Design",
            "CI/CD",
            "Jenkins",
            "CircleCI",
            "GitLab"
        ],
        "tech_stack": [
            "AWS",
            "S3",
            "CLI",
            "Lambda",
            "SNS",
            "SQS",
            "Docker",
            "Containers",
            "Kubernetes",
            "EKS",
            "Elasticsearch",
            "ELK",
            "Jenkins",
            "CircleCI",
            "GitLab"
        ],
        "programming_languages": [
            "SQL",
            "Python",
            "C#"
        ],
        "experience": 4,
        "education": {
            "min_degree": "MS",
            "fields": [
                "Computer Science",
                "Software/Computer Engineering",
                "Applied Math"
            ]
        },
        "salary": {
            "max": 9543,
            "min": 7634
        },
        "benefits": [
            "Health",
            "Vision",
            "Dental",
            "401k",
            "Paid Time Off",
            "Sick Leave",
            "Long-Term Disability",
            "Life Insurance",
            "Wellness Programs",
            "Discount Programs"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "California, United States",
        "job_id": 3817683828,
        "company": "MARSHALL TECHNOLOGIES INC",
        "title": "Data Engineer",
        "created_on": 1720635259.8631392,
        "description": "Hi, Greetings for the day, This is Anthony from Marshall Technologies and I have a Contract position with my client. Below is the job description, which can be filled immediately. Kindly respond to this requirement with your resume, contact and current location info to speed up the further process Looking for Senior Data Engineer with Spark, Scala, GCP experience. Mandatory Areas Must Have Skills – Spark – 8+ Yrs of Exp Scala – 8+ Yrs of Exp GCP –5+ Yrs of Exp Hive– 8+Yrs of Exp SQL - 8+ Yrs of Exp ETL Process / Data Pipeline - 8+ Years of experience Location – Sunnyvale, CA Onsite Requirement - Yes Number of days onsite – 2 to 3 days to start with If Onsite – Office Address – 860 W California Ave, Sunnyvale, CA 94086 UST Global® is looking for a highly energetic and collaborative Senior Data Engineer(10+ yrs) for a 12-month engagement. Responsibilities As a Senior Data Engineer, you will Design and develop big data applications using the latest open source technologies. Desired working in offshore model and Managed outcome Develop logical and physical data models for big data platforms. Automate workflows using Apache Airflow. Create data pipelines using Apache Hive, Apache Spark, Scala, Apache Kafka. Provide ongoing maintenance and enhancements to existing systems and participate in rotational on-call support. Learn our business domain and technology infrastructure quickly and share your knowledge freely and actively with others in the team. Mentor junior engineers on the team Lead daily standups and design reviews Groom and prioritize backlog using JIRA Act as the point of contact for your assigned business domain Requirements 8+ years of hands-on experience with developing data warehouse solutions and data products. 4+ years of hands-on experience developing a distributed data processing platform with Hadoop, Hive,Scala, Airflow or a workflow orchestration solution are required 4 + years of experience in GCP,GCS Data proc, BIG Query 2+ years of hands-on experience in modeling(Erwin) and designing schema for data lakes or for RDBMS platforms. Experience with programming languages: Python, Java, Scala, etc. Experience with scripting languages: Perl, Shell, etc. Practice working with, processing, and managing large data sets (multi TB/PB scale). Exposure to test driven development and automated testing frameworks. Background in Scrum/Agile development methodologies. Capable of delivering on multiple competing priorities with little supervision. Excellent verbal and written communication skills. Bachelor's Degree in computer science or equivalent experience. The most successful candidates will also have experience in the following: Gitflow Atlassian products – BitBucket, JIRA, Confluence etc. Continuous Integration tools such as Bamboo, Jenkins, or TFS",
        "url": "https://www.linkedin.com/jobs/view/3817683828",
        "summary": "Senior Data Engineer with 8+ years of experience in Spark, Scala, GCP, Hive, and SQL. Responsible for designing and developing big data applications, data models, automating workflows using Apache Airflow, creating data pipelines, and providing ongoing maintenance. Experience with Hadoop, Hive, Scala, Airflow, GCP, GCS Data proc, BIG Query, Erwin, Python, Java, Perl, Shell, and Scrum/Agile methodologies is required. ",
        "industries": [
            "Data Engineering",
            "Software Development",
            "Technology"
        ],
        "soft_skills": [
            "Collaborative",
            "Energetic",
            "Communication",
            "Problem Solving",
            "Teamwork",
            "Leadership",
            "Mentorship"
        ],
        "hard_skills": [
            "Spark",
            "Scala",
            "GCP",
            "Hive",
            "SQL",
            "ETL",
            "Hadoop",
            "Airflow",
            "Kafka",
            "Python",
            "Java",
            "Perl",
            "Shell",
            "Erwin",
            "Gitflow",
            "Jira",
            "Confluence",
            "Bamboo",
            "Jenkins",
            "TFS"
        ],
        "tech_stack": [
            "Spark",
            "Scala",
            "GCP",
            "Hive",
            "Hadoop",
            "Airflow",
            "Kafka",
            "Python",
            "Java",
            "Perl",
            "Shell",
            "Erwin",
            "Gitflow",
            "Jira",
            "Confluence",
            "Bamboo",
            "Jenkins",
            "TFS"
        ],
        "programming_languages": [
            "Python",
            "Java",
            "Scala"
        ],
        "experience": 8,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3895822095,
        "company": "Unreal Staffing, Inc",
        "title": "Big Data Engineer",
        "created_on": 1720635263.1376193,
        "description": "Company Overview: Welcome to the forefront of big data innovation! At our company, we're leading the charge in harnessing the power of big data to drive insights and innovation. Our mission is to develop scalable and robust big data solutions that empower organizations to extract value from their data at scale. Join us and be part of a dynamic team committed to shaping the future of big data engineering. Position Overview: As a Big Data Engineer, you'll play a pivotal role in designing, building, and maintaining our big data infrastructure and platforms. Working closely with cross-functional teams of data scientists, analysts, and software engineers, you'll ensure the reliability, scalability, and efficiency of our big data pipelines and systems. If you're passionate about big data engineering and eager to drive innovation through scalable data solutions, we want you on our team. Requirements Key Responsibilities: Big Data Infrastructure Design: Design and architect scalable and reliable big data infrastructure and platforms, including data lakes, data warehouses, and streaming data pipelines Data Ingestion and Processing: Develop and maintain data ingestion pipelines to collect and process large volumes of data from diverse sources, ensuring data quality and integrity Data Storage and Management: Implement and manage big data storage solutions, including distributed file systems (e.g., HDFS), NoSQL databases (e.g., Cassandra, MongoDB), and cloud storage services (e.g., Amazon S3, Google Cloud Storage) Data Transformation and ETL: Design and implement data transformation and ETL (Extract, Transform, Load) processes to standardize, cleanse, and enrich data for analysis and modeling Data Querying and Analysis: Develop and optimize data querying and analysis tools and frameworks, enabling data scientists and analysts to extract insights from large datasets Data Governance and Security: Implement data governance policies and security controls to ensure compliance with regulatory requirements and protect sensitive data Monitoring and Optimization: Monitor big data pipelines and systems for performance, reliability, and efficiency, proactively identifying and addressing issues to minimize downtime and optimize resource utilization Documentation and Collaboration: Document big data infrastructure and processes, including architecture diagrams, data lineage, and workflow documentation, and collaborate with cross-functional teams to ensure alignment and transparency Qualifications: Bachelor's degree or higher in Computer Science, Engineering, or related field Strong background in big data engineering, with hands-on experience in designing, building, and maintaining big data infrastructure and platforms Proficiency in programming languages such as Java, Python, or Scala, and experience with big data processing frameworks such as Apache Hadoop, Apache Spark, or Apache Flink Experience with distributed computing technologies and concepts, including MapReduce, Spark RDDs, and streaming data processing Familiarity with big data storage solutions and databases, including HDFS, Cassandra, MongoDB, Amazon S3, and Google Cloud Storage Strong problem-solving abilities and analytical thinking, with a keen attention to detail and a passion for tackling complex technical challenges Excellent communication and collaboration skills, with the ability to work effectively in cross-functional teams and communicate technical concepts to non-technical stakeholders Benefits Competitive salary: The industry standard salary for Big Data Engineers typically ranges from $150,000 to $230,000 per year, depending on experience and qualifications. Exceptional candidates may be eligible for higher compensation packages Comprehensive health, dental, and vision insurance plans Flexible work hours and remote work options Generous vacation and paid time off Professional development opportunities, including access to training programs, conferences, and workshops State-of-the-art technology environment with access to cutting-edge tools and resources Vibrant and inclusive company culture with team-building activities and social events Opportunities for career growth and advancement within the company Exciting projects with real-world impact across diverse industries Chance to work alongside top talent and industry experts in the field of big data engineering Join Us: Ready to shape the future of big data engineering? Apply now to join our team and be part of an exciting journey of innovation and discovery!",
        "url": "https://www.linkedin.com/jobs/view/3895822095",
        "summary": "This role involves designing, building, and maintaining big data infrastructure and platforms. Responsibilities include data ingestion and processing, data storage and management, data transformation and ETL, data querying and analysis, data governance and security, monitoring and optimization, and documentation and collaboration. The ideal candidate will have a strong background in big data engineering, proficiency in programming languages such as Java, Python, or Scala, and experience with big data processing frameworks such as Apache Hadoop, Apache Spark, or Apache Flink. The position offers a competitive salary, comprehensive benefits, flexible work hours, professional development opportunities, and a vibrant and inclusive company culture.",
        "industries": [
            "Big Data",
            "Data Engineering",
            "Data Science",
            "Software Engineering"
        ],
        "soft_skills": [
            "Problem Solving",
            "Analytical Thinking",
            "Communication",
            "Collaboration",
            "Detail Oriented",
            "Passionate",
            "Teamwork",
            "Leadership"
        ],
        "hard_skills": [
            "Java",
            "Python",
            "Scala",
            "Apache Hadoop",
            "Apache Spark",
            "Apache Flink",
            "MapReduce",
            "Spark RDDs",
            "Streaming Data Processing",
            "HDFS",
            "Cassandra",
            "MongoDB",
            "Amazon S3",
            "Google Cloud Storage"
        ],
        "tech_stack": [
            "Apache Hadoop",
            "Apache Spark",
            "Apache Flink",
            "HDFS",
            "Cassandra",
            "MongoDB",
            "Amazon S3",
            "Google Cloud Storage"
        ],
        "programming_languages": [
            "Java",
            "Python",
            "Scala"
        ],
        "experience": 0,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Engineering"
            ]
        },
        "salary": {
            "max": 230000,
            "min": 150000
        },
        "benefits": [
            "Competitive salary",
            "Comprehensive health, dental, and vision insurance plans",
            "Flexible work hours",
            "Remote work options",
            "Generous vacation and paid time off",
            "Professional development opportunities",
            "State-of-the-art technology environment",
            "Vibrant and inclusive company culture",
            "Opportunities for career growth and advancement",
            "Exciting projects",
            "Chance to work alongside top talent"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Sunnyvale, CA",
        "job_id": 3926422276,
        "company": "Walmart",
        "title": "Data Engineer III",
        "created_on": 1720635264.8376722,
        "description": "Position Summary... What you'll do... We are seeking a highly motivated and talented Data Engineer to join our dynamic team. As a Data Engineer, you will play a critical role in designing, developing, and implementing data pipelines and data integration solutions using Spark, Scala, and Google Cloud Platform (GCP). You will be responsible for building scalable and efficient data processing systems, optimizing data workflows, and ensuring data quality and integrity. About Team Everyone has data, but the sheer volume of data at Walmart can be limitless. In the Data Engineering team, we help Walmart manage this data by building pipelines and data lakes to prepare big data for analysis and unlocking actionable insights in real-time. We also use cross-departmental data and machine learning to build a holistic view of true profitability, saving millions of dollars across item categories and geographies while assisting our leadership in making better decisions faster. What You'll Do Collaborate with cross-functional teams to understand data requirements and design data solutions that meet business needs Develop and maintain data pipelines and ETL processes using Spark and Scala Design, build, and optimize data models and data architecture for efficient data processing and storage Implement data integration and data transformation workflows to ensure data quality and consistency Monitor and troubleshoot data pipelines to ensure data availability and reliability Conduct performance tuning and optimization of data processing systems for improved efficiency and scalability Work closely with data scientists and analysts to provide them with the necessary data sets and tools for analysis and reporting Stay up-to-date with the latest industry trends and technologies in data engineering and apply them to enhance the data infrastructure What You'll Bring Proven working experience as a Data Engineer with a minimum of 3 years in the field. Strong programming skills in Scala and experience with Spark for data processing and analytics Familiarity with Google Cloud Platform (GCP) services such as BigQuery, GCS, Dataproc, Pub/Sub, etc. Experience with data modeling, data integration, and ETL processes Strong knowledge of SQL and database systems Understanding of data warehousing concepts and best practices Proficiency in working with large-scale data sets and distributed computing frameworks Strong problem-solving and analytical skills Excellent communication and teamwork abilities About Walmart Global Tech Imagine working in an environment where one line of code can make life easier for hundreds of millions of people. That's what we do at Walmart Global Tech. We're a team of software engineers, data scientists, cybersecurity expert's and service professionals within the world's leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered. We train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail. Flexible, Hybrid Work We use a hybrid way of working that is primarily in office coupled with virtual when not onsite. Our campuses serve as a hub to enhance collaboration, bring us together for purpose and deliver on business needs. This approach helps us make quicker decisions, remove location barriers across our global team and be more flexible in our personal lives. Benefits Benefits: Beyond our great compensation package, you can receive incentive awards for your performance. Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and much more. Equal Opportunity Employer Walmart, Inc. is an Equal Opportunity Employer - By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing diversity- unique styles, experiences, identities, ideas and opinions - while being inclusive of all people. The above information has been designed to indicate the general nature and level of work performed in the role. It is not designed to contain or be interpreted as a comprehensive inventory of all responsibilities and qualifications required of employees assigned to this job. The full Job Description can be made available as part of the hiring process. At Walmart, we offer competitive pay as well as performance-based bonus awards and other great benefits for a happier mind, body, and wallet. Health benefits include medical, vision and dental coverage. Financial benefits include 401(k), stock purchase and company-paid life insurance. Paid time off benefits include PTO (including sick leave), parental leave, family care leave, bereavement, jury duty, and voting. Other benefits include short-term and long-term disability, company discounts, Military Leave Pay, adoption and surrogacy expense reimbursement, and more. ‎ ‎ ‎ You will also receive PTO and/or PPTO that can be used for vacation, sick leave, holidays, or other purposes. The amount you receive depends on your job classification and length of employment. It will meet or exceed the requirements of paid sick leave laws, where applicable. ‎ For information about PTO, see https://one.walmart.com/notices . ‎ ‎ Live Better U is a Walmart-paid education benefit program for full-time and part-time associates in Walmart and Sam's Club facilities. Programs range from high school completion to bachelor's degrees, including English Language Learning and short-form certificates. Tuition, books, and fees are completely paid for by Walmart. ‎ Eligibility requirements apply to some benefits and may depend on your job classification and length of employment. Benefits are subject to change and may be subject to a specific plan or program terms. ‎ For Information About Benefits And Eligibility, See One.Walmart . ‎ The annual salary range for this position is $117,000.00-$234,000.00 ‎ Additional Compensation Includes Annual Or Quarterly Performance Bonuses. ‎ Additional Compensation For Certain Positions May Also Include ‎ ‎ Stock ‎ ‎ Minimum Qualifications... Outlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications. Option 1: Bachelor's degree in Computer Science and 2 years' experience in software engineering or related field. Option 2: 4 years' experience in software engineering or related field. Option 3: Master's degree in Computer Science. Preferred Qualifications... Outlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications. Data engineering, database engineering, business intelligence, or business analytics, Master's degree in Computer Science or related field and 2 years' experience in software engineering or related field, We value candidates with a background in creating inclusive digital experiences, demonstrating knowledge in implementing Web Content Accessibility Guidelines (WCAG) 2.2 AA standards, assistive technologies, and integrating digital accessibility seamlessly. The ideal candidate would have knowledge of accessibility best practices and join us as we continue to create accessible products and services following Walmart's accessibility standards and guidelines for supporting an inclusive culture. Primary Location... 840 W California Ave, Sunnyvale, CA 94086-4828, United States of America",
        "url": "https://www.linkedin.com/jobs/view/3926422276",
        "summary": "Walmart Global Tech is seeking a Data Engineer to design, develop, and implement data pipelines and data integration solutions using Spark, Scala, and Google Cloud Platform (GCP). This role involves building scalable data processing systems, optimizing workflows, and ensuring data quality.",
        "industries": [
            "Retail",
            "E-commerce",
            "Data Engineering",
            "Technology"
        ],
        "soft_skills": [
            "Highly motivated",
            "Talented",
            "Collaborative",
            "Problem-solving",
            "Analytical",
            "Communication",
            "Teamwork"
        ],
        "hard_skills": [
            "Spark",
            "Scala",
            "Google Cloud Platform (GCP)",
            "BigQuery",
            "GCS",
            "Dataproc",
            "Pub/Sub",
            "Data Modeling",
            "Data Integration",
            "ETL",
            "SQL",
            "Database Systems",
            "Data Warehousing",
            "Distributed Computing Frameworks",
            "Git"
        ],
        "tech_stack": [
            "Spark",
            "Scala",
            "Google Cloud Platform (GCP)",
            "BigQuery",
            "GCS",
            "Dataproc",
            "Pub/Sub"
        ],
        "programming_languages": [
            "Scala",
            "SQL"
        ],
        "experience": 3,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Computer Science",
                "Software Engineering",
                "Related Field"
            ]
        },
        "salary": {
            "max": 234000,
            "min": 117000
        },
        "benefits": [
            "401(k) match",
            "Stock purchase plan",
            "Paid maternity and parental leave",
            "PTO",
            "Multiple health plans",
            "Performance-based bonus awards",
            "Company-paid life insurance",
            "Short-term and long-term disability",
            "Company discounts",
            "Military Leave Pay",
            "Adoption and surrogacy expense reimbursement",
            "Live Better U education benefit program",
            "Tuition, books, and fees paid for by Walmart"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Sunnyvale, CA",
        "job_id": 3926424415,
        "company": "Walmart",
        "title": "Senior, Data Engineer",
        "created_on": 1720635272.2286398,
        "description": "Position Summary... What you'll do... We are in search of a detail-oriented and experienced Senior Data Engineer to join our dynamic team. The ideal candidate will have extensive knowledge and hands-on experience in using Apache Spark, Scala, and Google Cloud Platform (GCP). A background in Generative Artificial Intelligence (Gen AI) is desirable but not essential. About Team Everyone has data, but the sheer volume of data at Walmart can be limitless. In the Data Engineering team, we help Walmart manage this data by building pipelines and data lakes to prepare big data for analysis and unlocking actionable insights in real-time. We also use cross-departmental data and machine learning to build a holistic view of true profitability, saving millions of dollars across item categories and geographies while assisting our leadership in making better decisions faster. What You'll Do Design, build, and manage data pipelines using Apache Spark and Scala. Utilize Google Cloud Platform (GCP) for the deployment, operation, and monitoring of large-scale data processing systems. Analyze complex data sets to meet functional and non-functional business requirements. Identify, design, and implement internal process improvements, including re-designing infrastructure for greater scalability, optimization data delivery, and automation. Collaborate with data science team to strive for greater functionality in our data systems. Ensure data architecture will support the requirements of the business. Implement measures to secure data such as access controls and encryption. Provide technical leadership and coaching to junior team members. Deepen and broaden relationships with peer groups and customers. What You'll Bring Proven working experience as a Data Engineer with a minimum of 5 years in the field. Strong analytic skills related to working with structured and unstructured datasets. Mastery of at least one general-purpose programming language(preferably Scala), and SQL proficiency Experience with Google Cloud Platform(preferably) and big data tools. Experience with data pipeline, workflow management, and ETL tools. Experience with deploying and managing CI pipeline with Jenkins or similar tools Knowledge of Gen AI is a plus. About Walmart Global Tech Imagine working in an environment where one line of code can make life easier for hundreds of millions of people. That's what we do at Walmart Global Tech. We're a team of software engineers, data scientists, cybersecurity expert's and service professionals within the world's leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered. We train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail. Flexible, Hybrid Work We use a hybrid way of working that is primarily in office coupled with virtual when not onsite. Our campuses serve as a hub to enhance collaboration, bring us together for purpose and deliver on business needs. This approach helps us make quicker decisions, remove location barriers across our global team and be more flexible in our personal lives. Benefits Benefits: Beyond our great compensation package, you can receive incentive awards for your performance. Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and much more. Equal Opportunity Employer Walmart, Inc. is an Equal Opportunity Employer - By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing diversity- unique styles, experiences, identities, ideas and opinions - while being inclusive of all people. The above information has been designed to indicate the general nature and level of work performed in the role. It is not designed to contain or be interpreted as a comprehensive inventory of all responsibilities and qualifications required of employees assigned to this job. The full Job Description can be made available as part of the hiring process. At Walmart, we offer competitive pay as well as performance-based bonus awards and other great benefits for a happier mind, body, and wallet. Health benefits include medical, vision and dental coverage. Financial benefits include 401(k), stock purchase and company-paid life insurance. Paid time off benefits include PTO (including sick leave), parental leave, family care leave, bereavement, jury duty, and voting. Other benefits include short-term and long-term disability, company discounts, Military Leave Pay, adoption and surrogacy expense reimbursement, and more. ‎ ‎ ‎ You will also receive PTO and/or PPTO that can be used for vacation, sick leave, holidays, or other purposes. The amount you receive depends on your job classification and length of employment. It will meet or exceed the requirements of paid sick leave laws, where applicable. ‎ For information about PTO, see https://one.walmart.com/notices . ‎ ‎ Live Better U is a Walmart-paid education benefit program for full-time and part-time associates in Walmart and Sam's Club facilities. Programs range from high school completion to bachelor's degrees, including English Language Learning and short-form certificates. Tuition, books, and fees are completely paid for by Walmart. ‎ Eligibility requirements apply to some benefits and may depend on your job classification and length of employment. Benefits are subject to change and may be subject to a specific plan or program terms. ‎ For Information About Benefits And Eligibility, See One.Walmart . ‎ The annual salary range for this position is $117,000.00-$234,000.00 ‎ Additional Compensation Includes Annual Or Quarterly Performance Bonuses. ‎ Additional Compensation For Certain Positions May Also Include ‎ ‎ Stock ‎ ‎ Minimum Qualifications... Outlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications. Option 1: Bachelor's degree in Computer Science and 3 years' experience in software engineering or related field. Option 2: 5 years' experience in software engineering or related field. Option 3: Master's degree in Computer Science and 1 year's experience in software engineering or related field. 2 years' experience in data engineering, database engineering, business intelligence, or business analytics. Preferred Qualifications... Outlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications. Data engineering, database engineering, business intelligence, or business analytics, ETL tools and working with large data sets in the cloud, Master's degree in Computer Science or related field and 3 years' experience in software engineering, We value candidates with a background in creating inclusive digital experiences, demonstrating knowledge in implementing Web Content Accessibility Guidelines (WCAG) 2.2 AA standards, assistive technologies, and integrating digital accessibility seamlessly. The ideal candidate would have knowledge of accessibility best practices and join us as we continue to create accessible products and services following Walmart's accessibility standards and guidelines for supporting an inclusive culture. Primary Location... 840 W California Ave, Sunnyvale, CA 94086-4828, United States of America",
        "url": "https://www.linkedin.com/jobs/view/3926424415",
        "summary": "Walmart is seeking a Senior Data Engineer with experience in Apache Spark, Scala, and Google Cloud Platform (GCP) to design, build, and manage data pipelines. This role involves analyzing complex datasets, optimizing data delivery, and collaborating with data scientists. Experience with Generative Artificial Intelligence (Gen AI) is a plus. ",
        "industries": [
            "Retail",
            "Data Engineering",
            "Machine Learning",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Detail-oriented",
            "Experienced",
            "Analytical",
            "Problem-solving",
            "Communication",
            "Collaboration",
            "Leadership",
            "Coaching"
        ],
        "hard_skills": [
            "Apache Spark",
            "Scala",
            "Google Cloud Platform",
            "SQL",
            "Data Pipelines",
            "ETL",
            "CI/CD",
            "Jenkins",
            "Gen AI"
        ],
        "tech_stack": [
            "Apache Spark",
            "Scala",
            "Google Cloud Platform",
            "SQL",
            "Jenkins"
        ],
        "programming_languages": [
            "Scala",
            "SQL"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Computer Science"
            ]
        },
        "salary": {
            "max": 234000,
            "min": 117000
        },
        "benefits": [
            "Performance-based bonus",
            "401(k) match",
            "Stock purchase plan",
            "Paid maternity and parental leave",
            "PTO",
            "Health plans",
            "Dental coverage",
            "Vision coverage",
            "Company-paid life insurance",
            "Short-term and long-term disability",
            "Company discounts",
            "Military Leave Pay",
            "Adoption and surrogacy expense reimbursement",
            "Live Better U education program"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3887685562,
        "company": "Unreal Staffing, Inc",
        "title": "Big Data Engineer",
        "created_on": 1720635273.9274206,
        "description": "Unleash the Power of Data We believe in the transformative power of data. As a Big Data Engineer, you'll play a pivotal role in our mission to harness vast amounts of information, turning it into actionable insights that can change the world. Our data isn't just big; it's colossal, and we need your skills to help us navigate, analyze, and innovate in an ocean of bytes. What You'll Be Doing: Building and managing robust, scalable big data infrastructures that support groundbreaking AI and machine learning projects Working with petabytes of data, your role will involve processing, cleaning, and structuring it to unlock its true potential Collaborating with data scientists and AI engineers to design and deploy models that make sense of complex patterns and predictions Innovating and implementing cutting-edge technologies and methodologies in data storage, data processing, and data visualization Playing a key role in decision-making processes by providing insights and data-driven recommendations that guide strategic directions Why Join Us: Be at the forefront of the data revolution, working on projects that have a real impact on society A competitive salary, comprehensive benefits, and stock options in a rapidly growing company Flexible working arrangements to support your work-life balance Continuous learning opportunities, with access to courses, workshops, and conferences to enhance your skills A vibrant, inclusive culture that celebrates diversity and innovation Requirements Who's Right for the Job: Someone with a deep understanding of big data technologies like Hadoop, Spark, Kafka, and ecosystems around them A degree in Computer Science, Data Science, Mathematics, or a related field is highly valued Experience in cloud computing platforms (AWS, Google Cloud Platform, Azure) is crucial for designing and deploying scalable solutions Proficiency in programming languages used in data engineering, such as Python, Scala, or Java An analytical mindset and a problem-solving approach, passionate about finding hidden patterns and insights within data Benefits Unlimited Paid Time Off (PTO): Encourages work-life balance and shows trust in your employees to manage their time effectively Professional Development Fund: Allocate a certain budget for each employee to attend conferences, workshops, or courses of their choice, fostering continuous learning and growth Remote Work Options: Even post-pandemic, the flexibility to work from anywhere can be a huge draw for talent who value the ability to travel or prefer not to relocate Wellness Programs: Offer subscriptions to mental health apps, fitness memberships, or even in-office wellness activities like yoga and meditation sessions Pet-Friendly Workplace: Allow employees to bring their pets to work, reducing stress and promoting a more relaxed atmosphere Tech Stipend: Provide a budget for employees to set up or upgrade their home office setup, ensuring they have the tools they need to succeed Four-Day Work Week: Implement a condensed workweek to promote productivity and work-life balance Parental Support: Offer generous parental leave policies, childcare assistance, or even on-site childcare services Sabbatical Leave: Allow employees to take an extended leave of absence after a certain period of employment to travel, pursue personal projects, or simply recharge Equity or Stock Options: Give employees a stake in the company's success, aligning their interests with the company's long-term goals Transportation and Parking Benefits: Provide subsidies for public transportation, electric vehicle charging stations, or free parking to ease the commute Customized Career Pathing: Work with employees to design personalized career development plans, acknowledging their unique strengths and ambitions Health and Safety: Offer comprehensive health insurance, including mental health coverage, and ergonomic workplace assessments to ensure physical well-being Food and Snacks: Provide free meals, a well-stocked kitchen, or food allowances, especially for teams working late or in crunch times Team Retreats and Offsites: Organize annual retreats in exciting locales to build team cohesion and reward hard work",
        "url": "https://www.linkedin.com/jobs/view/3887685562",
        "summary": "We are looking for a Big Data Engineer to help us build and manage robust, scalable big data infrastructures. You will work with petabytes of data to process, clean, and structure it to unlock its true potential. You will collaborate with data scientists and AI engineers to design and deploy models that make sense of complex patterns and predictions. You will also be responsible for innovating and implementing cutting-edge technologies and methodologies in data storage, data processing, and data visualization.",
        "industries": [
            "Data Science",
            "Machine Learning",
            "Artificial Intelligence",
            "Cloud Computing",
            "Technology",
            "Software Development"
        ],
        "soft_skills": [
            "Analytical",
            "Problem-solving",
            "Communication",
            "Collaboration",
            "Decision-making",
            "Innovation"
        ],
        "hard_skills": [
            "Hadoop",
            "Spark",
            "Kafka",
            "Python",
            "Scala",
            "Java",
            "AWS",
            "Google Cloud Platform",
            "Azure"
        ],
        "tech_stack": [
            "Hadoop",
            "Spark",
            "Kafka",
            "AWS",
            "Google Cloud Platform",
            "Azure",
            "Python",
            "Scala",
            "Java"
        ],
        "programming_languages": [
            "Python",
            "Scala",
            "Java"
        ],
        "experience": 0,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Computer Science",
                "Data Science",
                "Mathematics"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Unlimited Paid Time Off",
            "Professional Development Fund",
            "Remote Work Options",
            "Wellness Programs",
            "Pet-Friendly Workplace",
            "Tech Stipend",
            "Four-Day Work Week",
            "Parental Support",
            "Sabbatical Leave",
            "Equity or Stock Options",
            "Transportation and Parking Benefits",
            "Customized Career Pathing",
            "Health and Safety",
            "Food and Snacks",
            "Team Retreats and Offsites"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3899639547,
        "company": "Notion",
        "title": "Software Engineer, Data Platform",
        "created_on": 1720635275.7584472,
        "description": "About Us We're on a mission to make it possible for every person, team, and company to be able to tailor their software to solve any problem and take on any challenge. Computers may be our most powerful tools, but most of us can't build or modify the software we use on them every day. At Notion, we want to change this with focus, design, and craft. We've been working on this together since 2016, and have customers like Pixar, Mitsubishi, Figma, Plaid, Match Group, and thousands more on this journey with us. Today, we're growing fast and excited for new teammates to join us who are the best at what they do. We're passionate about building a company as diverse and creative as the millions of people Notion reaches worldwide. About The Role You’ll join a team of talented engineers who will design and own foundational data capabilities that are key to the company’s business and product. Notion’s data platform and infrastructure are vital to both empowering every team at Notion to make decisions using data and experimentation, and enabling our product features, like search, user notifications, workspace analytics and Notion AI. We are hiring engineers across multiple levels. What You'll Achieve You'll work cross-functionally with partners from the Data Science, Data Engineering, AI, Product, Go-to-Market, Legal and Finance organizations to deliver short- and long-term impact You’ll help in influencing / executing the roadmap for data infrastructure and systems to power high volume product features using Notion’s data. You'll play a pivotal role in the development of tools and infrastructure that democratize data access and enable analytics capabilities across the organization You'll determine the best ways to handle Notion's unique data model and usage patterns to derive insights and bring intelligence to product features like search and discovery. Skills You'll Need To Bring You have experience with big data ingestion, compute, and storage systems based on Spark, Kafka, Flink, and data lake and data warehouse technologies. You have worked on data or infrastructure-focused engineering teams, particularly ones that own a wide swath of software platforms (hosted or built in-house). You've experienced the challenges of scaling and re-architecting data platforms and infrastructure through orders of magnitude of growth and scaling data volume. Nice To Haves You've built out data infrastructure from, or nearly from, scratch at a fast-growing startup. You've led a Data Platform / Infrastructure / Data Engineering Team. You have experience building MLOps and ML serving infrastructure. Our customers come from all walks of life and so do we. We hire great people from a wide variety of backgrounds, not just because it's the right thing to do, but because it makes our company stronger. If you share our values and our enthusiasm for small businesses, you will find a home at Notion. Notion is proud to be an equal opportunity employer. We do not discriminate in hiring or any employment decision based on race, color, religion, national origin, age, sex (including pregnancy, childbirth, or related medical conditions), marital status, ancestry, physical or mental disability, genetic information, veteran status, gender identity or expression, sexual orientation, or other applicable legally protected characteristic. Notion considers qualified applicants with criminal histories, consistent with applicable federal, state and local law. Notion is also committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. If you need assistance or an accommodation made due to a disability, please let your recruiter know. Notion is committed to providing highly competitive cash compensation, equity, and benefits. The compensation offered for this role will be based on multiple factors such as location, the role’s scope and complexity, and the candidate’s experience and expertise, and may vary from the range provided below. For roles based in San Francisco, the estimated base salary range for this role is $150,000 - $250,000 per year.",
        "url": "https://www.linkedin.com/jobs/view/3899639547",
        "summary": "Notion is seeking Data Engineers to design and own foundational data capabilities that are key to the company’s business and product. You will work cross-functionally with partners from the Data Science, Data Engineering, AI, Product, Go-to-Market, Legal and Finance organizations to deliver short- and long-term impact. You will play a pivotal role in the development of tools and infrastructure that democratize data access and enable analytics capabilities across the organization. You'll determine the best ways to handle Notion's unique data model and usage patterns to derive insights and bring intelligence to product features like search and discovery.",
        "industries": [
            "Software",
            "Technology",
            "Data",
            "Artificial Intelligence",
            "SaaS"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Influence",
            "Decision Making",
            "Leadership"
        ],
        "hard_skills": [
            "Spark",
            "Kafka",
            "Flink",
            "Data Lake",
            "Data Warehouse",
            "Data Modeling",
            "Data Infrastructure",
            "Data Engineering",
            "MLOps",
            "ML Serving Infrastructure"
        ],
        "tech_stack": [
            "Spark",
            "Kafka",
            "Flink",
            "Data Lake",
            "Data Warehouse"
        ],
        "programming_languages": [],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 250000,
            "min": 150000
        },
        "benefits": [
            "Competitive cash compensation",
            "Equity",
            "Benefits"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Los Angeles, CA",
        "job_id": 3933228348,
        "company": "Crexi",
        "title": "Data Engineer",
        "created_on": 1720635277.2794764,
        "description": "About Crexi Crexi is reimagining commercial real estate, building industry-leading software for professionals to more effectively market, analyze, and trade commercial property. With a suite of due diligence tools, transaction services, deal pipeline support, and a dynamic listing marketplace, Crexi accelerates deal efficiency and has empowered millions of monthly users to close more than $615 billion in deals and market over $7 trillion in property value. What You'll Do Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of SQL/NoSQL databases and data stores like MSSQL , Dynamodb. Building and optimizing 'big data' data pipelines, architectures and data sets using AWS cloud services and SnowFlake Design & support a multi-tenant SnowFlake data warehouse Performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. Build processes supporting data transformation, data structures, metadata, dependency and workload management using DBT and Airflow or similar tools. Working knowledge of message queuing, stream processing, and highly scalable 'big data' data stores using Hadoop, Spark, Kafka, etc. Creating a strong test suite, alerting, monitoring and documentation for all the pipelines Who You Are BS degree in Computer Science or relevant work experience 5+ years of experience in developing data pipelines and databases Proficient in Python and SQL Experience with Data Engineering best practices including Test Automation, Quality Controls, Reconciliation, Documenting data flows, etc. Have thorough knowledge of data structures and algorithms Have experience working with customers to understand and capture requirements Experience building highly scalable and reliable data pipelines using technologies like Spark, Hive, Trino/Presto, Airflow, Kafka, and/or Flink. Experience with data stores like Snowflake, Postgres, Feast Pluses Experience with cloud service providers, including AWS, Azure, or Google Experience supporting multi-tenant data warehouses Familiarity with Analytical tools like Tableau Knowledge of data service and deployment frameworks, including Docker, and Kubernetes Experience with data compliance and governance tools like OneTrust or DataGrail Experience with the Kafka The anticipated salary range for candidates who will work in our Playa Vista, California location is $145,000 to $180,000. The final salary offered to a successful candidate will depend on several factors, which may include, but are not limited to, the type and length of experience applicable to the role and within the industry, education, geographic location, etc. Commercial Real Estate Exchange, Inc (“Crexi”) is a multi-state employer, and this salary range may not reflect positions that work in other states. Crexi is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Crexi will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.",
        "url": "https://www.linkedin.com/jobs/view/3933228348",
        "summary": "Crexi is a commercial real estate software company seeking a Data Engineer to build and optimize 'big data' data pipelines using AWS and Snowflake. The role involves designing and supporting a multi-tenant Snowflake data warehouse, performing root cause analysis, and building processes for data transformation, metadata, and workload management. The ideal candidate will have 5+ years of experience, be proficient in Python and SQL, and have experience with technologies like Spark, Hive, Airflow, Kafka, and Snowflake. ",
        "industries": [
            "Commercial Real Estate",
            "Software",
            "Technology"
        ],
        "soft_skills": [
            "Communication",
            "Problem Solving",
            "Analytical",
            "Teamwork",
            "Collaboration",
            "Data Analysis",
            "Requirement Gathering",
            "Customer Focus"
        ],
        "hard_skills": [
            "SQL",
            "Python",
            "AWS",
            "Snowflake",
            "DBT",
            "Airflow",
            "Hadoop",
            "Spark",
            "Kafka",
            "Docker",
            "Kubernetes",
            "Tableau",
            "Data Structures",
            "Algorithms",
            "Test Automation",
            "Quality Controls",
            "Reconciliation",
            "Data Flow Documentation",
            "Data Compliance",
            "Data Governance",
            "OneTrust",
            "DataGrail",
            "MSSQL",
            "DynamoDB"
        ],
        "tech_stack": [
            "AWS",
            "Snowflake",
            "DBT",
            "Airflow",
            "Hadoop",
            "Spark",
            "Kafka",
            "Docker",
            "Kubernetes",
            "Tableau",
            "OneTrust",
            "DataGrail",
            "MSSQL",
            "DynamoDB"
        ],
        "programming_languages": [
            "Python",
            "SQL"
        ],
        "experience": 5,
        "education": {
            "min_degree": "BS",
            "fields": [
                "Computer Science"
            ]
        },
        "salary": {
            "max": 180000,
            "min": 145000
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Irvine, CA",
        "job_id": 3835540025,
        "company": "Wise Skulls",
        "title": "Data Engineer",
        "created_on": 1720635278.8484275,
        "description": "Title: Data Engineer Location: Irvine CA (On-site) Duration: 6+ months Implementation Partner: Infosys End Client: To be disclosed Jd Minimum Years of Experience: 8+ Years Must have hands-on experience with Adobe Analytics + SQL",
        "url": "https://www.linkedin.com/jobs/view/3835540025",
        "summary": "Data Engineer with 8+ years of experience in Adobe Analytics and SQL. On-site position in Irvine, CA, for 6+ months. Implementation Partner: Infosys.",
        "industries": [
            "Data Engineering",
            "Analytics",
            "Software Development",
            "Technology"
        ],
        "soft_skills": [],
        "hard_skills": [
            "Adobe Analytics",
            "SQL"
        ],
        "tech_stack": [
            "Adobe Analytics"
        ],
        "programming_languages": [
            "SQL"
        ],
        "experience": 8,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Fremont, CA",
        "job_id": 3818364035,
        "company": "Info Way Solutions",
        "title": "Data engineer with python",
        "created_on": 1720635280.4443245,
        "description": "Role : Data Engineer (with strong Python and AWS experience) Locations: DC/VA/MD (Onsite) Job Description Data Pipeline Development: Design, implement, and manage robust data pipelines using Python, PySpark, SQL to efficiently extract, transform, and load data from diverse sources(Batch & Streaming) AWS Expertise Demonstrate expertise in core AWS services such as AWS DMS, AWS Glue, AWS Step Functions, Amazon S3, Amazon Redshift, Amazon RDS, Amazon EMR, AWS IAM, AWS LAMBDA etc., and apply them to build scalable and reliable data solutions. Data Modeling Develop and maintain efficient data models to support the analytical and reporting needs. Database Management Administer databases using AWS services like Amazon RDS or Amazon Redshift, focusing on schema design, performance optimization, and monitoring. Data Warehousing Utilize Amazon Redshift or Amazon Snowflake to create high-performing analytical databases that empower data-driven decision-making. ETL Best Practices Implement industry best practices for ETL processes, including data validation, error handling, and data quality checks. Performance Optimization Optimize query performance through continuous tuning of databases and leveraging AWS's scalability capabilities. Monitoring And Logging Establish robust monitoring and logging mechanisms using AWS CloudWatch, Amazon CloudTrail, or comparable tools to ensure pipeline reliability. Security And Compliance Ensure adherence to security best practices and relevant compliance standards, tailoring solutions to meet GDPR, HIPAA, or other regulatory requirements. Automation Drive automation of deployment and scaling of data pipelines using infrastructure as code (IaC) tools like AWS CloudFormation and Terraform. Collaboration Collaborate closely with cross-functional teams, including data scientists, analysts, and other stakeholders, to understand their data needs and provide effective solutions. Continuous Learning Stay updated on the latest developments in AWS services and data engineering methodologies, applying new insights to enhance our data infrastructure. Soft Skills Exhibit strong communication skills to facilitate effective teamwork and interaction with diverse groups. Thanks & Regards, Saravanan.R |Infowaygroup.com| Direct: (925)464-1116 Work: (925)-592-6160 Ext 111 Saravanan @infowaygroup.com Info Way Solutions LLC, Fremont, CA",
        "url": "https://www.linkedin.com/jobs/view/3818364035",
        "summary": "This is a data engineering role requiring strong Python and AWS experience. Responsibilities include designing, implementing, and managing data pipelines using Python, PySpark, and SQL for batch and streaming data.  The role involves expertise in various AWS services like DMS, Glue, Step Functions, S3, Redshift, RDS, EMR, IAM, and Lambda. Additionally, it encompasses data modeling, database administration, data warehousing, ETL best practices, performance optimization, monitoring and logging, security and compliance, automation, collaboration, and continuous learning.",
        "industries": [
            "Data Engineering",
            "Cloud Computing",
            "Data Analytics",
            "Software Development"
        ],
        "soft_skills": [
            "Communication",
            "Teamwork",
            "Collaboration",
            "Problem Solving",
            "Analytical Thinking",
            "Decision Making",
            "Continuous Learning"
        ],
        "hard_skills": [
            "Python",
            "PySpark",
            "SQL",
            "AWS",
            "AWS DMS",
            "AWS Glue",
            "AWS Step Functions",
            "Amazon S3",
            "Amazon Redshift",
            "Amazon RDS",
            "Amazon EMR",
            "AWS IAM",
            "AWS LAMBDA",
            "Data Modeling",
            "Schema Design",
            "Performance Optimization",
            "Monitoring",
            "Logging",
            "Security",
            "Compliance",
            "GDPR",
            "HIPAA",
            "AWS CloudFormation",
            "Terraform"
        ],
        "tech_stack": [
            "AWS",
            "Python",
            "PySpark",
            "SQL",
            "AWS DMS",
            "AWS Glue",
            "AWS Step Functions",
            "Amazon S3",
            "Amazon Redshift",
            "Amazon RDS",
            "Amazon EMR",
            "AWS IAM",
            "AWS LAMBDA",
            "AWS CloudFormation",
            "Terraform",
            "CloudWatch",
            "Amazon CloudTrail"
        ],
        "programming_languages": [
            "Python",
            "SQL",
            "PySpark"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Fremont, CA",
        "job_id": 3818359259,
        "company": "Info Way Solutions",
        "title": "Data Engineer",
        "created_on": 1720635282.0893264,
        "description": "Required : Python, SQL, Data Engineering, Rest based web services, Pandas, GIT, Mongo DB Nice to have : Snowflake, Kubernetes, Docker, Big Data Data Engineer Strong Python development and software design Dremio/Presto/Trino Snowflake (or other data warehouse systems) Tableau Docker & Kubernetes SQL MongoDB Object store (S3) & data lake concepts Git Shell & CLI tools REST APIs Pandas",
        "url": "https://www.linkedin.com/jobs/view/3818359259",
        "summary": "Data Engineer with strong Python development and software design skills. Experience with data warehousing, data lakes, and REST APIs. Knowledge of cloud technologies like Snowflake, Docker, and Kubernetes is a plus.",
        "industries": [
            "Data Engineering",
            "Software Development",
            "Analytics",
            "Technology"
        ],
        "soft_skills": [
            "Strong communication",
            "Problem-solving",
            "Teamwork",
            "Collaboration"
        ],
        "hard_skills": [
            "Python",
            "SQL",
            "Data Engineering",
            "REST APIs",
            "Pandas",
            "GIT",
            "Mongo DB",
            "Snowflake",
            "Kubernetes",
            "Docker",
            "Big Data",
            "Dremio",
            "Presto",
            "Trino",
            "Tableau",
            "Object Store",
            "Shell",
            "CLI"
        ],
        "tech_stack": [
            "Python",
            "SQL",
            "Pandas",
            "GIT",
            "Mongo DB",
            "Snowflake",
            "Docker",
            "Kubernetes",
            "Dremio",
            "Presto",
            "Trino",
            "Tableau",
            "S3"
        ],
        "programming_languages": [
            "Python",
            "SQL"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": [
                "Computer Science",
                "Data Science",
                "Engineering"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Santa Clarita, CA",
        "job_id": 3888468101,
        "company": "Zortech Solutions",
        "title": "Data Engineer with Python-US",
        "created_on": 1720635285.772837,
        "description": "Role: Data Engineer with Python Location: Bellevue, WA or Santa Clara, CA (Onsite day 1) Duration: 6+ Months Job Description 6+ years within specific technology domain areas (e.g. software development, cloud computing, systems engineering, infrastructure, security, networking, data & analytics) Minimum 3 years' experience with Python, PySpark Minimum 3 years' experience with AWS, DynamoDB",
        "url": "https://www.linkedin.com/jobs/view/3888468101",
        "summary": "Data Engineer with 6+ years of experience in technology domains like software development, cloud computing, systems engineering, infrastructure, security, networking, and data & analytics. Requires 3+ years of Python and PySpark experience, as well as 3+ years of experience with AWS and DynamoDB. Onsite role in Bellevue, WA or Santa Clara, CA.",
        "industries": [
            "Software Development",
            "Cloud Computing",
            "Systems Engineering",
            "Infrastructure",
            "Security",
            "Networking",
            "Data & Analytics"
        ],
        "soft_skills": [],
        "hard_skills": [
            "Python",
            "PySpark",
            "AWS",
            "DynamoDB"
        ],
        "tech_stack": [
            "AWS",
            "DynamoDB"
        ],
        "programming_languages": [
            "Python",
            "PySpark"
        ],
        "experience": 6,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Diego, CA",
        "job_id": 3960941953,
        "company": "Axos Bank",
        "title": "Analytics Engineer, Data Analytics",
        "created_on": 1720635287.434051,
        "description": "Axos Bank Target Range: $80,000.00/Yr. - $100,000.00/Yr. Actual starting pay will vary based on factors including, but not limited to, geographic location, experience, skills, specialty, and education. Eligible for an Annual Discretionary Cash Bonus Target: 10% Eligible for an Annual Discretionary Restricted Stock Units Bonus Target: 10% These discretionary target bonuses may be awarded semi-annually based upon your achievement of performance goals and targets. About This Job Analytics Engineers are tasked with designing, constructing, and maintaining the robust technical infrastructure needed for data analysis. Analytics engineers work closely with business professionals, data scientists, and analysts to collect and organize data, as well as, develop data visualization tools. This role also performs production support and Change Management functions. Responsibilities Partner with Stakeholders and Enterprise Data Warehouse to create best-in-class Data Analytics solutions for the Consumer Bank Collaborate with and support data scientists, data engineers, business and data analysts Designing, building, testing, and deploying logical and physical data models across on-premise and AWS solutions Use business intelligence, data science, and visualization tools to build data products Build, and document automated and scalable data pipelines Participate in the design, building, and maintenance of data driven engines Provide insight and recommendations to the data team and stakeholders Performs code reviews, continuous integration and delivery, release management, testing activities, DevOps, and version control Requirement Bachelors degree 1 + years experience in Data Engineering, BI Engineering and similar role Experience in SQL programming and data transformation skills Experience making sense of data for analytical insights Experience building multi-step ETL jobs/data pipelines Axos Employee Benefits May Include: Medical, Dental, Vision, and Life Insurance Paid Sick Leave, 3 weeks’ Vacation, and Holidays (about 11 a year) HSA or FSA account and other voluntary benefits 401(k) Retirement Saving Plan with Employer Match Program and 529 Savings Plan Employee Mortgage Loan Program and free access to an Axos Bank Account with Self-Directed Trading About Axos Born digital-first, Axos delivers financial tools and services that allow individuals, small businesses, and companies to access and manage their money how, when, and where they want. We’re a diverse team of dynamic, insightful, and independent innovators who are excited to provide technology-driven solutions that offer unbeatable value to our customers. Axos Financial is our holding company and is publicly traded on the New York Stock Exchange under the symbol \"AX\" (NYSE: AX). Learn more about working at Axos Pre-Employment Background Check and Drug Test: All offers are contingent upon the candidate successfully passing a credit check, criminal background check, and pre-employment drug screening, which includes screening for marijuana. Axos Bank is a federally regulated banking institution. At the federal level, marijuana is an illegal schedule 1 drug; therefore, we will not employ any person who tests positive for marijuana, regardless of state legalization. Equal Employment Opportunity: Axos is an Equal Opportunity employer. We are committed to providing equal employment opportunities to all employees and applicants without regard to race, religious creed, color, sex (including pregnancy, breast feeding and related medical conditions), gender, gender identity, gender expression, sexual orientation, national origin, ancestry, citizenship status, military and veteran status, marital status, age, protected medical condition, genetic information, physical disability, mental disability, or any other protected status in accordance with all applicable federal, state, and local laws. Job Functions and Work Environment: While performing the duties of this position, the employee is required to sit for extended periods of time. Manual dexterity and coordination are required while operating standard office equipment such as computer keyboard and mouse, calculator, telephone, copiers, etc. The work environment characteristics described here are representative of those an employee may encounter while performing the essential functions of this position. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions of this position.",
        "url": "https://www.linkedin.com/jobs/view/3960941953",
        "summary": "Analytics Engineers at Axos Bank are responsible for designing, building, and maintaining data analytics infrastructure. They work with business professionals, data scientists, and analysts to collect and organize data, build data visualization tools, and support data-driven engines. They also contribute to code reviews, continuous integration and delivery, and DevOps.",
        "industries": [
            "Banking",
            "Finance",
            "Financial Services",
            "Data Analytics",
            "Data Science"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Analytical Thinking",
            "Time Management",
            "Organization",
            "Attention to Detail",
            "Teamwork",
            "Leadership",
            "Decision Making"
        ],
        "hard_skills": [
            "SQL",
            "Data Transformation",
            "ETL",
            "Data Pipelines",
            "Data Modeling",
            "Data Visualization",
            "AWS",
            "Business Intelligence",
            "DevOps",
            "Version Control",
            "Continuous Integration and Delivery",
            "Code Review",
            "Change Management",
            "Production Support"
        ],
        "tech_stack": [
            "AWS",
            "SQL",
            "ETL",
            "Data Pipelines",
            "Data Modeling",
            "Data Visualization",
            "Business Intelligence"
        ],
        "programming_languages": [
            "SQL"
        ],
        "experience": 1,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Data Engineering",
                "BI Engineering",
                "Computer Science",
                "Analytics",
                "Information Technology"
            ]
        },
        "salary": {
            "max": 100000,
            "min": 80000
        },
        "benefits": [
            "Medical Insurance",
            "Dental Insurance",
            "Vision Insurance",
            "Life Insurance",
            "Paid Sick Leave",
            "Vacation",
            "Holidays",
            "HSA",
            "FSA",
            "401(k) with Match",
            "529 Savings Plan",
            "Employee Mortgage Loan Program",
            "Free Axos Bank Account with Self-Directed Trading"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3941343616,
        "company": "Pronto",
        "title": "Junior Software Engineer",
        "created_on": 1720635289.1132991,
        "description": "While most Autonomous Vehicle (AV) technology companies are stuck in R&D mode, Pronto is a world-leader in commercializing AV tech via our Autonomous Haulage System, which is automating haulage operations at mines and quarries around the world. Pronto’s team of Silicon Valley veterans has been at the forefront of every major AV development over the past 20 years, with a relentless focus on commercializing the technology, leading to our current specialization in off-road applications. This focus and our decades of experience have put Pronto on a track to become the world’s first profitable AV technology company. We are seeking a skilled and motivated Junior Software Engineer to join our dynamic team. This person sits at the intersection between hardware and software, working with engineers from both sides to create solutions. While covering a range of topics, this role is mostly focused on writing software to interface with hardware, automating processes, embedded software, and creating tests. What You’ll Work On: Automating linux device bring-up and configuration Writing scripts for hardware testing and creating debug/diagnostic tooling Creating tooling to record and update device configuration, serial numbers, etc Reworking and organizing our device database Building device tracking dashboards for status, location, etc Working with the electrical engineering team to create configuration and testing for new custom hardware Your work will directly contribute to our fleet of autonomous trucks and auxiliary hardware This role has opportunities for growth You'll Achieve This By: Solving challenging systems problems Writing scripts for setup, configuration, and testing Creating and managing large databases Automating hardware and provisioning testing Working with the team to deploy solutions in a mission critical environment Building and maintaining core software infrastructure Creating efficient processes for device provisioning and testing Requirements: Internship or similar software experience highly suggested Programming experience with python Linux environment and bash scripting experience Experience with AWS is a plus Experience interfacing with hardware is a plus Able to create and debug software systems: build maintainable code with good diagnostics that scales Excellent at handling ambiguous or undefined problems Excellent communication and organizational skills Effective collaboration with members of the team Experience or general interest in embedded software Attention to detail and overall fastidiousness a huge plus Our Stack: This part of the stack is mostly bash and python Compensation and Benefits This is a full time position based in San Francisco, CA, with a salary range between $90,000 - $120,000. Actual compensation offered will depend on work experience, education, skill level, and/or other business and organizational needs. Please note that it is not typical for an individual to be hired at or near the top of the range. Pronto reserves the right to modify this compensation range at any time. In addition to your salary, as a full-time Pronto employee you are eligible for the following benefits: Medical, Dental, Vision, Disability, and Life Insurance 401k with matching contributions Equity Sick Time, Unlimited Vacation, and Paid Holidays Paid Parental Leave Pre-Tax Commuter Benefit Plan Team lunch in our SOMA office every Tuesday and Thursday Pronto is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. We offer generous pay, equity, medical, vision and dental insurance, 401k benefits, unlimited PTO, and team lunch in our SOMA office every Tuesday and Thursday. We’re an equal opportunity employer and do not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status.",
        "url": "https://www.linkedin.com/jobs/view/3941343616",
        "summary": "Pronto, a leading company in commercializing Autonomous Vehicle (AV) technology for off-road applications like mining and quarries, is seeking a Junior Software Engineer. The role focuses on writing software to interface with hardware, automating processes, embedded software, and creating tests. This includes automating device configuration, building device tracking dashboards, and working with electrical engineers to configure and test new hardware. Experience with Python, Linux, bash scripting, and AWS is required.",
        "industries": [
            "Autonomous Vehicles",
            "Mining",
            "Quarrying",
            "Technology",
            "Software Development",
            "Hardware"
        ],
        "soft_skills": [
            "Problem Solving",
            "Communication",
            "Organization",
            "Collaboration",
            "Attention to Detail",
            "Fastidiousness"
        ],
        "hard_skills": [
            "Python",
            "Linux",
            "Bash Scripting",
            "AWS",
            "Embedded Software",
            "Hardware Interfacing",
            "Database Management",
            "Software Testing",
            "Software Development",
            "Debugging"
        ],
        "tech_stack": [
            "Python",
            "Bash",
            "Linux",
            "AWS"
        ],
        "programming_languages": [
            "Python"
        ],
        "experience": 1,
        "education": {
            "min_degree": "Internship",
            "fields": [
                "Software Engineering",
                "Computer Science"
            ]
        },
        "salary": {
            "max": 120000,
            "min": 90000
        },
        "benefits": [
            "Medical Insurance",
            "Dental Insurance",
            "Vision Insurance",
            "Disability Insurance",
            "Life Insurance",
            "401k with Matching",
            "Equity",
            "Sick Time",
            "Unlimited Vacation",
            "Paid Holidays",
            "Paid Parental Leave",
            "Pre-Tax Commuter Benefit Plan",
            "Team Lunch"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Burbank, CA",
        "job_id": 3910097236,
        "company": "Sonitalent Corp",
        "title": "Senior Data Engineer",
        "created_on": 1720635290.6837046,
        "description": "Job title: Senior Data Engineer Location: Burbank, CA ( Onsite ) Local only Interview: Phone/ Skype Visa: No H1B, CPT&OPT Job Description Skill sets needed: ETL/ELT, Python/SQL, Big Data. , Docker, Gitlab, Airflow, Snowflake, Looker and PostgreSQL, AWS, S3 Interview Schedule/Availability: Zoom followed by zoom panel interview Misc. Info: Must give last four SSN, Birth mm/dd and email address Make Sure To Answer These Questions Has this candidate been submitted by us to Disney in the past? If so, is the attached resume new or the same one we used last submittal? Have They Interviewed For Disney In The Past Have they worked for Disney in the past: As a Contractor or as a FTE? When and who was their manager? External Job Description The team is involved in various activities ranging from data acquisition and validation, designing and implementing ETL/ELT data pipelines, designing and implementing databases, and evolving our next generation data platform to fulfill the needs of our applications, data services, ad-hoc analytics and self-service/POC initiatives. Responsibilities The Data Engineering team (within DDSI) is looking to fill a Senior Data Engineer responsible for designing and implementing ETL/ELT data pipelines, designing and implementing database schema/tables/views, and building batch processes leveraging Airflow for several projects. This Senior Data Engineer will be responsible for partnering with our Studios or Enterprise Technology team members in various activities around data requirements gathering, data validation scripting and review, developing and monitoring ETL/ELT data pipelines, designing and implementing database schema/tables/views, plus deployment across multiple environments such as DEV, QA/UAT and PROD. The role will leverage a multitude of technologies to fulfill the work including, but not limited to SQL, Python, Docker, Gitlab, Airflow, Snowflake, Looker and PostgreSQL Basic Qualifications Basic Qualifications 3+ years’ experience with Python 4+ years’ experience with SQL 3+ years’ experience designing, building and maintaining ETL/ELT data pipelines 2+ years’ experience with cloud based technologies, preferably AWS and S3 Experience using Apache Airflow, Databricks 2+ years leveraging, designing and building relational databases 2+ years of experience using Gitlab/Github Preferred Qualifications Preferred Qualifications Preferred Qualifications Experience with Snowflake, Airflow, Gitlab, , Tableau Experience in streaming, media, or digital marketing domain3+ years experience in a Data Engineering role SQL, Python, Snowflake, Gitlab, Airflow, AWS S3 and Relational Databases 2+ years’ experience working on a cloud platform 2+ years’ experience working with data lakes, data warehouses and application databases Experience working with large datasets and big data technologies, preferably cloud-based, such as Snowflake, Databricks, or similar Experience with SQL Query optimization based on runtime and cost Participate in driving best practices around data engineering software development processes Required Education Bachelor degree in Computer Science, Mathematics",
        "url": "https://www.linkedin.com/jobs/view/3910097236",
        "summary": "Senior Data Engineer role at Disney in Burbank, CA responsible for designing and implementing ETL/ELT data pipelines, designing and implementing database schema/tables/views, and building batch processes using Airflow.  The role requires strong skills in Python, SQL, AWS, Snowflake, Airflow, Gitlab, Looker and PostgreSQL.",
        "industries": [
            "Media",
            "Entertainment",
            "Technology"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Analytical Thinking",
            "Teamwork"
        ],
        "hard_skills": [
            "ETL/ELT",
            "Python",
            "SQL",
            "Docker",
            "Gitlab",
            "Airflow",
            "Snowflake",
            "Looker",
            "PostgreSQL",
            "AWS",
            "S3",
            "Databricks",
            "Tableau"
        ],
        "tech_stack": [
            "Python",
            "SQL",
            "Docker",
            "Gitlab",
            "Airflow",
            "Snowflake",
            "Looker",
            "PostgreSQL",
            "AWS",
            "S3",
            "Databricks",
            "Tableau"
        ],
        "programming_languages": [
            "Python",
            "SQL"
        ],
        "experience": 3,
        "education": {
            "min_degree": "Bachelor",
            "fields": [
                "Computer Science",
                "Mathematics"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3897984044,
        "company": "Unreal Staffing, Inc",
        "title": "Senior Data Engineer",
        "created_on": 1720635292.4881015,
        "description": "Company Overview: Welcome to the forefront of data-driven innovation! Our company is dedicated to harnessing the power of data to drive transformative change and solve complex problems across industries. We're committed to building scalable and reliable data infrastructure that enables advanced analytics, machine learning, and business intelligence. Join us and be part of a dynamic team shaping the future of data engineering. Position Overview: As a Senior Data Engineer, you'll play a critical role in designing, building, and maintaining our data infrastructure and pipelines. You'll work on challenging projects, from data ingestion and processing to data storage and retrieval, to support the needs of our data-driven organization. If you're a seasoned engineer with expertise in data engineering technologies and a passion for building robust data systems, we want you on our team. Requirements Key Responsibilities: Data Pipeline Development: Design, build, and maintain scalable and efficient data pipelines for ingesting, processing, and transforming large volumes of structured and unstructured data from diverse sources Data Modeling: Design and implement data models and schemas to support analytical and operational requirements, ensuring data integrity, consistency, and performance Data Warehousing: Design, build, and optimize data warehouses and data lakes to store and manage structured and unstructured data for analytics and reporting purposes Data Integration: Integrate data from disparate sources and systems, ensuring data consistency, quality, and completeness throughout the data lifecycle Data Governance: Establish and enforce data governance policies and best practices to ensure data quality, security, and compliance with regulatory requirements Performance Optimization: Optimize data pipelines and queries for performance and efficiency, identifying and addressing bottlenecks and inefficiencies to improve system scalability and reliability Monitoring and Alerting: Implement monitoring and alerting systems to track data pipeline performance and health, detecting and mitigating issues proactively to minimize downtime and data loss Documentation: Document data infrastructure and pipelines, providing clear and comprehensive documentation to facilitate understanding and collaboration among team members Collaboration: Collaborate with cross-functional teams, including data scientists, software engineers, and business stakeholders, to understand requirements and deliver data solutions that meet business needs Mentorship and Leadership: Mentor junior engineers, providing guidance, support, and technical leadership in data engineering best practices and technologies Qualifications: Bachelor's degree or higher in Computer Science, Engineering, Mathematics, or related field 5+ years of experience in data engineering, with a focus on designing, building, and maintaining data infrastructure and pipelines Proficiency in programming languages such as Python, Java, or Scala, and experience with data engineering frameworks and tools such as Apache Spark, Apache Kafka, Apache Airflow, or similar Strong understanding of data modeling concepts and techniques, with experience designing and implementing data models and schemas for relational and non-relational databases Experience with cloud platforms such as AWS, Azure, or Google Cloud Platform, and familiarity with cloud-based data services such as Amazon Redshift, Google BigQuery, or Azure Synapse Analytics Experience with SQL and NoSQL databases, data warehousing, and ETL/ELT processes Strong problem-solving skills and analytical thinking, with the ability to troubleshoot complex data issues and optimize system performance Excellent communication and collaboration skills, with the ability to work effectively in cross-functional teams and communicate technical concepts to non-technical stakeholders Benefits Competitive salary: The industry standard salary for Senior Data Engineers typically ranges from $170,000 to $230,000 per year, depending on experience and qualifications Comprehensive health, dental, and vision insurance plans Flexible work hours and remote work options Generous vacation and paid time off Professional development opportunities, including access to training programs, conferences, and workshops State-of-the-art technology environment with access to cutting-edge tools and resources Vibrant and inclusive company culture with opportunities for growth and advancement Exciting projects with real-world impact at the forefront of data-driven innovation Join Us: Ready to shape the future of data engineering? Apply now to join our team and be part of the data revolution!",
        "url": "https://www.linkedin.com/jobs/view/3897984044",
        "summary": "Senior Data Engineer position seeking a skilled engineer with 5+ years of experience in designing, building, and maintaining data infrastructure and pipelines. Responsibilities include data pipeline development, data modeling, data warehousing, data integration, data governance, performance optimization, monitoring & alerting, documentation, collaboration, mentorship, and leadership. Proficiency in Python, Java, or Scala, experience with Apache Spark, Apache Kafka, Apache Airflow, and cloud platforms like AWS, Azure, or GCP is required. Experience with SQL, NoSQL databases, data warehousing, and ETL/ELT processes is essential. The ideal candidate will have strong problem-solving skills and analytical thinking, excellent communication and collaboration skills, and a passion for building robust data systems.",
        "industries": [
            "Data Engineering",
            "Data Science",
            "Technology",
            "Software Development",
            "Business Intelligence",
            "Machine Learning",
            "Analytics"
        ],
        "soft_skills": [
            "Problem-solving",
            "Analytical thinking",
            "Communication",
            "Collaboration",
            "Leadership",
            "Mentorship"
        ],
        "hard_skills": [
            "Python",
            "Java",
            "Scala",
            "Apache Spark",
            "Apache Kafka",
            "Apache Airflow",
            "Data Modeling",
            "SQL",
            "NoSQL",
            "Data Warehousing",
            "ETL/ELT",
            "AWS",
            "Azure",
            "Google Cloud Platform",
            "Amazon Redshift",
            "Google BigQuery",
            "Azure Synapse Analytics"
        ],
        "tech_stack": [
            "Apache Spark",
            "Apache Kafka",
            "Apache Airflow",
            "AWS",
            "Azure",
            "Google Cloud Platform",
            "Amazon Redshift",
            "Google BigQuery",
            "Azure Synapse Analytics"
        ],
        "programming_languages": [
            "Python",
            "Java",
            "Scala"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Computer Science",
                "Engineering",
                "Mathematics"
            ]
        },
        "salary": {
            "max": 230000,
            "min": 170000
        },
        "benefits": [
            "Competitive salary",
            "Health insurance",
            "Dental insurance",
            "Vision insurance",
            "Flexible work hours",
            "Remote work options",
            "Vacation",
            "Paid time off",
            "Professional development opportunities",
            "State-of-the-art technology",
            "Vibrant company culture",
            "Growth opportunities",
            "Exciting projects"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Santa Clara, CA",
        "job_id": 3857209203,
        "company": "Amick Brown - SAP, Cloud Technologies and Business Intelligence Staffing & Consulting",
        "title": "Data Engineer 3",
        "created_on": 1720635294.4481695,
        "description": "Description To perform the responsibilities of this position, the individual must have: M.S. in Computer Science, Software/Computer Engineering, Information Technology, Electronics, Data Management or Applied Math with minimum of 7 years industry experience or B.S. degree with minimum (10) years industry experience. Demonstrated excellent communication skills both written and verbal. Strong data engineering and programming skills Hands on experience with C# and/or Python Proficient with SQL query language Strong experience working with relational SQL (Microsoft SQL , MySQL, Postgres, Snowflake etc.) and non-relational SQL ( MongoDB, Kafka etc.) Very Strong experience in Snowflake Hands on experience on providing Cloud data solutions on AWS, GCP, Azure. Hands on experience with Data application life cycle Solid knowledge of database modelling and data warehouse event driven and data streaming architecture Application Scalability Application security - SAML, OAUTH, Kerberos, JWT Token, SSO API Development Experience working with Windows and Linux OS Strong experience as a must:  CI/CD pipeline and Build tools such as GitLab, Jenkins, CircleCI, etc.  Modeling and transformation tools – DBT - AirFlow  Understanding and ability to work with Kubernetes Additional Details Pre-identified worker (First Name, Last Name) & Supplier Name : (No Value) Job Posting Type : Agency Recruited Worker Required Worker Legal Name (For Manager Sourced Only) : (No Value)",
        "url": "https://www.linkedin.com/jobs/view/3857209203",
        "summary": "This role requires a strong data engineer with expertise in SQL, C#, Python, and cloud platforms like AWS, GCP, and Azure.  They will be responsible for building and maintaining data solutions, implementing CI/CD pipelines, and working with relational and non-relational databases.",
        "industries": [
            "Information Technology",
            "Software Development",
            "Data Engineering",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Communication",
            "Problem Solving",
            "Teamwork",
            "Collaboration"
        ],
        "hard_skills": [
            "C#",
            "Python",
            "SQL",
            "Data Engineering",
            "Data Warehousing",
            "Database Modeling",
            "Event Driven Architecture",
            "Data Streaming",
            "Application Scalability",
            "Application Security",
            "API Development",
            "CI/CD",
            "GitLab",
            "Jenkins",
            "CircleCI",
            "DBT",
            "Airflow",
            "Kubernetes",
            "AWS",
            "GCP",
            "Azure",
            "Snowflake",
            "Microsoft SQL",
            "MySQL",
            "Postgres",
            "MongoDB",
            "Kafka",
            "SAML",
            "OAUTH",
            "Kerberos",
            "JWT Token",
            "SSO",
            "Windows",
            "Linux"
        ],
        "tech_stack": [
            "C#",
            "Python",
            "SQL",
            "Snowflake",
            "AWS",
            "GCP",
            "Azure",
            "GitLab",
            "Jenkins",
            "CircleCI",
            "DBT",
            "Airflow",
            "Kubernetes",
            "Microsoft SQL",
            "MySQL",
            "Postgres",
            "MongoDB",
            "Kafka"
        ],
        "programming_languages": [
            "C#",
            "Python",
            "SQL"
        ],
        "experience": 7,
        "education": {
            "min_degree": "M.S.",
            "fields": [
                "Computer Science",
                "Software/Computer Engineering",
                "Information Technology",
                "Electronics",
                "Data Management",
                "Applied Math"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3895816941,
        "company": "Unreal Staffing, Inc",
        "title": "Real-Time Data Engineer",
        "created_on": 1720635296.247043,
        "description": "Company Overview: Welcome to the forefront of real-time data engineering! At our company, we're dedicated to harnessing the power of real-time data to drive insights and enable real-time decision-making. Our mission is to develop scalable and efficient real-time data solutions that empower organizations to leverage data in real-time. Join us and be part of a dynamic team committed to shaping the future of real-time data engineering. Position Overview: As a Real-Time Data Engineer, you'll play a pivotal role in designing, building, and optimizing our real-time data infrastructure and systems. Working closely with cross-functional teams of data scientists, software engineers, and business analysts, you'll ensure the reliability, scalability, and efficiency of our real-time data solutions. If you're passionate about real-time data engineering and eager to drive innovation through scalable data solutions, we want you on our team. Requirements Key Responsibilities: Real-Time Data Pipeline Development: Design and develop real-time data pipelines to ingest, process, and analyze streaming data from various sources, ensuring low-latency and high-throughput Stream Processing: Implement stream processing applications using technologies such as Apache Kafka, Apache Flink, Apache Spark Streaming, or AWS Kinesis to process and analyze real-time data streams Data Integration: Integrate real-time data streams with existing data systems and applications, ensuring data consistency and coherence across systems Data Transformation and Enrichment: Transform and enrich real-time data streams to derive meaningful insights and enable real-time decision-making, using techniques such as complex event processing (CEP) and real-time analytics Data Quality and Governance: Implement data quality checks, validation rules, and error handling mechanisms to ensure the accuracy, completeness, and consistency of real-time data Performance Optimization: Optimize real-time data processing pipelines for speed, scalability, and efficiency, tuning streaming applications and leveraging distributed computing technologies Monitoring and Maintenance: Monitor real-time data infrastructure and systems for performance, reliability, and availability, proactively identifying and addressing issues to minimize downtime and ensure data integrity Documentation and Collaboration: Document real-time data architecture, processes, and best practices, and collaborate with cross-functional teams to ensure alignment and transparency Qualifications: Bachelor's degree or higher in Computer Science, Engineering, or related field Strong background in real-time data engineering, with hands-on experience in designing, building, and optimizing real-time data solutions Proficiency in programming languages such as Python, Java, or Scala, and experience with stream processing frameworks such as Apache Kafka, Apache Flink, or Apache Spark Streaming Experience with cloud platforms such as AWS, Google Cloud Platform, or Microsoft Azure, and familiarity with cloud services for real-time data processing (e.g., AWS Kinesis, Google Cloud Dataflow, Azure Stream Analytics) Knowledge of distributed computing technologies and concepts, including message brokers, distributed databases, and microservices architecture Strong problem-solving abilities and analytical thinking, with a keen attention to detail and a passion for tackling complex technical challenges Excellent communication and collaboration skills, with the ability to work effectively in cross-functional teams and communicate technical concepts to non-technical stakeholders Benefits Competitive salary: The industry standard salary for Real-Time Data Engineers typically ranges from $140,000 to $210,000 per year, depending on experience and qualifications. Exceptional candidates may be eligible for higher compensation packages Comprehensive health, dental, and vision insurance plans Flexible work hours and remote work options Generous vacation and paid time off Professional development opportunities, including access to training programs, conferences, and workshops State-of-the-art technology environment with access to cutting-edge tools and resources Vibrant and inclusive company culture with team-building activities and social events Opportunities for career growth and advancement within the company Exciting projects with real-world impact in the field of real-time data engineering Chance to work alongside top talent and industry experts in real-time data engineering Join Us: Ready to shape the future of real-time data engineering? Apply now to join our team and be part of an exciting journey of innovation and discovery!",
        "url": "https://www.linkedin.com/jobs/view/3895816941",
        "summary": "We are looking for a Real-Time Data Engineer to design, build, and optimize our real-time data infrastructure and systems. You will work closely with data scientists, software engineers, and business analysts to ensure the reliability, scalability, and efficiency of our real-time data solutions. This role involves developing real-time data pipelines, implementing stream processing applications, integrating real-time data streams with existing data systems, transforming and enriching real-time data streams, implementing data quality checks and validation rules, optimizing real-time data processing pipelines, monitoring real-time data infrastructure and systems, documenting real-time data architecture, processes, and best practices, and collaborating with cross-functional teams.",
        "industries": [
            "Data Engineering",
            "Software Engineering",
            "Technology",
            "Big Data",
            "Analytics",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Problem-solving",
            "Analytical thinking",
            "Attention to detail",
            "Communication",
            "Collaboration"
        ],
        "hard_skills": [
            "Real-time data engineering",
            "Data pipeline development",
            "Stream processing",
            "Data integration",
            "Data transformation",
            "Data enrichment",
            "Data quality",
            "Data governance",
            "Performance optimization",
            "Monitoring",
            "Maintenance",
            "Documentation",
            "Python",
            "Java",
            "Scala",
            "Apache Kafka",
            "Apache Flink",
            "Apache Spark Streaming",
            "AWS Kinesis",
            "AWS",
            "Google Cloud Platform",
            "Microsoft Azure",
            "Distributed computing",
            "Message brokers",
            "Distributed databases",
            "Microservices architecture"
        ],
        "tech_stack": [
            "Apache Kafka",
            "Apache Flink",
            "Apache Spark Streaming",
            "AWS Kinesis",
            "AWS",
            "Google Cloud Platform",
            "Microsoft Azure",
            "Python",
            "Java",
            "Scala"
        ],
        "programming_languages": [
            "Python",
            "Java",
            "Scala"
        ],
        "experience": 0,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Computer Science",
                "Engineering"
            ]
        },
        "salary": {
            "max": 210000,
            "min": 140000
        },
        "benefits": [
            "Competitive salary",
            "Health insurance",
            "Dental insurance",
            "Vision insurance",
            "Flexible work hours",
            "Remote work options",
            "Vacation",
            "Paid time off",
            "Professional development opportunities",
            "Training programs",
            "Conferences",
            "Workshops",
            "Technology environment",
            "Cutting-edge tools",
            "Resources",
            "Company culture",
            "Team-building activities",
            "Social events",
            "Career growth",
            "Advancement opportunities",
            "Projects with real-world impact",
            "Top talent",
            "Industry experts"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Mountain View, CA",
        "job_id": 3951085489,
        "company": "LinkedIn",
        "title": "Staff Software Engineer (Azure Data Engineering)",
        "created_on": 1720635297.8723078,
        "description": "LinkedIn is the world’s largest professional network, built to help members of all backgrounds and experiences achieve more in their careers. Our vision is to create economic opportunities for every member of the global workforce. Every day our members use our products to make connections, discover opportunities, build skills, and gain insights. We believe amazing things happen when we work together in an environment where everyone feels a true sense of belonging, and that what matters most in a candidate is having the skills needed to succeed. It inspires us to invest in our talent and support career growth. Join us to challenge yourself with work that matters. At LinkedIn, we trust each other to do our best work where it works best for us and our teams. This role offers a hybrid work option, meaning you can both work from home and commute to a LinkedIn office, depending on what is best for you and when it is important for your team to be together. Productivity Engineering is a team at LinkedIn that builds products that power LinkedIn’s business. We drive technology vision, architecture, and design systems that help the company deliver on major business processes (go-to-market, sales, finance, and customer support etc.). We deliver applications and data products that support our external and internal customers, do business with us in a seamless way, help grow our top line, and increase our efficiency. We are seeking a highly skilled, passionate, and experienced Staff Software Engineer to join our dynamic team engaged in building a finance data platform to enable data driven decisions across various finance flows. The ideal candidate will have a background in Azure Data Engineering, Azure Data Lake, Azure Databricks, MS Fabric Ecosystems and Oracle E-Business Suite (EBS). You will play a crucial role in designing, developing, and maintaining our Finance Data Platform, integrating with EBS and other upstream applications, ensuring seamless integration and optimal performance. Key Responsibilities: Architecture and Development: Design, develop, and implement data engineering solutions using Azure Data Services (Azure Data Factory, Azure Databricks, Azure Synapse Analytics, etc.). Integrate with Oracle EBS modules, Get Paid (credit & collection), Workday, Salesforce and Dynamics ensuring alignment with business requirements. Integrate Azure data solutions with Oracle EBS for efficient data flow and processing. Knowledge of Oracle Golden Gate and Streaming is a plus. Data Management: Create and manage ETL pipelines to extract, transform, and load data from various sources into Azure and Oracle EBS/upstream systems. Ensure data quality, integrity, and consistency across all systems. Optimize database performance and manage storage solutions. Collaboration and Leadership: Work closely with cross-functional teams, including business analysts, project managers, and other stakeholders, to gather requirements and deliver solutions. Mentor and guide junior engineers, fostering a collaborative and innovative team environment. Participate in code reviews and ensure the best practices are followed. System Maintenance and Support: Monitor and troubleshoot issues within the Azure data ecosystem Provide ongoing support and maintenance for existing solutions. Implement security best practices to protect sensitive data. Continuous Improvement: Stay updated with the latest industry trends and technologies in Azure Data Engineering and Oracle EBS. Propose and implement improvements to current processes and systems. Basic Qualifications: Bachelor’s or Master’s degree in computer science, Information Technology, or a related field. 7+ years of experience in software engineering with a focus on data engineering, Business Intelligence and SQL. Experience with Azure Data Services and Oracle E-Business Suite modules (Financials, Supply Chain, HRMS, etc.). Excellent craftsmanship skills across the SDLC including design, development, deployment, testing and observability. Experience on visualization tools like Power BI/Tableau is a plus Technical Skills: Proficiency in Azure Data Factory, Azure Databricks, Azure Synapse Analytics, and other Azure services. Experience in data engineering skills using SSIS and/or ADF. Experience with data modeling and data governance skills Experience with SQL skills and experience in analytics using Python is a plus Near Real Time data engineering on a huge dataset is a plus. Experience in Performance tuning to meet the SLAs is a must Knowledge of Oracle EBS architecture is a plus Experience with data warehousing, data lakes, and big data technologies. Familiarity with programming languages such as Python, Java, or C#. Excellent problem-solving and analytical skills. Strong communication and interpersonal skills. Ability to work independently and as part of a team. Leadership skills with the ability to mentor and guide team members. Preferred Qualifications: Certifications in Azure Data Engineering and related Azure components Experience with cloud migration projects. Knowledge of DevOps best practices and tools. Data Modeling Experience on High Volume & Near Real time Data Ingestion You will Benefit from our Culture: We strongly believe in the well-being of our employees and their families. That is why we offer generous health and wellness programs and time away for employees of all levels. LinkedIn is committed to fair and equitable compensation practices. The pay range for this role is $144,000 to $235,000. Actual compensation packages are based on several factors that are unique to each candidate, including but not limited to skill set, depth of experience, certifications, and specific work location. This may be different in other locations due to differences in the cost of labor. Equal Opportunity Statement LinkedIn is committed to diversity in its workforce and is proud to be an equal opportunity employer. LinkedIn considers qualified applicants without regard to race, color, religion, creed, gender, national origin, age, disability, veteran status, marital status, pregnancy, sex, gender expression or identity, sexual orientation, citizenship, or any other legally protected class. LinkedIn is an Affirmative Action and Equal Opportunity Employer as described in our equal opportunity statement here: https://microsoft.sharepoint.com/:b:/t/LinkedInGCI/EeE8sk7CTIdFmEp9ONzFOTEBM62TPrWLMHs4J1C_QxVTbg?e=5hfhpE. Please reference https://www.eeoc.gov/sites/default/files/2023-06/22-088_EEOC_KnowYourRights6.12ScreenRdr.pdf and https://www.dol.gov/ofccp/regs/compliance/posters/pdf/OFCCP_EEO_Supplement_Final_JRF_QA_508c.pdf for more information LinkedIn is committed to offering an inclusive and accessible experience for all job seekers, including individuals with disabilities. Our goal is to foster an inclusive and accessible workplace where everyone has the opportunity to be successful. If you need a reasonable accommodation to search for a job opening, apply for a position, or participate in the interview process, connect with us at accommodations@linkedin.com and describe the specific accommodation requested for a disability-related limitation. Reasonable accommodations are modifications or adjustments to the application or hiring process that would enable you to fully participate in that process. Examples of reasonable accommodations include but are not limited to: -Documents in alternate formats or read aloud to you -Having interviews in an accessible location -Being accompanied by a service dog -Having a sign language interpreter present for the interview A request for an accommodation will be responded to within three business days. However, non-disability related requests, such as following up on an application, will not receive a response. LinkedIn will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by LinkedIn, or (c) consistent with LinkedIn's legal duty to furnish information. Pay Transparency Policy Statement As a federal contractor, LinkedIn follows the Pay Transparency and non-discrimination provisions described at this link: https://lnkd.in/paytransparency. Global Data Privacy Notice for Job Candidates This document provides transparency around the way in which LinkedIn handles personal data of employees and job applicants: https://lnkd.in/GlobalDataPrivacyNotice Equal Opportunity Statement LinkedIn is committed to diversity in its workforce and is proud to be an equal opportunity employer. LinkedIn considers qualified applicants without regard to race, color, religion, creed, gender, national origin, age, disability, veteran status, marital status, pregnancy, sex, gender expression or identity, sexual orientation, citizenship, or any other legally protected class. LinkedIn is an Affirmative Action and Equal Opportunity Employer as described in our equal opportunity statement here: https://microsoft.sharepoint.com/:b:/t/LinkedInGCI/EeE8sk7CTIdFmEp9ONzFOTEBM62TPrWLMHs4J1C_QxVTbg?e=5hfhpE. Please reference https://www.eeoc.gov/sites/default/files/2023-06/22-088_EEOC_KnowYourRights6.12ScreenRdr.pdf and https://www.dol.gov/ofccp/regs/compliance/posters/pdf/OFCCP_EEO_Supplement_Final_JRF_QA_508c.pdf for more information. LinkedIn is committed to offering an inclusive and accessible experience for all job seekers, including individuals with disabilities. Our goal is to foster an inclusive and accessible workplace where everyone has the opportunity to be successful. If you need a reasonable accommodation to search for a job opening, apply for a position, or participate in the interview process, connect with us at accommodations@linkedin.com and describe the specific accommodation requested for a disability-related limitation. Reasonable accommodations are modifications or adjustments to the application or hiring process that would enable you to fully participate in that process. Examples of reasonable accommodations include but are not limited to: -Documents in alternate formats or read aloud to you -Having interviews in an accessible location -Being accompanied by a service dog -Having a sign language interpreter present for the interview A request for an accommodation will be responded to within three business days. However, non-disability related requests, such as following up on an application, will not receive a response. LinkedIn will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by LinkedIn, or (c) consistent with LinkedIn's legal duty to furnish information. Pay Transparency Policy Statement As a federal contractor, LinkedIn follows the Pay Transparency and non-discrimination provisions described at this link: https://lnkd.in/paytransparency. Global Data Privacy Notice for Job Candidates This document provides transparency around the way in which LinkedIn handles personal data of employees and job applicants: https://lnkd.in/GlobalDataPrivacyNotice",
        "url": "https://www.linkedin.com/jobs/view/3951085489",
        "summary": "LinkedIn is seeking a Staff Software Engineer to join their Productivity Engineering team and build a finance data platform. The role involves designing, developing, and maintaining the platform using Azure Data Services and integrating with Oracle EBS, Workday, Salesforce, and Dynamics.  Candidates need strong experience in Azure Data Engineering, Azure Data Lake, Azure Databricks, MS Fabric Ecosystems, and Oracle E-Business Suite. The role offers a hybrid work option.",
        "industries": [
            "Technology",
            "Software",
            "Data Engineering",
            "Finance"
        ],
        "soft_skills": [
            "Problem-solving",
            "Analytical",
            "Communication",
            "Interpersonal",
            "Leadership",
            "Teamwork",
            "Mentorship",
            "Collaboration"
        ],
        "hard_skills": [
            "Azure Data Factory",
            "Azure Databricks",
            "Azure Synapse Analytics",
            "SSIS",
            "ADF",
            "Data Modeling",
            "Data Governance",
            "SQL",
            "Python",
            "Oracle EBS",
            "Oracle Golden Gate",
            "Streaming",
            "Power BI",
            "Tableau",
            "Java",
            "C#",
            "DevOps"
        ],
        "tech_stack": [
            "Azure Data Services",
            "Azure Data Factory",
            "Azure Databricks",
            "Azure Synapse Analytics",
            "Oracle EBS",
            "Oracle Golden Gate",
            "SSIS",
            "ADF",
            "Power BI",
            "Tableau",
            "Python",
            "Java",
            "C#"
        ],
        "programming_languages": [
            "Python",
            "Java",
            "C#"
        ],
        "experience": 7,
        "education": {
            "min_degree": "Bachelor’s",
            "fields": [
                "Computer Science",
                "Information Technology"
            ]
        },
        "salary": {
            "max": 235000,
            "min": 144000
        },
        "benefits": [
            "Health and wellness programs",
            "Time away"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Los Angeles, CA",
        "job_id": 3925969190,
        "company": "StubHub",
        "title": "Analytics Engineer II",
        "created_on": 1720635299.5707726,
        "description": "StubHub is on a mission to redefine the live event experience on a global scale. Whether someone is looking to attend their first event or their hundredth, we’re here to delight them all the way from the moment they start looking for a ticket until they step through the gate. The same goes for our sellers. From fans selling a single ticket to the promoters of a worldwide stadium tour, we want StubHub to be the safest, most convenient way to offer a ticket to the millions of fans who browse our platform around the world. We are seeking talented analytics engineers from mid to senior levels to join StubHub’s Data Engineering & Analytics organization. In this role you will be responsible for scaling both analytic products (data assets, dashboards, tools/services) and analytic frameworks (decision frameworks, metrics, analytical models). You will be a “force multiplier” who significantly improves and speeds up StubHub’s ability to make great data-informed decisions. This position is a hybrid role, 2 days remote and 3 days in-office per week, situated in either Los Angeles, CA or New York, NY. About the Team: The Analytics Engineering team exists to enable robust data-informed decision making that steers our business growth goals, through the creation of high-quality data products and scaled insights. Analytics Engineering helps solve the scale problems and common data access patterns encountered by analysts, data scientists, and other business data consumers. All data modules and tools owned by AE are then leveraged not only by members of that business domain, but also data consumers from across the company. We are looking for the right person who can operate and translate across all parts of the business. What You've Done: 3-5 years of relevant analytics engineering, data engineering, or business intelligence experience in a fast paced, high growth environment Proficiency with transforming and analyzing large scale data with modern cloud computing platforms (e.g. SparkSQL, BigQuery, Snowflake, Databricks) High proficiency with SQL and experience with one or more programming languages (e.g. Python, Java) and markup/configuration languages (e.g. YAML) Proficiency in building data models and pipelines using orchestration software (e.g. Airflow, dbt) Experience building reports/dashboards with business intelligence (BI) tools, such as Tableau and Looker Exposure to both batch data processing and real-time streaming technologies Familiarity with data cataloging and metadata management tools Passionate about working with non-technical stakeholders to understand,anticipate, and deliver on their data needs What You'll Do: Manage cross-functional analytical data models, metric frameworks and implementations, and self-serve dashboards/tools Specialize in one or more business domains, building deep expertise and anticipating the needs of that business vertical Collaborate cross functionally with business and product teams while simultaneously \"speaking the language” of engineering teams, oftentimes acting as proxy for one or the other What We Offer: Accelerated Growth Environment: Immerse yourself in an environment designed for swift skill and knowledge enhancement, where you have the autonomy to lead experiments and tests on a massive scale. Top Tier Compensation Package: Enjoy a rewarding compensation package that includes enticing stock incentives, aligning with our commitment to recognizing and valuing your contributions. Flexible Time Off: Embrace a healthy work-life balance with unlimited Flex Time Off, providing you the flexibility to manage your schedule and recharge as needed. Comprehensive Benefits Package: Prioritize your well-being with a comprehensive benefits package, featuring 401k, and premium Health, Vision, and Dental Insurance options. Team-Building Events: Engage in vibrant team events that foster camaraderie and collaboration, creating an atmosphere where your professional and personal growth are celebrated. The anticipated gross annual base salary range for this role is $175,000 – $230,000 per year. Actual compensation will vary depending on factors such as a candidate’s qualifications, skills, experience, and competencies. Base annual salary is one component of StubHub’s total compensation and competitive benefits package, which also includes equity, 401(k), paid time off, paid parental leave, and comprehensive health benefits. California Job Applicant Privacy Notice found here About Us StubHub is the world’s leading marketplace to buy and sell tickets to any live event, anywhere. Through StubHub in North America and viagogo, our international platform, we service customers in 195 countries in 33 languages and 49 available currencies. With more than 300 million tickets available annually on our platform to events around the world -- from sports to music, comedy to dance, festivals to theater -- StubHub offers the safest, most convenient way to buy or sell tickets to the most memorable live experiences. Come join our team for a front-row seat to the action. We are an equal opportunity employer and value diversity on our team. We do not discriminate on the basis of race, color, religion, sex, national origin, gender, sexual orientation, age, disability, veteran status, or any other legally protected status.",
        "url": "https://www.linkedin.com/jobs/view/3925969190",
        "summary": "StubHub seeks Analytics Engineers to build data models, frameworks, and dashboards to drive business decisions.  They'll work cross-functionally with business and product teams, translating between engineering and business needs. The role requires experience with cloud platforms like BigQuery, SparkSQL, and Snowflake, as well as proficiency in SQL and Python. Experience with data pipelines and BI tools like Tableau is essential.",
        "industries": [
            "E-commerce",
            "Entertainment",
            "Ticketing",
            "Data & Analytics"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Analytical Skills",
            "Data-Driven Decision Making",
            "Stakeholder Management",
            "Time Management",
            "Passionate"
        ],
        "hard_skills": [
            "SparkSQL",
            "BigQuery",
            "Snowflake",
            "Databricks",
            "SQL",
            "Python",
            "Java",
            "YAML",
            "Airflow",
            "dbt",
            "Tableau",
            "Looker",
            "Batch Data Processing",
            "Real-time Streaming Technologies",
            "Data Cataloging",
            "Metadata Management"
        ],
        "tech_stack": [
            "SparkSQL",
            "BigQuery",
            "Snowflake",
            "Databricks",
            "SQL",
            "Python",
            "Java",
            "YAML",
            "Airflow",
            "dbt",
            "Tableau",
            "Looker",
            "Batch Data Processing",
            "Real-time Streaming Technologies",
            "Data Cataloging",
            "Metadata Management"
        ],
        "programming_languages": [
            "SQL",
            "Python",
            "Java"
        ],
        "experience": 3,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 230000,
            "min": 175000
        },
        "benefits": [
            "Accelerated Growth Environment",
            "Top Tier Compensation Package",
            "Stock Incentives",
            "Flexible Time Off",
            "401k",
            "Health, Vision, and Dental Insurance",
            "Team-Building Events"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Alameda, CA",
        "job_id": 3877326713,
        "company": "Abbott",
        "title": "Data Engineer",
        "created_on": 1720635303.042549,
        "description": "Abbott is a global healthcare leader that helps people live more fully at all stages of life. Our portfolio of life-changing technologies spans the spectrum of healthcare, with leading businesses and products in diagnostics, medical devices, nutritionals and branded generic medicines. Our 114,000 colleagues serve people in more than 160 countries. Data Engineer About Lingo Meet Lingo, a new biosensing technology that provides users a window into their body. Lingo tracks key biomarkers – such as glucose, ketones, and lactate – to help people make better decisions about their health and nutrition. Biowearable technology will digitize, decentralize and democratize healthcare, enabling consumers to take control of their own health. Working at Abbott At Abbott, You Can Do Work That Matters, Grow, And Learn, Care For Yourself And Family, Be Your True Self And Live a Full Life. You’ll Also Have Access To Career development with an international company where you can grow the career you dream of . Free medical coverage for employees* via the Health Investment Plan (HIP) PPO An excellent retirement savings plan with high employer contribution Tuition reimbursement, the Freedom 2 Save student debt program and FreeU education benefit - an affordable and convenient path to getting a bachelor’s degree. A company recognized as a great place to work in dozens of countries around the world and named one of the most admired companies in the world by Fortune. A company that is recognized as one of the best big companies to work for as well as a best place to work for diversity, working mothers, female executives, and scientists. The opportunity Personalized healthcare is the future. Working on Lingo, you will help build a next-generation technology that enables individuals to make decisions about how to improve energy, lose weight or enhance athletic performance. The Lingo team embodies a start-up culture and mindset with the backing of Abbott, a company with a rich history of healthcare innovation. Join us and grow your career as you help Abbott shape the future of healthcare. This position works out of our  location in Lingo. You will work closely with a multidisciplinary agile team to build high-quality data pipelines for high-end analytics solutions. These solutions will generate insights from the organization’s connected data and enable data-driven decision-making capabilities for the organization. What You’ll Do Design, develop, optimize, and maintain data architecture and pipelines that adhere to ETL principles and business goals Solve complex data problems to deliver insights that help the organization achieve its goals Code in Python and Scala with tools like Apache Spark/Kafka to build a multi-cluster data warehouse Interact with other technology teams to define, prioritize, and ensure smooth deployments for other operational components Advise, consult, mentor, and coach other data and analytics professionals on data standards and practices Foster a culture of sharing, reuse, design for scale stability, and operational efficiency of data and analytical solutions Codify best practices for future reuse in the form of accessible, reusable patterns, templates, and code bases to facilitate data capturing and management Qualifications 4+ years of relevant experience in data engineering/analytics space Expertise in SQL and data analysis and strong hands-on expertise with at least one programming language: Python and/or Scala Strong knowledge in one or more of the following big data tools: Hive, Hadoop Impala, Spark, Kafka Strong expertise in ETL, reporting tools, data governance, data warehousing, and hands-on experience Experience developing solutions for cloud computing services and infrastructure Experience developing and maintaining data warehouses in big data solutions Up-to-date on industry trends within the analytics space from a data acquisition processing, engineering, and management perspective Experience in agile development Strong people skills, specifically in collaboration and teamwork High level of curiosity, creativity, and problem-solving capabilities What We Offer At Abbott, you can have a good job that can grow into a great career. We offer: Training and career development, with onboarding programs for new employees and tuition assistance Financial security through competitive compensation, incentives and retirement plans Health care and well-being programs including medical, dental, vision, wellness and occupational health programs Paid time off 401(k) retirement savings with a generous company match The stability of a company with a record of strong financial performance and history of being actively involved in local communities Learn more about our benefits that add real value to your life to help you live fully: http://www.abbottbenefits.com/pages/candidate.aspx Follow your career aspirations to Abbott for diverse opportunities with a company that provides the growth and strength to build your future. Abbott is an Equal Opportunity Employer, committed to employee diversity. Connect with us at www.abbott.com, on Facebook at www.facebook.com/Abbott and on Twitter @AbbottNews and @AbbottGlobal. The base pay for this position is $95,500.00 – $190,900.00. In specific locations, the pay range may vary from the range posted.",
        "url": "https://www.linkedin.com/jobs/view/3877326713",
        "summary": "Abbott is seeking a Data Engineer to join their Lingo team, a new biosensing technology that provides users a window into their body. The ideal candidate will have 4+ years of relevant experience in data engineering/analytics, expertise in SQL and data analysis, strong hands-on expertise with Python and/or Scala, and knowledge in big data tools like Hive, Hadoop Impala, Spark, and Kafka. They will design, develop, optimize, and maintain data architecture and pipelines, solve complex data problems, code in Python and Scala, and interact with other technology teams.  This role is based in Lingo and offers a competitive salary range of $95,500.00 – $190,900.00.",
        "industries": [
            "Healthcare",
            "Biotechnology",
            "Technology",
            "Data Analytics",
            "Software Development",
            "Engineering",
            "Biosensing"
        ],
        "soft_skills": [
            "Problem-solving",
            "Collaboration",
            "Teamwork",
            "Communication",
            "Mentoring",
            "Consulting",
            "Curiosity",
            "Creativity"
        ],
        "hard_skills": [
            "SQL",
            "Data Analysis",
            "Python",
            "Scala",
            "Apache Spark",
            "Kafka",
            "Hive",
            "Hadoop Impala",
            "ETL",
            "Data Warehousing",
            "Data Governance",
            "Cloud Computing",
            "Agile Development"
        ],
        "tech_stack": [
            "Apache Spark",
            "Kafka",
            "Hive",
            "Hadoop Impala",
            "Python",
            "Scala",
            "SQL"
        ],
        "programming_languages": [
            "Python",
            "Scala",
            "SQL"
        ],
        "experience": 4,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 190900,
            "min": 95500
        },
        "benefits": [
            "Career development",
            "Free medical coverage",
            "Retirement savings plan",
            "Tuition reimbursement",
            "Student debt program",
            "Education benefit",
            "Paid time off",
            "401(k) with company match"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Los Angeles, CA",
        "job_id": 3946957381,
        "company": "Adobe",
        "title": "Finance Data Engineer",
        "created_on": 1720635304.6312397,
        "description": "Our Company Changing the world through digital experiences is what Adobe’s all about. We give everyone—from emerging artists to global brands—everything they need to design and deliver exceptional digital experiences! We’re passionate about empowering people to create beautiful and powerful images, videos, and apps, and transform how companies interact with customers across every screen. We’re on a mission to hire the very best and are committed to creating exceptional employee experiences where everyone is respected and has access to equal opportunity. We realize that new ideas can come from everywhere in the organization, and we know the next big idea could be yours! The Opportunity This is an exciting opportunity to join the Digital Media Finance team as we continue to propel the business through data-driven forecasts and provide influential insights. In this role, you will have the opportunity to help drive Adobe’s pivotal initiatives by data engineering solutions that connect systems to visualization platforms for management reporting. The position will report to the Group Manager, Finance Automation to drive our systems and automation effort. This position will provide an outstanding opportunity to drive timely insights across Digital Media (DMe) Business at Adobe and help drive the Finance Transformational efforts undertaken by the Finance Automation team. The ideal candidate for this role is a well-rounded top performer with exceptional data engineering, analysis, and visualization skills, capable of driving growth in a fast-paced environment. You should possess a continuous learning mentality to explore and implement innovations in the team's reporting and transformational projects. Strong cross-functional collaboration within Adobe is essential. You will be responsible for automating complex financial models, developing engaging and interactive dashboards, streamlining processes, and enhancing efficiencies in management reporting What you'll do: Responsible for the strategy and execution of DMe lifecycle of financial and analytics dashboards and ensuring scalability of systems and processes. Manage data transitions and platform migrations between SAP HANA/DataBricks, Tableau/PowerBI, and related ETL processes. Develop data pipelines, connections, and infrastructure to better enable forecasting and data science modelling. Correct data errors via systematic, logical fixes and provide updates within the data pipelines, connections and infrastructures that are built to support the teams. Create documentation and support enablement for the teams who are interested and able to help themselves. Accountable for ad-hoc support & participation in critical business analytics and key project support as directed by management. Partner closely with IT, Finance Systems, Finance Transformation Office, and other Finance counterparts to continually improve, streamline & enhance planning and reporting processes. What you need to succeed : BS/BA with preferred focuses in areas of Business, Finance, or Information Systems. Master’s in Data Science a plus. 3+ years of demonstrated experience working with large and complex data structures and develop efficient queries to create calculated fields and data aggregates. 2+ years of proven experience designing and developing dashboards using PowerBI or Tableau. 3+ years of relevant experience in data science and analytics in creating/maintaining financial models for a subscription/SaaS business a plus. Proficient in data platforms/systems (such as SQL, Databricks), ETL tools (such as Python, SnapLogic), process automation, and standardization. Self-starter with high attention to details, excellent interpersonal skills, and ability to take charge, set objectives, and deliver results. Strong project management skills with ability to juggle multiple priorities. Strong team orientation and a learning mentality. Our compensation reflects the cost of labor across several  U.S. geographic markets, and we pay differently based on those defined markets. The U.S. pay range for this position is $73,900 -- $170,300 annually. Pay within this range varies by work location and may also depend on job-related knowledge, skills, and experience. Your recruiter can share more about the specific salary range for the job location during the hiring process. At Adobe, for sales roles starting salaries are expressed as total target compensation (TTC = base + commission), and short-term incentives are in the form of sales commission plans. Non-sales roles starting salaries are expressed as base salary and short-term incentives are in the form of the Annual Incentive Plan (AIP). In addition, certain roles may be eligible for long-term incentives in the form of a new hire equity award. Adobe will consider qualified applicants with arrest or conviction records for employment in accordance with state and local laws and “fair chance” ordinances. Adobe is proud to be an Equal Employment Opportunity and affirmative action employer. We do not discriminate based on gender, race or color, ethnicity or national origin, age, disability, religion, sexual orientation, gender identity or expression, veteran status, or any other applicable characteristics protected by law. Learn more. Adobe aims to make Adobe.com accessible to any and all users. If you have a disability or special need that requires accommodation to navigate our website or complete the application process, email accommodations@adobe.com or call (408) 536-3015. Adobe values a free and open marketplace for all employees and has policies in place to ensure that we do not enter into illegal agreements with other companies to not recruit or hire each other’s employees.",
        "url": "https://www.linkedin.com/jobs/view/3946957381",
        "summary": "Adobe is looking for a Data Engineer to join their Digital Media Finance team. The ideal candidate will have experience with large and complex data structures, data platforms/systems (SQL, Databricks), ETL tools (Python, SnapLogic), and dashboard design using PowerBI or Tableau. This role involves building data pipelines, managing data transitions, automating financial models, creating interactive dashboards, and collaborating with cross-functional teams.",
        "industries": [
            "Software",
            "Technology",
            "Finance",
            "Data Engineering",
            "Analytics"
        ],
        "soft_skills": [
            "communication",
            "collaboration",
            "problem-solving",
            "analytical",
            "project management",
            "time management",
            "organization",
            "critical thinking",
            "attention to detail",
            "teamwork",
            "learning",
            "self-starter",
            "leadership"
        ],
        "hard_skills": [
            "SQL",
            "Databricks",
            "Python",
            "SnapLogic",
            "PowerBI",
            "Tableau",
            "SAP HANA",
            "DataBricks",
            "ETL",
            "data engineering",
            "data science",
            "analytics",
            "financial modeling",
            "process automation",
            "standardization"
        ],
        "tech_stack": [
            "SQL",
            "Databricks",
            "Python",
            "SnapLogic",
            "PowerBI",
            "Tableau",
            "SAP HANA",
            "DataBricks",
            "ETL"
        ],
        "programming_languages": [
            "SQL",
            "Python"
        ],
        "experience": 3,
        "education": {
            "min_degree": "BS/BA",
            "fields": [
                "Business",
                "Finance",
                "Information Systems",
                "Data Science"
            ]
        },
        "salary": {
            "max": 170300,
            "min": 73900
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Fremont, CA",
        "job_id": 3818360381,
        "company": "Info Way Solutions",
        "title": "Data Engineer�",
        "created_on": 1720635306.2755933,
        "description": "Hi Professionals, Hope you are doing good Job Description This is Sangeetha from Info Way Solutions, LLC We have job opening for Data Engineer and the detailed Job description is given below: Kindly check the JD and share your views Role Data Engineer Location : Austin,TX Strong Python development and software design Dremio/Presto/Trino Snowflake (or other data warehouse systems) Tableau Docker & Kubernetes SQL MongoDB Object store (S3) & data lake concepts Git Shell & CLI tools REST APIs Pandas Thanks & Regards, Sangeetha Email: sangeetha@Infowaygroup.com Direct: (925)241-4886 Work: (925)-592-6160 Ext 104 https://www.linkedin.com/in/sangeetha-kannan-291636206/ Info Way Solutions LLC | 46520 Fremont Blvd, Suite 614 | Fremont, CA - 94538",
        "url": "https://www.linkedin.com/jobs/view/3818360381",
        "summary": "Data Engineer role in Austin, TX requiring strong Python development, Dremio/Presto/Trino, Snowflake, Tableau, Docker & Kubernetes, SQL, MongoDB, Object store (S3) & data lake concepts, Git, Shell & CLI tools, REST APIs, and Pandas.",
        "industries": [
            "Information Technology",
            "Software Development",
            "Data Engineering",
            "Data Analytics",
            "Cloud Computing"
        ],
        "soft_skills": [],
        "hard_skills": [
            "Python",
            "Dremio",
            "Presto",
            "Trino",
            "Snowflake",
            "Tableau",
            "Docker",
            "Kubernetes",
            "SQL",
            "MongoDB",
            "S3",
            "Git",
            "Shell",
            "CLI",
            "REST APIs",
            "Pandas"
        ],
        "tech_stack": [
            "Python",
            "Dremio",
            "Presto",
            "Trino",
            "Snowflake",
            "Tableau",
            "Docker",
            "Kubernetes",
            "SQL",
            "MongoDB",
            "S3",
            "Git",
            "Shell",
            "CLI",
            "REST APIs",
            "Pandas"
        ],
        "programming_languages": [
            "Python",
            "SQL"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Sunnyvale, CA",
        "job_id": 3818815198,
        "company": "Keylent Inc",
        "title": "Data Engineer TECHM-JOB-28410",
        "created_on": 1720635309.674812,
        "description": "Data Engineer TECHM-JOB-28410 Skill: SQL,Python,Kubernetes,MongoDB,Snowf... We are looking for Data Engineer in Sunnyvale, CA - Required Onsite Job Description Strong Python development and software design Dremio/Presto/Trino Snowflake (or other data warehouse systems) Tableau Docker & Kubernetes SQL MongoDB Object store (S3) & data lake concepts Git Shell & CLI tools REST APIs Pandas",
        "url": "https://www.linkedin.com/jobs/view/3818815198",
        "summary": "Data Engineer position in Sunnyvale, CA requiring strong Python development skills, experience with Dremio/Presto/Trino, Snowflake, Tableau, Docker & Kubernetes, SQL, MongoDB, Object storage (S3) & data lake concepts, Git, Shell & CLI tools, and REST APIs.",
        "industries": [
            "Technology",
            "Data Science",
            "Software Development"
        ],
        "soft_skills": [
            "Problem Solving",
            "Communication",
            "Teamwork",
            "Collaboration",
            "Analytical Thinking",
            "Time Management"
        ],
        "hard_skills": [
            "Python",
            "SQL",
            "Kubernetes",
            "MongoDB",
            "Dremio",
            "Presto",
            "Trino",
            "Snowflake",
            "Tableau",
            "Docker",
            "Git",
            "Shell",
            "CLI",
            "REST APIs",
            "Pandas"
        ],
        "tech_stack": [
            "Python",
            "SQL",
            "Kubernetes",
            "MongoDB",
            "Dremio",
            "Presto",
            "Trino",
            "Snowflake",
            "Tableau",
            "Docker",
            "Git",
            "Shell",
            "CLI",
            "REST APIs",
            "Pandas"
        ],
        "programming_languages": [
            "Python",
            "SQL"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Mountain View, CA",
        "job_id": 3899991632,
        "company": "Ampcus Inc",
        "title": "Data Engineer",
        "created_on": 1720635311.349126,
        "description": "Location: Mountain View, CA (Hybrid) Experience: 10+ yrs. Skills Python (Numpy, Pandas) SQL Hadoop, Hive, Pyspark RDBMS Tableau/Qliksense/Power BI - Any one of those Advanced Excel Visuals (Pivot and Regression)",
        "url": "https://www.linkedin.com/jobs/view/3899991632",
        "summary": "Seeking a data scientist with 10+ years of experience in Python (Numpy, Pandas), SQL, Hadoop, Hive, Pyspark, RDBMS, and data visualization tools like Tableau, Qliksense, or Power BI. Strong skills in advanced Excel visuals, including pivot tables and regression analysis are required. Location: Mountain View, CA (Hybrid).",
        "industries": [
            "Data Science",
            "Analytics",
            "Software"
        ],
        "soft_skills": [],
        "hard_skills": [
            "Python",
            "Numpy",
            "Pandas",
            "SQL",
            "Hadoop",
            "Hive",
            "Pyspark",
            "RDBMS",
            "Tableau",
            "Qliksense",
            "Power BI",
            "Excel",
            "Pivot Tables",
            "Regression"
        ],
        "tech_stack": [
            "Python",
            "SQL",
            "Hadoop",
            "Hive",
            "Pyspark",
            "Tableau",
            "Qliksense",
            "Power BI",
            "Excel"
        ],
        "programming_languages": [
            "Python",
            "SQL"
        ],
        "experience": 10,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Jose, CA",
        "job_id": 3840482615,
        "company": "Mendel.ai",
        "title": "Senior Data Platform Engineer",
        "created_on": 1720635314.7583418,
        "description": "About Mendel: At Mendel, we are enabling different stakeholders in healthcare to make better decisions by learning from every patient's journey— from a physician deciding on the best treatment for a patient to a pharma company discovering the next blockbuster drug. In creating depth and breadth for a patient’s health record journey, Mendel enables the right care to be delivered at the right time to the right person. This is an incredible opportunity to join a unique vision to improve healthcare using AI technology. About the Role: We seek an experienced and highly skilled Senior Data Platform Engineer to join our dynamic team. In this crucial role, you will be responsible for designing, building, and maintaining our organization's data platforms and infrastructure, ensuring efficient and reliable data processing, storage, and analysis capabilities. Your expertise in data engineering will contribute to our organization's data-driven decision-making and overall success. Responsibilities: Architect and build scalable and performing data platforms by leveraging technologies like Apache Spark, Elastic, Kafka, Kubernetes, and various cloud-based data services. Design and develop data pipelines for ingesting, transforming, and processing large volumes of structured and unstructured data from multiple sources. Collaborate with data engineers, data scientists, and business stakeholders to understand data requirements and translate them into robust and efficient scalable data solutions. Optimize data workflows and processes for performance, reliability, and cost-effectiveness. Implement data governance policies, data quality controls, and data security measures to ensure data integrity and compliance with regulatory requirements. Automate and streamline data operations through scripting, containerization, and orchestration tools. Participate in code reviews, knowledge sharing, and mentoring of junior team members. Stay up-to-date with emerging data technologies, trends, and best practices, and contribute to the continuous improvement of our data platform. Qualifications: Bachelor's degree in Computer Science, Information Technology, or a related field, or equivalent professional experience. Minimum of 5 years of experience in data engineering or a related role, with a strong focus on building and maintaining data platforms and infrastructure. Proficient in programming languages such as Python, Scala, or Java, and experience with SQL and NoSQL databases. Hands-on experience in distributed computing frameworks like Apache Spark, Ray, and related ecosystem tools. Experience cloud platforms (e.g., AWS, GCP, Azure) and their data services. Experience with containerization technologies like Docker and orchestration tools like Kubernetes. Experience in solving complex problems using both structured and unstructured data. Strong understanding of data modeling, data warehousing, and data integration concepts. Excellent problem-solving, analytical, and troubleshooting skills. Ability to work collaboratively in a team environment and communicate complex technical concepts effectively. Preferred Skills: Master's degree in a relevant field or equivalent experience. Experience with real-time data processing and streaming technologies (e.g., Apache Kafka, Apache Flink). Experience in data governance, data quality, and data security best practices. Experience in agile software development methodologies and DevOps practices. Experience with machine learning and data science workflows is a plus. P.S.: This role will require you working out of our San Jose office. Please feel free to apply if you are local or open to relocation. Why should you join our team: Benjamin Franklin once said: “If you would not be forgotten as soon as you are dead, either write something worth reading, or do something worth writing.” — at Mendel you can do the latter. There are two things more devastating than learning that you or your loved ones have a terminal illness; getting the wrong treatment or not finding any. At Mendel, we are on a sincere mission to solve many never-solved-before technology challenges that can enable prescribers and drug makers to do their best",
        "url": "https://www.linkedin.com/jobs/view/3840482615",
        "summary": "Mendel is seeking a Senior Data Platform Engineer to design, build, and maintain data platforms and infrastructure using technologies like Apache Spark, Elastic, Kafka, Kubernetes, and cloud-based data services. This role involves data pipeline development, collaboration with data engineers and scientists, optimizing data workflows, implementing data governance and security, and staying up-to-date with emerging technologies. The ideal candidate will have 5+ years of experience in data engineering, strong programming skills (Python, Scala, Java), experience with distributed computing frameworks, cloud platforms, containerization, data modeling, and excellent problem-solving abilities.",
        "industries": [
            "Healthcare",
            "Biotechnology",
            "Pharmaceuticals",
            "Data Science",
            "Artificial Intelligence"
        ],
        "soft_skills": [
            "Collaboration",
            "Communication",
            "Problem-solving",
            "Analytical",
            "Troubleshooting",
            "Teamwork"
        ],
        "hard_skills": [
            "Apache Spark",
            "Elastic",
            "Kafka",
            "Kubernetes",
            "Python",
            "Scala",
            "Java",
            "SQL",
            "NoSQL",
            "Ray",
            "AWS",
            "GCP",
            "Azure",
            "Docker",
            "Data Modeling",
            "Data Warehousing",
            "Data Integration"
        ],
        "tech_stack": [
            "Apache Spark",
            "Elastic",
            "Kafka",
            "Kubernetes",
            "AWS",
            "GCP",
            "Azure",
            "Docker",
            "Ray"
        ],
        "programming_languages": [
            "Python",
            "Scala",
            "Java",
            "SQL"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Computer Science",
                "Information Technology"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Menlo Park, CA",
        "job_id": 3922129923,
        "company": "SPECTRAFORCE",
        "title": "Data Engineer",
        "created_on": 1720635317.923836,
        "description": "Data Engineer Menlo Park, CA 12 months Job Description: Perform analysis of data extracted from testbeds / test campaigns Writing/scripting queries to collect data across the testbeds and various test equipment (e.g, traffic generators, cellular), systems (e.g, linux systems) and other types (e.g, telecom, cellular, wifi) Data extracting results, observations, insights - Integration with internal tools (database, hive tables, visualization tools) Use of open-source / external tools for visualization: eg., grafana, influxdb Data extraction from mobile devices (e.g. iphone, samsung etc…) lab equipments (e.g. switches, wireless access point, telecom equipment) and packet capture tools like wireshark/pcap, sflow, netflow Merging data from a number of different data sources (mobile devices, lab equipments, Meta content delivery networks, mobile devices)",
        "url": "https://www.linkedin.com/jobs/view/3922129923",
        "summary": "Data Engineer role focusing on extracting, analyzing, and visualizing data from testbeds and campaigns. The position involves writing queries, integrating data with internal tools, and using visualization tools like Grafana.  Experience with mobile devices, lab equipment, and packet capture tools is required.",
        "industries": [
            "Technology",
            "Telecommunications",
            "Data Analysis",
            "Data Engineering"
        ],
        "soft_skills": [
            "Analysis",
            "Problem Solving",
            "Communication",
            "Collaboration",
            "Integration"
        ],
        "hard_skills": [
            "Query Writing",
            "Data Extraction",
            "Data Integration",
            "Data Visualization",
            "Grafana",
            "InfluxDB",
            "Wireshark",
            "PCAP",
            "SFlow",
            "Netflow",
            "Linux Systems"
        ],
        "tech_stack": [
            "Grafana",
            "InfluxDB",
            "Wireshark",
            "PCAP",
            "SFlow",
            "Netflow",
            "Hive",
            "Linux"
        ],
        "programming_languages": [],
        "experience": 12,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Fremont, CA",
        "job_id": 3822195459,
        "company": "Info Way Solutions",
        "title": "Data Engineer",
        "created_on": 1720635319.5552785,
        "description": "Hello Professionals Hope you are doing well, i am sharing a below job requirement with you, do let me know if you are open for a new opportunity/have a consultant open for a new opportunity. Location - NYC / NJ WA: W2 only Client: Mizuho Bank Job Description Must be 8/10 Years in SQL Server as it involves replicating functionality in Snowflake SSIS strongly preferred Python is a must Banking and Finance background is a must DBT, Mulesoft, and Alation - good to have Thanks and Regards Girish Tanwar Sr. Executive - Recruitment & Operations Mobile: +19253019833 Email: girish.t@infowaygroup.com LinkedIn: https://www.linkedin.com/in/girishtan007/ Info Way Solutions LLC. | 46520 Fremont Blvd, Suite 614 | Fremont, CA - 94538",
        "url": "https://www.linkedin.com/jobs/view/3822195459",
        "summary": "Mizuho Bank is seeking a SQL Server expert with 8+ years of experience to replicate functionality in Snowflake. Strong SSIS experience is preferred, and Python is a must.  Banking and Finance background is required.  Experience with DBT, Mulesoft, and Alation is a plus.",
        "industries": [
            "Banking",
            "Finance"
        ],
        "soft_skills": [],
        "hard_skills": [
            "SQL Server",
            "Snowflake",
            "SSIS",
            "Python",
            "DBT",
            "Mulesoft",
            "Alation"
        ],
        "tech_stack": [
            "SQL Server",
            "Snowflake",
            "SSIS",
            "Python",
            "DBT",
            "Mulesoft",
            "Alation"
        ],
        "programming_languages": [
            "SQL",
            "Python"
        ],
        "experience": 8,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Sunnyvale, CA",
        "job_id": 3931509883,
        "company": "Jobs Malaysia - Two95 HR HUB",
        "title": "Senior Data Engineer",
        "created_on": 1720635322.7914307,
        "description": "Title: Senior Data Engineer Location: Sunnyvale, CA Position: 2 Month (Contract) Summary Provide business insights, while leveraging internal tools and systems, databases and industry data Roles & Responsibilities Ability to document requirements, data lineage, subject matter in both business and technical terminology. Guide and learn from other team members. Demonstrated ability to transform business requirements to code, specific analytical reports and tools This role will involve coding, analytical modeling, root cause analysis, investigation, debugging, testing and collaboration with the business partners, product managers other engineering team. Minimum Qualifications Very Strong engineering skills. Should have an analytical approach and have good programming skills Minimum of 5+ years' experience. Experience in retail business will be a plus. Excellent written and verbal communication skills for varied audiences on engineering subject matter Must Have Strong analytical background Self-starter Must be able to reach out to others and thrive in a fast-paced environment. Strong background in transforming big data into business insights Technical Requirements Knowledge/experience on Teradata Physical Design and Implementation, Teradata SQL Performance Optimization Experience with Teradata Tools and Utilities (FastLoad, MultiLoad, BTEQ, FastExport) Advanced SQL (preferably Teradata) Experience working with large data sets, experience working with distributed computing (MapReduce, Hadoop, Hive, Pig, Apache Spark, etc.). Strong Hadoop scripting skills to process petabytes of data Experience in Unix/Linux shell scripting or similar programming/scripting knowledge Experience in ETL/ processes Real time data ingestion (Kafka) BS degree in specific technical fields like computer science, math, statistics preferred Education",
        "url": "https://www.linkedin.com/jobs/view/3931509883",
        "summary": "This role involves transforming business requirements into code, analytical reports, and tools. Responsibilities include coding, analytical modeling, root cause analysis, investigation, debugging, testing, and collaboration with business partners and product managers.  The position requires strong engineering skills, analytical thinking, and experience with data warehousing technologies.",
        "industries": [
            "Retail",
            "Technology",
            "Data Analytics",
            "Data Engineering"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Analytical Thinking",
            "Problem Solving",
            "Self-starter",
            "Teamwork",
            "Guide and Learn"
        ],
        "hard_skills": [
            "Teradata Physical Design",
            "Teradata Implementation",
            "Teradata SQL Performance Optimization",
            "Teradata Tools",
            "FastLoad",
            "MultiLoad",
            "BTEQ",
            "FastExport",
            "Advanced SQL",
            "Hadoop",
            "MapReduce",
            "Hive",
            "Pig",
            "Apache Spark",
            "Unix/Linux shell scripting",
            "ETL",
            "Kafka"
        ],
        "tech_stack": [
            "Teradata",
            "Hadoop",
            "Hive",
            "Pig",
            "Apache Spark",
            "Kafka",
            "Unix/Linux"
        ],
        "programming_languages": [
            "SQL"
        ],
        "experience": 5,
        "education": {
            "min_degree": "BS",
            "fields": [
                "Computer Science",
                "Math",
                "Statistics"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Newport Beach, CA",
        "job_id": 3960331887,
        "company": "Obsidian Security",
        "title": "Data Software Engineer",
        "created_on": 1720635326.2330709,
        "description": "About Us: Obsidian Security was founded in 2017 to solve the unaddressed blindspot of SaaS Security. SaaS applications provide the tools employees need to succeed and hold the business’ most critical information. If those tools become unavailable or that data is jeopardized, there is a detrimental impact on the organization. Obsidian proudly offers the industry's most comprehensive and powerful SaaS defense solution. We are committed to solving the challenge of SaaS Security for our customers as efficiently and effectively as possible. We’re a passionate team optimizing for impact by solving some of the biggest challenges in cybersecurity today. We listen closely to our customers, iterate quickly, and (over) deliver to delight them. Working at Obsidian means contributing to an industry-leading cybersecurity product in an environment where customer satisfaction, privacy, and data ethics are paramount. Role Summary: As a Data Software Engineer, you will be integral in helping to build the future of Obsidian. This position combines engineering, data analysis and security research to deliver new service integrations into the Obsidian platform. You will be responsible for engaging with internal and external stakeholders to build solutions that deliver security insights on customer critical SaaS applications as well as helping to build the frameworks and tools that we use to do it. This role is perfect for gaining experience in software development and data analysis. You will be working in a growing multidisciplinary talented team at the heart of our product. Key Responsibilities: Analyze SaaS services to build integrations covering onboarding, data retrieval, analytics and presentation Deliver security insights that help our customers secure and govern their services Engineer solutions for tooling and frameworks that enable us to deliver service integrations Work with our product team to engage with customers and deliver their requirements and insights into the Obsidian platform Work with our cross-functional team to help drive technical requirements and specifications Required Skills And Qualifications Bachelor's or Master's degree in Computer Science, Engineering, or a related field. 1+ years of experience with software development, data analysis or security research Familiarity with Python and SQL Strong data analysis skills and an inquisitive mind Nice to Have Experience with dbt, Dagster, Postgres is beneficial Experience with LinuxDocker/Kubernetes is beneficial Golang Experience in a startup environment, with the ability to thrive in a fast-paced and dynamic setting Must be based in NPB office 3 days a week, Mon/Wed/Thurs Employee Benefits: Our competitive benefits packages are designed to support our employees' well-being, both at work and at home. Competitive compensation with equity and 401k Comprehensive healthcare with dental and vision coverage Flexible paid time off and paid holiday time off 12 weeks of new parent or family leave Personal and professional development resources Comp $110k At Obsidian, we are proud to be an equal-opportunity employer. We value diversity and hire for talent, passion, and compassion. In compliance with federal law, all persons hired will be required to submit satisfactory proof of identity and legal authorization.",
        "url": "https://www.linkedin.com/jobs/view/3960331887",
        "summary": "Obsidian Security is seeking a Data Software Engineer to build service integrations for their SaaS security platform. The role involves data analysis, security research, and software development to deliver security insights to customers. Responsibilities include analyzing SaaS services, building integrations, delivering security insights, engineering tooling and frameworks, and collaborating with product and cross-functional teams. Ideal candidate has 1+ years of experience with software development, data analysis or security research, familiarity with Python and SQL, strong data analysis skills, and experience with dbt, Dagster, Postgres, LinuxDocker/Kubernetes, and Golang.",
        "industries": [
            "Cybersecurity",
            "Software Development",
            "Data Analysis",
            "SaaS"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Critical Thinking",
            "Inquisitiveness",
            "Adaptability",
            "Teamwork",
            "Passion",
            "Compassion"
        ],
        "hard_skills": [
            "Python",
            "SQL",
            "dbt",
            "Dagster",
            "Postgres",
            "Linux",
            "Docker",
            "Kubernetes",
            "Golang"
        ],
        "tech_stack": [
            "Python",
            "SQL",
            "dbt",
            "Dagster",
            "Postgres",
            "Linux",
            "Docker",
            "Kubernetes",
            "Golang"
        ],
        "programming_languages": [
            "Python",
            "SQL",
            "Golang"
        ],
        "experience": 1,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Computer Science",
                "Engineering"
            ]
        },
        "salary": {
            "max": 110000,
            "min": 0
        },
        "benefits": [
            "Competitive compensation",
            "Equity",
            "401k",
            "Comprehensive healthcare",
            "Dental and vision coverage",
            "Flexible paid time off",
            "Paid holiday time off",
            "New parent or family leave",
            "Personal and professional development resources"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Sunnyvale, CA",
        "job_id": 3956004615,
        "company": "Keylent Inc",
        "title": "Data Engineer MAHIN-JOB-35556",
        "created_on": 1720635327.844611,
        "description": "Data Engineer MAHIN-JOB-35556 We are looking for Data Engineer in Sunnyvale,CA Day 1 onsite ( 3 days a week hybrid) FTE- 130k per annum + benefits Subcon-73 $ per hour ( no layers are accepted) Job Description Strong Python development and software design Dremio/Presto/Trino Snowflake (or other data warehouse systems) Tableau Docker & Kubernetes SQL MongoDB Object store (S3) & data lake concepts Git Shell & CLI tools REST APIs Pandas",
        "url": "https://www.linkedin.com/jobs/view/3956004615",
        "summary": "Data Engineer position in Sunnyvale, CA offering both FTE and Subcontractor options. Requires strong Python skills, experience with Dremio/Presto/Trino, Snowflake, Tableau, Docker/Kubernetes, SQL, MongoDB, Object store (S3) and data lake concepts, Git, Shell/CLI tools, REST APIs, and Pandas.",
        "industries": [
            "Software Development",
            "Data Engineering",
            "Technology",
            "Analytics",
            "Data Science"
        ],
        "soft_skills": [
            "Problem Solving",
            "Analytical Skills",
            "Communication",
            "Collaboration",
            "Time Management"
        ],
        "hard_skills": [
            "Python",
            "Dremio",
            "Presto",
            "Trino",
            "Snowflake",
            "Tableau",
            "Docker",
            "Kubernetes",
            "SQL",
            "MongoDB",
            "S3",
            "Git",
            "Shell",
            "CLI",
            "REST API",
            "Pandas"
        ],
        "tech_stack": [
            "Python",
            "Dremio",
            "Presto",
            "Trino",
            "Snowflake",
            "Tableau",
            "Docker",
            "Kubernetes",
            "SQL",
            "MongoDB",
            "S3",
            "Git",
            "Shell",
            "CLI",
            "REST API",
            "Pandas"
        ],
        "programming_languages": [
            "Python",
            "SQL"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 130000,
            "min": 73
        },
        "benefits": [
            "Benefits"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3895816950,
        "company": "Unreal Staffing, Inc",
        "title": "Data Warehouse Engineer",
        "created_on": 1720635331.4849632,
        "description": "Company Overview: Welcome to the forefront of data-driven decision-making! At our company, we're committed to harnessing the power of data to drive insights and innovation. Our mission is to develop scalable and efficient data warehouse solutions that empower organizations to make informed decisions based on high-quality data. Join us and be part of a dynamic team dedicated to shaping the future of data warehouse engineering. Position Overview: As a Data Warehouse Engineer, you'll play a crucial role in designing, building, and maintaining our data warehouse infrastructure and systems. Working closely with cross-functional teams of data analysts, business intelligence developers, and software engineers, you'll ensure the reliability, performance, and scalability of our data warehouse solutions. If you're passionate about data engineering and eager to drive innovation through scalable data warehouse solutions, we want you on our team. Requirements Key Responsibilities: Data Warehouse Design: Design and architect scalable and efficient data warehouse solutions, including data models, schemas, and storage structures ETL Development: Develop and maintain ETL (Extract, Transform, Load) processes to extract data from source systems, transform it to meet business requirements, and load it into the data warehouse Data Modeling: Design and implement dimensional and normalized data models to support reporting, analytics, and business intelligence applications Data Integration: Integrate data from diverse sources, including databases, applications, APIs, and flat files, ensuring data consistency and integrity Performance Optimization: Optimize data warehouse performance for query speed, concurrency, and scalability, tuning SQL queries, indexing strategies, and partitioning schemes Data Quality and Governance: Implement data quality checks and validation rules to ensure the accuracy, completeness, and consistency of data in the warehouse Monitoring and Maintenance: Monitor data warehouse performance and health, proactively identifying and addressing issues to minimize downtime and optimize resource utilization Documentation and Collaboration: Document data warehouse architecture, processes, and best practices, and collaborate with cross-functional teams to ensure alignment and transparency Qualifications: Bachelor's degree or higher in Computer Science, Engineering, or related field Strong background in data engineering, with hands-on experience in designing, building, and maintaining data warehouse solutions Proficiency in SQL and database technologies such as PostgreSQL, MySQL, SQL Server, or Oracle Experience with data warehouse platforms and technologies such as Amazon Redshift, Google BigQuery, Snowflake, or Microsoft Azure SQL Data Warehouse Familiarity with ETL tools and technologies such as Apache Airflow, Talend, Informatica, or AWS Glue Strong problem-solving abilities and analytical thinking, with a keen attention to detail and a passion for tackling complex technical challenges Excellent communication and collaboration skills, with the ability to work effectively in cross-functional teams and communicate technical concepts to non-technical stakeholders Benefits Competitive salary: The industry standard salary for Data Warehouse Engineers typically ranges from $140,000 to $210,000 per year, depending on experience and qualifications. Exceptional candidates may be eligible for higher compensation packages Comprehensive health, dental, and vision insurance plans Flexible work hours and remote work options Generous vacation and paid time off Professional development opportunities, including access to training programs, conferences, and workshops State-of-the-art technology environment with access to cutting-edge tools and resources Vibrant and inclusive company culture with team-building activities and social events Opportunities for career growth and advancement within the company Exciting projects with real-world impact across diverse industries Chance to work alongside top talent and industry experts in the field of data warehouse engineering Join Us: Ready to shape the future of data warehouse engineering? Apply now to join our team and be part of an exciting journey of innovation and discovery!",
        "url": "https://www.linkedin.com/jobs/view/3895816950",
        "summary": "This role involves designing, building, and maintaining a data warehouse infrastructure. Responsibilities include data modeling, ETL development, data integration, performance optimization, and data quality assurance. The ideal candidate has a strong background in data engineering, proficiency in SQL, and experience with data warehouse platforms like Amazon Redshift, Google BigQuery, or Snowflake. This position offers a competitive salary, comprehensive benefits, and opportunities for professional development.",
        "industries": [
            "Data Warehousing",
            "Data Engineering",
            "Software Engineering",
            "Business Intelligence",
            "Analytics"
        ],
        "soft_skills": [
            "Problem-solving",
            "Analytical Thinking",
            "Attention to Detail",
            "Communication",
            "Collaboration",
            "Teamwork"
        ],
        "hard_skills": [
            "SQL",
            "PostgreSQL",
            "MySQL",
            "SQL Server",
            "Oracle",
            "Amazon Redshift",
            "Google BigQuery",
            "Snowflake",
            "Microsoft Azure SQL Data Warehouse",
            "Apache Airflow",
            "Talend",
            "Informatica",
            "AWS Glue"
        ],
        "tech_stack": [
            "Amazon Redshift",
            "Google BigQuery",
            "Snowflake",
            "Microsoft Azure SQL Data Warehouse",
            "Apache Airflow",
            "Talend",
            "Informatica",
            "AWS Glue",
            "PostgreSQL",
            "MySQL",
            "SQL Server",
            "Oracle"
        ],
        "programming_languages": [
            "SQL"
        ],
        "experience": 0,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Computer Science",
                "Engineering"
            ]
        },
        "salary": {
            "max": 210000,
            "min": 140000
        },
        "benefits": [
            "Competitive salary",
            "Health insurance",
            "Dental insurance",
            "Vision insurance",
            "Flexible work hours",
            "Remote work options",
            "Vacation",
            "Paid time off",
            "Professional development opportunities",
            "Training programs",
            "Conferences",
            "Workshops",
            "State-of-the-art technology environment",
            "Team-building activities",
            "Social events",
            "Career growth opportunities",
            "Advancement opportunities",
            "Exciting projects",
            "Work alongside top talent",
            "Industry experts"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco Bay Area",
        "job_id": 3930694451,
        "company": "Sigmaways Inc",
        "title": "Cloud Data Engineer",
        "created_on": 1720635333.1140993,
        "description": "We seek a Cloud Data Engineer with experience in developing and analyzing complex SQL on various RDBMS (Microsoft SQL Server, Oracle) and Expert knowledge of data modeling, ETL(Informatica), and AWS Cloud Data Warehousing technologies for our direct client. Responsibilities Contributes to designing, developing, testing, implementing, and reviewing complex data warehouse and business intelligence solutions. Develops all or part of complex data warehouse applications, develops software from established requirements, builds reports and dashboards, plans and coordinates work with fellow programmers to meet delivery commitments, creates prototypes; and offers insight on the feasibility of system designs. Designs, develops, modifies, tests, and automates the data warehouse and business intelligence applications solutions. This includes design, development, architecture recommendations, quality management, metadata and repository creation, troubleshooting problems, and tuning warehouse applications. Develops transition and implementation plans. Recommends changes in development, maintenance, and standards. Advanced analytical ability and technical skill as well as the ability to provide innovative solutions to technical needs and business requirements. Contributes to the design of technology infrastructure and configurations, and recommends process improvements. Reviews complex patches and new versions of data warehouse applications. Implements complex software packages and deploys code. Key participant in cross-functional team initiatives and process improvement projects. Qualifications: Bachelor’s degree in computer science, Information Systems, or another related field Expert in developing and analyzing complex SQL on various RDBMS (Microsoft SQL Server, Oracle). 9+ years of experience in data engineering, data science, and software engineering. Expert knowledge of data modeling and understanding of different data structures and their benefits and limitations under particular use cases. Experience with Business Objects, ETL tools (Informatica). Experience using core AWS services to build and support data warehouse solutions leveraging AWS architecture best practices (S3, DMS, Glue, Lambda). Development/modeling experience with Amazon Redshift. Proficiency in developing, deploying, and debugging cloud-based applications using AWS. Ability to use a CI/CD pipeline to deploy applications on AWS (GitLab, Terraform, DBMaestro) Ability to apply a basic understanding of cloud-native applications to write code. Experience with visualization tools (Tableau). Experience creating scripts with Python. Experience working on an Agile team.",
        "url": "https://www.linkedin.com/jobs/view/3930694451",
        "summary": "We are seeking a Cloud Data Engineer with expertise in SQL, data modeling, ETL, and AWS data warehousing technologies. Responsibilities include designing, developing, and analyzing data warehouse solutions, building reports and dashboards, and recommending process improvements. 9+ years of experience in data engineering, data science, and software engineering are required. Expertise in RDBMS (Microsoft SQL Server, Oracle), ETL tools (Informatica), and AWS services (S3, DMS, Glue, Lambda) is essential. Experience with Amazon Redshift, CI/CD pipeline (GitLab, Terraform, DBMaestro), visualization tools (Tableau), and Python scripting is preferred.",
        "industries": [
            "Data Engineering",
            "Software Engineering",
            "Cloud Computing",
            "Business Intelligence"
        ],
        "soft_skills": [
            "Analytical",
            "Technical",
            "Innovative",
            "Problem-Solving",
            "Communication",
            "Teamwork"
        ],
        "hard_skills": [
            "SQL",
            "Data Modeling",
            "ETL",
            "AWS",
            "RDBMS",
            "Microsoft SQL Server",
            "Oracle",
            "Informatica",
            "S3",
            "DMS",
            "Glue",
            "Lambda",
            "Amazon Redshift",
            "GitLab",
            "Terraform",
            "DBMaestro",
            "Tableau",
            "Python",
            "Agile"
        ],
        "tech_stack": [
            "AWS",
            "S3",
            "DMS",
            "Glue",
            "Lambda",
            "Amazon Redshift",
            "GitLab",
            "Terraform",
            "DBMaestro",
            "Tableau",
            "Python"
        ],
        "programming_languages": [
            "SQL",
            "Python"
        ],
        "experience": 9,
        "education": {
            "min_degree": "Bachelor’s degree",
            "fields": [
                "Computer Science",
                "Information Systems"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3602426714,
        "company": "Arc",
        "title": "Data Engineer",
        "created_on": 1720635334.6558144,
        "description": "We are seeking a Data Engineer to join the Arc family. As a senior member of the technical team, you'll work alongside the CTO to design, architect and implement data infrastructure and pipelines to power our automated underwriting process. You'll solve hard problems and have a deep impact on the end-product. Expect to work in a fast-paced environment, with amazing engineers who believe in our mission. Requirements 4 years of professional experience Experience with working on a modern data stack with tools such as AWS S3, Apache Spark, Jupyter, Databricks, Snowflake Collaborative and enjoys working in a team Passion for engineering and learning",
        "url": "https://www.linkedin.com/jobs/view/3602426714",
        "summary": "Data Engineer to design, architect and implement data infrastructure and pipelines for automated underwriting process. Requires 4 years of experience with AWS S3, Apache Spark, Jupyter, Databricks, Snowflake.",
        "industries": [
            "Financial Technology",
            "Insurance",
            "Data Science"
        ],
        "soft_skills": [
            "Collaborative",
            "Teamwork",
            "Passion for engineering",
            "Learning"
        ],
        "hard_skills": [
            "Data Engineering",
            "Data Infrastructure",
            "Data Pipelines",
            "Automated Underwriting"
        ],
        "tech_stack": [
            "AWS S3",
            "Apache Spark",
            "Jupyter",
            "Databricks",
            "Snowflake"
        ],
        "programming_languages": [
            "Python",
            "Scala"
        ],
        "experience": 4,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Computer Science",
                "Data Science",
                "Engineering"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Menlo Park, CA",
        "job_id": 3907891397,
        "company": "SPECTRAFORCE",
        "title": "Data Engineer",
        "created_on": 1720635336.3838086,
        "description": "Data Engineer Menlo Park, CA (onsite) 12 months Job Description: Perform analysis of data extracted from testbeds / test campaigns Writing/scripting queries to collect data across the testbeds and various test equipment (e.g, traffic generators, cellular), systems (e.g, linux systems) and other types (e.g, telecom, cellular, wifi) Data extracting results, observations, insights - Integration with internal tools (database, hive tables, visualization tools) Use of open-source / external tools for visualization: eg., grafana, influxdb Data extraction from mobile devices (e.g. iphone, samsung etc…) lab equipment's (e.g. switches, wireless access point, telecom equipment) and packet capture tools like wireshark/pcap, sflow, netflow Merging data from a number of different data sources (mobile devices, lab equipments, content delivery networks, mobile devices) About Us Established in 2004, SPECTRA FORCE is one of the largest and fastest-growing diversity-owned staffing firms in the US. SPECTRA FORCE is built on the concept of “human connection,” defined by our branding tagline NEWJOBPHORIA®, which is the excitement of bringing joy and freedom to the work lifestyle so our people (and clients) can reach their highest potential. Our entire workflow cultivates NEWJOBPHORIA® with candidates and employees throughout their engagement with SPECTRA FORCE . http://www.spectraforce.com We have built our business by providing talent and project-based solutions, including Contingent, Permanent, and Statement of Work (SOW) services to over 130 clients in the US, Canada, Puerto Rico, Costa Rica, and India. Key industries that we service include Technology, Financial Services, Life Sciences, Healthcare, Telecom, Retail, Utilities and Transportation. Benefits: SPECTRA FORCE offers ACA compliant health benefits as well as dental, vision, accident, EAP (Employee Assistance Program) and hospital indemnity insurances. Additional benefits SPECTRA FORCE offers to the eligible employees include commuter benefits, 401K plan with matching and a referral bonus program. SPECTRA FORCE offers unpaid leave as well as paid sick leave when required by law. Employees who will be filling this role will also be eligible for PTO as per the client’s specific policy. Equal Opportunity Employer : SPECTRA FORCE is an equal opportunity employer and does not discriminate against any employee or applicant for employment because of race, religion, color, sex, national origin, age, sexual orientation, gender identity, genetic information, disability or veteran status, or any other category protected by applicable federal, state, or local laws. Please contact Human Resources at nahr@spectraforce.com if you require reasonable accommodation.",
        "url": "https://www.linkedin.com/jobs/view/3907891397",
        "summary": "Data Engineer needed to analyze data from testbeds and test campaigns, write queries to collect data from various sources, extract results, and integrate with internal tools. Experience with visualization tools like Grafana and InfluxDB, data extraction from mobile devices and lab equipment, packet capture tools, and merging data from various sources is required.",
        "industries": [
            "Technology",
            "Telecom",
            "Financial Services",
            "Life Sciences",
            "Healthcare",
            "Retail",
            "Utilities",
            "Transportation"
        ],
        "soft_skills": [
            "Analytical",
            "Problem-solving",
            "Communication",
            "Teamwork",
            "Collaboration"
        ],
        "hard_skills": [
            "Data Analysis",
            "Query Writing",
            "SQL",
            "Hive",
            "Visualization Tools",
            "Grafana",
            "InfluxDB",
            "Data Extraction",
            "Mobile Devices",
            "Lab Equipment",
            "Packet Capture Tools",
            "Wireshark",
            "Pcap",
            "Sflow",
            "Netflow",
            "Data Merging",
            "Content Delivery Networks"
        ],
        "tech_stack": [
            "SQL",
            "Hive",
            "Grafana",
            "InfluxDB",
            "Wireshark",
            "Pcap",
            "Sflow",
            "Netflow"
        ],
        "programming_languages": [
            "SQL"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Health Insurance",
            "Dental Insurance",
            "Vision Insurance",
            "Accident Insurance",
            "EAP (Employee Assistance Program)",
            "Hospital Indemnity Insurance",
            "Commuter Benefits",
            "401K Plan with Matching",
            "Referral Bonus Program",
            "Unpaid Leave",
            "Paid Sick Leave",
            "PTO"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Sunnyvale, CA",
        "job_id": 3943329727,
        "company": "Diverse Lynx",
        "title": "Data Engineer _ Python",
        "created_on": 1720635343.842674,
        "description": "Sunnyvale, CA (Day one on site) work from office 3 days a week Required Skills Strong Python development and software design Dremio/Presto/Trino Snowflake (or other data warehouse systems) Tableau Docker & Kubernetes SQL MongoDB Object store (S3) & data lake concepts Git Shell & CLI tools REST APIs Pandas Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.",
        "url": "https://www.linkedin.com/jobs/view/3943329727",
        "summary": "Diverse Lynx LLC seeks a Python developer with experience in data warehousing, data visualization, and cloud technologies. The role requires strong Python development, Dremio/Presto/Trino, Snowflake, Tableau, Docker/Kubernetes, SQL, MongoDB, object storage (S3), Git, shell scripting, REST APIs, and Pandas.  The position is based in Sunnyvale, CA, with a hybrid work model of 3 days in the office and 2 days remote.",
        "industries": [
            "Software Development",
            "Data Analytics",
            "Cloud Computing"
        ],
        "soft_skills": [],
        "hard_skills": [
            "Python",
            "Software Design",
            "Dremio",
            "Presto",
            "Trino",
            "Snowflake",
            "Tableau",
            "Docker",
            "Kubernetes",
            "SQL",
            "MongoDB",
            "S3",
            "Git",
            "Shell",
            "CLI",
            "REST APIs",
            "Pandas"
        ],
        "tech_stack": [
            "Python",
            "Dremio",
            "Presto",
            "Trino",
            "Snowflake",
            "Tableau",
            "Docker",
            "Kubernetes",
            "SQL",
            "MongoDB",
            "S3",
            "Git",
            "Shell",
            "REST APIs",
            "Pandas"
        ],
        "programming_languages": [
            "Python"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Fremont, CA",
        "job_id": 3829611875,
        "company": "Info Way Solutions",
        "title": "Data Engineer",
        "created_on": 1720635345.4564521,
        "description": "Hi Friends, I am sending requirement, kindly get back to me if the job description suits you. Job Title: Data Engineer Job Location: Remote Experience: 10 +Years. Contract Type: C2C Overall Skills 10+ years of overall experience in data management space and at least 5 years of working in large data sets in a data lake environment : Highly proficient in SQL, Solid understanding of Spark including performance tuning. Solid understanding of the AWS Platform Experience in Python Non Technical Skills Ability to work with Business and technical stake holders independently with minimal guidance - Must have Strong written and verbal communication and ability to articulate data with the business stake holders - Must have Info Way Solutions LLC | 46520 Fremont Blvd, Suite 614 | Fremont, CA - 94538",
        "url": "https://www.linkedin.com/jobs/view/3829611875",
        "summary": "Data Engineer with 10+ years of experience in data management and at least 5 years in data lake environments. Proficient in SQL, Spark, and Python. Strong understanding of AWS platform. Excellent communication and collaboration skills.",
        "industries": [
            "Data Management",
            "Data Engineering",
            "Cloud Computing",
            "Software Development"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Analytical Thinking",
            "Independent Work",
            "Stakeholder Management"
        ],
        "hard_skills": [
            "SQL",
            "Spark",
            "AWS",
            "Python"
        ],
        "tech_stack": [
            "SQL",
            "Spark",
            "AWS",
            "Python"
        ],
        "programming_languages": [
            "SQL",
            "Python"
        ],
        "experience": 10,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3912067766,
        "company": "Discord",
        "title": "Senior Data Engineer - Advertising",
        "created_on": 1720635347.1399097,
        "description": "This position is US based only. Discord is about giving people the power to create space to find belonging in their lives. We want to make it easier for you to talk regularly with the people you care about. We want you to build genuine relationships with your friends and communities close to home or around the world. Original, reliable, playful, and relatable. These are the values that connect our users and our employees at Discord. Discord, an inclusive community for all, invites an experienced data engineer to join our Data team, focusing on our marketing solutions and advertising products. As a Senior Data Engineer, your critical role will involve building and maintaining data pipelines, datasets, and analytical tools. These resources will guide insights and decision-making to revamp our advertising products. We are looking for someone with previous experience in a similar domain who can deliver valuable insights and expertise to optimize our advertising products and strategies. If harnessing data, creating impact, and teamwork ignites a spark in you, we encourage you to make a move! What You'll Be Doing Create and maintain data pipelines and foundational datasets to support analytics, modeling, experimentation, and product/business needs Design and build database architectures with massive and complex data, balancing ergonomic benefits with computational load and cost Collaborate closely with data science, machine learning, and engineering teams to improve the coverage, accuracy, and reliability of instrumentation Develop audits for data quality at scale, implementing alerting and anomaly detection as necessary Create scalable dashboards and reports to support business objectives and enable data-driven decision making Partner with data scientists, engineers, and product teams to accomplish all of the above! You Will Thrive In This Role If 4+ years of experience building data pipelines in production environments with high volumes of consumer data in marketing solutions and advertising 4+ years of experience in designing, developing, and maintaining robust data models from structured and unstructured sources 4+ years of experience writing accurate and effective code in SQL and Python Experience implementing and monitoring audits for data quality with massive data sets (e.g. billions of rows) Experience proactively identifying opportunities to improve ETL & dashboard performance and cost Experience leveraging your excellent communication and stakeholder management skills to thrive in ambiguous environments where problems are not well-defined and evolve quickly A desire to work with amazing, passionate people who care deeply about solving challenging problems to improve Discord Last but not least - a collaborative attitude and a healthy dose of natural curiosity! Bonus Points Passion for Discord or gaming in general Experience owning and proactively improving the data models for a functional area leveraging deep knowledge of performant scalable patterns Experience collaborating directly with data science, machine learning, and product engineering teams Experience with modern data storage and processing technologies (i.e. BigQuery SQL, Airflow, and DBT or similar) Experience with data visualization and dashboarding technologies (i.e. Looker, Tableau, or similar) Experience with designing data architecture to power a variety of use cases, including experimentation Experience with external data ingestion processes and technologies The US base salary range for this full-time position is $183,000- $201,500 + equity + benefits. Our salary ranges are determined by role and level. Within the range, individual pay is determined by additional factors, including job-related skills, experience, and relevant education or training. Please note that the compensation details listed in US role postings reflect the base salary only, and do not include equity, or benefits. Please see our Applicant and Candidate Privacy Policy for details regarding Discord's collection and use of personal information relating to the application and recruitment process by clicking here. Discord is an equal opportunity employer that believes diversity inspires creativity and innovation. We are committed to fostering an inclusive environment where employees can do their best work free of harassment or discrimination, regardless of race, ethnicity, religion, gender identity or expression, sexual orientation, age, disability, military status, or any other protected characteristics or identities. We encourage all candidates to apply for roles where they may make the most impact. About Us Discord is a voice, video and text app that helps friends come together to hang out, have fun, and play games. With over 150 million monthly users, Discord has grown to become one of the most popular communications services in the world. Discord offers a premium subscription called Nitro that gives users special perks like higher quality streams and fun customizations. We’re working toward an inclusive world where no one feels like an outsider, where genuine human connection is a click, text chat, or voice call away. A place where everyone can find belonging. Challenging? Heck yes. Rewarding? Double heck yes. It’s a mission that gives us the chance to positively impact millions of people all over the world. So if this strikes a chord with you, come build belonging with us!",
        "url": "https://www.linkedin.com/jobs/view/3912067766",
        "summary": "Discord seeks a Senior Data Engineer to build and maintain data pipelines, datasets, and analytical tools for their marketing solutions and advertising products.  This role involves working closely with data science, machine learning, and engineering teams to improve data quality and reliability, develop data audits, and create scalable dashboards and reports. Ideal candidates have 4+ years of experience in data pipeline development, data modeling, SQL and Python coding, data quality auditing, and performance optimization, along with excellent communication and stakeholder management skills.",
        "industries": [
            "Technology",
            "Software",
            "Marketing",
            "Advertising",
            "Data",
            "Analytics"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Stakeholder Management",
            "Problem Solving",
            "Curiosity"
        ],
        "hard_skills": [
            "Data Pipelines",
            "Data Modeling",
            "SQL",
            "Python",
            "Data Quality Auditing",
            "ETL",
            "Dashboarding",
            "Data Visualization",
            "BigQuery SQL",
            "Airflow",
            "DBT",
            "Looker",
            "Tableau"
        ],
        "tech_stack": [
            "BigQuery SQL",
            "Airflow",
            "DBT",
            "Looker",
            "Tableau"
        ],
        "programming_languages": [
            "SQL",
            "Python"
        ],
        "experience": 4,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 201500,
            "min": 183000
        },
        "benefits": [
            "Equity"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Diego, CA",
        "job_id": 3857815031,
        "company": "The Shipyard",
        "title": "Senior Data Engineer",
        "created_on": 1720635348.6427796,
        "description": "The Shipyard is a leading, independent agency that builds performance-driven brands audiences can't help but love. By applying modern mindsets to established models, we fuel brand and marketing decisions that are more courageous and more validated. Our ability to align bold creativity with individual consumer motivations lets us do more than hope for brand love - we methodically engineer it throughout the consumer journey. That's how we activate the synergistic power of brand building and performance marketing. Engineering Brand Love through the courageous ambitions of our people and our clients. The agency has significant momentum and is on a roll with new business. Our team continues to expand across four cities (Columbus, Newport Beach, Sacramento, and San Diego) and we're still growing. Come join our team! Summary We are looking for a smart, experienced, and motivated Senior Data Engineer to join our San Diego team. The Senior Data Engineer will serve as a coach for other Data Developers, the role requires daily hands-on development execution and technology roadmap leadership. The Senior Data Engineer is responsible for leading end-to-end data development, from building data architectures for robust data sets to collecting & modeling data. You will work with internal departments to define their analytic requirements and set up their corresponding data environments. You will be a valued member with the opportunity to shape the way that we collect and share data, influencing processes and efficiency across the agency and its clientele. This is a hybrid work-from-home/office position (in-office attendance is required on Tuesdays & Thursdays) and the candidate must live near San Diego, CA . At The Shipyard, we know that it's your unique talents, backgrounds, and perspectives that make you who you are, just like our team, who come from various career paths and experiences. We believe we can't be truly diverse without bringing your most authentic self to the agency. Studies show that people from marginalized communities may not always apply for positions if they don't meet every single requirement in a job posting. At The Shipyard, we encourage you to apply even if you don't meet all the requirements. We believe that your passion for what we stand for — our values and purpose — is just as important as meeting every checkbox. What You'll Do Lead data needs for clients and internal teams in an agile work environment Collect data requirements from internal teams to assist in the design of a scalable infrastructure to drive decision-making using technology and BI tools Manage the retrieval, compiling, and formatting of large volumes of data Set up data environments, in partnership with the Data Visualization team, within business intelligence platforms, ultimately creating advertising reports Learn new technologies as they evolve, understand their impact to systems, and lead the team in proposed tech stack evolutions Leadership in implementing technological advancements as well as educating internal and external partners Collaborate with internal teams and users to understand new opportunities for support Documentation of data environments and infrastructures developed What You'll Bring 5-6 years equivalent work experience in a Data Developer or similar Data & Analytics Role Experience in data collection, transformation, development, and/or reporting with optimized outputs for BI tools Experience developing and optimizing data ingestion processes with tools such as Fivetran, Stitch, and even writing custom API scripts. Experience designing, building, and maintaining robust data pipelines (ETL/ELT) using technologies like Postgres, dbt, and Airflow. Experience with AWS services, Kubernetes, and Docker. Experience with data modeling / semantic modeling Experience with data warehousing architecture (Postgres, AWS Redshift) Experience in applying data processes and deploying BI updates at scale A testing mindset - Knowledge and experience of CI/CD pipelines and test environments Proficiency in SQL, Python, and/or Scala/Java Experience contributing to and influencing tech stack evolutions and infrastructure roadmap Ability to write code with clean documentation Strong entrepreneurial drive Strong verbal and written communication skills and a collaborative, problem-solving mindset Self-starting with initiative and passionate about learning the world of data strategy, architecture, processing, and development Experience balancing multiple deliverables across multiple clients and data sets Experience with /awareness of cloud-based visualization tools such as Tableau, Looker, or Periscope Marketing & advertising knowledge (preferred, not required) What You'll Get The overall target range of base compensation for this role is $110,000 - $137,000. Compensation offered will be determined by additional factors such as location and experience 40 hours of paid sick time (annually) Open PTO Flexible work hours and remote work Paid holidays + holiday closure between Christmas Eve and New Year's Day Company-paid medical, dental, and vision insurance Life insurance and disability benefits 401k program with employer matching 6 weeks paid parental leave Employee bonus referrals Dog-friendly offices Company-provided snacks and beverages (yes, beer/wine included) ... and lots more! Requirements Must be authorized to work in the U.S. without the need for visa sponsorship.",
        "url": "https://www.linkedin.com/jobs/view/3857815031",
        "summary": "The Shipyard is seeking a Senior Data Engineer to join their San Diego team. This role involves leading end-to-end data development, from building data architectures to collecting and modeling data, and coaching other data developers. The candidate will work with internal departments to define their analytic requirements and set up data environments. This is a hybrid work-from-home/office position requiring in-office attendance on Tuesdays and Thursdays.",
        "industries": [
            "Marketing",
            "Advertising",
            "Data Analytics",
            "Technology"
        ],
        "soft_skills": [
            "Leadership",
            "Communication",
            "Collaboration",
            "Problem-solving",
            "Entrepreneurial",
            "Self-starter",
            "Passionate",
            "Teamwork"
        ],
        "hard_skills": [
            "Data Collection",
            "Data Transformation",
            "Data Development",
            "Data Reporting",
            "BI Tools",
            "Data Ingestion",
            "Fivetran",
            "Stitch",
            "API Scripting",
            "Data Pipelines",
            "ETL",
            "ELT",
            "Postgres",
            "dbt",
            "Airflow",
            "AWS",
            "Kubernetes",
            "Docker",
            "Data Modeling",
            "Semantic Modeling",
            "Data Warehousing",
            "Postgres",
            "AWS Redshift",
            "CI/CD Pipelines",
            "Test Environments",
            "SQL",
            "Python",
            "Scala",
            "Java",
            "Tableau",
            "Looker",
            "Periscope",
            "Marketing",
            "Advertising"
        ],
        "tech_stack": [
            "Fivetran",
            "Stitch",
            "Postgres",
            "dbt",
            "Airflow",
            "AWS",
            "Kubernetes",
            "Docker",
            "Tableau",
            "Looker",
            "Periscope"
        ],
        "programming_languages": [
            "SQL",
            "Python",
            "Scala",
            "Java"
        ],
        "experience": 5,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 137000,
            "min": 110000
        },
        "benefits": [
            "Paid Sick Time",
            "Open PTO",
            "Flexible Work Hours",
            "Remote Work",
            "Paid Holidays",
            "Medical Insurance",
            "Dental Insurance",
            "Vision Insurance",
            "Life Insurance",
            "Disability Benefits",
            "401k",
            "Employer Matching",
            "Parental Leave",
            "Employee Referral Bonuses",
            "Dog-Friendly Offices",
            "Snacks",
            "Beverages"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3960419142,
        "company": "Finix",
        "title": "Senior Data Engineer",
        "created_on": 1720635350.2594419,
        "description": "About Us Move money. Make money. Finix processes billions of dollars every year for leading SaaS, marketplace, and e-commerce platforms. With one developer-friendly API, Finix helps companies accept payments, manage payouts, and onboard merchants—everything you need to enable payment processing, and grow revenue. Finix has raised over $100M from American Express Ventures, Bain Capital Ventures, Homebrew, Inspired Capital, Lightspeed Venture Partners, Sequoia Capital, Visa, and others. About The Role Finix’s reporting cluster is the source of truth for operational metrics, customer-facing reports, billing, actionable insights, and a host of business-critical KPIs. As a Data Engineer in the Data Engineering team, you will own the tooling, frameworks, data pipelines, integrations, databases, schemas and the data integrity of this cluster. You are critical to both Finix, and our customers, in supplying the data that underpins key operations decisions, uncovers hidden issues, and powers insight through reports, analytics, and dashboards owned and managed by the Data Engineering team. Our operations teams will depend on you to find, source, prepare and supply the right data allowing them to run Finix. Our product team will depend on you to serve the right data quickly and reliably to our customers so they can make timely business decisions. Data Analysts will rely on you to source and channel data where it needs to be and how it needs to be organized to power daily operations and decisions. You will Enable our operations team to make data-driven decisions based on timely, consistent, complete, and correct data, reports, and analytics Work closely with engineering team members to build reporting and analytics infrastructure, solving our and our customer’s analytics needs Delight our customers by providing leading reporting and analytics capabilities supporting their operations and accounting teams Be the subject matter expert in all things data pipelines, data lakes and warehouse, tooling & frameworks, integration and sourcing of data Design, build and maintain all code, pipelines, schemas, databases and frameworks that power Finix’s data analytics and reporting capabilities You are Detail-oriented, with a healthy dose of mistrust, driving you to investigate even the slightest data discrepancy Thrive in a fast-paced environment around common goals and priorities but able to self-direct and problem solve with others to make progress Fast learner of new languages, tools & frameworks. You are willing to go wherever necessary to accomplish your goals even if you haven’t been there before. Curious about new approaches and pushing the boundaries of what exists. You have 6+ years of experience in a Data Engineering, ETL, Data Science or related role Expert knowledge of SQL, data modeling, Python and frameworks such as Spark, Storm, Airflow, Glue etc. Java would also be great B.S in computer science, mathematics, statistics, physics, or a related data field Experience with BI tools such as Looker, Sisense, Superset, Tableau Finix is an equal opportunity employer and values diversity. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, disability status, or any other protected class. Role: Data Engineer IV Level: IC4 Location: San Francisco, CA Base Salary Range: $170,000/yr to $210,000/yr + equity + benefits Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries at our headquarters in San Francisco, California. Individual pay is determined by work location, job related skills, experience, and relevant education or training.",
        "url": "https://www.linkedin.com/jobs/view/3960419142",
        "summary": "Finix is seeking a Data Engineer IV to join their Data Engineering team. The ideal candidate will have 6+ years of experience in data engineering, ETL, or related roles, and expert knowledge of SQL, data modeling, Python, and frameworks like Spark, Storm, Airflow, and Glue. The role involves owning and maintaining the reporting cluster, including data pipelines, databases, and schemas. They will also work closely with engineering and product teams to ensure data-driven decisions and customer satisfaction.",
        "industries": [
            "Financial Technology",
            "Software",
            "SaaS",
            "Payments",
            "E-commerce"
        ],
        "soft_skills": [
            "Detail-oriented",
            "Trustworthy",
            "Self-directed",
            "Problem-solver",
            "Fast learner",
            "Curious",
            "Communicative",
            "Collaborative"
        ],
        "hard_skills": [
            "SQL",
            "Data Modeling",
            "Python",
            "Spark",
            "Storm",
            "Airflow",
            "Glue",
            "Java",
            "BI tools"
        ],
        "tech_stack": [
            "SQL",
            "Python",
            "Spark",
            "Storm",
            "Airflow",
            "Glue",
            "Java",
            "Looker",
            "Sisense",
            "Superset",
            "Tableau"
        ],
        "programming_languages": [
            "SQL",
            "Python",
            "Java"
        ],
        "experience": 6,
        "education": {
            "min_degree": "B.S",
            "fields": [
                "Computer Science",
                "Mathematics",
                "Statistics",
                "Physics",
                "Data"
            ]
        },
        "salary": {
            "max": 210000,
            "min": 170000
        },
        "benefits": [
            "Equity"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Irvine, CA",
        "job_id": 3888450280,
        "company": "IT Minds LLC",
        "title": "Data Engineer",
        "created_on": 1720635353.6616352,
        "description": "Position: Data Engineer Location: Irvine/Los Angeles, CA Duration: Long Term Contract Qualifications Experience building data pipelines in a cloud-based environment Prior experience working on Data Science projects with a focus on Data Analysis and Data Quality 3+ years of experience in development with a focus on backend programming, ETL, and building data pipelines 2+ years of experience engineering complex, high-volume data pipelines using SQL and Python 2+ years of experience in Python (Java is fine as well if we can't find resources with Python experience) Prior experience with AWS, Jupyter/SageMaker, PostgreSQL (or MS SQL Server), Amazon S3 Mandatory skills: Python(or Java), Postgre (or MS SQL), AWS, Data Analysis Desired Skills: Jupyter, SageMaker, ETL, Amazon S3 Thanks & Regards Krishna | IT Minds LLC | Phone: (949)534-3939 Ext 406 Direct: 949-200-7533 | Email: krishna@itminds.net | : 9070 Irvine Centre DR, Suite 220 | Irvine, CA 92618 | 44075 Pipeline Plaza, Suite 305 | Ashburn, VA 20147| 102, Manjeera Trinity Corporate, Kukatpally, Hyderabad 500072| www.itminds.net",
        "url": "https://www.linkedin.com/jobs/view/3888450280",
        "summary": "This is a long-term contract position for a Data Engineer with a focus on building data pipelines in a cloud-based environment. The ideal candidate will have 3+ years of experience in backend programming, ETL, and data pipeline development, as well as experience working on Data Science projects with a focus on Data Analysis and Data Quality. Experience with AWS, Jupyter/SageMaker, PostgreSQL or MS SQL Server, and Amazon S3 is required.",
        "industries": [
            "Data Engineering",
            "Cloud Computing",
            "Data Science",
            "Software Development"
        ],
        "soft_skills": [
            "Data Analysis",
            "Data Quality",
            "Problem-solving",
            "Communication",
            "Collaboration"
        ],
        "hard_skills": [
            "Python",
            "Java",
            "PostgreSQL",
            "MS SQL Server",
            "AWS",
            "ETL",
            "Data Pipelines",
            "SQL",
            "Jupyter",
            "SageMaker",
            "Amazon S3"
        ],
        "tech_stack": [
            "AWS",
            "Jupyter/SageMaker",
            "PostgreSQL",
            "MS SQL Server",
            "Amazon S3"
        ],
        "programming_languages": [
            "Python",
            "Java"
        ],
        "experience": 3,
        "education": {
            "min_degree": null,
            "fields": [
                "Computer Science",
                "Data Science",
                "Engineering"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Palo Alto, CA",
        "job_id": 3968584524,
        "company": "Collab,ai",
        "title": "Senior Data Engineer",
        "created_on": 1720635355.3410306,
        "description": "Senior Data Enginee r Collab.Ai is changing the way teams work by building collaborative tools for projects, wikis, and note s Responsibilities As a key player in our team, you will be responsible for designing, building, and managing our data infrastructure, supporting our data-driven decision-making capability • Design, develop and optimize data ingestion pipelines to handle real-time and batch data streams using a variety of sources • Utilize your extensive knowledge of Python and AWS to engineer solutions for the transformation, consolidation, and storage of large datasets • Collaborate with data scientists and other stakeholders to understand data needs and translate them into data systems and pipelines • Enhance our data ecosystem by leveraging industry best practices for testing, deployment, and runtime environments • Drive continuous improvements to data reliability, efficiency, and quality Experience • We are seeking an experienced Data Engineer with a minimum of 4 years of experience in building data ingestion pipelines for large datasets • You'll need to be proficient in Python, AWS, and common frameworks used for data ingestion, transformation, and consolidation • The ideal candidate will be passionate about data, a strong team player, and have a continuous learning mentality • Hands-on experience building and optimizing large scale data pipelines, architectures, and data sets • Knowledge of data warehousing concepts, including data modeling, data cleaning, and ETL processes • Strong understanding of database design and data management principles • Experience with AWS cloud services • Strong problem-solving skills, and the ability to analyze data and design solutions to complex data issues • Excellent communication and teamwork skills, and a passion for data Collab.ai offer a super competitive cash compensation package including base salary + bonus, equity in the company after one year, free healthcare, gym membership, unlimited vacation and sick days",
        "url": "https://www.linkedin.com/jobs/view/3968584524",
        "summary": "Collab.Ai is seeking a Senior Data Engineer with 4+ years of experience to design, build, and manage their data infrastructure. The role involves designing data ingestion pipelines, utilizing Python and AWS, collaborating with data scientists, and enhancing data reliability and quality.  They offer a competitive compensation package including base salary, bonus, equity, healthcare, gym membership, unlimited vacation and sick days.",
        "industries": [
            "Software",
            "Technology",
            "Data Engineering",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Communication",
            "Teamwork",
            "Problem Solving",
            "Analytical",
            "Passion for Data",
            "Continuous Learning"
        ],
        "hard_skills": [
            "Python",
            "AWS",
            "Data Ingestion",
            "Data Transformation",
            "Data Consolidation",
            "Data Warehousing",
            "Data Modeling",
            "Data Cleaning",
            "ETL",
            "Database Design",
            "Data Management",
            "Cloud Services"
        ],
        "tech_stack": [
            "Python",
            "AWS"
        ],
        "programming_languages": [
            "Python"
        ],
        "experience": 4,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Base Salary",
            "Bonus",
            "Equity",
            "Healthcare",
            "Gym Membership",
            "Unlimited Vacation",
            "Sick Days"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Jose, CA",
        "job_id": 3968178397,
        "company": "Amtex Systems Inc.",
        "title": "Data Engineer",
        "created_on": 1720635356.8714685,
        "description": "Position: Data Engineer with SAP HANA Native Location: San Jose, CA- 2-3 days Onsite Duration: 12+ Months Key Skill: SQL Expertise, SQL Query writing, SQL Structures Hana Modeling - Calculation Views ECC / S4H Knowledge Business objects Designer / Data Services as a ETL tool Not worried about BI skills, Even if the candidate has good exp with SQL and slight exp in SAP ECC, S4H the HM is willing to consider them Not looking for any cloud exp. The Challenge: The SAP HANA Developer performs a wide range of job duties utilizing technical know-how and develop an analytics product that will generate insights into customer journey. Maintains a very hands-on focus for technology matters combined with an affinity for solving complex technical issues, enabling tools for self-service data analyst community and delivering projects on time with high quality. Who you are: • You have the ability to work independently, prioritize individual work load, troubleshoot and remedy problems under pressing deadlines and with minimal need for escalation • You exercise good judgment and a pragmatic approach to delivering solutions that optimizes architecture activities across company needs, business constraints and varying technological realities • Document and demonstrate solutions by developing design documentation, code comments, presentations and code execution. Qualifications: • 7+ years’ experience with strong SAP S4/HANA, HANA Modeling, SQL Skills, and big data with enterprise data warehousing experience • Strong SAP HANA data modeling and performance optimization skills • Ability to design, develop, and migrate data analytics solutions by sourcing data from SAP ECC, S/4 HANA, and SAP Data archival systems. • Experience with S4/HANA migration initiatives and exposure to SAP OCM/SOM modules is a plus. • Experience working with cross-functional teams to manage the impact of SAP ECC to S/4HANA migration on analytics reporting for sales, bookings, finance, and subscription business areas. • Strong experience on SQL and PLSQL skills. • Ability to design, develop, and test ETL solutions, including modeling data warehouse metadata and automating data loading processes. • Experience with big data technologies like Azure and Data Bricks is a plus. • Experience monitoring, troubleshooting and tuning services and applications and operational expertise such as good troubleshooting skills, understanding of systems capacity, bottlenecks, and basics of memory, CPU, OS, storage, and networks.",
        "url": "https://www.linkedin.com/jobs/view/3968178397",
        "summary": "This is a 12+ month contract position for a Data Engineer with strong SAP S4/HANA, HANA Modeling, and SQL skills. The role requires experience designing, developing, and migrating data analytics solutions, particularly those involving SAP ECC, S/4 HANA, and SAP Data archival systems. The candidate will work on projects related to customer journey insights, data warehouse metadata, and data loading process automation. Experience with S4/HANA migration initiatives, SAP OCM/SOM modules, Azure, and Data Bricks is a plus. ",
        "industries": [
            "Information Technology",
            "Software Development",
            "Data Analytics"
        ],
        "soft_skills": [
            "Independent work",
            "Prioritization",
            "Troubleshooting",
            "Problem-solving",
            "Deadline-oriented",
            "Judgment",
            "Pragmatic approach",
            "Solution delivery",
            "Documentation",
            "Presentation",
            "Teamwork",
            "Communication"
        ],
        "hard_skills": [
            "SAP S4/HANA",
            "HANA Modeling",
            "SQL",
            "SAP ECC",
            "SAP Data Archival",
            "ETL",
            "Data Warehousing",
            "Performance Optimization",
            "Data Analytics",
            "Data Migration",
            "OCM/SOM",
            "PL/SQL",
            "Azure",
            "Data Bricks",
            "Troubleshooting",
            "System Capacity",
            "Bottlenecks",
            "Memory Management",
            "CPU",
            "OS",
            "Storage",
            "Networking"
        ],
        "tech_stack": [
            "SAP S4/HANA",
            "SAP ECC",
            "SAP Data Archival",
            "Azure",
            "Data Bricks"
        ],
        "programming_languages": [
            "SQL",
            "PL/SQL"
        ],
        "experience": 7,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3894436196,
        "company": "Legalist",
        "title": "Data Engineer",
        "created_on": 1720635358.6616862,
        "description": "Intro description: Legalist is an institutional alternative asset management firm. Founded in 2016 and incubated at Y Combinator, the firm uses data-driven technology to invest in credit assets at scale. We are always looking for talented people to join our team. As a highly collaborative organization, our data engineers work cross-functionally with software engineering, data science, and product management to optimize growth and strategy of our data pipeline. In this position, you will be joining the data engineering team in an effort to take our data pipeline to the next level. Where you come in: Design and develop scalable data pipelines to collect, process, and analyze large volumes of data efficiently. Collaborate with cross-functional teams including data scientists, software engineers, and product managers to understand data requirements and deliver solutions that meet business needs. Develop ELT processes to transform raw data into actionable insights, leveraging tools and frameworks such as Airbyte, BigQuery, Dagster, DBT or similar technologies. Participate in agile development processes, including sprint planning, daily stand-ups, and retrospective meetings, to deliver iterative improvements and drive continuous innovation. Apply best practices in data modeling and schema design to ensure data integrity, consistency, and efficiency. Continuously monitor and optimize data pipelines and systems for performance, availability, scalability, and cost-effectiveness. What you'll be bringing to the team: Bachelor's degree (BA or BS) or equivalent. A minimum of 2 years of work experience in data engineering or similar role. Advanced SQL knowledge and experience working with a variety of databases (SQL, NoSQL, Graph, Multi-model). A minimum of 2 years professional experience with ETL//ELT, data modeling and Python. Familiarity with cloud environments like GCP, AWS, as well as cloud solutions like Kubernetes, Docker, BigQuery, etc. You have a pragmatic, data-driven mindset and are not dogmatic or overly idealistic about technology choices and trade-offs. You have an aptitude for learning new things quickly and have the confidence and humility to ask clarifying questions. Even better if you have, but not necessary: Experience with one or more of the following: data processing automation, data quality, data warehousing, data governance, business intelligence, data visualization. Experience working with TB scale data.",
        "url": "https://www.linkedin.com/jobs/view/3894436196",
        "summary": "Legalist, an institutional alternative asset management firm, is seeking a Data Engineer to design and develop scalable data pipelines for large-scale data processing and analysis. This role involves collaborating with cross-functional teams, implementing ELT processes using tools like Airbyte, BigQuery, and Dagster, and optimizing data pipelines for performance and cost-effectiveness. The ideal candidate will have at least 2 years of data engineering experience, strong SQL skills, familiarity with cloud environments like GCP and AWS, and experience with ETL/ELT processes.",
        "industries": [
            "Financial Services",
            "Asset Management",
            "Technology",
            "Data Science"
        ],
        "soft_skills": [
            "Collaboration",
            "Communication",
            "Problem Solving",
            "Data-Driven",
            "Pragmatic",
            "Learning",
            "Teamwork"
        ],
        "hard_skills": [
            "Data Engineering",
            "SQL",
            "NoSQL",
            "Graph Databases",
            "Multi-Model Databases",
            "ETL",
            "ELT",
            "Data Modeling",
            "Python",
            "Cloud Computing",
            "GCP",
            "AWS",
            "Kubernetes",
            "Docker",
            "BigQuery",
            "Data Processing Automation",
            "Data Quality",
            "Data Warehousing",
            "Data Governance",
            "Business Intelligence",
            "Data Visualization"
        ],
        "tech_stack": [
            "Airbyte",
            "BigQuery",
            "Dagster",
            "DBT",
            "GCP",
            "AWS",
            "Kubernetes",
            "Docker",
            "SQL",
            "Python"
        ],
        "programming_languages": [
            "SQL",
            "Python"
        ],
        "experience": 2,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Computer Science",
                "Data Science",
                "Engineering"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Sunnyvale, CA",
        "job_id": 3912089041,
        "company": "Walmart",
        "title": "(USA) Data Engineer III",
        "created_on": 1720635368.6860366,
        "description": "Position Summary... What you'll do... At Walmart, our goal is to assist individuals in saving money, enabling them to enhance their lives. This objective is the basis for all our choices and motivates us to revolutionize the retail industry. Achieving this requires the most exceptional talent - individuals who are innovative, curious, and motivated to provide extraordinary experiences for our customers. Are you filled with endless enthusiasm and dedication towards engineering data to tackle ever-changing challenges that will shape the future of retail? Walmart's extensive operations come with vast and complex sets of data, presenting unparalleled opportunities for analysis and insights. About The Data And Customer Analytics (DCA) Organization Our organization focuses on managing and delivering world-class data assets, including creating and maintaining data standards, driving policy compliance, creating partnerships, and developing pipelines and self-service tools. We empower our business to leverage data to fuel growth, driving revenue in our core and building new business model opportunities. About Team Everyone has data, but the sheer volume of data at Walmart can be limitless. In the Data Engineering team, we help Walmart manage this data by building pipelines and data lakes to prepare big data for analysis and unlocking actionable insights in real-time. We also use cross-departmental data and machine learning to build a holistic view of true profitability, saving millions of dollars across item categories and geographies while assisting our leadership in making better decisions faster. What You'll Do Your primary responsibility will be to oversee and manage data pipeline processes that utilize cutting-edge data engineering techniques to generate critical datasets for Walmart. You will be responsible for promptly addressing any issues or failures in data streaming and batch applications, ensuring adherence to SLAs. Your role will involve acting as a gatekeeper to maintain the integrity and reliability of production data systems. You will develop automation tools to monitor and alert platform components and data pipelines. You will contribute to the ongoing design, implementation, and maintenance of systems and tools within our data platform. You will assess and provide recommendations for optimizing resource usage in data pipelines built on a big data stack. You will collaborate closely with engineering teams to contribute to their roadmap and assist in capacity planning for our big data stack. You will implement data quality checks, data validation, and data cleaning processes to ensure the accuracy and high quality of ingested and processed data. You will participate in a 12/7 on-call rotation and act as the initial responder for any issues with the data pipelines. You will lead root cause analysis (RCA) and incident management efforts. You will support and uphold company policies, procedures, mission, values, and ethical standards. What You'll Bring At least 4+ years of experience development of big data technologies/data pipelines Experience with big data technologies like Hadoop, Apache Spark (Scala preferred), Apache Hive, or similar frameworks on the cloud (GCP preferred, AWS, Azure etc.) to build batch data pipelines with strong focus on optimization, SLA adherence and fault tolerance. Experience in programming languages like Java or Python Experience in managing Spark Streaming, Kafka, Apache Storm, Flume etc. Experience in writing SQL to analyze, optimize, profile data preferably in BigQuery or SPARK SQL Strong troubleshooting skills to analyze Spark Event logs and provide recommendations on Spark container optimization, Cluster configurations etc. Ability to move at a rapid pace with quality and start delivering with minimal ramp up time will be crucial to succeed in this initiative. Effective communication and collaboration skills are necessary for working in a team environment and coordinating efforts between different stakeholders involved in the project. Nice To Have From You Good understanding of REST APIs - working knowledge on Apache Druid, Redis, Elastic search, GraphQL or similar technologies. Understanding of API contracts, building telemetry, stress testing etc. Exposure in UI development using React. Exposure in developing reports/dashboards using Looker/Tableau About Walmart Global Tech Imagine working in an environment where one line of code can make life easier for hundreds of millions of people. That's what we do at Walmart Global Tech. We're a team of software engineers, data scientists, cybersecurity expert's and service professionals within the world's leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered. We train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail. Flexible, Hybrid Work We use a hybrid way of working that is primarily in office coupled with virtual when not onsite. Our campuses serve as a hub to enhance collaboration, bring us together for purpose and deliver on business needs. This approach helps us make quicker decisions, remove location barriers across our global team and be more flexible in our personal lives. Benefits Benefits: Beyond our great compensation package, you can receive incentive awards for your performance. Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and much more. Equal Opportunity Employer Walmart, Inc. is an Equal Opportunity Employer - By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing diversity- unique styles, experiences, identities, ideas and opinions - while being inclusive of all people. The above information has been designed to indicate the general nature and level of work performed in the role. It is not designed to contain or be interpreted as a comprehensive inventory of all responsibilities and qualifications required of employees assigned to this job. The full Job Description can be made available as part of the hiring process. At Walmart, we offer competitive pay as well as performance-based bonus awards and other great benefits for a happier mind, body, and wallet. Health benefits include medical, vision and dental coverage. Financial benefits include 401(k), stock purchase and company-paid life insurance. Paid time off benefits include PTO (including sick leave), parental leave, family care leave, bereavement, jury duty, and voting. Other benefits include short-term and long-term disability, company discounts, Military Leave Pay, adoption and surrogacy expense reimbursement, and more. ‎ ‎ ‎ You will also receive PTO and/or PPTO that can be used for vacation, sick leave, holidays, or other purposes. The amount you receive depends on your job classification and length of employment. It will meet or exceed the requirements of paid sick leave laws, where applicable. ‎ For information about PTO, see https://one.walmart.com/notices . ‎ ‎ Live Better U is a Walmart-paid education benefit program for full-time and part-time associates in Walmart and Sam's Club facilities. Programs range from high school completion to bachelor's degrees, including English Language Learning and short-form certificates. Tuition, books, and fees are completely paid for by Walmart. ‎ Eligibility requirements apply to some benefits and may depend on your job classification and length of employment. Benefits are subject to change and may be subject to a specific plan or program terms. ‎ For Information About Benefits And Eligibility, See One.Walmart . ‎ The annual salary range for this position is $117,000.00-$234,000.00 ‎ Additional Compensation Includes Annual Or Quarterly Performance Bonuses. ‎ Additional Compensation For Certain Positions May Also Include ‎ ‎ Stock ‎ ‎ Minimum Qualifications... Outlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications. Option 1: Bachelor's degree in Computer Science and 2 years' experience in software engineering or related field. Option 2: 4 years' experience in software engineering or related field. Option 3: Master's degree in Computer Science. Preferred Qualifications... Outlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications. Data engineering, database engineering, business intelligence, or business analytics, Master's degree in Computer Science or related field and 2 years' experience in software engineering or related field, We value candidates with a background in creating inclusive digital experiences, demonstrating knowledge in implementing Web Content Accessibility Guidelines (WCAG) 2.2 AA standards, assistive technologies, and integrating digital accessibility seamlessly. The ideal candidate would have knowledge of accessibility best practices and join us as we continue to create accessible products and services following Walmart's accessibility standards and guidelines for supporting an inclusive culture. Primary Location... 640 W California Avenue, Sunnyvale, CA 94086-4828, United States of America",
        "url": "https://www.linkedin.com/jobs/view/3912089041",
        "summary": "Walmart is seeking a Data Engineer to oversee and manage data pipeline processes utilizing cutting-edge techniques to generate critical datasets. Responsibilities include addressing data streaming and batch application issues, maintaining data system integrity, developing automation tools, optimizing resource usage, collaborating with engineering teams, implementing data quality checks, and participating in on-call rotations. The ideal candidate will have 4+ years of experience in big data technologies like Hadoop, Apache Spark, Hive, and cloud platforms (GCP preferred). They should also be proficient in programming languages like Java or Python, managing Spark Streaming, Kafka, and writing SQL for data analysis and optimization.",
        "industries": [
            "Retail",
            "Data Analytics",
            "Software Engineering",
            "Technology",
            "E-commerce"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Troubleshooting",
            "Problem Solving",
            "Analytical",
            "Decision Making",
            "Time Management",
            "Organization",
            "Detail Oriented",
            "Teamwork",
            "Leadership",
            "Critical Thinking",
            "Adaptability",
            "Innovation"
        ],
        "hard_skills": [
            "Hadoop",
            "Apache Spark",
            "Apache Hive",
            "GCP",
            "AWS",
            "Azure",
            "Java",
            "Python",
            "Spark Streaming",
            "Kafka",
            "Apache Storm",
            "Flume",
            "SQL",
            "BigQuery",
            "Spark SQL",
            "REST APIs",
            "Apache Druid",
            "Redis",
            "Elasticsearch",
            "GraphQL",
            "React",
            "Looker",
            "Tableau"
        ],
        "tech_stack": [
            "Hadoop",
            "Apache Spark",
            "Apache Hive",
            "GCP",
            "AWS",
            "Azure",
            "Java",
            "Python",
            "Spark Streaming",
            "Kafka",
            "Apache Storm",
            "Flume",
            "SQL",
            "BigQuery",
            "Spark SQL",
            "REST APIs",
            "Apache Druid",
            "Redis",
            "Elasticsearch",
            "GraphQL",
            "React",
            "Looker",
            "Tableau"
        ],
        "programming_languages": [
            "Java",
            "Python",
            "Scala",
            "SQL"
        ],
        "experience": 4,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Computer Science",
                "Software Engineering",
                "Data Engineering",
                "Database Engineering",
                "Business Intelligence",
                "Business Analytics"
            ]
        },
        "salary": {
            "max": 234000,
            "min": 117000
        },
        "benefits": [
            "401(k) match",
            "stock purchase plan",
            "paid maternity and parental leave",
            "PTO",
            "multiple health plans",
            "performance-based bonus awards",
            "medical, vision and dental coverage",
            "company-paid life insurance",
            "short-term and long-term disability",
            "company discounts",
            "Military Leave Pay",
            "adoption and surrogacy expense reimbursement",
            "Live Better U (education benefit program)"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Jose, CA",
        "job_id": 3592061167,
        "company": "TikTok",
        "title": "Data Engineer - Data Platform",
        "created_on": 1720635370.4504864,
        "description": "Responsibilities TikTok is the leading destination for short-form mobile video. Our mission is to inspire creativity and bring joy. TikTok has global offices including Los Angeles, New York, London, Paris, Berlin, Dubai, Mumbai, Singapore, Jakarta, Seoul and Tokyo. Why Join Us Creation is the core of TikTok's purpose. Our platform is built to help imaginations thrive. This is doubly true of the teams that make TikTok possible. Together, we inspire creativity and bring joy - a mission we all believe in and aim towards achieving every day. To us, every challenge, no matter how difficult, is an opportunity; to learn, to innovate, and to grow as one team. Status quo? Never. Courage? Always. At TikTok, we create together and grow together. That's how we drive impact - for ourselves, our company, and the communities we serve. Join us. Team Introduction The Data Platform team works on building data infrastructures and data products to support business engineering teams at TikTok. As a data engineer in the data platform team, you will have the opportunity to build, optimize and grow one of the largest data platforms in the world. You'll have the opportunity to gain hands-on experience on all kinds of systems in the data platform ecosystem. Your work will have a direct and huge impact on the company's core products as well as hundreds of millions of users. Responsibility • Design and build data transformations efficiently and reliably for different purposes (e.g. reporting, growth analysis, and multi-dimensional analysis) • Design and implement reliable, scalable, robust and extensible big data systems that support core products and business • Establish solid design and best engineering practice for engineers as well as non-technical people Qualifications • BS or MS degree in Computer Science or related technical field or equivalent practical experience • Experience in the Big Data technologies(Hadoop, M/R, Hive, Spark, Metastore, Presto, Flume, Kafka, ClickHouse, Flink etc.) • Experience with performing data analysis, data ingestion and data integration • Solid communication and collaboration skills • Experience with ETL (Extraction, Transformation & Loading) and architecting data systems • Experience with schema design and data modeling • Experience in writing, analyzing and debugging SQL queries • Deep understanding of various Big Data technologies • Passionate and self-motivated about technologies in the Big Data area TikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too. TikTok is committed to providing reasonable accommodations in our recruitment processes for candidates with disabilities, pregnancy, sincerely held religious beliefs or other reasons protected by applicable laws. If you need assistance or a reasonable accommodation, please reach out to us at dataecommerce.accommodation@tiktok.com Job Information: 【For Pay Transparency】Compensation Description (annually) The base salary range for this position in the selected city is $145000 - $355000 annually. ​ Compensation may vary outside of this range depending on a number of factors, including a candidate’s qualifications, skills, competencies and experience, and location. Base pay is one part of the Total Package that is provided to compensate and recognize employees for their work, and this role may be eligible for additional discretionary bonuses/incentives, and restricted stock units. ​ Our company benefits are designed to convey company culture and values, to create an efficient and inspiring work environment, and to support our employees to give their best in both work and life. We offer the following benefits to eligible employees: ​ We cover 100% premium coverage for employee medical insurance, approximately 75% premium coverage for dependents and offer a Health Savings Account(HSA) with a company match. As well as Dental, Vision, Short/Long term Disability, Basic Life, Voluntary Life and AD&D insurance plans. In addition to Flexible Spending Account(FSA) Options like Health Care, Limited Purpose and Dependent Care. ​ Our time off and leave plans are: 10 paid holidays per year plus 17 days of Paid Personal Time Off (PPTO) (prorated upon hire and increased by tenure) and 10 paid sick days per year as well as 12 weeks of paid Parental leave and 8 weeks of paid Supplemental Disability. ​ We also provide generous benefits like mental and emotional health benefits through our EAP and Lyra. A 401K company match, gym and cellphone service reimbursements. The Company reserves the right to modify or change these benefits programs at any time, with or without notice. ​",
        "url": "https://www.linkedin.com/jobs/view/3592061167",
        "summary": "TikTok is seeking a Data Engineer to build and optimize their data platform, supporting business engineering teams.  Responsibilities include designing data transformations, building scalable big data systems, and establishing engineering best practices.  The ideal candidate has experience with Big Data technologies (Hadoop, Hive, Spark, Kafka, etc.), data analysis, data ingestion, and ETL processes.",
        "industries": [
            "Technology",
            "Social Media",
            "Data Engineering"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Passionate",
            "Self-motivated"
        ],
        "hard_skills": [
            "Hadoop",
            "MapReduce",
            "Hive",
            "Spark",
            "Metastore",
            "Presto",
            "Flume",
            "Kafka",
            "ClickHouse",
            "Flink",
            "Data Analysis",
            "Data Ingestion",
            "Data Integration",
            "ETL",
            "Data Systems Architecture",
            "Schema Design",
            "Data Modeling",
            "SQL",
            "Debugging"
        ],
        "tech_stack": [
            "Hadoop",
            "MapReduce",
            "Hive",
            "Spark",
            "Metastore",
            "Presto",
            "Flume",
            "Kafka",
            "ClickHouse",
            "Flink"
        ],
        "programming_languages": [
            "SQL"
        ],
        "experience": 0,
        "education": {
            "min_degree": "BS",
            "fields": [
                "Computer Science",
                "Related Technical Field"
            ]
        },
        "salary": {
            "max": 355000,
            "min": 145000
        },
        "benefits": [
            "Medical Insurance",
            "Dental Insurance",
            "Vision Insurance",
            "Short/Long Term Disability",
            "Basic Life Insurance",
            "Voluntary Life Insurance",
            "AD&D Insurance",
            "Flexible Spending Account (FSA)",
            "Paid Time Off (PTO)",
            "Paid Sick Days",
            "Paid Parental Leave",
            "Paid Supplemental Disability",
            "Employee Assistance Program (EAP)",
            "Mental and Emotional Health Benefits",
            "401K Company Match",
            "Gym Reimbursement",
            "Cellphone Service Reimbursement"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3867558532,
        "company": "VARITE INC",
        "title": "Data Engineer",
        "created_on": 1720635372.062937,
        "description": "Pay rate range $65-75/hr. Job Descriptions: Designs, develops, modifies, tests and automation used to provide data warehouse and business intelligence applications solutions. This includes design, development, architecture recommendations, quality management, metadata and repository creation, trouble-shooting problems and tuning warehouse applications. Develops transition and implementation plans. Recommends changes in development, maintenance and standards. Contributes to the design, development, testing, implementation, and review of complex data warehouse and business intelligence solutions Develops all or part of complex data warehouse applications, develops software from established requirements, builds reports and dashboards, plans and coordinates work of lower level programmers to meet delivery commitments, creates prototypes; offers insight on the feasibility of system designs Contributes to the design of technology infrastructure and configurations, recommends process improvements Reviews complex patches and new versions of data warehouse applications. Implements complex software packages and deploys code Key participant in cross-functional team initiatives and process improvement projects. Qualifications: Advanced analytical ability and technical skill as well as the ability to provide innovative solutions to technical needs and business requirements Ability to exercise independent judgment in making complex business decisions Acute attention to detail with a high level of data integrity and accuracy Excellent verbal and written communication, with interpersonal skills to work with people at all levels of the organization Ability to translate highly technical information into non-technical terms Excellent computer skills including Microsoft Office along with various other software applications as needed for the role Broad knowledge of the programming tools, concepts, practices, and principles including design, implementation, and testing Position requires continuous visual concentration and manual dexterity to operate PC Requires prolonged sitting and minimal standing/walking Minor lifting and carrying, not likely to exceed ten pounds (laptop + charger) May require extended work hours and/or schedule flexibility as unexpected situations and/or workflow dictate May require on-call status Rare domestic travel including overnight stays may be necessary 5 to 9+ years of relevant work experience Bachelor's degree or equivalent experience Skills: Expert in developing and analyzing complex SQL on a variety of RDBMS (Microsoft SQL Server, Oracle) Expert knowledge of data modeling and understanding of different data structures and their benefits and limitations under particular use cases Experience with ETL tools (Informatica) Ability to create quality ERD's (entity-relationship diagrams) Excellent writing skills for writing user and system documentation AWS Cloud Data Warehousing Technologies Experience using core AWS services to build and support data warehouse solutions leveraging AWS architecture best practices (S3, DMS, Glue, Lambda) Development/modeling experience with Amazon Redshift Experience using the AWS service APIs, AWS CLI, and SDKs to build applications Proficiency in developing, deploying, and debugging cloud-based applications using AWS Ability to use a CI/CD pipeline to deploy applications on AWS (GitLab, Terraform, DBMaestro) Ability to apply a basic understanding of cloud-native applications to write code Proficiency writing code for serverless applications Ability to write code using AWS security best practices (e.g., not using secret and access keys in the code, instead using IAM roles) Ability to author, maintain, and debug code modules on AWS Experience with visualization tools (Tableau) Experience creating scripts with Python Experience working on an Agile team Understanding of application lifecycle management Understanding of the use of containers in the development process.",
        "url": "https://www.linkedin.com/jobs/view/3867558532",
        "summary": "This role involves designing, developing, and implementing data warehouse and business intelligence solutions using various technologies including SQL, AWS services (S3, DMS, Glue, Lambda), and Redshift. It requires expertise in data modeling, ETL processes, and visualization tools. The individual will contribute to cross-functional teams and participate in process improvement projects. They will also be responsible for writing user and system documentation, working in an Agile environment, and understanding containerization in the development process.",
        "industries": [
            "Information Technology",
            "Data Analytics",
            "Business Intelligence"
        ],
        "soft_skills": [
            "Analytical Ability",
            "Problem Solving",
            "Communication",
            "Interpersonal Skills",
            "Technical Communication",
            "Teamwork",
            "Process Improvement",
            "Attention to Detail",
            "Decision Making",
            "Time Management",
            "Flexibility"
        ],
        "hard_skills": [
            "SQL",
            "RDBMS",
            "Microsoft SQL Server",
            "Oracle",
            "Data Modeling",
            "ETL",
            "Informatica",
            "ERD",
            "AWS",
            "S3",
            "DMS",
            "Glue",
            "Lambda",
            "Redshift",
            "AWS Service APIs",
            "AWS CLI",
            "AWS SDKs",
            "CI/CD Pipeline",
            "GitLab",
            "Terraform",
            "DBMaestro",
            "Cloud-Native Applications",
            "Serverless Applications",
            "Python",
            "Tableau",
            "Agile",
            "Application Lifecycle Management",
            "Containers"
        ],
        "tech_stack": [
            "AWS",
            "S3",
            "DMS",
            "Glue",
            "Lambda",
            "Redshift",
            "GitLab",
            "Terraform",
            "DBMaestro",
            "Tableau"
        ],
        "programming_languages": [
            "SQL",
            "Python"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Data Science",
                "Information Technology",
                "Business Intelligence",
                "Related Fields"
            ]
        },
        "salary": {
            "max": 75,
            "min": 65
        },
        "benefits": [
            "On-Call Status",
            "Travel",
            "Extended Work Hours",
            "Schedule Flexibility"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco Bay Area",
        "job_id": 3928526588,
        "company": "Peak Design",
        "title": "Analytics Engineer",
        "created_on": 1720635373.6557107,
        "description": "About Us The purpose of Peak Design is to create happy, meaningful lives for the people that work here. We believe this purpose can only be achieved when self-actualizing, highly stoked people enthusiastically step into (or log onto) the Peak Design office every day. Our purpose and our mission go hand-in-hand and we encourage and celebrate authenticity and the unique perspective each of our employees brings. We’re a close-knit team that thrives on mutual respect and the belief that every voice matters—especially when it’s got something interesting to say. We make radical, meticulously-engineered gear for detail-obsessed people. Our backpacks, travel bags, camera gear, and phone accessories are used dang-near everywhere. If you’ve visited Machu Picchu, Tokyo, or an REI store in the last 10 years, you’ve been within ogling distance of a Peak Design product. Alongside our award-winning gear is a brand that truly reflects who we are as people—passionate about design, deeply caring about our environmental and social impact, unafraid to speak up, radically transparent, and generally down to clown. Whether we’re explaining a product , running a sale , launching a nonprofit , sponsoring a film , or razzing the biggest company on Earth , we do it with our trademark honesty, warmth, and wit. Through our products and our brand, we aim to create delight, and leave this world better than how we found it. About The Job Our small but mighty Business Technology team is responsible for building and streamlining business processes by leveraging technology to optimize operations, enhance productivity, and propel business growth. We’re opening a new role - Analytics Engineer - to help us design, build, and maintain scalable and robust data pipelines that support the needs of different stakeholders, data analysts, and scientists. You will also help us build out dashboards for key visualizations throughout the company . This is our first data hire at Peak Design so the ideal candidate will require a blend of technical proficiency, an analytical mindset, and the ability to communicate effectively across teams. What You'll Do: Design and construct robust, scalable data models and data warehousing solutions to support our business analytics needs. Ensure data architecture will support the requirements of the business Define the architecture for our data warehouse and data lake Manage and oversee data processes such as extract, transform, load (ETL) infrastructure to transfer and make data readily available across the company. Collaborate with various business stakeholders and analysts to understand data needs Create and manage dashboards and reporting tools to visualize key metrics and insights for different stakeholder groups. Implement secure and compliant data processing practices. Identify ways to improve data reliability, efficiency, and quality About You: 2-4 years in an Analytics Engineering role, with a proven track record of building and optimizing data systems. Strong programming skills in Python or R, or similar. Ability to think critically about data and its implications for the business, including strong problem-solving skills and attention to detail. Experience with big data cloud services such as Redshift, BigQuery, or Snowflake or similar Experience in building and optimizing big data pipelines, architectures, and datasets Strong organizational skills and project management Proficient in data visualization tools such as Tableau, Looker, or PowerBI $115,000 - $145,000 a year The base salary for this role is targeted between $115,000 - $145,000 USD (for candidates based in the San Francisco Bay Area) . Final offer amounts are determined by multiple factors and may vary from the amounts listed above. Equal Opportunity Employer Having a team of wildly-talented, fully-stoked people with diverse backgrounds, perspectives, and skillsets is how we make this whole shindig possible. Peak Design complies with all laws set force by the U.S. Equal Employment Opportunity Commission (EEOC), which enforces Federal laws that protect you from discrimination in employment: https://www.eeoc.gov/poster Peak Design also complies with all laws set forth by the California Civil Rights Department (CRD), which is responsible for enforcing state laws that make it illegal to discriminate against a job applicant or employee because of protected characteristics: https://calcivilrights.ca.gov/Posters/",
        "url": "https://www.linkedin.com/jobs/view/3928526588",
        "summary": "Peak Design is seeking an Analytics Engineer to design, build, and maintain scalable data pipelines and dashboards for various stakeholders. The role involves data modeling, warehousing, ETL infrastructure, collaborating with stakeholders, and creating dashboards for visualization. The ideal candidate will have 2-4 years of experience in Analytics Engineering, strong programming skills in Python or R, experience with big data cloud services, and proficiency in data visualization tools.",
        "industries": [
            "Consumer Goods",
            "Retail",
            "Technology",
            "E-commerce",
            "Photography"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem-solving",
            "Critical Thinking",
            "Attention to detail",
            "Organizational skills",
            "Project Management"
        ],
        "hard_skills": [
            "Python",
            "R",
            "Redshift",
            "BigQuery",
            "Snowflake",
            "ETL",
            "Data Modeling",
            "Data Warehousing",
            "Data Visualization",
            "Tableau",
            "Looker",
            "Power BI"
        ],
        "tech_stack": [
            "Python",
            "R",
            "Redshift",
            "BigQuery",
            "Snowflake",
            "ETL",
            "Data Modeling",
            "Data Warehousing",
            "Data Visualization",
            "Tableau",
            "Looker",
            "Power BI"
        ],
        "programming_languages": [
            "Python",
            "R"
        ],
        "experience": 2,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 145000,
            "min": 115000
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3961130668,
        "company": "Atlassian",
        "title": "Senior Data Engineer",
        "created_on": 1720635375.7395875,
        "description": "Overview Our Data Engineering Team is comprised of data experts. We build world-class data solutions and applications that power crucial business decisions throughout the organisation. We manage multiple analytical data models and pipelines across Atlassian, covering finance, growth, product analysis, customer analysis, sales and marketing, and so on. We maintain Atlassian’s data lake that provide a unified way of analysing our customers, our products, our operations, and the interactions among them. We’re hiring a Senior Data Engineer, reporting to the Data Engineering Manager based in Sydney. Here, you’ll enable a world-class engineering practice, drive the approach with which we use data, develop backend systems and data models to serve the needs of insights, and help build Atlassian’s data-driven culture. You love thinking about the ways the business can consume data and then figuring out how to build it. Compensation At Atlassian, we strive to design equitable, explainable, and competitive compensation programs. To support this goal, the baseline of our range is higher than that of the typical market range, but in turn we expect to hire most candidates near this baseline. Base pay within the range is ultimately determined by a candidate's skills, expertise, or experience. In the United States, we have three geographic pay zones. For this role, our current base pay ranges for new hires in each zone are: Zone A: $163,300 - $217,700 Zone B: $147,000 - $196,000 Zone C: $135,600 - $180,700 This role may also be eligible for benefits, bonuses, commissions, and equity. Please visit go.atlassian.com/payzones for more information on which locations are included in each of our geographic pay zones. However, please confirm the zone for your specific location with your recruiter. Responsibilities You’ll partner with the data analytics and data scientist team to build the data solutions that allow them to obtain more insights from our data and use that to support important business decisions. You’ll work with different stakeholders to understand their needs and architect/build the data models, data acquisition/ingestion processes and data applications to address those requirements. You’ll add new sources, code business rules, and produce new metrics that support the product analysts and data scientist. You’ll be the data domain expert who understand all the nitty-gritty of our products. You’ll own a problem end-to-end. Requirements could be vague, and iterations will be rapid You’ll improve data quality by using & improving internal tools/frameworks to automatically detect DQ issues. Qualifications BS in Computer Science or equivalent experience with 8+ years as a Senior Data Engineer or similar role Strong programming skills using Python Working knowledge of relational databases and query authoring (SQL). Experience designing data models for optimal storage and retrieval to meet product and business requirements. Experience building scalable data pipelines using Spark (SparkSQL) with Airflow scheduler/executor framework or similar scheduling tools. Experience working with AWS data services or similar Apache projects (Spark, Flink, Hive, and Kafka). Understanding of Data Engineering tools/frameworks and standards to improve the productivity and quality of output for Data Engineers across the team. Well versed in modern software development practices (Agile, TDD, CICD)",
        "url": "https://www.linkedin.com/jobs/view/3961130668",
        "summary": "Senior Data Engineer role at Atlassian responsible for building and managing data solutions, pipelines, and models to support business decisions across various departments.  This role requires strong programming skills in Python and experience with relational databases, data modeling, scalable data pipelines using Spark and Airflow, AWS data services, and modern software development practices.",
        "industries": [
            "Software",
            "Technology",
            "Data",
            "Analytics",
            "Business Intelligence"
        ],
        "soft_skills": [
            "Problem solving",
            "Communication",
            "Collaboration",
            "Stakeholder management",
            "Data analysis",
            "Data quality",
            "Domain expertise"
        ],
        "hard_skills": [
            "Python",
            "SQL",
            "Data modeling",
            "Spark",
            "SparkSQL",
            "Airflow",
            "AWS data services",
            "Apache projects",
            "Data Engineering tools and frameworks",
            "Agile",
            "TDD",
            "CI/CD"
        ],
        "tech_stack": [
            "Python",
            "SQL",
            "Spark",
            "SparkSQL",
            "Airflow",
            "AWS data services",
            "Apache projects"
        ],
        "programming_languages": [
            "Python",
            "SQL"
        ],
        "experience": 8,
        "education": {
            "min_degree": "BS",
            "fields": [
                "Computer Science"
            ]
        },
        "salary": {
            "max": 217700,
            "min": 135600
        },
        "benefits": [
            "Bonuses",
            "Commissions",
            "Equity"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "California, United States",
        "job_id": 3888422766,
        "company": "Avani Tech Solutions Private Limited",
        "title": "Sr Data Engineer",
        "created_on": 1720635377.3787365,
        "description": "Job Title: Sr. Data Engineer - Remote Hours: 40 Would like a 6 hour overlap with each other. 9:00-3:00 timeframe and skew depending on where they are Payrate: $95.50/ Hr on W-2 Description: Top 3 Skills: 5+ years experience in Data Engineering/Analytics, Data Modeling, Experience with BigQuery, Python, Airflow, solution independently, write SQL, Experience with ETL & ELT Other Skills/Nice to Haves: Spark, Adobe Analytics Senior Data engineer with GCP, Airflow, Python experience. Media / streaming data experience strongly preferred. This is a remote position. The Senior Data Engineer should possess a deep sense of curiosity and a passion for building smart data pipelines, data structures and data products and the ability to communicate data structures and tools throughout the client Streaming organization. The candidate for this role will use their skills in reverse engineering, analytics, and creative, experimental solutions to devise data and BI solutions. This engineer supports data pipeline development which includes machine learning algorithms using disparate data sources. The ideal candidate will work closely with BI, Research, Engineering, Marketing, Finance, and Product teams to implement data-driven plans that drive the business. They will have good communication skills and possess the ability to convey knowledge of data structures and tools throughout the client Digital Media organization. This candidate will be expected to lead a project from inception to completion as well as help mentor junior members of the team on best practices and approaches around data. Your Day-to-Day: Works with large volumes of traffic data and user behaviors to build pipelines that enhance raw data. Able to break down and communicate highly complex data problems into simple, feasible solutions. Extract patterns from large datasets and transform data into an informational advantage. Find answers to business questions via hands-on exploration of data sets via Jupyter, SQL, dashboards, statistical analysis, and data visualizations. Partner with the internal product and business intelligence teams to determine the best approach around data ingestion, structure, and storage. Then, work with the team to ensure these are implemented correctly. Contributing ideas on how to make our data more effective and working with other members of the engineering, BI teams, and business units to implement changes. Ongoing development of technical solutions while developing and maintaining documentation, at times training impacted teams. Early on collaboration with the team on internal initiatives to create strategies that improve company processes. Look at ways of improving efficiency by staying current on the latest technology and trends and introducing team members to such. Develop prototypes to proof out strategies for data pipelines and products. Mentor members of the team and department on best practices and approaches. Lead initiatives in ways to improve the quality of our data as well as make the data more effective, with other members of engineering, BI teams, and business units to implement changes. Able to break down and communicate highly complex data problems into simple, feasible solutions. Qualifications: What you bring to the team: You have- Bachelor's degree and 5+ years of work experience in Data Engineering and Analytics fields or consulting roles with a focus on digital analytics implementations. Experience with large scale data warehouse management systems such as BigQuery for 3+ years with advanced level understanding of warehouse cost management and query optimization Proficient in Python. Experience with Apache Airflow or equivalent tools for orchestration of pipelines. Experience with Data Modeling of performant table structures. Able to write SQL to perform common types of analysis and transformations. Strong problem-solving and creative-thinking skills. Demonstrated development of ongoing technical solutions while developing and maintaining documentation, at times training impacted teams. Experience developing solutions to business requirements via hands-on discovery and exploration of data. Exceptional written and verbal communication skills, including the ability to communicate technical concepts to non-technical audiences, as well as translating business requirements into Data Solutions Strong Experience with ETL & ELT. Experience building and deploying applications on GCP cloud platform. Influences and applies data standards, policies, and procedures Builds strong commitment within the team to support the appropriate team priorities Stays current with new and evolving technologies via formal training and self-directed education You might also have: Experience with Snowflake, Redshift and other AWS technologies. Experience with Docker and container deployment. Experience with Marketing tools like Kochava, Braze, Branch, Salesforce Marketing Cloud is a plus. Experience with exploratory data analysis using tools like iPython Notebook, Pandas & matplotlib, etc. Familiarity in Hadoop pipelines using Spark, Kafka. Familiar with GIT. Familiar with Adobe Analytics (Omniture) or Google Analytics. Digital marketing strategy including site, video, social media, SEM, SEO, and display advertising. All candidates must successfully complete a background check prior to starting employment at client.",
        "url": "https://www.linkedin.com/jobs/view/3888422766",
        "summary": "Senior Data Engineer with 5+ years of experience in Data Engineering/Analytics, Data Modeling, BigQuery, Python, Airflow. Strong skills in SQL, ETL/ELT.  Experience with GCP, media/streaming data preferred.  Responsible for building data pipelines, data structures, and data products.  Will work with various teams (BI, Research, Engineering, Marketing, Finance, and Product) to implement data-driven plans.  Experience with machine learning algorithms, Jupyter, dashboards, statistical analysis, and data visualizations.  Lead projects and mentor junior team members.",
        "industries": [
            "Data Engineering",
            "Analytics",
            "Digital Media",
            "Marketing",
            "Finance",
            "Product"
        ],
        "soft_skills": [
            "communication",
            "problem-solving",
            "creative thinking",
            "leadership",
            "mentoring",
            "collaboration",
            "curiosity",
            "passion",
            "analytical thinking",
            "data visualization",
            "presentation"
        ],
        "hard_skills": [
            "BigQuery",
            "Python",
            "Airflow",
            "SQL",
            "ETL",
            "ELT",
            "Data Modeling",
            "Jupyter",
            "Spark",
            "Docker",
            "Kochava",
            "Braze",
            "Branch",
            "Salesforce Marketing Cloud",
            "Pandas",
            "Matplotlib",
            "Hadoop",
            "Kafka",
            "Git",
            "Adobe Analytics",
            "Google Analytics"
        ],
        "tech_stack": [
            "BigQuery",
            "Python",
            "Airflow",
            "Spark",
            "GCP",
            "Hadoop",
            "Kafka",
            "Docker",
            "Jupyter",
            "Pandas",
            "Matplotlib",
            "Snowflake",
            "Redshift",
            "AWS"
        ],
        "programming_languages": [
            "Python",
            "SQL"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Data Engineering",
                "Analytics",
                "Consulting"
            ]
        },
        "salary": {
            "max": 9550,
            "min": 9550
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3969210401,
        "company": "Faire",
        "title": "Senior Data Engineer",
        "created_on": 1720635379.0372298,
        "description": "About Faire Faire is an online wholesale marketplace built on the belief that the future is local — independent retailers around the globe are doing more revenue than Walmart and Amazon combined. At Faire, we're using the power of tech, data, and machine learning to connect this thriving community of entrepreneurs across the globe. Picture your favorite boutique in town — we help them discover the best products from around the world to sell in their stores. With the right tools and insights, we believe that we can level the playing field so that small businesses everywhere can compete with these big box and e-commerce giants. By supporting the growth of independent businesses, Faire is driving positive economic impact in local communities, globally. We’re looking for smart, resourceful and passionate people to join us as we power the shop local movement. If you believe in community, come join ours. About The Role Are you passionate about using data to shape the future of Faire's product features for small and medium businesses (SMBs)? If so, we want to talk to you! We're looking for a Senior Data Engineer with a deep expertise in designing and building data systems and a dedication to applying software development best practices to data-driven solutions. In this role, you will design, build, and maintain the critical infrastructure that drives our data-driven decision-making, product development, and business insights. Your work will have a direct impact on our company's growth and user satisfaction. You'll collaborate with A players, work with one of the richest data sets in the world, utilize cutting-edge technology, and see your efforts positively influence our products and SMBs on a regular basis. Join us and be part of a team that values data-driven innovation and excellence. Job Description & What You Will Do The ideal candidate will possess a blend of software and data engineering expertise, along with strong skills in data infrastructure and architecture. They should be comfortable working independently and effectively coordinating with stakeholders. Design and Build Data Capabilities: Design and implement cutting edge technologies for Faire’s data platform Architect Robust Data Pipelines: Design and implement highly reliable, scalable, and performant data pipelines (ETL/ELT) to process structured and semi-structured data from diverse sources into our data lake and data warehouse. Champion Data Quality: Build frameworks that ensure quality, monitoring and alerting in data systems. Collaborate Cross-Functionally: Work closely with data scientists, analysts, product managers, and software engineers to translate business requirements into effective data solutions on marketplace quality, pricing parity & catalog completeness. Mentor and Guide: Provide technical leadership and mentorship to junior data engineers/co-op interns, fostering a data-driven engineering culture. Minimum qualification Over 3+ years of Software Engineering + Data Engineering expertise. Comfortable working with AWS technologies Experience using Terraform Experience using Flink and Spark Experience working with Airflow Familiarity with Docker and Kubernetes Hands-on development experience in at least one object-oriented language and experience in writing Python. SQL proficiency with proven competencies in designing well-architected data models, and optimizing query performance Experience and strong knowledge of data warehousing concepts, big data technologies, and analytics platforms. Holds a BA/BS degree in Computer Science, Math, Physics, or a related technical field. Salary Range San Francisco: the pay range for this role is $177,500 to $244,000 per year. This role will also be eligible for equity and benefits. Actual base pay will be determined based on permissible factors such as transferable skills, work experience, market demands, and primary work location. The base pay range provided is subject to change and may be modified in the future. Faire’s flexible work model aims to meet the needs of our diverse employee community by making work more flexible, connected, and inclusive. Depending on the role and needs of the team, Faire employees have the flexibility to choose how they work–whether that’s mainly in the office, remotely, or a mix of both. Roles that list only a country in the location are eligible for fully remote work in that country or in- office work at a Faire office in that country, provided employees are located in the registered country/province/state. Roles with only a city location are eligible for in-office or hybrid office work in that city. Our talent team will work with candidates to determine what locations and roles are eligible for each option. Applications for this position will be accepted for a minimum of 30 days from the posting date. Why you’ll love working at Faire We are entrepreneurs: Faire is being built for entrepreneurs, by entrepreneurs. We believe entrepreneurship is a calling and our mission is to empower entrepreneurs to chase their dreams. Every member of our team is taking part in the founding process. We are using technology and data to level the playing field: We are leveraging the power of product innovation and machine learning to connect brands and boutiques from all over the world, building a growing community of more than 350,000 small business owners. We build products our customers love: Everything we do is ultimately in the service of helping our customers grow their business because our goal is to grow the pie - not steal a piece from it. Running a small business is hard work, but using Faire makes it easy. We are curious and resourceful: Inquisitive by default, we explore every possibility, test every assumption, and develop creative solutions to the challenges at hand. We lead with curiosity and data in our decision making, and reason from a first principles mentality. Faire was founded in 2017 by a team of early product and engineering leads from Square. We’re backed by some of the top investors in retail and tech including: Y Combinator, Lightspeed Venture Partners, Forerunner Ventures, Khosla Ventures, Sequoia Capital, Founders Fund, and DST Global. We have headquarters in San Francisco and Kitchener-Waterloo, and a global employee presence across offices in Salt Lake City, Atlanta, Toronto, London, New York, LA, and Sao Paulo. To learn more about Faire and our customers, you can read more on our blog. Faire provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability, genetics, sexual orientation, gender identity or gender expression. Faire is committed to providing access, equal opportunity and reasonable accommodation for individuals with disabilities in employment, its services, programs, and activities. To request reasonable accommodation, please fill out our Accommodation Request Form (https://bit.ly/faire-form)",
        "url": "https://www.linkedin.com/jobs/view/3969210401",
        "summary": "Faire is seeking a Senior Data Engineer to design, build, and maintain the critical data infrastructure that powers its platform. The ideal candidate will have extensive experience in data engineering, AWS technologies, data pipelines, and data warehousing, along with strong communication and collaboration skills.",
        "industries": [
            "E-commerce",
            "Retail",
            "Technology",
            "Software",
            "Wholesale"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Leadership",
            "Mentorship",
            "Resourcefulness",
            "Curiosity",
            "Data-Driven"
        ],
        "hard_skills": [
            "AWS",
            "Terraform",
            "Flink",
            "Spark",
            "Airflow",
            "Docker",
            "Kubernetes",
            "Python",
            "SQL",
            "Data Warehousing",
            "Big Data",
            "Analytics Platforms"
        ],
        "tech_stack": [
            "AWS",
            "Terraform",
            "Flink",
            "Spark",
            "Airflow",
            "Docker",
            "Kubernetes",
            "Python",
            "SQL",
            "Data Warehousing",
            "Big Data",
            "Analytics Platforms"
        ],
        "programming_languages": [
            "Python",
            "SQL"
        ],
        "experience": 3,
        "education": {
            "min_degree": "BA/BS",
            "fields": [
                "Computer Science",
                "Math",
                "Physics",
                "Related Technical Fields"
            ]
        },
        "salary": {
            "max": 244000,
            "min": 177500
        },
        "benefits": [
            "Equity",
            "Flexible Work Model"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Sunnyvale, CA",
        "job_id": 3933850620,
        "company": "DivIHN Integration Inc",
        "title": "Database Engineer",
        "created_on": 1720635380.6600296,
        "description": "DivIHN (pronounced “divine”) is a CMMI ML3-certified Technology and Talent solutions firm. Driven by a unique Purpose, Culture, and Value Delivery Model, we enable meaningful connections between talented professionals and forward-thinking organizations. Since our formation in 2002, organizations across commercial and public sectors have been trusting us to help build their teams with exceptional temporary and permanent talent. Visit us at https://divihn.com/find-a-job/ to learn more and view our open positions. Please apply or call one of us to learn more For further inquiries regarding the following opportunity, please contact one of our Talent Specialist Vinod | 224-507-1294 Muskan | 224-369-4238 Abdul | 224 - 369-0755 Title: Database Engineer Location: Sunnyvale, CA Or Sylmar, CA Duration: 6 Months (Contract to Hire) Job Description Database Developer works under general supervision, performs specialized level software project activities in such areas as Development, Tools, Test, Requirements and/or related areas. Assignments may include, but are not limited to, analyzing requirements, planning and scheduling data, updating and maintaining different software specifications based on analysis of specifications, needs and improvements, as well as troubleshooting, debugging, and maintaining of software, test, or tool designs, cases, scripts, procedures, and/or code based on strategic analysis. Work requires the application of theoretical engineering principles, evaluation, ingenuity and creative/analytical techniques typically acquired in a recognized four-year or more academic course of specialized study. Applies engineering and scientific principles to the evaluation and solution of technical problems in a creative manner unique to each study. Exercises level appropriate technical judgment in planning, organizing, performing, and/or coordinating engineering work. What You'll Do Work on database application design, coding/scripting, and unit testing, supporting verification and validation testing, and ensuring that all application development conforms to design specifications and coding standards for medical devices. Support the development, enhancement, and modifications to source code. Contribute to, and support reviews held by other engineers on related projects and provide useful feedback. Contribute to and supports a variety of test phase efforts. Design and implement database applications in SQL and other programming languages such as Python/Java/C# and database systems such as Postgres/Oracle. Assist in the preparation of detailed design specifications for implementation per Client design development process. Review project deliverables such as specifications, code, and test/tool procedures for feasibility, thoroughness, clarity, correctness, and accuracy. Presents at design reviews; documents and resolves issues as directed. Support all Company initiatives as identified by management and in support of Quality Management Systems (QMS), Environmental Management Systems (EMS), and other regulatory requirements. Comply with U.S. Food and Drug Administration (FDA) regulations, other regulatory requirements, Company policies, operating procedures, processes, and task assignments. Maintains positive and cooperative communications and collaboration with all levels of employees, customers, contractors, and vendors. Required EDUCATION AND EXPERIENCE YOU'LL BRING Bachelor’s Degree in Engineering discipline, computer science, related engineering field, or an equivalent combination of education and work experience. 3 years of hands-on experience on database application development. Experience with any of Oracle, Postgres, or other database system. Experience with SQL, Python, Java or C#, scripting, multi-threading, formal software development methodologies, and source code management. Experience on one or more of the major cloud platforms, such as Azure, Google Cloud, AWS. Working knowledge on cloud computing technology, familiar with Docker, Kubernetes, and other cloud platforms/tools. Working knowledge on Linux and Windows operating system. Ability to work in a highly matrixed and geographically diverse business environment. Ability to work within a team and as an individual contributor in a fast-paced, changing environment. Strong verbal and written communications with ability to effectively communicate at multiple levels in the organization. Multitasks, prioritizes, and meets deadlines in timely manner. Strong organizational and follow-up skills, as well as attention to detail. Ability to maintain regular and predictable attendance Preferred Master’s Degree preferred Experience in database design, application development and database operations. Experience in data migration, data governance, data quality and data analytics. Experience with AI applications. About Us DivIHN , the 'IT Asset Performance Services' organization, provides Professional Consulting, Custom Projects, and Professional Resource Augmentation services to clients in the Mid-West and beyond. The strategic characteristics of the organization are Standardization, Specialization, and Collaboration. DivIHN is an equal opportunity employer. DivIHN does not and shall not discriminate against any employee or qualified applicant on the basis of race, color, religion (creed), gender, gender expression, age, national origin (ancestry), disability, marital status, sexual orientation, or military status.",
        "url": "https://www.linkedin.com/jobs/view/3933850620",
        "summary": "Database Engineer with 3+ years of experience in database application development, SQL, Python, Java, C#, scripting, multi-threading, and source code management. Experience with Oracle, Postgres, or other database systems. Experience with cloud platforms like Azure, Google Cloud, AWS. Knowledge of Docker, Kubernetes, Linux, and Windows. Strong communication and organizational skills. ",
        "industries": [
            "Information Technology",
            "Technology",
            "Software Development",
            "Talent Acquisition",
            "Professional Services",
            "Consulting",
            "Medical Device"
        ],
        "soft_skills": [
            "Communication",
            "Organization",
            "Teamwork",
            "Problem Solving",
            "Time Management",
            "Analytical Thinking",
            "Detail Oriented",
            "Multitasking",
            "Collaboration",
            "Adaptability",
            "Problem Solving",
            "Decision Making"
        ],
        "hard_skills": [
            "Database Application Development",
            "SQL",
            "Python",
            "Java",
            "C#",
            "Scripting",
            "Multi-threading",
            "Source Code Management",
            "Oracle",
            "Postgres",
            "Azure",
            "Google Cloud",
            "AWS",
            "Docker",
            "Kubernetes",
            "Linux",
            "Windows"
        ],
        "tech_stack": [
            "Oracle",
            "Postgres",
            "SQL",
            "Python",
            "Java",
            "C#",
            "Azure",
            "Google Cloud",
            "AWS",
            "Docker",
            "Kubernetes",
            "Linux",
            "Windows"
        ],
        "programming_languages": [
            "SQL",
            "Python",
            "Java",
            "C#"
        ],
        "experience": 3,
        "education": {
            "min_degree": "Bachelor’s Degree",
            "fields": [
                "Engineering",
                "Computer Science",
                "Related Engineering Field"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3901627233,
        "company": "Unreal Staffing, Inc",
        "title": "Senior Real-Time Data Engineer",
        "created_on": 1720635382.3592129,
        "description": "Company Overview: Welcome to the forefront of data-driven innovation! Our company is dedicated to harnessing the power of real-time data to drive transformative change and solve complex problems across industries. We're committed to building cutting-edge real-time data solutions that enable timely insights and actions. Join us and be part of a dynamic team shaping the future of real-time data engineering. Position Overview: As a Senior Real-Time Data Engineer, you'll play a crucial role in designing, building, and optimizing our real-time data infrastructure. You'll work on challenging projects, from architecting data streaming pipelines to developing real-time analytics systems, to support the needs of our data-driven organization. If you're a seasoned engineer with expertise in real-time data technologies and a passion for building scalable and reliable systems, we want you on our team. Requirements Key Responsibilities: Real-Time Data Architecture: Design and implement scalable and reliable architecture for real-time data processing and analytics, including data ingestion, processing, and serving layers Streaming Data Pipelines: Develop and maintain real-time data streaming pipelines using technologies such as Apache Kafka, Apache Flink, or Apache Spark Streaming, ensuring low-latency and high-throughput data processing Event-Driven Architecture: Design event-driven systems to enable real-time processing of data events and triggers, supporting use cases such as real-time monitoring, anomaly detection, and alerting Data Integration: Integrate real-time data streams from diverse sources and systems into the real-time data infrastructure, ensuring data consistency, integrity, and quality Real-Time Analytics: Develop real-time analytics systems and dashboards to enable real-time insights and decision-making, leveraging technologies such as Apache Druid, Elasticsearch, or Grafana Data Governance: Implement data governance policies and procedures to ensure data quality, security, and compliance with regulatory requirements in real-time data environments Performance Optimization: Optimize real-time data pipelines and processing workflows for performance, scalability, and efficiency, leveraging distributed computing and streaming processing techniques Monitoring and Alerting: Implement monitoring and alerting solutions to track the performance and health of real-time data infrastructure and pipelines, proactively identifying and resolving issues Documentation and Best Practices: Document real-time data architecture, pipelines, and best practices, providing clear and comprehensive documentation to facilitate understanding and collaboration among team members Collaboration: Collaborate with cross-functional teams, including data scientists, software engineers, and business analysts, to understand requirements and deliver real-time data solutions that meet business needs Mentorship and Development: Mentor junior engineers, sharing expertise and best practices in real-time data engineering, and facilitate knowledge sharing sessions within the team Qualifications: Bachelor's degree or higher in Computer Science, Engineering, Mathematics, or related field 5+ years of experience in data engineering, with a focus on real-time data technologies Proficiency in real-time data streaming technologies such as Apache Kafka, Apache Flink, or Apache Spark Streaming Strong programming skills in languages such as Python, Java, or Scala, with experience in distributed computing frameworks Experience with cloud platforms such as AWS, Azure, or Google Cloud Platform, and services like AWS Kinesis, Azure Stream Analytics, or Google Cloud Dataflow Strong understanding of event-driven architecture and stream processing concepts, with experience building event-driven systems Strong problem-solving skills and analytical thinking, with the ability to design and troubleshoot complex real-time data solutions Excellent communication and collaboration skills, with the ability to work effectively in cross-functional teams and communicate technical concepts to non-technical stakeholders Benefits Competitive salary: The industry standard salary for Senior Real-Time Data Engineers typically ranges from $170,000 to $230,000 per year, depending on experience and qualifications Comprehensive health, dental, and vision insurance plans Flexible work hours and remote work options Generous vacation and paid time off Professional development opportunities, including access to training programs, conferences, and workshops State-of-the-art technology environment with access to cutting-edge tools and resources Vibrant and inclusive company culture with opportunities for growth and advancement Exciting projects with real-world impact at the forefront of data-driven innovation Join Us: Ready to shape the future of real-time data engineering? Apply now to join our team and be part of the data revolution!",
        "url": "https://www.linkedin.com/jobs/view/3901627233",
        "summary": "This job posting seeks a Senior Real-Time Data Engineer to design, build, and optimize real-time data infrastructure for a company specializing in real-time data solutions.  Responsibilities include building data streaming pipelines using Apache Kafka, Apache Flink, or Apache Spark Streaming, implementing event-driven architectures, integrating data from various sources, developing real-time analytics systems, and ensuring data governance and performance optimization.  The ideal candidate will have 5+ years of experience in data engineering, proficiency in real-time data technologies, strong programming skills in Python, Java, or Scala, and experience with cloud platforms like AWS, Azure, or GCP.",
        "industries": [
            "Data Engineering",
            "Data Analytics",
            "Software Engineering",
            "Technology"
        ],
        "soft_skills": [
            "Problem-solving",
            "Analytical thinking",
            "Communication",
            "Collaboration",
            "Mentorship"
        ],
        "hard_skills": [
            "Apache Kafka",
            "Apache Flink",
            "Apache Spark Streaming",
            "Python",
            "Java",
            "Scala",
            "AWS",
            "Azure",
            "Google Cloud Platform",
            "AWS Kinesis",
            "Azure Stream Analytics",
            "Google Cloud Dataflow",
            "Event-Driven Architecture",
            "Stream Processing",
            "Data Governance",
            "Performance Optimization",
            "Distributed Computing"
        ],
        "tech_stack": [
            "Apache Kafka",
            "Apache Flink",
            "Apache Spark Streaming",
            "AWS",
            "Azure",
            "Google Cloud Platform",
            "AWS Kinesis",
            "Azure Stream Analytics",
            "Google Cloud Dataflow",
            "Apache Druid",
            "Elasticsearch",
            "Grafana"
        ],
        "programming_languages": [
            "Python",
            "Java",
            "Scala"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Engineering",
                "Mathematics"
            ]
        },
        "salary": {
            "max": 230000,
            "min": 170000
        },
        "benefits": [
            "Competitive salary",
            "Health insurance",
            "Dental insurance",
            "Vision insurance",
            "Flexible work hours",
            "Remote work options",
            "Vacation",
            "Paid time off",
            "Professional development opportunities",
            "State-of-the-art technology environment",
            "Company culture",
            "Growth opportunities",
            "Real-world impact"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Sunnyvale, CA",
        "job_id": 3948678198,
        "company": "Walmart Global Tech",
        "title": "Sr. Data Engineer R-1833900",
        "created_on": 1720635384.0971513,
        "description": "At Walmart, we help people save money, so they can live better. This mission serves as the foundation for every decision we make and drives us to create the future of retail. We can’t do that without the best talent – talent that is innovative, curious, and driven to create exceptional experiences for our customers. Do you have boundless energy and passion for engineering data used to solve dynamic problems that will shape the future of retail? With the sheer scale of Walmart’s environment comes the biggest of big data sets. As a Walmart Data Engineer in Marketplace, you will dig into our mammoth scale of data to help unleash the power of retail data science by imagining, developing, and maintaining data pipelines that our Data Scientists and Analysts can rely on. You will be responsible for contributing to an orchestration layer of complex data transformations, refining raw data from source into targeted, valuable data assets for consumption in a governed way. You will partner with Data Scientists, Analysts, other engineers and business stakeholders to solve complex and exciting challenges so that we can build out capabilities that evolve the retail business model while making a positive impact on our customers’ and sellers’ lives. About the Data and Customer Analytics (DCA) Organization: Our organization focuses on managing and delivering world-class data assets, including creating and maintaining data standards, driving policy compliance, creating partnerships, and developing pipelines and self-service tools. We empower our business to leverage data to fuel growth, driving revenue in our core and building new business model opportunities. What you'll do: You will use cutting edge data engineering techniques to create critical datasets and dig into our mammoth scale of data to help unleash the power of data science by imagining, developing, and maintaining data pipelines that our Data Scientists and Analysts can rely on. You will be responsible for contributing to an orchestration layer of complex data transformations, refining raw data from source into targeted, valuable data assets for consumption in a governed way. You will partner with Data Scientists, Analysts, other engineers, and business stakeholders to solve complex and exciting challenges so that we can build out capabilities that evolve the marketplace business model while making a positive impact on our customers' and sellers’ lives. You will participate with limited help in small to large sized projects by reviewing project requirements; gather requested information; write and develop code; conduct unit testing; communicate status and issues to team members and stakeholders; collaborate with project team and cross functional teams; troubleshoot open issues and bug-fixes; and ensure on-time delivery and hand-offs. You will design, develop and maintain highly scalable and fault-tolerant real time, near real time and batch data systems/pipelines that process, store, and serve large volumes of data with optimal performance. You will ensure data ingested and processed is accurate and of high quality by implementing data quality checks, data validation, and data cleaning processes. You will identify possible options to address business problems within one's discipline through analytics, big data analytics, and automation. You will build business domain knowledge to support the data need for product teams, analytics, data scientists and other data consumers. What you'll bring: At least 7+ years of experience development of big data technologies/data pipelines Experience in managing and manipulating huge datasets in the order of terabytes (TB) is essential. Experience with in big data technologies like Hadoop, Apache Spark (Scala preferred), Apache Hive, or similar frameworks on the cloud (GCP preferred, AWS, Azure etc.) to build batch data pipelines with strong focus on optimization, SLA adherence and fault tolerance. Experience in building idempotent workflows using orchestrators like Automic, Airflow, Luigi etc. Experience in writing SQL to analyze, optimize, profile data preferably in BigQuery or SPARK SQL Strong data modeling skills are necessary for designing a schema that can accommodate the evolution of data sources and facilitate seamless data joins across various datasets. Ability to work directly with stakeholders to understand data requirements and translate that to pipeline development / data solution work. Strong analytical and problem-solving skills are crucial for identifying and resolving issues that may arise during the data integration and schema evolution process. Ability to move at a rapid pace with quality and start delivering with minimal ramp up time will be crucial to succeed in this initiative. Effective communication and collaboration skills are necessary for working in a team environment and coordinating efforts between different stakeholders involved in the project. Nice to have from you: Experience building complex near real time (NRT) streaming data pipelines using Apache Kafka, Spark streaming, Kafka Connect with a strong focus on stability, scalability and SLA adherence. Good understanding of REST APIs – working knowledge on Apache Druid, Redis, Elastic search, GraphQL or similar technologies. Understanding of API contracts, building telemetry, stress testing etc. Exposure in developing reports/dashboards using Looker/Tableau Experience in eCommerce domain preferred. The above information has been designed to indicate the general nature and level of work performed in the role. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications required of employees assigned to this job. The full Job Description can be made available as part of the hiring process. About Global Tech Imagine working in an environment where one line of code can make life easier for hundreds of millions of people. That’s what we do at Walmart Global Tech. We’re a team of software engineers, data scientists, cybersecurity expert's and service professionals within the world’s leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered. We train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail. Flexible, hybrid work We use a hybrid way of working that is primarily in office coupled with virtual when not onsite. Our campuses serve as a hub to enhance collaboration, bring us together for purpose and deliver on business needs. This approach helps us make quicker decisions, remove location barriers across our global team and be more flexible in our personal lives. Benefits & Perks: Beyond our great compensation package, you can receive incentive awards for your performance. Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and much more. Equal Opportunity Employer: Walmart, Inc. is an Equal Opportunity Employer – By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing diversity- unique styles, experiences, identities, ideas and opinions – while being inclusive of all people. About Global Tech Imagine working in an environment where one line of code can make life easier for hundreds of millions of people and put a smile on their face. That’s what we do at Walmart Global Tech. We’re a team of 15,000+ software engineers, data scientists and service professionals within Walmart, the world’s largest retailer, delivering innovations that improve how our customers shop and empower our 2.2 million associates. To others, innovation looks like an app, service, or some code, but Walmart has always been about people. People are why we innovate, and people.",
        "url": "https://www.linkedin.com/jobs/view/3948678198",
        "summary": "Walmart is seeking a Data Engineer to join their Marketplace team and help develop and maintain data pipelines for data scientists and analysts. The role involves working with massive datasets, implementing data quality checks, and collaborating with stakeholders to understand data requirements. Experience with big data technologies such as Hadoop, Apache Spark, and Apache Hive is essential, as well as knowledge of data modeling and SQL. This is a challenging and rewarding opportunity to make a positive impact on the retail business model.",
        "industries": [
            "Retail",
            "Ecommerce",
            "Data Analytics",
            "Data Engineering"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Analytical",
            "Stakeholder Management",
            "Teamwork",
            "Time Management",
            "Organization"
        ],
        "hard_skills": [
            "Hadoop",
            "Apache Spark",
            "Apache Hive",
            "SQL",
            "BigQuery",
            "Spark SQL",
            "Automic",
            "Airflow",
            "Luigi",
            "Data Modeling",
            "Data Quality",
            "Data Validation",
            "Data Cleaning",
            "Data Integration",
            "Schema Evolution",
            "REST APIs",
            "Apache Kafka",
            "Spark Streaming",
            "Kafka Connect",
            "Apache Druid",
            "Redis",
            "Elasticsearch",
            "GraphQL",
            "Looker",
            "Tableau"
        ],
        "tech_stack": [
            "Hadoop",
            "Apache Spark",
            "Apache Hive",
            "BigQuery",
            "Spark SQL",
            "Automic",
            "Airflow",
            "Luigi",
            "Apache Kafka",
            "Spark Streaming",
            "Kafka Connect",
            "Apache Druid",
            "Redis",
            "Elasticsearch",
            "GraphQL",
            "Looker",
            "Tableau"
        ],
        "programming_languages": [
            "Scala",
            "SQL"
        ],
        "experience": 7,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "401(k) match",
            "stock purchase plan",
            "paid maternity and parental leave",
            "PTO",
            "multiple health plans"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Irvine, CA",
        "job_id": 3937585233,
        "company": "Thales",
        "title": "System Engineer - New Graduate",
        "created_on": 1720635398.2555196,
        "description": "Location: Irvine, United States of America Thales people architect solutions that enable two-thirds of planes to take off and land safely. We create in-flight entertainment systems that engross 50 million fliers every year and we develop the avionics that control the world’s largest commercial aircrafts. Our simulators train the next generation of pilots for fighter jets, transporters and search and rescue helicopters. And, together, each and every member of our aerospace team makes a difference. Systems Engineer – New Graduate Irvine, CA (Onsite) Position Summary Thales is looking for a System Engineer - New Graduate that will participate in producing consistent specifications according to customer needs. Will support developing solutions working with Project Design Authorities on critical projects leveraging foundational knowledge and skills while continuously learning and growing in a dynamic technical environment. In this position you will be a part of the PDA (Project Design Authority) team, directly responsible and in charge for all end to end technical activities leading to the development of solution for entertainment, connectivity and ground systems. This includes participation to Innovation workshops, project sizing (SOW, WBS/OBS…), technical interfaces with the OEM (Airbus, Boeing…), or Connectivity providers, technical coordination of all engineering disciplines, Compliance to Customer/Product requirements, definition of high level requirements and capabilities including performance aspects and high level design and flow down of all subsystems leading up to the successful solution delivery. Key Areas of Responsibility Support gathering Customer Requirements and document in Customer Requirement Specifications. Support reviews with customer, make updates, help with Release, and obtain signatures. Get familiar with requirements and traceability in requirement management tool JAMA, indirectly learning how system works. Support technical meetings with customer and get familiar with IFE architecture and how to integrate customer solutions and bring in innovative fresh ideas to customer workshops. Assist in creation of lessons learned and all process documentation, and learn overall in-flight Entertainment processes and domains. Support technical discussions, provide insight on customer needs, and testing at the rack and support and present customer needs to engineering and gather estimates to develop/deliver the feature. Minimum Qualifications Bachelor’s Degree in Computer Science, Electrical, Aerospace, or Aeronautics Engineering, or related discipline with 0-2 yrs of professional experience. Successful candidate will be expected to ramp-up quickly, take ownership of tasks, and operate effectively and autonomously across multiple entities. Ability to read, analyze, interpret and prepare technical procedures, documents, and an ability to solve complex problems and syntheses ideas into clear and concise statements. Have excellent interpersonal written and oral communication skills with the ability to communicate technical issues in an assertive and tactful manner. Have a strong customer focus to develop innovative solutions to fit your customers’ needs. Ability to work in a fast paced, autonomously driven, and demanding atmosphere, and be a team player and ability to collaborate with different team members. If you’re excited about working with Thales, but not meeting the requirements for this position, we encourage you to join our Talent Community! Special Position Requirements Schedule: Core Business Hours Monday-Friday. Travel: Very limited travel potentially to other Thales manufacturing sites within the USA. Regulatory Compliance Requirements: Pre-employment Drug Screening. What We Offer The anticipated TTC range for this role is $86,675.40 - $160,968.60 USD Annual. The Company reserves the right to ultimately pay more or less than the posted range and offer additional benefits and other compensation, depending on circumstances not related to an applicant’s status protected by local, state, or federal law. Thales provides an extensive benefits program for all full-time employees working 30 or more hours per week and their eligible dependents, including the following: Elective Health and Dental plans. Retirement Savings Plan with a company contribution and a match, and without vesting period. Company paid holidays, vacation days, and paid sick leave. Company provided Life Insurance. Why Join Us? Say HI and learn more about working at Thales click here . This position will require successfully completing a post-offer background check. Qualified candidates with [a] criminal history will be considered and are not automatically disqualified, consistent with federal law, state law, and local ordinances. Successful applicant must comply with federal contractor vaccine mandate requirements. Thales champions inclusion and we believe diversity strengthens the fabric of our culture. We are an equal opportunity/affirmative action employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected Veteran status, age, or any other characteristic protected by law. If you need an accommodation or assistance in order to apply for a position with Thales, please contact us at talentacquisition@us.thalesgroup.com.",
        "url": "https://www.linkedin.com/jobs/view/3937585233",
        "summary": "Thales is seeking a System Engineer - New Graduate to participate in creating specifications for customer needs, supporting solution development, and working with Project Design Authorities on critical projects. The role involves collaborating with OEMs like Airbus and Boeing, defining high-level requirements and capabilities, and ensuring successful solution delivery.",
        "industries": [
            "Aerospace",
            "Aviation",
            "Defense",
            "Technology",
            "Engineering",
            "Software Development",
            "Telecommunications",
            "Entertainment"
        ],
        "soft_skills": [
            "Communication",
            "Problem Solving",
            "Teamwork",
            "Collaboration",
            "Customer Focus",
            "Innovation",
            "Analytical Thinking",
            "Time Management",
            "Organization",
            "Detail Oriented",
            "Adaptability",
            "Problem Solving",
            "Interpersonal Skills"
        ],
        "hard_skills": [
            "Customer Requirements",
            "System Architecture",
            "Technical Documentation",
            "JAMA",
            "IFE (In-Flight Entertainment)",
            "Technical Meetings",
            "Requirements Gathering",
            "Process Documentation",
            "Technical Discussions",
            "Testing",
            "Rack Support",
            "Estimating"
        ],
        "tech_stack": [
            "JAMA"
        ],
        "programming_languages": [],
        "experience": 0,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Electrical Engineering",
                "Aerospace Engineering",
                "Aeronautics Engineering",
                "Related Discipline"
            ]
        },
        "salary": {
            "max": 160968,
            "min": 86675
        },
        "benefits": [
            "Health Insurance",
            "Dental Insurance",
            "Retirement Savings Plan",
            "Company Contribution",
            "Match",
            "Paid Holidays",
            "Vacation Days",
            "Paid Sick Leave",
            "Life Insurance"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3895816961,
        "company": "Unreal Staffing, Inc",
        "title": "Cloud Data Engineer",
        "created_on": 1720635401.6597228,
        "description": "Company Overview: Welcome to the forefront of cloud data engineering! At our company, we're dedicated to leveraging the power of cloud technology to drive data-driven innovation. Our mission is to develop scalable and robust cloud data solutions that enable organizations to harness the full potential of their data. Join us and be part of a dynamic team committed to shaping the future of cloud data engineering. Position Overview: As a Cloud Data Engineer, you'll play a pivotal role in designing, building, and maintaining our cloud-based data infrastructure and systems. Working closely with cross-functional teams of data engineers, architects, and business stakeholders, you'll ensure the reliability, scalability, and efficiency of our cloud data solutions. If you're passionate about cloud technology and eager to drive innovation through scalable data solutions, we want you on our team. Requirements Key Responsibilities: Cloud Data Infrastructure Design: Design and architect scalable and resilient cloud-based data infrastructure solutions using leading cloud platforms such as AWS, Google Cloud Platform, or Microsoft Azure Data Ingestion and Integration: Develop and maintain data ingestion pipelines to collect and integrate data from diverse sources into cloud storage and processing services Data Processing and Transformation: Implement data processing and transformation workflows using cloud-native services such as AWS Glue, Google Dataflow, or Azure Data Factory Data Storage and Management: Manage cloud-based data storage solutions, including data lakes, data warehouses, and NoSQL databases, optimizing for performance, cost, and scalability Data Governance and Security: Implement data governance policies and security controls to ensure compliance with regulatory requirements and protect sensitive data in the cloud Performance Optimization: Optimize cloud data solutions for performance, scalability, and cost-effectiveness, leveraging cloud-native services and technologies Monitoring and Maintenance: Monitor cloud data infrastructure and systems for performance, reliability, and security, proactively identifying and addressing issues to minimize downtime and optimize resource utilization Documentation and Collaboration: Document cloud data architecture, processes, and best practices, and collaborate with cross-functional teams to ensure alignment and transparency Qualifications: Bachelor's degree or higher in Computer Science, Engineering, or related field Strong background in data engineering, with hands-on experience in designing, building, and optimizing cloud-based data solutions Proficiency in cloud platforms such as AWS, Google Cloud Platform, or Microsoft Azure, and experience with cloud services such as S3, GCS, Azure Blob Storage, and others Experience with cloud data processing and analytics services such as AWS Glue, Google Dataflow, Apache Spark on cloud platforms, or Azure Data Lake Analytics Familiarity with cloud data storage solutions such as AWS S3, Google Cloud Storage, Azure Blob Storage, AWS Redshift, Google BigQuery, Azure SQL Data Warehouse, or others Strong problem-solving abilities and analytical thinking, with a keen attention to detail and a passion for tackling complex technical challenges Excellent communication and collaboration skills, with the ability to work effectively in cross-functional teams and communicate technical concepts to non-technical stakeholders Benefits Competitive salary: The industry standard salary for Cloud Data Engineers typically ranges from $140,000 to $210,000 per year, depending on experience and qualifications. Exceptional candidates may be eligible for higher compensation packages Comprehensive health, dental, and vision insurance plans Flexible work hours and remote work options Generous vacation and paid time off Professional development opportunities, including access to training programs, conferences, and workshops State-of-the-art technology environment with access to leading cloud platforms and tools Vibrant and inclusive company culture with team-building activities and social events Opportunities for career growth and advancement within the company Exciting projects with real-world impact across diverse industries Chance to work alongside top talent and industry experts in the field of cloud data engineering Join Us: Ready to shape the future of cloud data engineering? Apply now to join our team and be part of an exciting journey of innovation and discovery!",
        "url": "https://www.linkedin.com/jobs/view/3895816961",
        "summary": "This role involves designing, building, and maintaining cloud-based data infrastructure for a company focused on cloud data engineering. The Cloud Data Engineer will work with cross-functional teams to ensure the reliability, scalability, and efficiency of the cloud data solutions. Key responsibilities include designing cloud data infrastructure, developing data ingestion pipelines, implementing data processing workflows, managing cloud-based data storage, and ensuring data governance and security. The position requires experience in cloud platforms like AWS, Google Cloud Platform, or Microsoft Azure, and familiarity with cloud services such as S3, GCS, Azure Blob Storage, AWS Glue, Google Dataflow, Apache Spark, and cloud data storage solutions like AWS Redshift, Google BigQuery, and Azure SQL Data Warehouse. The company offers a competitive salary, comprehensive benefits, flexible work hours, professional development opportunities, and a vibrant company culture.",
        "industries": [
            "Cloud Computing",
            "Data Engineering",
            "Data Analytics",
            "Information Technology"
        ],
        "soft_skills": [
            "Problem Solving",
            "Analytical Thinking",
            "Communication",
            "Collaboration",
            "Attention to Detail"
        ],
        "hard_skills": [
            "AWS",
            "Google Cloud Platform",
            "Microsoft Azure",
            "S3",
            "GCS",
            "Azure Blob Storage",
            "AWS Glue",
            "Google Dataflow",
            "Apache Spark",
            "AWS Redshift",
            "Google BigQuery",
            "Azure SQL Data Warehouse",
            "Data Governance",
            "Data Security",
            "Performance Optimization",
            "Monitoring",
            "Documentation"
        ],
        "tech_stack": [
            "AWS",
            "Google Cloud Platform",
            "Microsoft Azure",
            "S3",
            "GCS",
            "Azure Blob Storage",
            "AWS Glue",
            "Google Dataflow",
            "Apache Spark",
            "AWS Redshift",
            "Google BigQuery",
            "Azure SQL Data Warehouse"
        ],
        "programming_languages": [],
        "experience": 0,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Computer Science",
                "Engineering"
            ]
        },
        "salary": {
            "max": 210000,
            "min": 140000
        },
        "benefits": [
            "Competitive Salary",
            "Health Insurance",
            "Dental Insurance",
            "Vision Insurance",
            "Flexible Work Hours",
            "Remote Work Options",
            "Vacation",
            "Paid Time Off",
            "Professional Development",
            "Training Programs",
            "Conferences",
            "Workshops",
            "State-of-the-art Technology",
            "Vibrant Culture",
            "Team-Building Activities",
            "Social Events",
            "Career Growth",
            "Advancement Opportunities",
            "Real-world Impact",
            "Work with Top Talent",
            "Industry Experts"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Fremont, CA",
        "job_id": 3841227188,
        "company": "Info Way Solutions",
        "title": "Data Engineer",
        "created_on": 1720635403.6505845,
        "description": "Hi Friends, I am sending requirement, kindly get back to me if the job description suits you. Job Title: Data Engineer Job Location: Montreal, Canada Experience: 8-10 Years. Job Description Skills: Data Engineer, SQL, Python , DW ,and banking or finance etc. Sagar Wagh Technical Recruiter Info Way Solutions LLC 9254952178 Email: wagh.s@infowaygroup.com LinkedIn: linkedin.com/in/sagar-wagh-604a0b292 Info Way Solutions LLC | 46520 Fremont Blvd, Suite 614 | Fremont, CA - 94538",
        "url": "https://www.linkedin.com/jobs/view/3841227188",
        "summary": "Data Engineer position in Montreal, Canada requiring 8-10 years of experience. Skills needed include Data Engineering, SQL, Python, DW, and experience in banking or finance.",
        "industries": [
            "Banking",
            "Finance"
        ],
        "soft_skills": [],
        "hard_skills": [
            "Data Engineering",
            "SQL",
            "Python",
            "DW"
        ],
        "tech_stack": [],
        "programming_languages": [
            "SQL",
            "Python"
        ],
        "experience": 8,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Los Angeles, CA",
        "job_id": 3894467190,
        "company": "System1",
        "title": "Data Engineer III",
        "created_on": 1720635405.5794852,
        "description": "System1 is one of the largest customer acquisition companies in the world whose growth depends heavily on a very talented data engineering team. The Data Operations team at System1 is an engineering team that is focused on building processes, procedures and automation to ensure smooth running data infrastructure. We process billions of records per day, providing a core component of many organizational functions. The data that flows through these systems supports business intelligence, data science and machine learning, traffic quality and analytics. You would be working in a fast-paced environment where enhancements to system scalability, reliability, usability, efficiency and performance are the goal. Come join us! The Role You Will Have: Gather requirements, understand the big picture, create detailed proposals in technical specification documents. Proof of concept evaluations of new technologies, new features, patterns, frameworks, API. Productizing data ingestion from various sources, data delivery to various destinations, and the creating well-orchestrated data pipelines. Consolidate and modernize the codebase. Continuously improve monitoring and alerting coverage. Communicate effectively with upstream / downstream stake-holders, with clear understanding of data contracts or dependencies. Conduct SQL data investigations, data quality analysis and optimizations. Work in a transparent, and agile team environment, supporting the peers. Perform maintenance of existing infrastructure, improving efficiency and costs. Contribute in peer code reviews, and help the team produce high quality code. What You Will Bring: Bachelor’s or Master’s degree in Computer Science/Engineering. Very strong SQL proficiency, and preferably SQL query optimization experience. Programming expertise in Python, Scala, Java is preferred. Excellent communication skills. Versed in software production engineering practices, version control, code peer reviews, automated testing, and CI/CD. Experience in AWS cloud is required. GCP, Azure are preferred. Proficiency with one of the Cloud data-warehouses / data-lakes like Snowflake, Google BigQuery, AWS Redshift, Azure Synapse, Databricks. Applications and data-flow development in the Hadoop ecosystem is a plus. Experience in databases like Postgres, MySQL, Oracle or SQL Server required. Good understanding of data engineering fundamentals, ELT / ETL, latency, observability, lineage, distributed storage and distributed computing. Experience with modern orchestration platforms, such as Airflow. Familiarity with docker, kubernetes containerization strategies and optimization is preferred. Good data modeling skills, database design, relational/non-relational. Working knowledge of dbt and jinja macros, dbt docs, dbt test is preferred, but not required. Any experience collaborating with business intelligence, data analytics, data science stake-holders is a plus. What We Have to Offer: Competitive salary + bonus + equity Generous PTO + 11 company holidays Open sick time 100% covered Medical, Dental, Vision for employees 401k with match Health & Dependent Care Flex Spending Account Paid professional development Leadership & growth opportunities Virtual company and team building events The U.S. base salary range for this full-time position is $141,000 - $211,000 + bonus + equity + benefits. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process. Please note that the compensation details listed in U.S. role postings reflect the base salary only, and do not include bonus, equity, or benefits. System1’s headquarters is located in Marina del Rey, CA with additional offices in Bellevue, WA and Guelph, ON, Canada. Employees near office locations are returning to the office. Location-specific policies and available accommodations will be discussed during the interview process. System1 allows fully-remote work in the following approved locations: Arizona, Colorado, Connecticut, Georgia, Hawaii, Minnesota, Missouri, New Jersey, New York, North Carolina, Oklahoma, Oregon, Pennsylvania, Tennessee, Texas and Virginia. Prospective U.S. employees who live outside of any of these states will need to establish residency in one of the approved states prior to employment. Reasonable accommodations will be provided as applicable. Equal Employment Opportunity: System1 provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.",
        "url": "https://www.linkedin.com/jobs/view/3894467190",
        "summary": "System1, a leading customer acquisition company, is seeking a Data Operations Engineer to join their team. This role will involve building and maintaining data infrastructure, ensuring smooth data flow, and contributing to the company's growth through data-driven insights.  Responsibilities include gathering requirements, designing data pipelines, consolidating code, improving monitoring, and collaborating with stakeholders.",
        "industries": [
            "Data Engineering",
            "Data Science",
            "Machine Learning",
            "Business Intelligence",
            "Customer Acquisition"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Analytical Thinking",
            "Attention to Detail"
        ],
        "hard_skills": [
            "SQL",
            "Python",
            "Scala",
            "Java",
            "AWS",
            "Snowflake",
            "Google BigQuery",
            "AWS Redshift",
            "Azure Synapse",
            "Databricks",
            "Hadoop",
            "Postgres",
            "MySQL",
            "Oracle",
            "SQL Server",
            "Airflow",
            "Docker",
            "Kubernetes",
            "dbt",
            "Jinja"
        ],
        "tech_stack": [
            "AWS",
            "Snowflake",
            "Google BigQuery",
            "AWS Redshift",
            "Azure Synapse",
            "Databricks",
            "Hadoop",
            "Airflow",
            "Docker",
            "Kubernetes",
            "dbt",
            "Jinja"
        ],
        "programming_languages": [
            "SQL",
            "Python",
            "Scala",
            "Java"
        ],
        "experience": 3,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Computer Science",
                "Engineering"
            ]
        },
        "salary": {
            "max": 211000,
            "min": 141000
        },
        "benefits": [
            "Competitive Salary",
            "Bonus",
            "Equity",
            "Generous PTO",
            "Company Holidays",
            "Open Sick Time",
            "Medical",
            "Dental",
            "Vision",
            "401k with Match",
            "Health & Dependent Care FSA",
            "Paid Professional Development",
            "Leadership & Growth Opportunities",
            "Virtual Company & Team Building Events"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Diego, CA",
        "job_id": 3857810816,
        "company": "The Shipyard",
        "title": "Senior Data Developer",
        "created_on": 1720635407.3177633,
        "description": "The Shipyard is a leading, independent agency that builds performance-driven brands audiences can't help but love. By applying modern mindsets to established models, we fuel brand and marketing decisions that are more courageous and more validated. Our ability to align bold creativity with individual consumer motivations lets us do more than hope for brand love - we methodically engineer it throughout the consumer journey. That's how we activate the synergistic power of brand building and performance marketing. Engineering Brand Love through the courageous ambitions of our people and our clients. The agency has significant momentum and is on a roll with new business. Our team continues to expand across four cities (Columbus, Newport Beach, Sacramento, and San Diego) and we're still growing. Come join our team! Summary We are looking for a smart, experienced, and motivated Senior Data Developer to join our San Diego team. The Senior Data Developer will serve as a coach for other Data Developers, the role requires daily hands-on development execution and technology roadmap leadership. The Senior Data Developer is responsible for leading end-to-end data development, from building data architectures for robust data sets to collecting & modeling data. You will work with internal departments to define their analytic requirements and set up their corresponding data environments. You will be a valued member with the opportunity to shape the way that we collect and share data, influencing processes and efficiency across the agency and its clientele. This is a hybrid work-from-home/office position (in-office attendance is required on Tuesdays & Thursdays) and the candidate must live near San Diego, CA . At The Shipyard, we know that it's your unique talents, backgrounds, and perspectives that make you who you are, just like our team, who come from various career paths and experiences. We believe we can't be truly diverse without bringing your most authentic self to the agency. Studies show that people from marginalized communities may not always apply for positions if they don't meet every single requirement in a job posting. At The Shipyard, we encourage you to apply even if you don't meet all the requirements. We believe that your passion for what we stand for — our values and purpose — is just as important as meeting every checkbox. What You'll Do Lead data needs for clients and internal teams in an agile work environment Collect data requirements from internal teams to assist in the design of a scalable infrastructure to drive decision-making using technology and BI tools Manage the retrieval, compiling, and formatting of large volumes of data Set up data environments, in partnership with the Data Visualization team, within business intelligence platforms, ultimately creating advertising reports Learn new technologies as they evolve, understand their impact to systems, and lead the team in proposed tech stack evolutions Leadership in implementing technological advancements as well as educating internal and external partners Collaborate with internal teams and users to understand new opportunities for support Documentation of data environments and infrastructures developed What You'll Bring 5-6 years equivalent work experience in a Data Developer or similar Data & Analytics Role Experience in data collection, transformation, development, and/or reporting with optimized outputs for BI tools Experience developing and optimizing data ingestion processes with tools such as Fivetran, Stitch, and even writing custom API scripts. Experience designing, building, and maintaining robust data pipelines (ETL/ELT) using technologies like Postgres, dbt, and Airflow. Experience with AWS services, Kubernetes, and Docker. Experience with data modeling / semantic modeling Experience with data warehousing architecture (Postgres, AWS Redshift) Experience in applying data processes and deploying BI updates at scale A testing mindset - Knowledge and experience of CI/CD pipelines and test environments Proficiency in SQL, Python, and/or Scala/Java Experience contributing to and influencing tech stack evolutions and infrastructure roadmap Ability to write code with clean documentation Strong entrepreneurial drive Strong verbal and written communication skills and a collaborative, problem-solving mindset Self-starting with initiative and passionate about learning the world of data strategy, architecture, processing, and development Experience balancing multiple deliverables across multiple clients and data sets Experience with /awareness of cloud-based visualization tools such as Tableau, Looker, or Periscope Marketing & advertising knowledge (preferred, not required) What You'll Get The overall target range of base compensation for this role is $110,000 - $137,000. Compensation offered will be determined by additional factors such as location and experience 40 hours of paid sick time (annually) Open PTO Flexible work hours and remote work Paid holidays + holiday closure between Christmas Eve and New Year's Day Company-paid medical, dental, and vision insurance Life insurance and disability benefits 401k program with employer matching 6 weeks paid parental leave Employee bonus referrals Dog-friendly offices Company-provided snacks and beverages (yes, beer/wine included) ... and lots more! Requirements Must be authorized to work in the U.S. without the need for visa sponsorship.",
        "url": "https://www.linkedin.com/jobs/view/3857810816",
        "summary": "The Shipyard, a leading marketing agency, is seeking a Senior Data Developer to join their San Diego team. The role involves leading data development, managing data pipelines, and working with internal teams to define analytic requirements. This is a hybrid work-from-home/office position, requiring in-office attendance on Tuesdays and Thursdays.",
        "industries": [
            "Marketing",
            "Advertising",
            "Data Analytics",
            "Data Science",
            "Business Intelligence"
        ],
        "soft_skills": [
            "Leadership",
            "Communication",
            "Collaboration",
            "Problem-solving",
            "Entrepreneurial",
            "Self-starter",
            "Initiative",
            "Passionate",
            "Organization",
            "Time Management"
        ],
        "hard_skills": [
            "Data Collection",
            "Data Transformation",
            "Data Development",
            "Data Reporting",
            "Data Ingestion",
            "Data Pipelines",
            "ETL/ELT",
            "SQL",
            "Python",
            "Scala",
            "Java",
            "Postgres",
            "dbt",
            "Airflow",
            "AWS",
            "Kubernetes",
            "Docker",
            "Data Modeling",
            "Semantic Modeling",
            "Data Warehousing",
            "CI/CD",
            "Tableau",
            "Looker",
            "Periscope"
        ],
        "tech_stack": [
            "Fivetran",
            "Stitch",
            "Postgres",
            "dbt",
            "Airflow",
            "AWS",
            "Kubernetes",
            "Docker",
            "Tableau",
            "Looker",
            "Periscope"
        ],
        "programming_languages": [
            "SQL",
            "Python",
            "Scala",
            "Java"
        ],
        "experience": 5,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 137000,
            "min": 110000
        },
        "benefits": [
            "40 hours of paid sick time",
            "Open PTO",
            "Flexible work hours",
            "Remote work",
            "Paid holidays",
            "Company-paid medical, dental, and vision insurance",
            "Life insurance",
            "Disability benefits",
            "401k with employer matching",
            "Paid parental leave",
            "Employee bonus referrals",
            "Dog-friendly offices",
            "Company-provided snacks and beverages"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3914344314,
        "company": "Speak",
        "title": "Data Engineer",
        "created_on": 1720635409.1045563,
        "description": "About Us Our mission is to reinvent the way people learn, starting with language. We begin by teaching the next billion people English, Spanish, and French. English is the global language of business, culture, and communication, and over 1.5 billion people around the world are actively trying to learn right now. Others dream of communicating with the half-billion native Spanish speakers across the globe. The problem is that it's nearly impossible to learn to speak a language without constant access to a speaking partner. Grammar and vocab apps don't really help – you need to actually converse with someone. Speak is on a journey to fix this. We're creating an AI-powered experience that replicates the flow of a conversation, without needing a human on the other end. The goal is to make it radically more accessible to be able to have conversations in a foreign language and eventually help hundreds of millions of people gain fluency who otherwise wouldn't be able to. We started on this journey over five years ago and we've still got a long ways to go. We're thoughtfully adding new team members only when we think they can truly play a big role in our mission. Speak launched first in South Korea where we have quickly grown to become the top grossing education app in the country. We have now delivered this winning product to more than 30 countries globally and are continuing to expand to more markets in the coming months. The company is well funded, raising a recent Series B backed by investors like OpenAI, Founders Fund, Y Combinator, Khosla Ventures, Lachy Groom, Josh Buckley, and others. We’re a team of 75 based primarily in SF, Seoul, Tokyo, and Ljubljana. About This Role As an Data Engineer at Speak, you'll play a pivotal role in shaping the future of digital language learning, propelling us towards our mission of making language proficiency accessible to millions worldwide. Your responsibilities will span the crucial intersection of data infrastructure and analytics, from managing scalable data pipelines that support real-time processing, to deploying sophisticated analytics solutions that drive personalized learning experiences. You'll work closely with our product, engineering, and data science teams to ensure that our platform, powered by cutting-edge technology, is not only robust but also delivers actionable insights that enhance user engagement and learning outcomes. What you'll be doing Design and Build Data Infrastructure: You'll architect and implement robust, scalable data pipelines that ensure efficient data flow and processing, supporting both real-time analytics and large-scale, batch processing needs. Your work will be critical in managing the ingestion, storage, and accessibility of data from various sources, ensuring our platform's backbone is strong and reliable. Enable Data-Driven Decisions: By collaborating with cross-functional teams, you will develop and deploy tools and frameworks that facilitate data access and analysis, empowering product and business teams to make informed decisions. This includes creating dashboards, reports, and advanced analytics models that reveal user behavior patterns, learning efficacy, and opportunities for product improvement. Optimize Data Architecture: Constantly evaluate and refine the data architecture to support our growing data needs and ensure optimal performance. This includes managing data lakes, data warehouses, and databases, as well as implementing best practices for data modeling, data quality, and data governance. Support Machine Learning Projects: Work closely with data scientists and machine learning engineers by providing them with clean, structured data for building and deploying predictive models that enhance personalized learning experiences and engagement strategies. Innovate and Experiment: Stay ahead of the curve by researching and implementing cutting-edge technologies and methodologies in data engineering and analytics. Collaborate Across Teams: As a key player in the engineering team, you'll work closely with product managers, analysts, and other engineers to bring data-driven products and features from concept to launch. What we're looking for Data Modeling: Deep understanding of data structures, theories, principles, and practices. Ability to design, implement, and manage data warehouses effectively. Big Data Technologies: Proficiency in big data technologies and frameworks such as DBT, Airflow, etc., to handle large-scale data processing and analysis. Programming Skills: Strong programming skills in languages relevant to data engineering (Python and SQL). Ability to write efficient, reliable, and maintainable code. Data Pipeline and ETL Development: Experience in building and optimizing data pipelines, architectures, and data sets. Familiarity with ETL (extract, transform, load) processes and tools. Cloud Computing: Knowledge of cloud services (GCP BigQuery, dbt) and understanding of how to leverage them for data processing and storage solutions. Data Analysis and Visualization: Ability to analyze data to identify patterns, anomalies, and insights. Proficiency in using data visualization tools(eg Mode) to communicate findings clearly. Debugging Skills: Strong problem-solving skills and the ability to approach complex challenges methodically including data inconsistency issues. Effective Communication: Ability to communicate technical information to non-technical stakeholders clearly and effectively. This includes writing documentation, presenting findings, and collaborating on projects. Bonuses: Experience with AppsFlyer, Segment, Split.io, Customer.io, Facebook Ads and Google Big Query Office San Francisco, CA Why work at Speak Join a fantastic, tight-knit team at the right time: we're growing very quickly, we've raised our Series B and an additional extension from some of the top investors in the valley, and we've achieved product-market fit in our initial markets. You'd join at a magical time when a single person could significantly change the course of the company. Do your life's work with people you’ll love working with: we care strongly about our craft and want every person at Speak to feel like they're growing every day. We believe in the idea that working with people you both enjoy and have respect for makes everything better. We hire thoughtfully and only work with people we admire deeply. Global in nature: We're live in over 40 countries and launching in a number of new markets soon. We have dedicated offices in San Francisco, Ljubljana, Seoul, and Tokyo, and you’ll have the opportunity to talk to users in each of these regions on a regular basis as well as travel. Impact people's lives in a major way: Learning a language is one of the single most life-changing skills one can learn, and right now 99% of people never achieve their goal because the process is broken. We’re helping millions of people achieve their goals and improve their lives. Speak does not discriminate based upon race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.",
        "url": "https://www.linkedin.com/jobs/view/3914344314",
        "summary": "Speak is an AI-powered language learning app that aims to make language proficiency accessible to millions worldwide. The company is seeking a Data Engineer to build and maintain scalable data pipelines, develop data-driven insights, and optimize data architecture to support their growth. The ideal candidate will have experience with big data technologies, data modeling, ETL development, cloud computing, and data visualization.",
        "industries": [
            "Education",
            "Technology",
            "Artificial Intelligence",
            "Language Learning"
        ],
        "soft_skills": [
            "Communication",
            "Problem Solving",
            "Collaboration",
            "Critical Thinking",
            "Decision Making"
        ],
        "hard_skills": [
            "Data Modeling",
            "Big Data Technologies",
            "Python",
            "SQL",
            "ETL",
            "Cloud Computing",
            "Data Visualization",
            "Debugging"
        ],
        "tech_stack": [
            "DBT",
            "Airflow",
            "GCP BigQuery",
            "Mode",
            "AppsFlyer",
            "Segment",
            "Split.io",
            "Customer.io",
            "Facebook Ads"
        ],
        "programming_languages": [
            "Python",
            "SQL"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Global in nature",
            "Impact people's lives in a major way"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Carlsbad, CA",
        "job_id": 3875000724,
        "company": "Callaway Golf",
        "title": "Sr. Data Engineer",
        "created_on": 1720635410.9249098,
        "description": "ABOUT THE BRAND: Callaway Golf is the leading manufacturer of premium golf clubs, balls, performance gear and accessories worldwide. Through an unwavering commitment to innovation, we push the limits of performance and create demonstrably superior products designed to make every golfer a better golfer. Our company is a blend of experience and diverse backgrounds, and together we look to move the game forward, and we want top-notch people to join us in that mission! By joining Callaway Golf, you also join part of the portfolio of brands within Topgolf Callaway Brands, an unrivaled tech-enabled Modern Golf and active lifestyle company delivering leading golf equipment, apparel, and entertainment, with a portfolio of global brands including Topgolf, Callaway Golf, TravisMathew, Toptracer, Odyssey, OGIO, Jack Wolfskin, and World Golf Tour (“WGT”). “Modern Golf” is the dynamic and inclusive ecosystem that includes both on-course and off-course golf. For more information, please visit https://www.topgolfcallawaybrands.com Job Overview As a Senior Data Engineer, you will play a critical role in helping the organization leverage data and technology to drive business growth, customer engagement, and data-driven decision-making. The primary responsibilities will be building, managing, and optimizing data pipelines as well as machine learning models for predictive analytics effectively into production for critical data and analytics consumers like business/data analysts, data scientists, or any persona needing curated data for data and analytics use cases across the enterprise. Roles And Responsibilities Design, build, and maintain our data architecture using technologies like Snowflake, Azure, and Databricks. Develop and maintain ELT/ETL workflows using Python, SQL, and Spark to ensure data accuracy, completeness, and consistency. Develop, deploy, and maintain machine learning models for various applications such as predictive analytics, recommendation systems, and anomaly detection. Collaborate with cross-functional teams to gather and analyze business requirements and recommend solutions. Ensure compliance with data privacy regulations and best practices. Implement and manage DevOps processes to automate the deployment and testing of data pipelines and workflows. Stay current with the latest data technologies, trends, and best practices, and recommend adoption where appropriate. Work in an Agile environment to deliver high-quality solutions on time and within budget. TECHNICAL COMPETENCIES (Knowledge, Skills & Abilities) Strong experience creating pipelines in Snowflake. Knowledge of the MarTech ecosystem. Able to work with complex, high-volume, high-dimensionality data from varying sources. Fluency with at least one scripting language, such as Python and R. Direct expertise in building and deploying machine learning models for predictive analytics using libraries such as TensorFlow, PyTorch, or scikit-learn. Strong knowledge of DevOps workflow, cloud-native platforms (containers, Kubernetes, serverless, etc.) and tools (such as Git), and infrastructure as code (IaC) tools (such as CloudFormation or Terraform) Good written and oral communication skills. Self-motivated and results-oriented attitude. Education And Experience Bachelor’s degree in computer science, mathematics, or statistics Minimum eight years equivalent work experience. Minimum five years of cloud platform experience Minimum five years of experience with Snowflake, Azure, and visualization tools. Experience with Data Integration tools like Spark/Databricks or equivalent. Experience in Linux/Unix shell scripting. DE&I and EEOC: Inclusion & Diversity: As a purpose-led, performance driven company, we strive to foster a culture of belonging based on respect, connection, openness and authenticity. We are committed to building and maintaining a workplace that celebrates the diversity of our associates, supporting them to bring their authentic selves to work every day. If your experience is close to what we’re looking for, please consider applying. Experience comes in many forms, skills are transferable, and passion goes a long way. We know that diverse backgrounds and experiences make for the best problem-solving and creative thinking, which is why we’re dedicated to adding new perspectives to the team and encourage everyone to apply. We look forward to learning more about you. ARE YOU READY TO MAKE THE TURN? APPLY TODAY! 102,200.00 - 127,700.00 - 153,200.00 USD Annual",
        "url": "https://www.linkedin.com/jobs/view/3875000724",
        "summary": "Callaway Golf seeks a Senior Data Engineer to design, build, and maintain data architecture using Snowflake, Azure, and Databricks.  Responsibilities include developing ELT/ETL workflows with Python, SQL, and Spark, building machine learning models for predictive analytics, collaborating with cross-functional teams, ensuring data privacy compliance, implementing DevOps processes, and staying up-to-date with data technologies. The role requires experience with Snowflake, Azure, data visualization tools, data integration tools like Spark/Databricks, and Linux/Unix shell scripting.",
        "industries": [
            "Golf",
            "Sports",
            "Technology",
            "Data Analytics",
            "Machine Learning",
            "Software"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem-Solving",
            "Analytical",
            "Self-Motivation",
            "Results-Oriented"
        ],
        "hard_skills": [
            "Snowflake",
            "Azure",
            "Databricks",
            "Python",
            "SQL",
            "Spark",
            "Machine Learning",
            "TensorFlow",
            "PyTorch",
            "Scikit-learn",
            "DevOps",
            "Cloud-Native Platforms",
            "Containers",
            "Kubernetes",
            "Serverless",
            "Git",
            "Infrastructure as Code",
            "CloudFormation",
            "Terraform",
            "Linux",
            "Unix"
        ],
        "tech_stack": [
            "Snowflake",
            "Azure",
            "Databricks",
            "Python",
            "SQL",
            "Spark",
            "TensorFlow",
            "PyTorch",
            "Scikit-learn",
            "Git",
            "CloudFormation",
            "Terraform",
            "Linux",
            "Unix"
        ],
        "programming_languages": [
            "Python",
            "SQL"
        ],
        "experience": 8,
        "education": {
            "min_degree": "Bachelor’s degree",
            "fields": [
                "Computer Science",
                "Mathematics",
                "Statistics"
            ]
        },
        "salary": {
            "max": 153200,
            "min": 102200
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3829514475,
        "company": "SoFi",
        "title": "Senior Software Engineer, Data Platform (Activation)",
        "created_on": 1720635412.8187692,
        "description": "Employee Applicant Privacy Notice Who we are: Shape a brighter financial future with us. Together with our members, we’re changing the way people think about and interact with personal finance. We’re a next-generation financial services company and national bank using innovative, mobile-first technology to help our millions of members reach their goals. The industry is going through an unprecedented transformation, and we’re at the forefront. We’re proud to come to work every day knowing that what we do has a direct impact on people’s lives, with our core values guiding us every step of the way. Join us to invest in yourself, your career, and the financial world. The Role: SoFi runs on data! We are seeking a highly motivated Senior Software Engineer to join our Data Platform team. As a Senior Software Engineer, you will work alongside our experienced team of data engineers and product managers to develop and maintain our cutting-edge data handling platform using Snowflake, dbt, Sagemaker, AWS, Tableau and Airflow. In this role you will be contributing to the long-term success of SoFi’s data vision by building out distributed systems and scalable data platforms. As a Senior engineer on the Data Platform team at SoFi, you'll be tasked with building critical components and features. You will implement battle-tested patterns and interfaces, squash bugs, refactor code and continually grow as an engineer. The ideal candidate has a strong software engineering background and problem-solving ability along with cloud computing (AWS) and data engineering skill set with prior experience on technologies such as Snowflake, Airflow, dbt, Kafka, Spark, Python, and Tableau. Additionally, you will demonstrate SoFi’s core values by honing your skills as an effective communicator, showing personal responsibility, and setting ambitious goals. If you like working on problems with tangible and lasting impact, we would love to have you in our team! What You’ll Do: Collaborate with cross-functional teams to understand data requirements and design scalable data solutions. Write high-quality, efficient, and scalable code to implement new features and functionality on the data platform by following Agile methodologies. Participate in code reviews and provide feedback to other team members to ensure code quality and maintainability. Work with product managers and other stakeholders to understand user requirements and implement solutions that meet their needs. Participate in team meetings and contribute to discussions on technology, design, and implementation. Keep up-to-date with the latest developments in data engineering, analytics and cloud technologies and promote best practices and drive innovation. Develop and optimize data pipelines using dbt and Airflow to ensure efficient data processing and data quality checks. Design and implement data ingestion patterns, reverseETL capabilities to promote data democratization and drive insights for business. Build, deploy and operate the products end-to-end as part of the team including on call rotation. Utilize your proficiency in Python to develop custom scripts and tools to enhance data operations and automation. Focussed on operational excellence to provide a reliable platform for SoFi use. Mentor and guide junior team members, providing technical expertise and fostering a culture of continuous learning. What You’ll Need: Bachelor's or Master's degree in Computer Science, Engineering, or a related field. 4+ years of experience as a Software Engineer, with a focus on Software/data engineering and data platform development. Extensive hands-on experience with Snowflake, AWS services, dbt and Airflow. Strong understanding of data ingestion, Orchestration, transformation and reverse ETL best practices and design principles. Proficiency in Java, Python for data manipulation, scripting, and automation. Experience with tableau administration or building dashboards. Excellent problem-solving skills and the ability to thrive in a fast-paced, collaborative environment. Strong communication skills to effectively work with diverse stakeholders and present technical concepts. Nice to Have: Interest in personal finance Experience with Machine Learning services like Sagemaker, Snowflake ML Compensation And Benefits The base pay range for this role is listed below. Final base pay offer will be determined based on individual factors such as the candidate’s experience, skills, and location. To view all of our comprehensive and competitive benefits, visit our Benefits at SoFi page! SoFi provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion (including religious dress and grooming practices), sex (including pregnancy, childbirth and related medical conditions, breastfeeding, and conditions related to breastfeeding), gender, gender identity, gender expression, national origin, ancestry, age (40 or over), physical or medical disability, medical condition, marital status, registered domestic partner status, sexual orientation, genetic information, military and/or veteran status, or any other basis prohibited by applicable state or federal law. The Company hires the best qualified candidate for the job, without regard to protected characteristics. Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records. New York applicants: Notice of Employee Rights SoFi is committed to embracing diversity. As part of this commitment, SoFi offers reasonable accommodations to candidates with physical or mental disabilities. If you need accommodations to participate in the job application or interview process, please let your recruiter know or email accommodations@sofi.com. Due to insurance coverage issues, we are unable to accommodate remote work from Hawaii or Alaska at this time. Internal Employees If you are a current employee, do not apply here - please navigate to our Internal Job Board in Greenhouse to apply to our open roles.",
        "url": "https://www.linkedin.com/jobs/view/3829514475",
        "summary": "SoFi is seeking a Senior Software Engineer to join its Data Platform team. The role involves developing and maintaining a data platform using technologies like Snowflake, dbt, Sagemaker, AWS, Tableau, and Airflow. The ideal candidate will have experience with data engineering, cloud computing, and software development, with specific skills in Python, Snowflake, Airflow, dbt, Kafka, Spark, and Tableau. The position offers opportunities for career growth, working on impactful projects, and contributing to SoFi's data vision.",
        "industries": [
            "Financial Services",
            "Technology",
            "Software Development",
            "Data Engineering",
            "Cloud Computing",
            "Banking"
        ],
        "soft_skills": [
            "Motivated",
            "Problem-solving",
            "Communication",
            "Collaboration",
            "Teamwork",
            "Ambitious",
            "Leadership",
            "Mentorship",
            "Continuous Learning",
            "Effective Communication",
            "Personal Responsibility"
        ],
        "hard_skills": [
            "Snowflake",
            "dbt",
            "Sagemaker",
            "AWS",
            "Tableau",
            "Airflow",
            "Data Ingestion",
            "Data Orchestration",
            "Data Transformation",
            "Reverse ETL",
            "Java",
            "Python",
            "Scripting",
            "Automation",
            "Machine Learning"
        ],
        "tech_stack": [
            "Snowflake",
            "dbt",
            "Sagemaker",
            "AWS",
            "Tableau",
            "Airflow",
            "Kafka",
            "Spark",
            "Python",
            "Java"
        ],
        "programming_languages": [
            "Python",
            "Java"
        ],
        "experience": 4,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Computer Science",
                "Engineering",
                "Related Fields"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Comprehensive and Competitive Benefits"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Gardena, CA",
        "job_id": 3813098215,
        "company": "1872 Consulting",
        "title": "Data Engineer",
        "created_on": 1720635416.0231838,
        "description": "Data Engineer Near Los Angeles, CA Full Time What you Should Expect Develop, integrate, and maintain APIs for our marketing data warehouse Develop and maintain reporting views in a common repository with automation to load into RedShift Creating a simple web UI for internal users to generate common data exports Assist in diagnosing and improving slow queries Work with the Data Insights Manager to architect and design our e-commerce analytics pipeline Contribute to marketing data insights strategy and development Help extend our operations software to include inventory management, barcoding, additional warehouse telemetry and automation Develop real time data visualizations to help stakeholders make better strategic decisions Help support our Shopify/Ecommerce portal with troubleshooting Qualifications and Skills Experience and Skills: 2+ years as a developer Experience in 1+ data warehouse / ETL-type projects Follows best practices around APIs and data processing pipelines Can maintain well-documented code and communicate effectively with non-technical stakeholders Experience with the following tools is highly preferred: Scripting / development: Python or Node.js Database: SQL + AWS Redshift",
        "url": "https://www.linkedin.com/jobs/view/3813098215",
        "summary": "Data Engineer responsible for developing and maintaining marketing data warehouse APIs, Redshift reporting views, a data export web UI, and real-time visualizations. Collaborates with the Data Insights Manager on data pipeline design and strategy. Supports Shopify/Ecommerce portal troubleshooting and contributes to operational software development.",
        "industries": [
            "E-commerce",
            "Marketing",
            "Data Analytics"
        ],
        "soft_skills": [
            "Communication",
            "Problem Solving",
            "Collaboration",
            "Documentation",
            "Data Analysis"
        ],
        "hard_skills": [
            "API Development",
            "Data Warehousing",
            "ETL",
            "Data Processing Pipelines",
            "SQL",
            "AWS Redshift",
            "Web UI Development",
            "Query Optimization",
            "Data Visualization",
            "Python",
            "Node.js",
            "Shopify"
        ],
        "tech_stack": [
            "AWS Redshift",
            "Python",
            "Node.js",
            "Shopify",
            "SQL"
        ],
        "programming_languages": [
            "Python",
            "Node.js",
            "SQL"
        ],
        "experience": 2,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Diego, CA",
        "job_id": 3958160662,
        "company": "Lockheed Martin",
        "title": "Associate Software Engineer",
        "created_on": 1720635417.5698032,
        "description": "Participate in development and integration of the Control and Management software used for submarine communications. Primary responsibilities include designing, implementing, unit testing, and integrating Java based software. In addition, the candidate will provide real-time support to government counterparts and customers for integration, testing, and troubleshooting of lab and deployed systems. The candidate should have a strong analytical background, well developed problem-solving skills, the ability to work in teams, and excellent communication and organizational skills. The candidate should have experience in Object Oriented Design in Java. The candidate should be able to interpret design specifications, create low-level software design, and translate design into working code. The candidate should be detail-oriented with troubleshooting skills to be able to analyze issues relating to all aspects of the software and provide resolution recommendations for them. Experience with developing software for Linux environments is desired. Familiarity with a Lean development approach as well as Agile software development processes is desired. Candidates must have or be able to acquire at least a Secret Clearance. Most software development is expected to be completed on-site in San Diego within a secure room. Candidates should expect to be on site full-time, with minimal opportunities for remote work.",
        "url": "https://www.linkedin.com/jobs/view/3958160662",
        "summary": "Develop and integrate Control and Management software for submarine communications, using Java, in a secure environment. Responsibilities include design, implementation, unit testing, integration, and real-time support for government customers.",
        "industries": [
            "Defense",
            "Military",
            "Technology",
            "Telecommunications",
            "Software Development"
        ],
        "soft_skills": [
            "Problem-solving",
            "Teamwork",
            "Communication",
            "Organizational Skills",
            "Analytical Skills",
            "Detail-Oriented"
        ],
        "hard_skills": [
            "Java",
            "Object Oriented Design",
            "Software Design",
            "Troubleshooting",
            "Linux",
            "Agile Software Development",
            "Lean Development"
        ],
        "tech_stack": [
            "Java",
            "Linux",
            "Agile",
            "Lean"
        ],
        "programming_languages": [
            "Java"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Secret Clearance"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3964671659,
        "company": "Trove",
        "title": "Senior Data Engineer",
        "created_on": 1720635419.3716357,
        "description": "About Trove Recommerce Trove powers resale for the world's most beloved brands, extending the life of millions of products and creating more inclusive, less wasteful business models. Trove is the market leader in branded resale and trade-in for world-class brands and retailers such as Canada Goose, lululemon, Patagonia, REI, Levi's, Arc'teryx, Allbirds, and more. Through its proprietary Recommerce Operating System, Trove is accelerating the shift toward more sustainable business models, foundational for circularity. Over the last decade, Trove has equipped leading brands with technology and operations to create and scale branded resale programs by enabling customer trade-in of items, single-SKU identification and condition grading, site build and maintenance, and customer data collection, analytics and reporting. A Certified B Corporation, Trove is pioneering a new era of retail essential to a more sustainable future. Recognized by FastCo in 2023, 2022 and 2020 as one of the World Changing Ideas Awards and one of the World's Most Innovative Companies, we continue to gather accolades for our innovative resale technology. In 2023, Trove was recognized as winner for Best Use of Emerging Technology by Modern Retail and selected by Inc as 2023 winner for larger businesses that have grown significantly without losing their focus on serving the best products. The Leading 100 selected Trove as 2023 Trailblazer of the Year for breakout technology startups that are driving innovation in fashion and retail. And most recently awarded the Reverse Logistics Operational Excellence Award for 2024 by the Reverse Logistics Association. Join us for the new era of conscious commerce. About the Sr. Data Engineer role We are seeking a skilled Sr. Data Engineer to join our data team. As the Sr. Data Engineer, you will be responsible for making architectural decisions to improve critical data processing, analytics pipelines, developing best practices, and building a platform to support the analytics and machine learning products. You will work to improve the data platform's reliability, resiliency, and scalability in order to support the data analysts, data scientists, and all of the data stakeholders. An ideal candidate brings curiosity, a passion for data, and a deep understanding of the technologies behind data pipelines, data warehousing, big data and analytics. Our current data tooling includes Redshift, Snowflake, AWS, Stitch, dbt, Looker, Hex, Terraform. Trove has a culture that values excellence, integrity, and creative solutions. We are looking for team members who respect each others' differences, pursue solutions to hard problems, learn from our mistakes, and challenge the status quo. The Trove team and our partners are working to grow a segment of the retail industry that is truly a win-win - our partners can grow their business, gain new customers, and make retail more sustainable by giving their products a second (or third, or fourth) life. What You'll Do Extend and optimize the current ELT pipeline to help assemble complex data sets to address a diverse set of business and data analytics requests. Monitor the performance of data pipelines, troubleshoot and resolve data issues. Design and develop data marts based on business requirements and data architecture principles. Optimize data mart schema and queries for performance and scalability. Perform data validation and quality assurance activities to ensure accuracy and integrity of data Create and maintain documentation for data mart designs, ETL processes, and data mappings Evolve tools, best practices and processes to enable the Catalog Integration team to ensure catalog quality and freshness Stay up-to-date with the latest data engineering technologies and best practices Experience You Bring Bachelor's degree in Engineering, or a related field (or equivalent practical experience) 8+ years of experience in data engineering or a similar role Strong expertise in data modeling, developing and implementing data mart and data warehouse solutions Proven experience as a data mart developer or similar role in data warehousing projects with strong working knowledge and understanding of the data warehouse design methods, including data vault and dimensional modeling Solid understanding of data modeling concepts (star schema, snowflake schema) and data warehouse design principles Strong experience and proficiency in SQL, dbt, Redshift administration, including performance tuning, WLM and DMS Proven experience in managing and administrating distributed databases Experience in building data lakes, AWS and data-related services (e.g., Athena, S3, Glue) Experience with Snowflake and Python is preferred Familiarity with data governance practices and principles Strong analytical skills and problem-solving aptitude with excellent communication and collaboration skills Bonus Points If You Have Experience with our stack: Redshift, dbt, AWS, Looker Experience with eCommerce Why Work Here? Flexible Remote Workplace - Trove's flexible schedule and remote workplace allows employees to take control over their schedule, which is invaluable when it comes to attending to the needs of personal and work life needs. Open PTO - Our flexible vacation company policy allows employees to take the time they need to rest and enjoy themselves outside work. 401K- Employees can set aside pre-tax dollars to invest in a qualified retirement investment plan with a company match. Medical- Trove covers 100% of employee monthly premium. Equity/Share Options - The longer the employee works, the more stocks become available to purchase. Learning Fund- Exempt employees have access to a pool of funds to support execution of their self-development goals and objectives. Partner Discounts- Enjoy eligibility for Pro Accounts and various partner discounts from Patagonia, Arcteryx and Eileen FIsher Mission Driven- Work alongside passionate people to build the leading recommerce infrastructure and the circular economy. Trove is committed to creating an equitable and transparent workplace. The annual compensation range for this position is $125,513-$185,850 plus equity. Final compensation will be determined based on experience, location and skills and may vary from the range listed above. Trove is proud to be an equal opportunity company but we don't want to stop there. We strive to be an anti-racist company and we believe that diversity and gender balance will help us reach our full potential. At Trove we are building a company where we are not only proud of our mission, but how we are pursuing it. We are committed to equal opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, or veteran status and we will consider qualified applicants with arrest and conviction records for employment.",
        "url": "https://www.linkedin.com/jobs/view/3964671659",
        "summary": "Trove is seeking a Sr. Data Engineer to join its data team. The role involves building and improving data processing pipelines, developing best practices, and supporting analytics and machine learning products. The ideal candidate has strong expertise in data modeling, data warehousing solutions, and experience with SQL, dbt, Redshift, AWS, and data-related services.",
        "industries": [
            "Retail",
            "Technology",
            "Ecommerce",
            "Sustainability",
            "Circular Economy",
            "Data Analytics",
            "Machine Learning",
            "Software Development"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Analytical",
            "Curiosity",
            "Passion for Data"
        ],
        "hard_skills": [
            "Data Modeling",
            "Data Warehousing",
            "Data Mart Development",
            "Data Vault",
            "Dimensional Modeling",
            "SQL",
            "dbt",
            "Redshift",
            "Performance Tuning",
            "WLM",
            "DMS",
            "Distributed Databases",
            "Data Lakes",
            "AWS",
            "Athena",
            "S3",
            "Glue",
            "Snowflake",
            "Python",
            "Data Governance"
        ],
        "tech_stack": [
            "Redshift",
            "Snowflake",
            "AWS",
            "Stitch",
            "dbt",
            "Looker",
            "Hex",
            "Terraform",
            "Athena",
            "S3",
            "Glue"
        ],
        "programming_languages": [
            "SQL",
            "Python"
        ],
        "experience": 8,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Engineering",
                "Related Field"
            ]
        },
        "salary": {
            "max": 185850,
            "min": 125513
        },
        "benefits": [
            "Flexible Remote Workplace",
            "Open PTO",
            "401K",
            "Medical",
            "Equity/Share Options",
            "Learning Fund",
            "Partner Discounts"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Diego, CA",
        "job_id": 3914855005,
        "company": "Gunderson Dettmer",
        "title": "Data Engineer",
        "created_on": 1720635421.4433599,
        "description": "Gunderson Dettme r is the only business law firm of its kind - exclusively serving the global venture capital and emerging technology marketplace. With 400 attorneys in eleven offices - from Silicon Valley to Singapore - we innovate for innovators, accelerate entrepreneurship, and help build companies at every stage of the growth lifecycle. We are committed to being the employer of choice by working together to create an environment, in which each of our people can grow, take initiative, and develop a fun, fulfilling and financially rewarding career. Benefits In addition to offering competitive salaries, we also offer an excellent benefit package, which includes full medical, dental and vision coverage; 401(k) Profit Sharing Plan; Flexible Spending Account and Paid Time Off. Job Description Join Gunderson Dettmer, the preeminent international law firm with an exclusive focus on the innovation economy, as a full time Data Engineer to work at the intersection of technology, law, data and innovation. We’re at the forefront of legal innovation and are actively developing and refining the law firm tech stack of the future. This unique position offers the opportunity to help architect the modern law firm’s data infrastructure, power key applications to solve problems for legal practitioners, and craft the integration layer between these bespoke tools and our data warehouse. We seek a talented individual who thrives in a collaborative, cross-functional environment and embodies a commitment to precision, repeatability, and quality. Responsibilities: Design, build and manage robust, end-to-end ETL processes with performance monitoring Create data integrations between various cloud-based software solutions and platforms (including our enterprise data warehouse) using iPaaS tools Manage and optimize enterprise data warehouse on RDBMS with a complex Dim/Fact data model for analytics Manage and optimize cloud infrastructure to ensure scalability, reliability, and cost-effectiveness Work with cross-functional teams—including legal engineers, data visualization specialists, business solutions and product specialists and subject-matter experts—to ensure efficient and coherent data processes and delivery of high-quality data products Improve and maintain quality control, implementing best practices to ensure data products meet our standards of reliability, usability, and performance Proactively identify opportunities to improve data quality, tooling, and version controls Requirements: 5+ years of relevant experience designing, building and managing ETL processes with a variety of tools Significant experience with: iPaaS tools like Apache AirFlow API technologies such as REST and GraphQL and transfering files using SFTP Data transformation tools like dbt RDBMS platforms like Snowflake noSQL databases like Mongo Dim/Fact data models for analytics Proficiency writing Python scripts and SQL queries (with experience writing Mongo Atlas aggregation queries considered a plus) Exceptional problem-solving skills, ability to work independently, and a collaborative spirit Excellent communication skills, underpinning effective collaboration with various teams and subject-matter experts Nice-to-haves: Experience with Google Cloud Platform (particularly IAM permissions, as well as log analytics, queries and dashboards) Experience with other Snowflake platform features like Snowsight and Cortex Experience in a financial services, professional services or law firm environment Experience in, or familiarity with, the venture-backed company or venture capital space, especially including familiarity with private company equity data schemas and transaction modeling Location: Work out of any of our firm’s U.S. offices or remotely. Our Offer: Join us as a Data Engineer and be at the forefront of innovation and change in the legal industry. We offer a dynamic, supportive, and collaborative work environment, where our contributions will shape the future of legal service delivery. Be part of our journey to optimize the way we serve our clients and make a lasting impact on the legal landscape. The expected starting salary for this position is $100,00 - $200,000 annually, dependent upon qualifications, experience and location. Gunderson Dettmer is an Equal Opportunity Employer and does not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class. By applying to this job you acknowledge that you have read the California Consumer Privacy Act Applicant Notice View Powered by JazzHR EsY8BbXWZf",
        "url": "https://www.linkedin.com/jobs/view/3914855005",
        "summary": "Gunderson Dettmer, a law firm specializing in venture capital and emerging technology, is seeking a Data Engineer to join their team. This role will involve designing, building, and managing data pipelines, integrating various software solutions, and optimizing data infrastructure. The ideal candidate will have 5+ years of experience in ETL processes, iPaaS tools, API technologies, data transformation tools, RDBMS platforms, and noSQL databases. Experience with Python scripting, SQL queries, and Google Cloud Platform is a plus. The position offers a competitive salary of $100,000 - $200,000 annually and the opportunity to work in a dynamic and collaborative environment.",
        "industries": [
            "Law",
            "Venture Capital",
            "Technology",
            "Financial Services",
            "Professional Services"
        ],
        "soft_skills": [
            "Problem-solving",
            "Collaboration",
            "Communication",
            "Independent Work"
        ],
        "hard_skills": [
            "ETL",
            "iPaaS",
            "API",
            "REST",
            "GraphQL",
            "SFTP",
            "Data Transformation",
            "RDBMS",
            "NoSQL",
            "Python",
            "SQL",
            "Mongo Atlas",
            "Google Cloud Platform",
            "IAM",
            "Log Analytics",
            "Snowflake",
            "Snowsight",
            "Cortex"
        ],
        "tech_stack": [
            "Apache AirFlow",
            "REST",
            "GraphQL",
            "SFTP",
            "dbt",
            "Snowflake",
            "Mongo",
            "Python",
            "SQL",
            "Google Cloud Platform",
            "IAM",
            "Snowsight",
            "Cortex"
        ],
        "programming_languages": [
            "Python",
            "SQL"
        ],
        "experience": 5,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 200000,
            "min": 100000
        },
        "benefits": [
            "Competitive Salary",
            "Full Medical, Dental and Vision Coverage",
            "401(k) Profit Sharing Plan",
            "Flexible Spending Account",
            "Paid Time Off"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Fremont, CA",
        "job_id": 3859905730,
        "company": "Info Way Solutions",
        "title": "Data engineer",
        "created_on": 1720635422.9654832,
        "description": "Data Engineer Montreal CA ( Hybrid - 3days in a week) Responsibilities Defining and evaluating key metrics and understanding what moves them and why Ownership of conceptualizing, developing, and maintaining dashboards and visualizations Investigating evolving fraud trends to extract patterns, identify root causes and propose actionable solutions Communicating analyses and recommendations to cross functional stakeholders for decision making Empowering the team to answer data questions quickly and easily by building high-quality ground truth data sets Build machine learning models to detect high risk users and financial crime activities such as money laundering Required Skills 8+ years of professional industry experience in a quantitative analysis role Comfortable in SQL and some experience with a programming language (Python or R a plus) Ability to communicate clearly and effectively to cross functional partners of varying technical levels Ability to define relevant metrics that can guide and influence stakeholders to the appropriate and accurate insights Experience or willingness to learn tools to create data pipelines using Airflow Building clear and easy to understand dashboards (Tableau) and presentations . Thanks & Regards, Saravanan.R |Infowaygroup.com| Direct: (925)464-1116 Work: (925)-592-6160 Ext 111 Saravanan @infowaygroup.com Info Way Solutions LLC, Fremont, CA",
        "url": "https://www.linkedin.com/jobs/view/3859905730",
        "summary": "Data Engineer needed for a hybrid role in Montreal. Responsibilities include defining metrics, building dashboards, investigating fraud trends, communicating analysis, building ground truth data sets, and developing machine learning models. Requires 8+ years of experience in a quantitative analysis role, proficiency in SQL, and Python or R. Experience with Airflow, Tableau, and creating dashboards and presentations is a plus.",
        "industries": [
            "Data Engineering",
            "Finance",
            "Fraud Detection",
            "Machine Learning"
        ],
        "soft_skills": [
            "Communication",
            "Data Analysis",
            "Problem Solving",
            "Collaboration",
            "Stakeholder Management"
        ],
        "hard_skills": [
            "SQL",
            "Python",
            "R",
            "Airflow",
            "Tableau"
        ],
        "tech_stack": [
            "SQL",
            "Python",
            "R",
            "Airflow",
            "Tableau"
        ],
        "programming_languages": [
            "Python",
            "R"
        ],
        "experience": 8,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Burbank, CA",
        "job_id": 3822847634,
        "company": "Accroid Inc",
        "title": "Lead Data Engineer",
        "created_on": 1720635425.0609148,
        "description": "Top Skills #1snowflake experience scala, spark and python java is NOT needed Contribute to maintaining, updating, and expanding existing Core Data platform data pipelines Build tools and services to support data discovery, lineage, governance, and privacy Collaborate with other software/data engineers and cross-functional teams Tech stack includes Airflow, Spark, Databricks, Delta Lake, and Snowflake Collaborate with product managers, architects, and other engineers to drive the success of the Core Data platform Contribute to developing and documenting both internal and external standards and best practices for pipeline configurations, naming conventions, and more Ensure high operational efficiency and quality of the Core Data platform datasets to ensure our solutions meet SLAs and project reliability and accuracy to all our stakeholders (Engineering, Data Science, Operations, and Analytics teams) Be an active participant and advocate of agile/scrum ceremonies to collaborate and improve processes for our team Engage with and understand our customers, forming relationships that allow us to understand and prioritize both innovative new offerings and incremental platform improvements Maintain detailed documentation of your work and changes to support data quality and data governance requirements Qualifications 5+ years of data engineering experience developing large data pipelines Proficiency in at least one major programming language (e.g. Python,Java, Scala) Strong SQL skills and ability to create queries to analyze complex datasets Hands-on production environment experience with distributed processing systems such as Spark Hands-on production experience with data pipeline orchestration systems such as Airflow for creating and maintaining data pipelines Experience with at least one major Massively Parallel Processing (MPP) or cloud database technology (Snowflake, Databricks, Big Query). Experience in developing APIs with GraphQL Deep Understanding of AWS or other cloud providers as well as infrastructure as code Familiarity with Data Modeling techniques and Data Warehousing standard methodologies and practices Strong algorithmic problem-solving expertise Excellent written and verbal communication Advance understanding of OLTP vs OLAP environments Willingness and ability to learn and pick up new skill sets Self-starting problem solver with an eye for detail and excellent analytical and communication skills Strong background in at least one of the following: distributed data processing or software engineering of data services, or data modeling Familiar with Scrum and Agile methodologies Bachelor’s Degree in Computer Science, Information Systems equivalent industry experience",
        "url": "https://www.linkedin.com/jobs/view/3822847634",
        "summary": "This is a data engineer role focusing on building and maintaining a Core Data platform with a focus on Snowflake, Spark, Databricks, and Airflow.  The role requires strong SQL skills, experience with distributed processing systems like Spark and data pipeline orchestration tools like Airflow.  The ideal candidate will have experience with MPP databases and cloud providers like AWS, along with knowledge of data modeling and warehousing methodologies.  Strong communication and collaboration skills are essential, as the role involves working with cross-functional teams.",
        "industries": [
            "Technology",
            "Software Development",
            "Data Analytics",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Analytical Skills",
            "Detail Oriented",
            "Self-Starter",
            "Teamwork",
            "Agile",
            "Scrum",
            "Customer Focus",
            "Relationship Building"
        ],
        "hard_skills": [
            "Snowflake",
            "Scala",
            "Spark",
            "Python",
            "SQL",
            "Airflow",
            "Databricks",
            "Delta Lake",
            "AWS",
            "GraphQL",
            "Data Modeling",
            "Data Warehousing",
            "Distributed Processing",
            "Software Engineering"
        ],
        "tech_stack": [
            "Snowflake",
            "Spark",
            "Databricks",
            "Delta Lake",
            "Airflow",
            "AWS",
            "GraphQL"
        ],
        "programming_languages": [
            "Scala",
            "Python",
            "SQL",
            "Java"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Information Systems"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Diego, CA",
        "job_id": 3888451479,
        "company": "Wise Skulls",
        "title": "Data Engineer",
        "created_on": 1720635429.2473485,
        "description": "Job Title : Data Engineer Location : San Diego, CA (Remote Is Fine) Duration : 6+ Months (Possibility Of Extension) Implementation Partner : TCS End Client : To Be Disclosed Jd Working with our stakeholders to develop end to end Cloud based solutions with a heavy focus on applications and data. Collaborate with BI/BA analyst, Data scientists, Data Engineers, Product Managers and other stakeholders across the organization. Ensure the delivery of reliable software and data pipelines using data engineering best practices, including secure automation, version control, continuous integration/delivery, proper testing. Ownership of the product and will significantly influence on our strategy by helping define the next wave of data insights and system architecture. A commitment to teamwork and excellent business and interpersonal skills are essential. You will be an essential part of our growing analytics and data insights team, and be responsible for our technological and architectural vision. The Ideal Candidate You have a minimum of 3 years' experience and hands on practical experience in data integration, engineering and technological analytics. You have a degree in Science, Technology, Engineering, or Mathematics Related Discipline Excellent skills in; SQL, Python, distributed source control such as GIT in an Agile-Scrum environment. Experience with ETL pipelines and Airflow Has a strong understanding of dimensional modelling and data warehousing methodologies. Can identify ways to improve data quality And reliability. Can use data to discover different tasks for automation. Is aligned with the latest data trends and ways to simplify data insights. Is passionate about data and the insights that large amounts of data sets can provide Experience within the retail industry is a plus.",
        "url": "https://www.linkedin.com/jobs/view/3888451479",
        "summary": "Data Engineer needed to develop end-to-end cloud-based solutions with a focus on applications and data.  Responsibilities include collaborating with stakeholders, ensuring reliable software and data pipelines, owning the product, and influencing strategy.  Experience with data integration, engineering, SQL, Python, GIT, ETL pipelines, Airflow, dimensional modeling, and data warehousing methodologies is required. Retail industry experience is a plus.",
        "industries": [
            "Retail",
            "Technology",
            "Data Analytics",
            "Cloud Computing",
            "Software Engineering"
        ],
        "soft_skills": [
            "Collaboration",
            "Teamwork",
            "Communication",
            "Problem-solving",
            "Strategic thinking",
            "Passion for Data"
        ],
        "hard_skills": [
            "SQL",
            "Python",
            "GIT",
            "ETL Pipelines",
            "Airflow",
            "Dimensional Modeling",
            "Data Warehousing",
            "Data Integration",
            "Data Engineering"
        ],
        "tech_stack": [
            "Cloud Computing",
            "Data Pipelines",
            "ETL",
            "Airflow",
            "GIT",
            "Agile-Scrum"
        ],
        "programming_languages": [
            "SQL",
            "Python"
        ],
        "experience": 3,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Science",
                "Technology",
                "Engineering",
                "Mathematics"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Los Angeles, CA",
        "job_id": 3952686009,
        "company": "Resource Logistics Inc.",
        "title": "Data Engineer",
        "created_on": 1720635430.89601,
        "description": "What Youll Do \" Lead technical data solutions, including custom development working with cross project teams in delivering projects. \" Define overall ETL/ELT Clienthitecture including key designs on integration standards such as loading real time/batch data, CDC, data validation, data enrichment etc. \" Clienthitect solutions based on specific project requirements, considering best practices and performance standards while promoting reusability \" Building data model and semantic layer with automated data exception framework \" Ability to manage and participate as a senior Data Analyst gathering and analysis of source data, processing logic, and operational system usage. \" Responsible for the solution design, hands-on development, technical tasks oversight, release management and implementation of data products and features. \" Deep hands-on knowledge of data integration and data pipeline methodologies and API platforms \" Hands-on engineer working with other cross functional teams following agile methodologies. \" Analyze issues, reverse engineer where needed to come up with solutions to resolve issues in a timely manner \" Responsible for maintaining teams commitment to excellence and high standards in a collaborative environment \" Work with team to align solutions and data integration with business strategy and objectives. \" Apply broad in depth business and technical knowledge advance technical direction. Skill Set Requirements \" 6+ years in a direct role as a Developer / designer / Clienthitect for ETL, data warehouse and data lake systems. \" 4+ years of solid data warehousing, integration methodology experience. \" 4+ Years - Data modeling experience to deliver both logical model & physical design for transaction and analytical systems. \" Candidate must have strong technical expertise in SQL and Snowflake. \" Must have advanced technical understanding with tools and products used in data warehouse and data integration development, such as Pyhton, Airflow, Glue, DBT, Workato \" Solid hands-on experience in data modeling, data feature engineering. \" Broad exposure in the new techniques in data warehousing and data integration technology \" Experience in data lake Clienthitecture and design \" Strong understanding of data Clienthitecture in a AWS cloud environment \" Able to communicate effectively with all levels of management in a clear and professional manner; verbally and written. \" Strong technical design and documentation skills",
        "url": "https://www.linkedin.com/jobs/view/3952686009",
        "summary": "Lead technical data solutions involving custom development, ETL/ELT architecture, data modeling, data integration, and data pipeline methodologies. This role requires strong technical expertise in SQL, Snowflake, Python, Airflow, Glue, DBT, and Workato, as well as experience in data warehousing, data lake architecture, and AWS cloud environments. Responsibilities include hands-on development, solution design, technical oversight, and collaboration with cross-functional teams.",
        "industries": [
            "Data Analytics",
            "Data Engineering",
            "Data Science",
            "Software Development",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Communication",
            "Leadership",
            "Collaboration",
            "Problem Solving",
            "Analytical Thinking",
            "Time Management"
        ],
        "hard_skills": [
            "SQL",
            "Snowflake",
            "Python",
            "Airflow",
            "Glue",
            "DBT",
            "Workato",
            "ETL",
            "ELT",
            "Data Warehousing",
            "Data Lake",
            "Data Modeling",
            "Data Integration",
            "Data Pipeline",
            "Data Feature Engineering",
            "AWS Cloud"
        ],
        "tech_stack": [
            "Snowflake",
            "Python",
            "Airflow",
            "Glue",
            "DBT",
            "Workato",
            "AWS"
        ],
        "programming_languages": [
            "Python",
            "SQL"
        ],
        "experience": 6,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Data Science",
                "Engineering"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Sacramento, CA",
        "job_id": 3857810815,
        "company": "The Shipyard",
        "title": "Senior Data Developer",
        "created_on": 1720635432.4209256,
        "description": "The Shipyard is a leading, independent agency that builds performance-driven brands audiences can't help but love. By applying modern mindsets to established models, we fuel brand and marketing decisions that are more courageous and more validated. Our ability to align bold creativity with individual consumer motivations lets us do more than hope for brand love - we methodically engineer it throughout the consumer journey. That's how we activate the synergistic power of brand building and performance marketing. Engineering Brand Love through the courageous ambitions of our people and our clients. The agency has significant momentum and is on a roll with new business. Our team continues to expand across four cities (Columbus, Newport Beach, Sacramento, and San Diego) and we're still growing. Come join our team! Summary We are looking for a smart, experienced, and motivated Senior Data Developer to join our Sacramento team. The Senior Data Developer will serve as a coach for other Data Developers, the role requires daily hands-on development execution and technology roadmap leadership. The Senior Data Developer is responsible for leading end-to-end data development, from building data architectures for robust data sets to collecting & modeling data. You will work with internal departments to define their analytic requirements and set up their corresponding data environments. You will be a valued member with the opportunity to shape the way that we collect and share data, influencing processes and efficiency across the agency and its clientele. This is a hybrid work-from-home/office position (in-office attendance is required on Tuesdays & Thursdays) and the candidate must live near Sacramento, CA . At The Shipyard, we know that it's your unique talents, backgrounds, and perspectives that make you who you are, just like our team, who come from various career paths and experiences. We believe we can't be truly diverse without bringing your most authentic self to the agency. Studies show that people from marginalized communities may not always apply for positions if they don't meet every single requirement in a job posting. At The Shipyard, we encourage you to apply even if you don't meet all the requirements. We believe that your passion for what we stand for — our values and purpose — is just as important as meeting every checkbox. What You'll Do Lead data needs for clients and internal teams in an agile work environment Collect data requirements from internal teams to assist in the design of a scalable infrastructure to drive decision-making using technology and BI tools Manage the retrieval, compiling, and formatting of large volumes of data Set up data environments, in partnership with the Data Visualization team, within business intelligence platforms, ultimately creating advertising reports Learn new technologies as they evolve, understand their impact to systems, and lead the team in proposed tech stack evolutions Leadership in implementing technological advancements as well as educating internal and external partners Collaborate with internal teams and users to understand new opportunities for support Documentation of data environments and infrastructures developed What You'll Bring 5-6 years equivalent work experience in a Data Developer or similar Data & Analytics Role Experience in data collection, transformation, development, and/or reporting with optimized outputs for BI tools Experience developing and optimizing data ingestion processes with tools such as Fivetran, Stitch, and even writing custom API scripts. Experience designing, building, and maintaining robust data pipelines (ETL/ELT) using technologies like Postgres, dbt, and Airflow. Experience with AWS services, Kubernetes, and Docker. Experience with data modeling / semantic modeling Experience with data warehousing architecture (Postgres, AWS Redshift) Experience in applying data processes and deploying BI updates at scale A testing mindset - Knowledge and experience of CI/CD pipelines and test environments Proficiency in SQL, Python, and/or Scala/Java Experience contributing to and influencing tech stack evolutions and infrastructure roadmap Ability to write code with clean documentation Strong entrepreneurial drive Strong verbal and written communication skills and a collaborative, problem-solving mindset Self-starting with initiative and passionate about learning the world of data strategy, architecture, processing, and development Experience balancing multiple deliverables across multiple clients and data sets Experience with /awareness of cloud-based visualization tools such as Tableau, Looker, or Periscope Marketing & advertising knowledge (preferred, not required) What You'll Get The overall target range of base compensation for this role is $110,000 - $137,000. Compensation offered will be determined by additional factors such as location and experience 40 hours of paid sick time (annually) Open PTO Flexible work hours and remote work Paid holidays + holiday closure between Christmas Eve and New Year's Day Company-paid medical, dental, and vision insurance Life insurance and disability benefits 401k program with employer matching 6 weeks paid parental leave Employee bonus referrals Dog-friendly offices Company-provided snacks and beverages (yes, beer/wine included) ... and lots more! Requirements Must be authorized to work in the U.S. without the need for visa sponsorship.",
        "url": "https://www.linkedin.com/jobs/view/3857810815",
        "summary": "The Shipyard, a leading marketing agency, is seeking a Senior Data Developer to join their Sacramento team. This role involves leading end-to-end data development, managing data pipelines, and providing technical leadership. The ideal candidate will have experience with data collection, transformation, and reporting, as well as proficiency in SQL, Python, and AWS services.",
        "industries": [
            "Marketing",
            "Advertising",
            "Data Analytics",
            "Software Development",
            "Business Intelligence"
        ],
        "soft_skills": [
            "Leadership",
            "Communication",
            "Collaboration",
            "Problem-solving",
            "Entrepreneurial",
            "Self-starter",
            "Passion for learning"
        ],
        "hard_skills": [
            "Data collection",
            "Data transformation",
            "Data development",
            "Data reporting",
            "Data warehousing",
            "Data modeling",
            "ETL/ELT",
            "SQL",
            "Python",
            "Scala",
            "Java",
            "AWS",
            "Kubernetes",
            "Docker",
            "Postgres",
            "dbt",
            "Airflow",
            "CI/CD",
            "Tableau",
            "Looker",
            "Periscope",
            "Fivetran",
            "Stitch",
            "API scripting"
        ],
        "tech_stack": [
            "AWS",
            "Kubernetes",
            "Docker",
            "Postgres",
            "dbt",
            "Airflow",
            "Fivetran",
            "Stitch",
            "Tableau",
            "Looker",
            "Periscope"
        ],
        "programming_languages": [
            "SQL",
            "Python",
            "Scala",
            "Java"
        ],
        "experience": 5,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 137000,
            "min": 110000
        },
        "benefits": [
            "Paid sick time",
            "Open PTO",
            "Flexible work hours",
            "Remote work",
            "Paid holidays",
            "Company-paid medical, dental, and vision insurance",
            "Life insurance",
            "Disability benefits",
            "401k with employer matching",
            "Paid parental leave",
            "Employee bonus referrals",
            "Dog-friendly offices",
            "Company-provided snacks and beverages"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3897977670,
        "company": "Unreal Staffing, Inc",
        "title": "Senior Big Data Engineer",
        "created_on": 1720635435.8520606,
        "description": "Company Overview: Welcome to the forefront of data-driven innovation! Our company is committed to harnessing the power of big data to drive transformative change and solve complex problems across industries. We're dedicated to building scalable and efficient big data solutions that enable advanced analytics, machine learning, and business intelligence. Join us and be part of a dynamic team shaping the future of big data engineering. Position Overview: As a Senior Big Data Engineer, you'll play a pivotal role in designing, building, and maintaining our big data infrastructure and pipelines. You'll work on challenging projects, from data ingestion and processing to data storage and retrieval, to support the needs of our data-driven organization. If you're a seasoned engineer with expertise in big data technologies and a passion for building robust data systems, we want you on our team. Requirements Key Responsibilities: Big Data Infrastructure Design: Design, architect, and implement scalable and efficient big data infrastructure, including data lakes, data warehouses, and streaming data platforms, leveraging technologies such as Hadoop, Spark, Kafka, and more Data Pipeline Development: Develop, deploy, and manage data pipelines for ingesting, processing, and transforming large volumes of structured and unstructured data from diverse sources, ensuring reliability, scalability, and performance Data Modeling: Design and implement data models and schemas to support analytical and operational requirements, ensuring data integrity, consistency, and performance in a distributed environment Data Integration: Integrate data from disparate sources and systems, ensuring data consistency, quality, and completeness throughout the data lifecycle Performance Optimization: Optimize data pipelines and queries for performance and efficiency, identifying and addressing bottlenecks and inefficiencies to improve system scalability and reliability Monitoring and Alerting: Implement monitoring and alerting systems to track big data platform performance and health, detecting and mitigating issues proactively to minimize downtime and data loss Security and Compliance: Implement security controls and data governance policies to ensure data security, privacy, and compliance with regulatory requirements Documentation and Best Practices: Document big data infrastructure and pipelines, providing clear and comprehensive documentation to facilitate understanding and collaboration among team members Collaboration: Collaborate with cross-functional teams, including data scientists, software engineers, and business stakeholders, to understand requirements and deliver big data solutions that meet business needs Mentorship and Leadership: Mentor junior engineers, providing guidance, support, and technical leadership in big data engineering best practices and technologies Qualifications: Bachelor's degree or higher in Computer Science, Engineering, Mathematics, or related field 5+ years of experience in big data engineering, with a focus on designing, building, and maintaining big data infrastructure and pipelines Proficiency in programming languages such as Java, Scala, or Python, and experience with big data technologies such as Hadoop, Spark, Kafka, Hive, HBase, and more Strong understanding of distributed systems and parallel processing, with experience designing and optimizing data pipelines for performance and scalability Experience with cloud platforms such as AWS, Azure, or Google Cloud Platform, and familiarity with cloud-based big data services such as Amazon EMR, Azure HDInsight, or Google Dataproc Experience with SQL and NoSQL databases, data warehousing, and ETL/ELT processes Strong problem-solving skills and analytical thinking, with the ability to troubleshoot complex data issues and optimize system performance Excellent communication and collaboration skills, with the ability to work effectively in cross-functional teams and communicate technical concepts to non-technical stakeholders Benefits Competitive salary: The industry standard salary for Senior Big Data Engineers typically ranges from $170,000 to $230,000 per year, depending on experience and qualifications Comprehensive health, dental, and vision insurance plans Flexible work hours and remote work options Generous vacation and paid time off Professional development opportunities, including access to training programs, conferences, and workshops State-of-the-art technology environment with access to cutting-edge tools and resources Vibrant and inclusive company culture with opportunities for growth and advancement Exciting projects with real-world impact at the forefront of big data innovation Join Us: Ready to shape the future of big data engineering? Apply now to join our team and be part of the data revolution!",
        "url": "https://www.linkedin.com/jobs/view/3897977670",
        "summary": "We are seeking a Senior Big Data Engineer to design, build, and maintain our big data infrastructure and pipelines, leveraging technologies like Hadoop, Spark, Kafka, and more. You will be responsible for data ingestion, processing, modeling, integration, optimization, monitoring, security, and collaboration with cross-functional teams. We offer competitive salaries, comprehensive benefits, flexible work hours, remote work options, professional development opportunities, and a vibrant company culture.",
        "industries": [
            "Big Data",
            "Data Engineering",
            "Data Analytics",
            "Machine Learning",
            "Business Intelligence",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Problem-solving",
            "Analytical thinking",
            "Communication",
            "Collaboration",
            "Leadership",
            "Mentorship"
        ],
        "hard_skills": [
            "Java",
            "Scala",
            "Python",
            "Hadoop",
            "Spark",
            "Kafka",
            "Hive",
            "HBase",
            "SQL",
            "NoSQL",
            "ETL/ELT",
            "AWS",
            "Azure",
            "Google Cloud Platform",
            "Amazon EMR",
            "Azure HDInsight",
            "Google Dataproc"
        ],
        "tech_stack": [
            "Hadoop",
            "Spark",
            "Kafka",
            "Hive",
            "HBase",
            "AWS",
            "Azure",
            "Google Cloud Platform",
            "Amazon EMR",
            "Azure HDInsight",
            "Google Dataproc"
        ],
        "programming_languages": [
            "Java",
            "Scala",
            "Python"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Computer Science",
                "Engineering",
                "Mathematics"
            ]
        },
        "salary": {
            "max": 230000,
            "min": 170000
        },
        "benefits": [
            "Competitive salary",
            "Health insurance",
            "Dental insurance",
            "Vision insurance",
            "Flexible work hours",
            "Remote work options",
            "Vacation",
            "Paid time off",
            "Professional development",
            "Training programs",
            "Conferences",
            "Workshops",
            "State-of-the-art technology",
            "Vibrant company culture",
            "Growth opportunities",
            "Advancement opportunities"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3905235890,
        "company": "A Society US",
        "title": "Backend Software Engineer, Python",
        "created_on": 1720635437.44316,
        "description": "The Data Team leverages data from our autonomous vehicles and operations to determine autonomy and service readiness. We provide the foundation for strategic decision-making. You will help develop and implement the next generation of our data pipeline to ensure visibility into our business as we scale toward the launch of an autonomous mobility service. You will define the system and build the pipeline to enable our company to develop and scale with a data-first culture. Responsibilities Design, build, and maintain a platform used by our teams to build large-scale data pipelines Drive platform efficiency improvements to reduce latency and improve data freshness Support data-driven engineering decisions through improved observability and key metrics Partner with engineering teams to support long-term scaling of their data pipelines Hadoop, ...) Requirements Qualifications 5+ years of software engineering experience Strong fluency with Python Strong understanding of distributed systems Strong written and verbal communication skills Good experience building and maintaining data intensive production systems at scale Good experience with relational databases and non-relational (NoSQL) databases Good experience with infrastructure-as-code tools (e.g. Terraform, ...) Experience with large scale streaming platforms (e.g. Kafka, Kinesis) Bonus Qualifications Experience with data warehouse platforms (e.g. Redshift, BigQuery, Databricks, ...) Experience building scalable and maintainable data pipelines Experience with AWS ECS and/or Kubernetes Experience with a workflow manager such as Airflow Experience with large scale processing frameworks (e.g. Spark, Benefits Daily free breakfasts and lunches Health Care Plan (Medical, Dental & Vision) Life Insurance (Basic, Voluntary & AD&D) Paid Time Off (Vacation, Sick & Public Holidays) Training & Development Retirement Plan (401k, IRA)",
        "url": "https://www.linkedin.com/jobs/view/3905235890",
        "summary": "As a Data Engineer, you will develop and implement the next generation of a data pipeline for an autonomous mobility service. You'll design, build, and maintain a platform for large-scale data pipelines, ensuring visibility into the business as the company scales.",
        "industries": [
            "Automotive",
            "Transportation",
            "Technology",
            "Software Development",
            "Data Science"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Decision Making",
            "Time Management"
        ],
        "hard_skills": [
            "Python",
            "Distributed Systems",
            "Relational Databases",
            "NoSQL Databases",
            "Infrastructure as Code",
            "Streaming Platforms",
            "Data Warehousing",
            "Data Pipelines",
            "AWS ECS",
            "Kubernetes",
            "Airflow",
            "Spark",
            "Hadoop"
        ],
        "tech_stack": [
            "Python",
            "Kafka",
            "Kinesis",
            "Terraform",
            "Redshift",
            "BigQuery",
            "Databricks",
            "AWS ECS",
            "Kubernetes",
            "Airflow",
            "Spark",
            "Hadoop"
        ],
        "programming_languages": [
            "Python"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Data Science",
                "Engineering"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Free Breakfasts and Lunches",
            "Health Insurance",
            "Life Insurance",
            "Paid Time Off",
            "Training and Development",
            "Retirement Plan"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Fremont, CA",
        "job_id": 3818361185,
        "company": "Info Way Solutions",
        "title": "Data engineer",
        "created_on": 1720635439.1147346,
        "description": "Role: Data engineers (with strong Python and AWS experience) Locations : DC/VA/Client(Onsite) GC ,US CITIZEN & EAD Visa only Need Strong Hands-on experience Role Description Data Pipeline Development: Design, implement, and manage robust data pipelines using Python, PySpark, SQL to efficiently extract, transform, and load data from diverse sources(Batch & Streaming) AWS Expertise Demonstrate expertise in core AWS services such as AWS DMS, AWS Glue, AWS Step Functions, Amazon S3, Amazon Redshift, Amazon RDS, Amazon EMR, AWS IAM, AWS LAMBDA etc., and apply them to build scalable and reliable data solutions. Data Modeling Develop and maintain efficient data models to support the analytical and reporting needs. Database Management Administer databases using AWS services like Amazon RDS or Amazon Redshift, focusing on schema design, performance optimization, and monitoring. Data Warehousing Utilize Amazon Redshift or Amazon Snowflake to create high-performing analytical databases that empower data-driven decision-making. ETL Best Practices Implement industry best practices for ETL processes, including data validation, error handling, and data quality checks. Performance Optimization Optimize query performance through continuous tuning of databases and leveraging AWS's scalability capabilities. Monitoring And Logging Establish robust monitoring and logging mechanisms using AWS CloudWatch, Amazon CloudTrail, or comparable tools to ensure pipeline reliability. Security And Compliance Ensure adherence to security best practices and relevant compliance standards, tailoring solutions to meet GDPR, HIPAA, or other regulatory requirements. Automation Drive automation of deployment and scaling of data pipelines using infrastructure as code (IaC) tools like AWS CloudFormation and Terraform. Collaboration Collaborate closely with cross-functional teams, including data scientists, analysts, and other stakeholders, to understand their data needs and provide effective solutions. Continuous Learning Stay updated on the latest developments in AWS services and data engineering methodologies, applying new insights to enhance our data infrastructure. Soft Skills Exhibit strong communication skills to facilitate effective teamwork and interaction with diverse groups. Thanks & Regards, Saravanan.R |Infowaygroup.com| Direct: (925)464-1116 Work: (925)-592-6160 Ext 111 Saravanan@infowaygroup.com Info Way Solutions LLC, Fremont, CA",
        "url": "https://www.linkedin.com/jobs/view/3818361185",
        "summary": "Data Engineer with strong Python and AWS experience required for a client onsite role in DC/VA. Responsibilities include designing, implementing, and managing data pipelines using Python, PySpark, and SQL, leveraging AWS services like AWS DMS, AWS Glue, AWS Step Functions, Amazon S3, Amazon Redshift, Amazon RDS, Amazon EMR, AWS IAM, AWS LAMBDA, and more. Additionally, the role involves data modeling, database management, data warehousing, ETL best practices implementation, performance optimization, monitoring and logging, security and compliance, automation, and collaborative work with cross-functional teams.",
        "industries": [
            "Data Engineering",
            "Software Development",
            "Information Technology"
        ],
        "soft_skills": [
            "Communication",
            "Teamwork",
            "Collaboration",
            "Problem-Solving",
            "Analytical Thinking",
            "Critical Thinking"
        ],
        "hard_skills": [
            "Python",
            "PySpark",
            "SQL",
            "AWS DMS",
            "AWS Glue",
            "AWS Step Functions",
            "Amazon S3",
            "Amazon Redshift",
            "Amazon RDS",
            "Amazon EMR",
            "AWS IAM",
            "AWS LAMBDA",
            "Data Modeling",
            "Database Administration",
            "Data Warehousing",
            "ETL",
            "Performance Optimization",
            "Monitoring",
            "Logging",
            "Security",
            "Compliance",
            "Automation",
            "Infrastructure as Code (IaC)",
            "AWS CloudFormation",
            "Terraform"
        ],
        "tech_stack": [
            "Python",
            "PySpark",
            "SQL",
            "AWS DMS",
            "AWS Glue",
            "AWS Step Functions",
            "Amazon S3",
            "Amazon Redshift",
            "Amazon RDS",
            "Amazon EMR",
            "AWS IAM",
            "AWS LAMBDA",
            "AWS CloudFormation",
            "Terraform"
        ],
        "programming_languages": [
            "Python",
            "SQL"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Sunnyvale, CA",
        "job_id": 3879239649,
        "company": "LinkedIn",
        "title": "Staff Engineer - AI/Machine Learning",
        "created_on": 1720635440.6825244,
        "description": "LinkedIn is the world’s largest professional network, built to help members of all backgrounds and experiences achieve more in their careers. Our vision is to create economic opportunity for every member of the global workforce. Every day our members use our products to make connections, discover opportunities, build skills and gain insights. We believe amazing things happen when we work together in an environment where everyone feels a true sense of belonging, and that what matters most in a candidate is having the skills needed to succeed. It inspires us to invest in our talent and support career growth. Join us to challenge yourself with work that matters. This role will be based in Sunnyvale or San Francisco. At LinkedIn, we trust each other to do our best work where it works best for us and our teams. This role offers a hybrid work option, meaning you can work from home and commute to a LinkedIn office, depending on what’s best for you and when it is important for your team to be together. This is a full-time engineering role based in Sunnyvale, CA LinkedIn's Machine Learning Engineers are both data/research scientists and software engineers, who develop and implement machine learning models and algorithms.  Unlike other companies that separate these roles, our engineers work on projects from ideation to implementation. As a Staff Engineer at LinkedIn, you will be responsible for the development and training of cutting-edge machine learning models and algorithms that can effectively leverage our members’ platform activities and industry trends to create personalized recommendations that are delivered to our members at the optimal moment. Responsibilities: • Work with BIG data, crunching millions of samples for statistical modeling, data mining, recommendation solutions • Write production quality code and influence the next generation of LinkedIn’s system • Collaborate with 10+ machine learning engineers to deliver impact on LinkedIn newsfeed • Build scalable AI innovations with foundation and infra partners Basic Qualifications: • Bachelor’s degree in Computer Science or related technical field or equivalent practical experience • 4+ years of industry experience in software design, development, and algorithm related solutions. • 4+ years experience programming languages such as Java, C/C++, Python, etc. Preferred Qualifications: • 6+ years of relevant machine learning experience • MS or PhD in Computer Science or related technical discipline • Experience with machine learning, optimization algorithms, deep-learning techniques • Experience developing large scale systems • Experience with Spark, Tensorflow and Java. • Previous leadership or mentorship experience is preferred • Ability to diagnose technical problems, debug code, and automate routine tasks • Analytical approach coupled with solid communication skills and a sense of ownership • Track record of producing papers in conferences such as KDD, WWW, WSDM, and Patenting Innovations Suggested Skills: • Experience leading engineers to tackle a large-scale AI problem • Strong technical background & Strategic thinking • Experience in Machine Learning, Big Data and Deep Learning You will Benefit from our Culture: We strongly believe in the well-being of our employees and their families. That is why we offer generous health and wellness programs and time away for employees of all levels. LinkedIn is committed to fair and equitable compensation practices. The pay range for this role is $156,000-$255,000. Actual compensation packages are based on several factors that are unique to each candidate, including but not limited to skill set, depth of experience, certifications, and specific work location. This may be different in other locations due to differences in the cost of labor. The total compensation package for this position may also include annual performance bonus, stock, benefits and/or other applicable incentive compensation plans. For more information, visit https://careers.linkedin.com/benefits. Equal Opportunity Statement LinkedIn is committed to diversity in its workforce and is proud to be an equal opportunity employer. LinkedIn considers qualified applicants without regard to race, color, religion, creed, gender, national origin, age, disability, veteran status, marital status, pregnancy, sex, gender expression or identity, sexual orientation, citizenship, or any other legally protected class. LinkedIn is an Affirmative Action and Equal Opportunity Employer as described in our equal opportunity statement here: https://microsoft.sharepoint.com/:b:/t/LinkedInGCI/EeE8sk7CTIdFmEp9ONzFOTEBM62TPrWLMHs4J1C_QxVTbg?e=5hfhpE. Please reference https://www.eeoc.gov/sites/default/files/2023-06/22-088_EEOC_KnowYourRights6.12ScreenRdr.pdf and https://www.dol.gov/ofccp/regs/compliance/posters/pdf/OFCCP_EEO_Supplement_Final_JRF_QA_508c.pdf for more information. LinkedIn is committed to offering an inclusive and accessible experience for all job seekers, including individuals with disabilities. Our goal is to foster an inclusive and accessible workplace where everyone has the opportunity to be successful. If you need a reasonable accommodation to search for a job opening, apply for a position, or participate in the interview process, connect with us at accommodations@linkedin.com and describe the specific accommodation requested for a disability-related limitation. Reasonable accommodations are modifications or adjustments to the application or hiring process that would enable you to fully participate in that process. Examples of reasonable accommodations include but are not limited to: -Documents in alternate formats or read aloud to you -Having interviews in an accessible location -Being accompanied by a service dog -Having a sign language interpreter present for the interview A request for an accommodation will be responded to within three business days. However, non-disability related requests, such as following up on an application, will not receive a response. LinkedIn will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by LinkedIn, or (c) consistent with LinkedIn's legal duty to furnish information. Pay Transparency Policy Statement As a federal contractor, LinkedIn follows the Pay Transparency and non-discrimination provisions described at this link: https://lnkd.in/paytransparency. Global Data Privacy Notice for Job Candidates This document provides transparency around the way in which LinkedIn handles personal data of employees and job applicants: https://lnkd.in/GlobalDataPrivacyNotice",
        "url": "https://www.linkedin.com/jobs/view/3879239649",
        "summary": "LinkedIn is looking for a Staff Machine Learning Engineer to develop and train cutting-edge machine learning models and algorithms that leverage member platform activities and industry trends for personalized recommendations. The role involves working with big data, writing production quality code, collaborating with a team of engineers, and building scalable AI innovations.",
        "industries": [
            "Technology",
            "Software",
            "Artificial Intelligence",
            "Machine Learning",
            "Data Science",
            "Social Media"
        ],
        "soft_skills": [
            "Communication",
            "Leadership",
            "Collaboration",
            "Problem Solving",
            "Analytical Thinking",
            "Strategic Thinking",
            "Ownership"
        ],
        "hard_skills": [
            "Java",
            "C/C++",
            "Python",
            "Spark",
            "TensorFlow",
            "Machine Learning",
            "Optimization Algorithms",
            "Deep Learning",
            "Big Data"
        ],
        "tech_stack": [
            "Spark",
            "TensorFlow",
            "Java",
            "Python",
            "C/C++"
        ],
        "programming_languages": [
            "Java",
            "C/C++",
            "Python"
        ],
        "experience": 4,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Computer Science",
                "Related technical field"
            ]
        },
        "salary": {
            "max": 255000,
            "min": 156000
        },
        "benefits": [
            "Health and wellness programs",
            "Time away"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Jose, CA",
        "job_id": 3599611819,
        "company": "TikTok",
        "title": "Data Engineer, E-Commerce",
        "created_on": 1720635442.4175248,
        "description": "Responsibilities TikTok is the leading destination for short-form mobile video. Our mission is to inspire creativity and bring joy. TikTok has global offices including Los Angeles, New York, London, Paris, Berlin, Dubai, Mumbai, Singapore, Jakarta, Seoul and Tokyo. Why Join Us At TikTok, our people are humble, intelligent, compassionate and creative. We create to inspire - for you, for us, and for more than 1 billion users on our platform. We lead with curiosity and aim for the highest, never shying away from taking calculated risks and embracing ambiguity as it comes. Here, the opportunities are limitless for those who dare to pursue bold ideas that exist just beyond the boundary of possibility. Join us and make impact happen with a career at TikTok. The Global E-Commerce team focuses on building data infrastructure and data product areas to support business engineering teams working directly on TikTok's E-Commerce platform. As a data engineer in the Global E-Commerce team, you will have the opportunity to build, optimize and grow one of the largest data platforms in the world. You'll have the opportunity to gain hands-on experience on all kinds of systems in the data platform ecosystem. Your work will have a direct and huge impact on the company's core products as well as hundreds of millions of users. Responsibilities - What You'll Do • Design and build data transformations efficiently and reliably for different purposes (e.g. reporting, growth analysis, multi-dimensional analysis); • Design and implement reliable, scalable, robust and extensible big data systems that support core products and business; • Establish solid design and best engineering practice for engineers as well as non-technical people. Qualifications • BS or MS degree in Computer Science or related technical field or equivalent practical experience; • Experience in the Big Data technologies(Hadoop, M/R, Hive, Spark, Metastore, Presto, Flume, Kafka, ClickHouse, Flink etc.); • Experience with performing data analysis, data ingestion and data integration; • Experience with ETL(Extraction, Transformation & Loading) and architecting data systems; • Experience with schema design, data modeling and SQL queries; • Passionate and self-motivated about technologies in the Big Data area. TikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too. TikTok is committed to providing reasonable accommodations during our recruitment process. If you need assistance or an accommodation, please reach out to us at Dennis.Chau@tiktok.com Job Information: 【For Pay Transparency】Compensation Description (annually) The base salary range for this position in the selected city is $136000 - $280000 annually. ​ Compensation may vary outside of this range depending on a number of factors, including a candidate’s qualifications, skills, competencies and experience, and location. Base pay is one part of the Total Package that is provided to compensate and recognize employees for their work, and this role may be eligible for additional discretionary bonuses/incentives, and restricted stock units. ​ Our company benefits are designed to convey company culture and values, to create an efficient and inspiring work environment, and to support our employees to give their best in both work and life. We offer the following benefits to eligible employees: ​ We cover 100% premium coverage for employee medical insurance, approximately 75% premium coverage for dependents and offer a Health Savings Account(HSA) with a company match. As well as Dental, Vision, Short/Long term Disability, Basic Life, Voluntary Life and AD&D insurance plans. In addition to Flexible Spending Account(FSA) Options like Health Care, Limited Purpose and Dependent Care. ​ Our time off and leave plans are: 10 paid holidays per year plus 17 days of Paid Personal Time Off (PPTO) (prorated upon hire and increased by tenure) and 10 paid sick days per year as well as 12 weeks of paid Parental leave and 8 weeks of paid Supplemental Disability. ​ We also provide generous benefits like mental and emotional health benefits through our EAP and Lyra. A 401K company match, gym and cellphone service reimbursements. The Company reserves the right to modify or change these benefits programs at any time, with or without notice. ​",
        "url": "https://www.linkedin.com/jobs/view/3599611819",
        "summary": "TikTok is seeking a Data Engineer to join their Global E-Commerce team. This role involves designing and building data transformations for reporting, analysis, and supporting core products. The ideal candidate will have experience with Big Data technologies (Hadoop, Hive, Spark, Kafka, etc.) and a passion for data systems architecture.",
        "industries": [
            "Technology",
            "Social Media",
            "E-Commerce",
            "Data"
        ],
        "soft_skills": [
            "Passionate",
            "Self-Motivated",
            "Analytical",
            "Problem-Solving",
            "Communication",
            "Collaboration",
            "Teamwork",
            "Adaptability",
            "Innovation"
        ],
        "hard_skills": [
            "Hadoop",
            "M/R",
            "Hive",
            "Spark",
            "Metastore",
            "Presto",
            "Flume",
            "Kafka",
            "ClickHouse",
            "Flink",
            "Data Analysis",
            "Data Ingestion",
            "Data Integration",
            "ETL",
            "Data Systems Architecture",
            "Schema Design",
            "Data Modeling",
            "SQL",
            "Git",
            "Python",
            "Java"
        ],
        "tech_stack": [
            "Hadoop",
            "M/R",
            "Hive",
            "Spark",
            "Metastore",
            "Presto",
            "Flume",
            "Kafka",
            "ClickHouse",
            "Flink"
        ],
        "programming_languages": [
            "Python",
            "Java",
            "SQL"
        ],
        "experience": 3,
        "education": {
            "min_degree": "BS",
            "fields": [
                "Computer Science",
                "Related Technical Field"
            ]
        },
        "salary": {
            "max": 280000,
            "min": 136000
        },
        "benefits": [
            "Medical Insurance",
            "Dental Insurance",
            "Vision Insurance",
            "Short/Long Term Disability",
            "Basic Life Insurance",
            "Voluntary Life Insurance",
            "AD&D Insurance",
            "Flexible Spending Account (FSA)",
            "Paid Time Off (PPTO)",
            "Paid Sick Days",
            "Parental Leave",
            "Supplemental Disability",
            "Employee Assistance Program (EAP)",
            "Lyra",
            "401k Company Match",
            "Gym Reimbursement",
            "Cellphone Service Reimbursement"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Redwood City, CA",
        "job_id": 3914850579,
        "company": "Gunderson Dettmer",
        "title": "Data Engineer",
        "created_on": 1720635444.0039566,
        "description": "Gunderson Dettme r is the only business law firm of its kind - exclusively serving the global venture capital and emerging technology marketplace. With 400 attorneys in eleven offices - from Silicon Valley to Singapore - we innovate for innovators, accelerate entrepreneurship, and help build companies at every stage of the growth lifecycle. We are committed to being the employer of choice by working together to create an environment, in which each of our people can grow, take initiative, and develop a fun, fulfilling and financially rewarding career. Benefits In addition to offering competitive salaries, we also offer an excellent benefit package, which includes full medical, dental and vision coverage; 401(k) Profit Sharing Plan; Flexible Spending Account and Paid Time Off. Job Description Join Gunderson Dettmer, the preeminent international law firm with an exclusive focus on the innovation economy, as a full time Data Engineer to work at the intersection of technology, law, data and innovation. We’re at the forefront of legal innovation and are actively developing and refining the law firm tech stack of the future. This unique position offers the opportunity to help architect the modern law firm’s data infrastructure, power key applications to solve problems for legal practitioners, and craft the integration layer between these bespoke tools and our data warehouse. We seek a talented individual who thrives in a collaborative, cross-functional environment and embodies a commitment to precision, repeatability, and quality. Responsibilities: Design, build and manage robust, end-to-end ETL processes with performance monitoring Create data integrations between various cloud-based software solutions and platforms (including our enterprise data warehouse) using iPaaS tools Manage and optimize enterprise data warehouse on RDBMS with a complex Dim/Fact data model for analytics Manage and optimize cloud infrastructure to ensure scalability, reliability, and cost-effectiveness Work with cross-functional teams—including legal engineers, data visualization specialists, business solutions and product specialists and subject-matter experts—to ensure efficient and coherent data processes and delivery of high-quality data products Improve and maintain quality control, implementing best practices to ensure data products meet our standards of reliability, usability, and performance Proactively identify opportunities to improve data quality, tooling, and version controls Requirements: 5+ years of relevant experience designing, building and managing ETL processes with a variety of tools Significant experience with: iPaaS tools like Apache AirFlow API technologies such as REST and GraphQL and transfering files using SFTP Data transformation tools like dbt RDBMS platforms like Snowflake noSQL databases like Mongo Dim/Fact data models for analytics Proficiency writing Python scripts and SQL queries (with experience writing Mongo Atlas aggregation queries considered a plus) Exceptional problem-solving skills, ability to work independently, and a collaborative spirit Excellent communication skills, underpinning effective collaboration with various teams and subject-matter experts Nice-to-haves: Experience with Google Cloud Platform (particularly IAM permissions, as well as log analytics, queries and dashboards) Experience with other Snowflake platform features like Snowsight and Cortex Experience in a financial services, professional services or law firm environment Experience in, or familiarity with, the venture-backed company or venture capital space, especially including familiarity with private company equity data schemas and transaction modeling Location: Work out of any of our firm’s U.S. offices or remotely. Our Offer: Join us as a Data Engineer and be at the forefront of innovation and change in the legal industry. We offer a dynamic, supportive, and collaborative work environment, where our contributions will shape the future of legal service delivery. Be part of our journey to optimize the way we serve our clients and make a lasting impact on the legal landscape. The expected starting salary for this position is $100,00 - $200,000 annually, dependent upon qualifications, experience and location. Gunderson Dettmer is an Equal Opportunity Employer and does not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class. By applying to this job you acknowledge that you have read the California Consumer Privacy Act Applicant Notice View Powered by JazzHR aagpO0wMKD",
        "url": "https://www.linkedin.com/jobs/view/3914850579",
        "summary": "Gunderson Dettmer, a law firm specializing in venture capital and emerging technology, seeks a Data Engineer to design, build, and manage ETL processes, integrate cloud-based software, and optimize data warehousing. The role involves collaborating with cross-functional teams and ensuring data quality and reliability. The ideal candidate will have 5+ years of experience in data engineering, proficiency in iPaaS tools, API technologies, and various database platforms.",
        "industries": [
            "Legal",
            "Venture Capital",
            "Emerging Technology",
            "Law",
            "Innovation",
            "Data",
            "Technology"
        ],
        "soft_skills": [
            "Problem-solving",
            "Collaboration",
            "Communication",
            "Independent work",
            "Quality focus"
        ],
        "hard_skills": [
            "ETL",
            "iPaaS",
            "Apache AirFlow",
            "REST",
            "GraphQL",
            "SFTP",
            "Data transformation",
            "dbt",
            "RDBMS",
            "Snowflake",
            "NoSQL",
            "Mongo",
            "Python",
            "SQL",
            "Mongo Atlas aggregation",
            "Google Cloud Platform",
            "IAM permissions",
            "Log analytics",
            "Snowsight",
            "Cortex"
        ],
        "tech_stack": [
            "iPaaS",
            "Apache AirFlow",
            "REST",
            "GraphQL",
            "SFTP",
            "dbt",
            "Snowflake",
            "Mongo",
            "Python",
            "SQL",
            "Mongo Atlas",
            "Google Cloud Platform",
            "IAM",
            "Log analytics",
            "Snowsight",
            "Cortex"
        ],
        "programming_languages": [
            "Python",
            "SQL"
        ],
        "experience": 5,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 200000,
            "min": 100000
        },
        "benefits": [
            "Competitive salaries",
            "Full medical, dental and vision coverage",
            "401(k) Profit Sharing Plan",
            "Flexible Spending Account",
            "Paid Time Off"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Sunnyvale, CA",
        "job_id": 3942693888,
        "company": "Logix Shapers Offshore Services Pvt. Ltd.",
        "title": "Data Engineer (Python coding + Dremio)",
        "created_on": 1720635447.8716257,
        "description": "Data Engineer (Python coding + Dremio)* Day 1 Onsite Sunnyvale, CA (Preferred Local) Hybrid - 3 days onsite / week Long term contract Work visa: H1B, H4EAD, GCEAD, TN, OPT CPT EAD, L1, L2 *Candidate must have LinkedIn account created before 2018/2019* Job Details Strong Python development and software design Dremio/Presto/Trino Snowflake (or other data warehouse systems) Tableau Docker & Kubernetes SQL MongoDB Object store (S3) & data lake concepts Git Shell & CLI tools REST APIs Pandas Skill Sets - Niche Skill - Experience - Preference python - No - 5-10 years - Is Required Snowflake - No - At least 1 year - Nice To Have SQL - No - 2-5 years - Is Required Data Engineering - No - 2-5 years - Is Required REST Based Web Services - No - At least 1 year - Is Required Pandas - No - At least 1 year - Is Required Git - No - 2-5 years - Is Required MongoDB - No - At least 1 year - Is Required Kubernetes - No - At least 1 year - Nice To Have Docker - No - At least 1 year - Nice To Have Tableau - No - At least 1 year - Is Required Big Data - No - 2-5 years - Nice To Have Please reach me out at: moumita@logixshaper.com",
        "url": "https://www.linkedin.com/jobs/view/3942693888",
        "summary": "This is a long-term contract position for a Data Engineer with strong Python development and software design skills. The role requires experience with Dremio/Presto/Trino, Snowflake, Tableau, Docker & Kubernetes, SQL, MongoDB, and Object Store (S3) & data lake concepts.  ",
        "industries": [
            "Technology",
            "Data Engineering",
            "Software Development"
        ],
        "soft_skills": [
            "Communication",
            "Problem-solving",
            "Teamwork",
            "Collaboration"
        ],
        "hard_skills": [
            "Python",
            "Dremio",
            "Presto",
            "Trino",
            "Snowflake",
            "Tableau",
            "Docker",
            "Kubernetes",
            "SQL",
            "MongoDB",
            "Object Store (S3)",
            "Git",
            "Shell",
            "CLI tools",
            "REST APIs",
            "Pandas"
        ],
        "tech_stack": [
            "Python",
            "Dremio",
            "Presto",
            "Trino",
            "Snowflake",
            "Tableau",
            "Docker",
            "Kubernetes",
            "SQL",
            "MongoDB",
            "S3",
            "Git",
            "REST APIs",
            "Pandas"
        ],
        "programming_languages": [
            "Python",
            "SQL"
        ],
        "experience": 5,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3915650409,
        "company": "OpenAI",
        "title": "Analytics Data Engineer, Applied Engineering",
        "created_on": 1720635449.463222,
        "description": "About The Team The Applied team works across research, engineering, product, and design to bring OpenAI’s technology to consumers and businesses. We seek to learn from deployment and distribute the benefits of AI, while ensuring that this powerful tool is used responsibly and safely. Safety is more important to us than unfettered growth. About The Role We're seeking a Data Engineer to take the lead in building our data pipelines and core tables for OpenAI. These pipelines are crucial for powering analyses, safety systems that guide business decisions, product growth, and prevent bad actors. If you're passionate about working with data and are eager to create solutions with significant impact, we'd love to hear from you. This role also provides the opportunity to collaborate closely with the researchers behind ChatGPT and help them train new models to deliver to users. As we continue our rapid growth, we value data-driven insights, and your contributions will play a pivotal role in our trajectory. Join us in shaping the future of OpenAI! In This Role, You Will Design, build and manage our data pipelines, ensuring all user event data is seamlessly integrated into our data warehouse. Develop canonical datasets to track key product metrics including user growth, engagement, and revenue. Work collaboratively with various teams, including, Infrastructure, Data Science, Product, Marketing, Finance, and Research to understand their data needs and provide solutions. Implement robust and fault-tolerant systems for data ingestion and processing. Participate in data architecture and engineering decisions, bringing your strong experience and knowledge to bear. Ensure the security, integrity, and compliance of data according to industry and company standards. You Might Thrive In This Role If You Have 3+ years of experience as a data engineer and 8+ years of any software engineering experience(including data engineering). Proficiency in at least one programming language commonly used within Data Engineering, such as Python, Scala, or Java. Experience with distributed processing technologies and frameworks, such as Hadoop, Flink and distributed storage systems (e.g., HDFS, S3). Expertise with any of ETL schedulers such as Airflow, Dagster, Prefect or similar frameworks. Solid understanding of Spark and ability to write, debug and optimize Spark code. This role is exclusively based in our San Francisco HQ. We offer relocation assistance to new employees. About OpenAI OpenAI is an AI research and deployment company dedicated to ensuring that general-purpose artificial intelligence benefits all of humanity. We push the boundaries of the capabilities of AI systems and seek to safely deploy them to the world through our products. AI is an extremely powerful tool that must be created with safety and human needs at its core, and to achieve our mission, we must encompass and value the many different perspectives, voices, and experiences that form the full spectrum of humanity. We are an equal opportunity employer and do not discriminate on the basis of race, religion, national origin, gender, sexual orientation, age, veteran status, disability or any other legally protected status. For US Based Candidates: Pursuant to the San Francisco Fair Chance Ordinance, we will consider qualified applicants with arrest and conviction records. We are committed to providing reasonable accommodations to applicants with disabilities, and requests can be made via this link. OpenAI Global Applicant Privacy Policy At OpenAI, we believe artificial intelligence has the potential to help people solve immense global challenges, and we want the upside of AI to be widely shared. Join us in shaping the future of technology.",
        "url": "https://www.linkedin.com/jobs/view/3915650409",
        "summary": "OpenAI seeks a Data Engineer to build and manage data pipelines, develop canonical datasets, and collaborate with various teams to ensure seamless data integration and utilization. This role is crucial for powering analyses, safety systems, product growth, and preventing bad actors. The ideal candidate has 3+ years of data engineering experience and proficiency in Python, Scala, or Java, with expertise in distributed processing technologies, ETL schedulers, and Spark.",
        "industries": [
            "Artificial Intelligence",
            "Technology",
            "Research",
            "Software Development"
        ],
        "soft_skills": [
            "Collaboration",
            "Communication",
            "Problem Solving",
            "Data-Driven",
            "Analytical Thinking",
            "Decision Making",
            "Passion for AI"
        ],
        "hard_skills": [
            "Python",
            "Scala",
            "Java",
            "Hadoop",
            "Flink",
            "HDFS",
            "S3",
            "Airflow",
            "Dagster",
            "Prefect",
            "Spark"
        ],
        "tech_stack": [
            "Hadoop",
            "Flink",
            "HDFS",
            "S3",
            "Airflow",
            "Dagster",
            "Prefect",
            "Spark"
        ],
        "programming_languages": [
            "Python",
            "Scala",
            "Java"
        ],
        "experience": 3,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Relocation Assistance"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3914848904,
        "company": "Gunderson Dettmer",
        "title": "Data Engineer",
        "created_on": 1720635451.0688891,
        "description": "Gunderson Dettme r is the only business law firm of its kind - exclusively serving the global venture capital and emerging technology marketplace. With 400 attorneys in eleven offices - from Silicon Valley to Singapore - we innovate for innovators, accelerate entrepreneurship, and help build companies at every stage of the growth lifecycle. We are committed to being the employer of choice by working together to create an environment, in which each of our people can grow, take initiative, and develop a fun, fulfilling and financially rewarding career. Benefits In addition to offering competitive salaries, we also offer an excellent benefit package, which includes full medical, dental and vision coverage; 401(k) Profit Sharing Plan; Flexible Spending Account and Paid Time Off. Job Description Join Gunderson Dettmer, the preeminent international law firm with an exclusive focus on the innovation economy, as a full time Data Engineer to work at the intersection of technology, law, data and innovation. We’re at the forefront of legal innovation and are actively developing and refining the law firm tech stack of the future. This unique position offers the opportunity to help architect the modern law firm’s data infrastructure, power key applications to solve problems for legal practitioners, and craft the integration layer between these bespoke tools and our data warehouse. We seek a talented individual who thrives in a collaborative, cross-functional environment and embodies a commitment to precision, repeatability, and quality. Responsibilities: Design, build and manage robust, end-to-end ETL processes with performance monitoring Create data integrations between various cloud-based software solutions and platforms (including our enterprise data warehouse) using iPaaS tools Manage and optimize enterprise data warehouse on RDBMS with a complex Dim/Fact data model for analytics Manage and optimize cloud infrastructure to ensure scalability, reliability, and cost-effectiveness Work with cross-functional teams—including legal engineers, data visualization specialists, business solutions and product specialists and subject-matter experts—to ensure efficient and coherent data processes and delivery of high-quality data products Improve and maintain quality control, implementing best practices to ensure data products meet our standards of reliability, usability, and performance Proactively identify opportunities to improve data quality, tooling, and version controls Requirements: 5+ years of relevant experience designing, building and managing ETL processes with a variety of tools Significant experience with: iPaaS tools like Apache AirFlow API technologies such as REST and GraphQL and transfering files using SFTP Data transformation tools like dbt RDBMS platforms like Snowflake noSQL databases like Mongo Dim/Fact data models for analytics Proficiency writing Python scripts and SQL queries (with experience writing Mongo Atlas aggregation queries considered a plus) Exceptional problem-solving skills, ability to work independently, and a collaborative spirit Excellent communication skills, underpinning effective collaboration with various teams and subject-matter experts Nice-to-haves: Experience with Google Cloud Platform (particularly IAM permissions, as well as log analytics, queries and dashboards) Experience with other Snowflake platform features like Snowsight and Cortex Experience in a financial services, professional services or law firm environment Experience in, or familiarity with, the venture-backed company or venture capital space, especially including familiarity with private company equity data schemas and transaction modeling Location: Work out of any of our firm’s U.S. offices or remotely. Our Offer: Join us as a Data Engineer and be at the forefront of innovation and change in the legal industry. We offer a dynamic, supportive, and collaborative work environment, where our contributions will shape the future of legal service delivery. Be part of our journey to optimize the way we serve our clients and make a lasting impact on the legal landscape. The expected starting salary for this position is $100,00 - $200,000 annually, dependent upon qualifications, experience and location. Gunderson Dettmer is an Equal Opportunity Employer and does not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class. By applying to this job you acknowledge that you have read the California Consumer Privacy Act Applicant Notice View Powered by JazzHR RC8kJYVx5l",
        "url": "https://www.linkedin.com/jobs/view/3914848904",
        "summary": "Gunderson Dettmer, a law firm specializing in venture capital and emerging technology, seeks a Data Engineer to design, build, and manage ETL processes.  This role involves data integration, warehouse management, and collaboration with various teams. Ideal candidates will have 5+ years of experience in ETL processes, iPaaS tools, API technologies, data transformation tools, RDBMS and noSQL databases, Python scripting, and SQL queries. Experience with Google Cloud Platform, Snowflake features, and the venture capital space is a plus.",
        "industries": [
            "Legal",
            "Technology",
            "Venture Capital",
            "Financial Services"
        ],
        "soft_skills": [
            "Collaboration",
            "Problem-Solving",
            "Communication"
        ],
        "hard_skills": [
            "ETL",
            "iPaaS",
            "API",
            "REST",
            "GraphQL",
            "SFTP",
            "Data Transformation",
            "RDBMS",
            "SQL",
            "Python",
            "Mongo",
            "Snowflake",
            "Google Cloud Platform",
            "IAM",
            "Log Analytics",
            "Snowsight",
            "Cortex"
        ],
        "tech_stack": [
            "Apache Airflow",
            "dbt",
            "Snowflake",
            "Mongo",
            "Google Cloud Platform"
        ],
        "programming_languages": [
            "Python",
            "SQL"
        ],
        "experience": 5,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 200000,
            "min": 100000
        },
        "benefits": [
            "Competitive Salary",
            "Medical Coverage",
            "Dental Coverage",
            "Vision Coverage",
            "401(k) Profit Sharing Plan",
            "Flexible Spending Account",
            "Paid Time Off"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3911394472,
        "company": "Anyscale",
        "title": "Software Engineer (Ray Data)",
        "created_on": 1720635452.8082006,
        "description": "About Anyscale: At Anyscale , we're on a mission to democratize distributed computing and make it accessible to software developers of all skill levels. We’re commercializing Ray , a popular open-source project that's creating an ecosystem of libraries for scalable machine learning. Companies like OpenAI , Uber , Spotify , Instacart , Cruise , and many more, have Ray in their tech stacks to accelerate the progress of AI applications out into the real world. With Anyscale, we’re building the best place to run Ray, so that any developer or data scientist can scale an ML application from their laptop to the cluster without needing to be a distributed systems expert. Proud to be backed by Andreessen Horowitz, NEA, and Addition with $250+ million raised to date. Anyscale is based in San Francisco, CA. Employees are required to come in office 3x a week. About the role: Ray aims to provide a universal API for building distributed applications (e.g. a machine learning pipeline of feature engineering, model training, and evaluation). Data is usually a core element connecting these different stages, and therefore plays a critical role in Ray’s usability, performance, and stability. We are looking for strong engineers to build, optimize, and scale Ray’s Datasets library and data processing capabilities in general. About the Ray Data team: The Ray Data team currently develops and maintains the Ray Datasets library, which is already powering critical production use cases (e.g. large scale data compaction at Amazon, and ML pipeline at Alibaba). Ray Datasets is a Python library built on top of Apache Arrow and Ray Core (Ray’s C++ backend), and the Ray Data team interacts closely with Ray Core components including the scheduler and the memory & I/O subsystems. The Ray Data team also works closely with Ray’s ML libraries including Train, RLlib, and Serve. A snapshot of projects you will work on: - Performance of Ray Datasets at large scale (leveraging Arrow primitives, optimizing Ray object manager, etc.) - Integration with ML training and data sources - Stability and stress testing infrastructure - Lead future work integrating streaming workloads into Ray such as Beam on Ray - Differentiate Data operations in Anyscale hosted Ray service As part of this role, you will: Develop high quality open source software to simplify distributed programming (Ray) Identify, implement, and evaluate architectural improvements to Ray core and Datasets Improve the testing process for Ray to make releases as smooth as possible Communicate your work to a broader audience through talks, tutorials, and blog posts We'd love to hear from you if have: At least 2 year of relevant work experience Solid background in algorithms, data structures, system design Experience in building scalable and fault-tolerant distributed systems Experience with data processing, database internals including Spark or Dask (streaming is a plus) Compensation At Anyscale, we take a market-based approach to compensation. We are data-driven, transparent, and consistent. The target salary for this role is $170,112 ~ $237,000. As the market data changes over time, the target salary for this role may be adjusted. This role is also eligible to participate in Anyscale's Equity and Benefits offerings, including the following: Stock Options Healthcare plans, with premiums covered by Anyscale at 99% 401k Retirement Plan Wellness stipend Education stipend Paid Parental Leave Flexible Time Off Commute reimbursement 100% of in office meals covered Anyscale Inc. is an Equal Opportunity Employer. Candidates are evaluated without regard to age, race, color, religion, sex, disability, national origin, sexual orientation, veteran status, or any other characteristic protected by federal or state law. Anyscale Inc. is an E-Verify company and you may review the Notice of E-Verify Participation and the Right to Work posters in English and Spanish",
        "url": "https://www.linkedin.com/jobs/view/3911394472",
        "summary": "Anyscale is hiring a Software Engineer to join their Ray Data team and build, optimize, and scale Ray's Datasets library and data processing capabilities. This role involves developing high-quality open-source software, improving the testing process, and communicating work through talks, tutorials, and blog posts. The team works on large-scale performance, integration with ML training and data sources, stability and stress testing infrastructure, and integrating streaming workloads into Ray. The ideal candidate will have experience with distributed systems, data processing, and databases like Spark or Dask.",
        "industries": [
            "Software Development",
            "Machine Learning",
            "Artificial Intelligence",
            "Data Science"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Critical Thinking",
            "Leadership",
            "Time Management",
            "Organization"
        ],
        "hard_skills": [
            "Distributed Systems",
            "Data Processing",
            "System Design",
            "Algorithms",
            "Data Structures",
            "Apache Arrow",
            "Spark",
            "Dask",
            "Streaming Data",
            "Testing",
            "Python"
        ],
        "tech_stack": [
            "Ray",
            "Apache Arrow",
            "Spark",
            "Dask",
            "Beam"
        ],
        "programming_languages": [
            "Python",
            "C++"
        ],
        "experience": 2,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 237000,
            "min": 170112
        },
        "benefits": [
            "Stock Options",
            "Healthcare",
            "401k",
            "Wellness Stipend",
            "Education Stipend",
            "Paid Parental Leave",
            "Flexible Time Off",
            "Commute Reimbursement",
            "Meals Covered"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3853615785,
        "company": "Nextdoor",
        "title": "Senior Analytics Engineer - Growth",
        "created_on": 1720635454.445951,
        "description": "#Team Nextdoor Nextdoor is where you connect to the neighborhoods that matter to you so you can belong. Our purpose is to cultivate a kinder world where everyone has a neighborhood they can rely on. Neighbors around the world turn to Nextdoor daily to receive trusted information, give and get help, get things done, and build real-world connections with those nearby — neighbors, businesses, and public services. Today, neighbors rely on Nextdoor in more than 315,000 neighborhoods across 11 countries. Meet Your Future Neighbors In the Analytics Engineering team at Nextdoor, we believe in the transformative power of information, bringing together data from a wide variety of sources and making it digestable and actionable by everyone in the company. We are a lean but powerful team with diverse backgrounds and perspectives, because that’s what we seek out and respect in others. We collaborate cross-functionally to make the numbers available and clear, playing a key role in the company’s overall effort to foster stronger and healthier communities. At Nextdoor, we offer a warm and inclusive work environment that embraces a hybrid employment experience, providing a flexible experience for our valued employees. The Impact You’ll Make As a Senior Analytics Engineer at Nextdoor, you’ll work to democratize data and reporting to drive strategy and measure performance against KPIs across the business. This role focuses on Nextdoor’s Growth Initiatives. You'll be responsible for understanding where our data comes from, transforming it, and making it available to stakeholders across the company via queries, tools, and dashboards. The ideal candidate is a tech-savvy strong communicator who understands how to make data understandable by everyone. You should be excited to bring your experience and expertise every day in order to: Own reporting tools, pipelines, and dashboards to ensure consistent availability and accuracy of mission-critical data Partner with key stakeholders on KPI development and tracking across the organization Build datasets and reports that represent the source of truth for our key metrics Keep data sources fresh even while requirements and definitions change. Address data quality issues and build in alerting Partner with data science, finance & strategy, marketing, sales, product, and engineering on analytics engineering strategy to enable data-driven decision making across the company Set expectations and SLAs on data availability, and own communications around SLA performance to the business Drive self-service and data literacy for business users on our data platform by creating and documenting data structures and sets, SQL and data tools training, and dashboard development Care deeply about data quality and empowering employees to leverage data to help them succeed in their careers and help Nextdoor grow and succeed Participate in in-person Nextdoor events, trainings, off-sites, volunteer days, and other team building exercises Build in-person relationships with team members and contribute to the KIND culture that Nextdoor values What You’ll Bring To The Team 5+ to 10+ years working in data engineering or analytics Expert knowledge of SQL, including writing and optimizing queries in multiple dialects Strong command of Looker (preferred) or another BI/visualization tool Experience working with ETL/ELT pipelines Strong communication skills both written and verbal Ability to understand, tackle, and solve problems from both technical and business perspectives Experience effectively presenting insights and summarizing complex data to diverse audiences through visualizations and other means Demonstrated experience working with and delivering to cross-functional stakeholders from multiple parts of the company (Finance, Engineering, etc.) Demonstrated experience driving projects end-to-end and can speak to outcome and impact It’s ok if you don’t have all of these requirements; if you think you’d be a good fit for the role, please get in touch with us! Bonus Points A background working for an ad-supported site with user-generated content Experience in data related to one of our target areas (i.e. finance, product) Experience with writing and reviewing version controlled code (github) Experience writing in a scripting language such as Python or R Experience in an agile or kanban workflow Rewards Compensation, benefits, perks, and recognition programs at Nextdoor come together to create one overall rewards package. The starting salary for this role is expected to range from $112,000 - $180,000 on an annualized basis, or potentially greater in the event that your 'level' of proficiency exceeds the level expected for the role. Compensation may also vary by geography. We also expect to award a meaningful equity grant for this role. With equal quarterly vesting, your first vest date would be within the first 3 months of your start date. Overall, total compensation will vary depending on your relevant skills, experience, and qualifications. We have you covered! Nextdoor employees can choose between a variety of great health plans. We cover 100% of your personal monthly premium for health, dental, and vision – and provide a OneMedical membership for concierge care. At Nextdoor, we empower our employees to build stronger local communities. To create a platform where all feel welcome, we want our workforce to reflect the diversity of the neighbors we seek to serve. We encourage everyone interested in our purpose to apply. We do not discriminate on the basis of race, gender, religion, sexual orientation, age, or any other trait that unfairly targets a group of people. In accordance with the San Francisco Fair Chance Ordinance, we always consider qualified applicants with arrest and conviction records. For information about our collection and use of applicants’ personal information, please see Nextdoor's Personnel Privacy Notice, found here.",
        "url": "https://www.linkedin.com/jobs/view/3853615785",
        "summary": "Nextdoor is looking for a Senior Analytics Engineer to join their Analytics Engineering team.  The role will focus on Nextdoor's Growth Initiatives and will be responsible for understanding, transforming, and making data available to stakeholders across the company. The ideal candidate will have 5+ years of experience in data engineering or analytics, expert knowledge of SQL, strong communication skills, and experience working with cross-functional stakeholders.",
        "industries": [
            "Technology",
            "Social Media",
            "Data",
            "Analytics",
            "Community"
        ],
        "soft_skills": [
            "Communication",
            "Problem-solving",
            "Collaboration",
            "Data Literacy",
            "Presentation"
        ],
        "hard_skills": [
            "SQL",
            "Looker",
            "ETL/ELT",
            "Data Quality",
            "Data Visualization",
            "Data Structures",
            "GitHub",
            "Python",
            "R",
            "Agile",
            "Kanban"
        ],
        "tech_stack": [
            "SQL",
            "Looker",
            "ETL/ELT",
            "GitHub",
            "Python",
            "R",
            "Agile",
            "Kanban"
        ],
        "programming_languages": [
            "SQL",
            "Python",
            "R"
        ],
        "experience": 5,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 180000,
            "min": 112000
        },
        "benefits": [
            "Health Insurance",
            "Dental Insurance",
            "Vision Insurance",
            "OneMedical Membership",
            "Equity"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Mountain View, CA",
        "job_id": 3948443183,
        "company": "LinkedIn",
        "title": "Sr. Software Engineer - Compute Infra SRE",
        "created_on": 1720635456.0809653,
        "description": "LinkedIn is the world’s largest professional network, built to create economic opportunity for every member of the global workforce. Our products help people make powerful connections, discover exciting opportunities, build necessary skills, and gain valuable insights every day. We’re also committed to providing transformational opportunities for our own employees by investing in their growth. We aspire to create a culture that’s built on trust, care, inclusion, and fun – where everyone can succeed. Join us to transform the way the world works. At LinkedIn, we trust each other to do our best work where it works best for us and our teams. This role offers a hybrid work option, meaning you can both work from home and commute to a LinkedIn office, depending on what’s best for you and when it is important for your team to be together. This role will be based in Mountain View, CA. In the Compute Infrastructure SRE team at LinkedIn, you will be charged with building the next-generation infrastructure and platforms for LinkedIn. This is a unique opportunity to work on a high-profile, high-impact ongoing project that will touch every aspect of our engineering organization. Specifically, the LinkedIn Kubernetes Infrastructure team provides an on-premises Kubernetes platform for the entire company. The team provides capability to efficiently create Kubernetes clusters on-demand, scale the clusters beyond the current industry limits, automate upgrades, and intelligently detect and remediate cluster health, etc. In this role, you will have the opportunity to enable LinkedIn to scale its Compute Infrastructure to meet the demands of a rapidly growing user base. This will involve working closely with cross functional teams and be comfortable operating in a fast-paced, dynamic environment. Responsibilities ·  Serve as a primary point responsible for the overall health, performance and capacity of the Kubernetes cluster management platform. ·  Develop tools and infra to support the entire Linkedin's stateless and stateful apps from the existing compute pools onto Kubernetes clusters. ·  Manage site critical private cloud infrastructure focussing capacity and fleet efficiency. ·  You will produce high quality software that is unit tested, code reviewed, and checked in regularly for continuous integration. ·  You will provide technical leadership, driving and performing best engineering practices to initiate, plan, and execute large-scale, cross functional, and company-wide critical programs. ·  Identify, leverage, and successfully evangelize opportunities to improve engineering productivity. Basic Qualifications ·  BS (or higher, e.g., MS, or PhD) in Computer Science or related technical field involving coding (e.g., physics or mathematics), or equivalent technical experience. ·  2+ years of industry experience. ·  Must have 2+ years of experience in managing and scaling Kubernetes infrastructure. ·  Experience programming language Golang or Java. Preferred Qualifications ·  BS and 5+ years of relevant work experience, MS and 4+ years of relevant work experience, or PhD and 2+ years of relevant work experience. ·  Experience in building and running distributed large-scale systems in production (24x7). ·  Experience with Kubernetes controller development, automating cluster management ·  Golang coding experience ·  Experience with Distributed Storage/Databases and/or analytics technologies (e.g., Pinot, Druid, Redshift, Hadoop, Spark, Presto, Kafka, Flink, etc., or similar) is highly valued. Suggested Skills: · API Development · Kubernetes Infrastructure · Distributed Large-Scale Systems in Production You will Benefit from our Culture: We strongly believe in the well-being of our employees and their families. That is why we offer generous health and wellness programs and time away for employees of all levels. LinkedIn is committed to fair and equitable compensation practices. The pay range for this role is $117,000 - $192,000. Actual compensation packages are based on a wide array of factors unique to each candidate, including but not limited to skill set, years & depth of experience, certifications and specific office location. This may differ in other locations due to cost of labor considerations. The total compensation package for this position may also include annual performance bonus, stock, benefits and/or other applicable incentive compensation plans. For additional information, visit: https://careers.linkedin.com/benefits. Equal Opportunity Statement LinkedIn is committed to diversity in its workforce and is proud to be an equal opportunity employer. LinkedIn considers qualified applicants without regard to race, color, religion, creed, gender, national origin, age, disability, veteran status, marital status, pregnancy, sex, gender expression or identity, sexual orientation, citizenship, or any other legally protected class. LinkedIn is an Affirmative Action and Equal Opportunity Employer as described in our equal opportunity statement here: https://microsoft.sharepoint.com/:b:/t/LinkedInGCI/EeE8sk7CTIdFmEp9ONzFOTEBM62TPrWLMHs4J1C_QxVTbg?e=5hfhpE. Please reference https://www.eeoc.gov/sites/default/files/2023-06/22-088_EEOC_KnowYourRights6.12ScreenRdr.pdf and https://www.dol.gov/ofccp/regs/compliance/posters/pdf/OFCCP_EEO_Supplement_Final_JRF_QA_508c.pdf for more information. LinkedIn is committed to offering an inclusive and accessible experience for all job seekers, including individuals with disabilities. Our goal is to foster an inclusive and accessible workplace where everyone has the opportunity to be successful. If you need a reasonable accommodation to search for a job opening, apply for a position, or participate in the interview process, connect with us at accommodations@linkedin.com and describe the specific accommodation requested for a disability-related limitation. Reasonable accommodations are modifications or adjustments to the application or hiring process that would enable you to fully participate in that process. Examples of reasonable accommodations include but are not limited to: -Documents in alternate formats or read aloud to you -Having interviews in an accessible location -Being accompanied by a service dog -Having a sign language interpreter present for the interview A request for an accommodation will be responded to within three business days. However, non-disability related requests, such as following up on an application, will not receive a response. LinkedIn will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by LinkedIn, or (c) consistent with LinkedIn's legal duty to furnish information. Pay Transparency Policy Statement As a federal contractor, LinkedIn follows the Pay Transparency and non-discrimination provisions described at this link: https://lnkd.in/paytransparency. Global Data Privacy Notice for Job Candidates This document provides transparency around the way in which LinkedIn handles personal data of employees and job applicants: https://lnkd.in/GlobalDataPrivacyNotice",
        "url": "https://www.linkedin.com/jobs/view/3948443183",
        "summary": "LinkedIn is seeking a highly skilled and experienced Site Reliability Engineer (SRE) to join their Compute Infrastructure team in Mountain View, CA. This role involves building and managing the next-generation infrastructure and platforms for LinkedIn, specifically focusing on their on-premises Kubernetes platform. The ideal candidate will have extensive experience with Kubernetes infrastructure, distributed systems, and Golang programming. They will be responsible for the overall health, performance, and capacity of the Kubernetes cluster management platform, as well as developing tools and infrastructure to support LinkedIn's applications. This is an opportunity to work on a high-impact project that touches every aspect of LinkedIn's engineering organization.",
        "industries": [
            "Technology",
            "Software Development",
            "Cloud Computing",
            "IT",
            "Infrastructure"
        ],
        "soft_skills": [
            "Communication",
            "Problem-solving",
            "Leadership",
            "Collaboration",
            "Technical Expertise",
            "Analytical Skills",
            "Decision-making",
            "Teamwork",
            "Time Management"
        ],
        "hard_skills": [
            "Kubernetes",
            "Golang",
            "Java",
            "Distributed Systems",
            "Cloud Infrastructure",
            "Site Reliability Engineering (SRE)",
            "Capacity Planning",
            "Performance Tuning",
            "System Monitoring",
            "Troubleshooting",
            "Automation",
            "CI/CD",
            "Agile Development",
            "DevOps"
        ],
        "tech_stack": [
            "Kubernetes",
            "Golang",
            "Java",
            "Distributed Storage",
            "Databases",
            "Analytics Technologies",
            "Pinot",
            "Druid",
            "Redshift",
            "Hadoop",
            "Spark",
            "Presto",
            "Kafka",
            "Flink",
            "API Development",
            "Private Cloud Infrastructure"
        ],
        "programming_languages": [
            "Golang",
            "Java"
        ],
        "experience": 2,
        "education": {
            "min_degree": "BS",
            "fields": [
                "Computer Science",
                "Technical Field involving coding (e.g., physics or mathematics)"
            ]
        },
        "salary": {
            "max": 192000,
            "min": 117000
        },
        "benefits": [
            "Health and Wellness Programs",
            "Time Away",
            "Annual Performance Bonus",
            "Stock",
            "Other Applicable Incentive Compensation Plans"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "California, United States",
        "job_id": 3901876233,
        "company": "LanceSoft, Inc.",
        "title": "Senior Data Engineer/Developer",
        "created_on": 1720635457.8639433,
        "description": "Title: Senior Data Engineer/Developer Position: Remote Enter Job Description... Proficiency in WhereScape RED for data warehouse automation, including designing, building, and managing data warehouses. Expertise in Snowflake's cloud data platform, including data loading, transformation, and querying using Snowflake SQL. Experience with SQL-based development, optimization, and tuning for large-scale data processing. Strong understanding of dimensional modeling concepts and experience in designing and implementing data models for analytics and reporting purposes. Ability to optimize data pipelines and queries for performance and scalability. Familiarity with Snowflake's features such as virtual warehouses, data sharing, and data governance capabilities. Knowledge of WhereScape scripting language (WSL) for customizing and extending automation processes. Experience with data integration tools and techniques to ingest data from various sources into Snowflake. Understanding of data governance principles and experience implementing data governance frameworks within Snowflake. Ability to implement data quality checks and ensure data integrity within the data warehouse environment. Strong SQL skills for data manipulation, optimization, and performance tuning. Experience with data visualization tools such as Power BI.",
        "url": "https://www.linkedin.com/jobs/view/3901876233",
        "summary": "Senior Data Engineer/Developer with expertise in WhereScape RED for data warehouse automation, Snowflake cloud data platform, SQL development and optimization, dimensional modeling, data pipeline optimization, Snowflake features, data integration, data governance, and data visualization tools.",
        "industries": [
            "Data Engineering",
            "Data Warehousing",
            "Data Analytics",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Problem Solving",
            "Communication",
            "Teamwork",
            "Analytical Thinking"
        ],
        "hard_skills": [
            "WhereScape RED",
            "Snowflake",
            "SQL",
            "Dimensional Modeling",
            "Data Pipeline Optimization",
            "Data Integration",
            "Data Governance",
            "Data Quality",
            "Power BI",
            "WSL (WhereScape Scripting Language)"
        ],
        "tech_stack": [
            "WhereScape RED",
            "Snowflake",
            "SQL",
            "Power BI"
        ],
        "programming_languages": [
            "SQL",
            "WSL (WhereScape Scripting Language)"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Data Science",
                "Information Technology",
                "Engineering"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3901754527,
        "company": "Nightfall AI",
        "title": "Senior Data Engineer",
        "created_on": 1720635462.4027565,
        "description": "Nightfall AI (www.nightfall.ai) is the unified platform that prevents data leaks and enables secure collaboration by protecting sensitive data and controlling how it's shared. For decades, legacy data leak prevention (DLP) solutions have failed to adequately protect sensitive information. Traditional DLP is outdated, intrusive, and complex - it wasn't designed for today's modern enterprise where users continuously share data across interconnected SaaS applications, endpoints, and now generative AI. Nightfall AI is the first AI-native DLP solution. We leverage AI to achieve twice the accuracy with a fraction of the false positives that overwhelm security teams. Nightfall does this without disrupting modern work patterns. Our AI-native platform spans sensitive data protection across SaaS, email, data exfiltration prevention on SaaS, endpoints, and data encryption. Nightfall's Developer Platform provides an open, flexible environment for developers to integrate our data classification and protection capabilities anywhere, including establishing trust boundaries for AI model building and consumption. We're looking for a Senior Data Engineer to enhance Nightfall's core data models and infrastructure supporting our real-time and data annotation pipelines, analytics warehouse, MLOps, real-time monitoring, and business intelligence reporting. At Nightfall, we use ML/NLP to detect and protect sensitive personal, health, financial, and credential information in real time. You will contribute directly to the core product. Working with Engineers, Data Scientists, and Data Analysts, you will ensure high technical standards for data management and drive Nightfall's short and long-term data strategies. This is a hybrid role based out of our San Francisco office. Responsibilities Design, build, and own the core data models and infrastructure to manage data across Nightfall's analytics stack - AWS, Kafka, Glue, S3, Snowflake, Cassandra, Cockroach DB, Tableau, and Datadog. Collaborate closely with Data Analysts to unlock the data needed to power dashboards, ad-hoc analyses, monitoring, and high-quality decision-making across the company. Collaborate closely with ML Engineering and Data Science to scale our data pipelines for machine learning, analytics, and data annotation. Design and implement a data quality monitoring system with source-to-target data validations and anomaly detection. Collaborate with Security and Engineering to meet data governance requirements. Qualifications SQL Expert. Solid experience in Python or similar. Experience (5+ years) with analytical data warehouses/lakes. AWS/Snowflake preferred. Experience (5+ years) developing data pipelines supporting ML/NLP projects. Experience with popular MLOps and data annotation platforms is a plus. Experience (5+ years) analyzing complex datasets using SQL, optimizing queries, working with Data Analysts to automate dashboards (Tableau preferred), and performing ad-hoc analyses that lead to actionable business insights. Bachelor's degree in Computer Science or related field. MS/PhD a plus. Experience with source code repositories and cloud monitoring tools (Datadog preferred). Possess excellent communication skills, strong analytical skills, and the ability to work independently and as part of a team. The ability to thrive in a dynamic environment, being flexible and willing to jump in and do whatever it takes to be successful. About Nightfall: Nightfall is a cybersecurity startup dedicated to helping organizations secure and manage their sensitive data. As a leading enterprise technology company, our product affects the personal data that people entrust businesses to store & process with care every day. Critical data in modern organizations is often sprayed across a broad set of cloud data silos, and it's a herculean task for security & compliance teams to monitor, manage, and protect this highly sensitive data. Via machine learning, our product makes it easy for organizations to discover, classify, and protect this sensitive data across their cloud footprint - such as their corporate SaaS, data infrastructure, and even their own apps. In doing so, we prevent data leakage, provide unprecedented data visibility & protection across the cloud, and enable compliance. We're a technology startup founded in San Francisco, well-funded by leading institutional investors like Bain and Venrock, and a cadre of security & IT leaders from Okta, Salesforce, Atlassian, Splunk, FireEye, and more. Learn more on our website www.nightfall.ai or by reaching out via email at careers@nightfall.ai.",
        "url": "https://www.linkedin.com/jobs/view/3901754527",
        "summary": "Nightfall AI is seeking a Senior Data Engineer to manage and enhance its data models and infrastructure supporting real-time and data annotation pipelines, analytics warehouse, MLOps, real-time monitoring, and business intelligence reporting. The role involves designing, building, and owning core data models and infrastructure across various technologies including AWS, Kafka, Glue, S3, Snowflake, Cassandra, Cockroach DB, Tableau, and Datadog. The ideal candidate will have experience with analytical data warehouses/lakes, developing data pipelines supporting ML/NLP projects, analyzing complex datasets, and optimizing queries. Excellent communication skills, strong analytical skills, and the ability to work independently and as part of a team are essential.",
        "industries": [
            "Cybersecurity",
            "Data Security",
            "Machine Learning",
            "Artificial Intelligence",
            "Software Development",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Communication",
            "Analytical",
            "Problem Solving",
            "Teamwork",
            "Collaboration",
            "Independent",
            "Flexibility",
            "Decision Making"
        ],
        "hard_skills": [
            "SQL",
            "Python",
            "AWS",
            "Snowflake",
            "Kafka",
            "Glue",
            "S3",
            "Cassandra",
            "Cockroach DB",
            "Tableau",
            "Datadog",
            "MLOps",
            "Data Annotation",
            "Data Pipelines",
            "Data Governance",
            "Data Quality Monitoring",
            "Source Code Repositories",
            "Cloud Monitoring"
        ],
        "tech_stack": [
            "AWS",
            "Kafka",
            "Glue",
            "S3",
            "Snowflake",
            "Cassandra",
            "Cockroach DB",
            "Tableau",
            "Datadog"
        ],
        "programming_languages": [
            "SQL",
            "Python"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Computer Science",
                "Related Field"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3897979850,
        "company": "Unreal Staffing, Inc",
        "title": "Senior Data Warehouse Engineer",
        "created_on": 1720635464.3160088,
        "description": "Company Overview: Welcome to the forefront of data-driven innovation! Our company is dedicated to leveraging the power of data to drive transformative change and solve complex problems across industries. We're committed to building scalable and efficient data warehousing solutions that enable advanced analytics, reporting, and business intelligence. Join us and be part of a dynamic team shaping the future of data warehouse engineering. Position Overview: As a Senior Data Warehouse Engineer, you'll play a critical role in designing, building, and maintaining our data warehousing solutions. You'll work on challenging projects, from data modeling and ETL processes to performance optimization and data governance, to support the needs of our data-driven organization. If you're a seasoned engineer with expertise in data warehousing technologies and a passion for building robust data systems, we want you on our team. Requirements Key Responsibilities: Data Warehouse Design: Design and implement data warehouse solutions, including data models, schemas, and architecture, to meet business requirements and enable efficient data storage and retrieval ETL/ELT Processes: Develop and maintain ETL/ELT processes to extract, transform, and load data from various sources into the data warehouse, ensuring data quality, integrity, and consistency Performance Optimization: Optimize data warehouse performance through indexing, partitioning, and query optimization techniques, ensuring scalability and responsiveness for analytical and reporting needs Data Governance: Implement data governance policies and procedures to ensure data quality, security, and compliance with regulatory requirements, including access controls, data masking, and encryption Monitoring and Alerting: Implement monitoring and alerting systems to track data warehouse performance and health, proactively identifying and resolving issues to minimize downtime and data loss Documentation and Best Practices: Document data warehouse design, processes, and best practices, providing clear and comprehensive documentation to facilitate understanding and collaboration among team members Collaboration: Collaborate with cross-functional teams, including data scientists, business analysts, and software engineers, to understand requirements and deliver data solutions that meet business needs Mentorship and Knowledge Sharing: Mentor junior engineers, sharing expertise and best practices in data warehouse engineering, and facilitate knowledge sharing sessions within the team Qualifications: Bachelor's degree or higher in Computer Science, Engineering, Mathematics, or related field 5+ years of experience in data engineering, with a focus on designing, building, and maintaining data warehousing solutions Proficiency in SQL and experience with data warehousing technologies such as Snowflake, Amazon Redshift, Google BigQuery, or similar Strong understanding of data modeling principles and techniques, with experience designing and implementing data models and schemas for data warehouses Experience with ETL/ELT processes, data integration, and data governance in a data warehouse environment Strong problem-solving skills and analytical thinking, with the ability to troubleshoot complex data issues and optimize system performance Excellent communication and collaboration skills, with the ability to work effectively in cross-functional teams and communicate technical concepts to non-technical stakeholders Benefits Competitive salary: The industry standard salary for Senior Data Warehouse Engineers typically ranges from $170,000 to $230,000 per year, depending on experience and qualifications Comprehensive health, dental, and vision insurance plans Flexible work hours and remote work options Generous vacation and paid time off Professional development opportunities, including access to training programs, conferences, and workshops State-of-the-art technology environment with access to cutting-edge tools and resources Vibrant and inclusive company culture with opportunities for growth and advancement Exciting projects with real-world impact at the forefront of data-driven innovation Join Us: Ready to shape the future of data warehouse engineering? Apply now to join our team and be part of the data revolution!",
        "url": "https://www.linkedin.com/jobs/view/3897979850",
        "summary": "Senior Data Warehouse Engineer responsible for designing, building, and maintaining data warehousing solutions. Responsibilities include data modeling, ETL/ELT processes, performance optimization, data governance, monitoring and alerting, documentation, and collaboration with cross-functional teams. Requires 5+ years of experience in data engineering, proficiency in SQL, and experience with data warehousing technologies like Snowflake, Amazon Redshift, or Google BigQuery.",
        "industries": [
            "Data Engineering",
            "Data Warehousing",
            "Business Intelligence",
            "Analytics",
            "Technology",
            "Software Development"
        ],
        "soft_skills": [
            "Problem-solving",
            "Analytical Thinking",
            "Communication",
            "Collaboration",
            "Mentorship",
            "Knowledge Sharing"
        ],
        "hard_skills": [
            "SQL",
            "Snowflake",
            "Amazon Redshift",
            "Google BigQuery",
            "Data Modeling",
            "ETL",
            "ELT",
            "Data Integration",
            "Data Governance",
            "Performance Optimization",
            "Query Optimization",
            "Indexing",
            "Partitioning",
            "Monitoring",
            "Alerting",
            "Documentation"
        ],
        "tech_stack": [
            "Snowflake",
            "Amazon Redshift",
            "Google BigQuery",
            "SQL"
        ],
        "programming_languages": [
            "SQL"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Computer Science",
                "Engineering",
                "Mathematics"
            ]
        },
        "salary": {
            "max": 230000,
            "min": 170000
        },
        "benefits": [
            "Competitive salary",
            "Health insurance",
            "Dental insurance",
            "Vision insurance",
            "Flexible work hours",
            "Remote work options",
            "Vacation",
            "Paid time off",
            "Professional development opportunities",
            "Training programs",
            "Conferences",
            "Workshops",
            "State-of-the-art technology",
            "Cutting-edge tools",
            "Vibrant company culture",
            "Growth opportunities",
            "Advancement opportunities",
            "Exciting projects",
            "Real-world impact"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Los Angeles, CA",
        "job_id": 3812812801,
        "company": "MAVAN",
        "title": "Data Engineer - (Full Time / Part Time)",
        "created_on": 1720635466.0203714,
        "description": "Data Engineer Mavan is seeking a data engineer who will build data solutions for various use cases including tracking, reporting, product analytics, marketing optimization and financial reporting. By implementing dashboards, data pipelines, data structures, and data warehouse architecture, you will help shape how Mavan functions and empower clients to do the same. Responsibilities Work with business partners and stakeholders to understand data/reporting requirements Work with engineering, analytics and marketing, and 3rd parties to build pipelines and collect required data Design, develop and implement large scale, high volume, high performance data models and pipelines for Data Lake and Data Warehouse. Develop and implement data quality checks, conduct QA and implement monitoring routines. Build and implement ETL frameworks to improve code quality and reliability Build and enforce common design patterns to increase code maintainability Manage reliability and scaling of portfolio of pipelines and data marts Document new and existing models, solutions, and implementations Code new automations for common processes Mentor and coach team members to improve their designs and solutions Qualifications 7+ years of professional experience 5+ years experience working in data engineering, business intelligence, or a similar role Proficiency in programming languages such as Python/Java 3+ years of experience in ETL orchestration and workflow management tools Expert in Database fundamentals, SQL and distributed computing Experience working with Snowflake, BigQuery, Redshift, and/or PostgreSQL Familiarity with CDPs like Segment.io and mParticle Familiarity with Data Pipelines such as Funnel.io, Fivetran, and Stitch Excellent communication skills and experience working with technical and non-technical teams Strong in Google tracking products - particularly Google Tag Manager and Google Analytics - with comfort setting up new instances from scratch Knowledge of reporting tools such as Tableau, Google Data Studio, and Looker Comfortable working in fast paced environment, self starter and self organizing Ability to think strategically, analyze and interpret market and consumer information About MAVAN MAVAN is a growth studio that operates across the entire funnel. We unlock growth through consumer/competitive research, creative strategy/production, paid acquisition/data and analytics, landing page/product optimization, lifecycle marketing, and technology. MAVAN's exclusive on-demand talent network of 250+ specialized experts, allows us to pull in specialists as needed to scale with the unique needs of any company. Our specialists are from some of the most successful tech and consumer companies, including Google, Apple, Uber, Square, Activision, Nike, Red Bull, Dropbox, and more. We've unlocked multibillion-dollar businesses, scaled massive global teams, and collectively managed over $2.7 billion dollars in paid media over the last 5 years. Our process is informed by structured testing, competitive research, and detailed analysis informed by experts across the entire funnel. We are profitable, have shown exponential growth over the last 2 years, and have a wide variety of benefits including: Full Health, Medical, Dental, Vision 401k matching (up to 5%) Unlimited Vacation \"No meeting Fridays\" (Experimenting with moving to a 4-day work week in 2023 All of the fun events, swag, joys of working at a fast growing start-up Our Website Explainer Video DISCLAIMER: MAVAN will only contact you via LinkedIn or email using the mavan.com domain for job openings and job offers. Any communication from other domains, applications, or platforms is NOT from the MAVAN team and is not representative of any communication with the MAVAN team. If you receive any communication from parties pretending to be MAVAN using domains other than mavan.com, MAVAN is not responsible for the communications contained within. If you suspect someone is impersonating the MAVAN team, please forward those communications to legal@mavan.com.",
        "url": "https://www.linkedin.com/jobs/view/3812812801",
        "summary": "Mavan is looking for a Data Engineer to build data solutions for various use cases like tracking, reporting, product analytics, marketing optimization, and financial reporting. Responsibilities include working with stakeholders to understand data requirements, building data pipelines, designing and implementing large-scale data models, conducting data quality checks, building ETL frameworks, managing data mart reliability, and mentoring team members.",
        "industries": [
            "Marketing",
            "Analytics",
            "Data Engineering",
            "Technology"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem-Solving",
            "Strategic Thinking",
            "Analytical",
            "Self-Starter",
            "Teamwork",
            "Mentoring",
            "Leadership"
        ],
        "hard_skills": [
            "Python",
            "Java",
            "SQL",
            "ETL",
            "Data Warehousing",
            "Data Lake",
            "Data Modeling",
            "Data Pipelines",
            "Data Quality",
            "Data Visualization",
            "Snowflake",
            "BigQuery",
            "Redshift",
            "PostgreSQL",
            "Google Analytics",
            "Google Tag Manager",
            "Tableau",
            "Google Data Studio",
            "Looker",
            "Segment.io",
            "mParticle",
            "Funnel.io",
            "Fivetran",
            "Stitch"
        ],
        "tech_stack": [
            "Snowflake",
            "BigQuery",
            "Redshift",
            "PostgreSQL",
            "Segment.io",
            "mParticle",
            "Funnel.io",
            "Fivetran",
            "Stitch",
            "Google Tag Manager",
            "Google Analytics",
            "Tableau",
            "Google Data Studio",
            "Looker"
        ],
        "programming_languages": [
            "Python",
            "Java",
            "SQL"
        ],
        "experience": 7,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Health Insurance",
            "Dental Insurance",
            "Vision Insurance",
            "401k matching",
            "Unlimited Vacation",
            "No meeting Fridays",
            "Company Events",
            "Swag"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "California, United States",
        "job_id": 3888467692,
        "company": "Laksan Technologies",
        "title": "Data Engineer - Hadoop /Cloudera",
        "created_on": 1720635467.8073933,
        "description": "Role : Hadoop Data Engineer Years of Experience : 8+ yrs Strong experience needed in building end to end pipelines, SQL and Pyspark along with large scale migrations. Requirement Need Senior Hadoop developer with focus on cloudera Hadoop. Should be able to demonstrate work experience in hive/Impala and Spark, pyspark. Strong spark hands on skill a must. Very Strong SQL skills. Working knowledge of Python, shell scripting and bash. Must have work experience in Hadoop as data warehouse/data lake implementations. Must be able to work from onsite santa clara ( no exceptions)",
        "url": "https://www.linkedin.com/jobs/view/3888467692",
        "summary": "Seeking a senior Hadoop Data Engineer with 8+ years of experience to build end-to-end pipelines, work with SQL and PySpark, and handle large-scale data migrations.  Must have strong experience with Cloudera Hadoop, Hive/Impala, Spark, and PySpark.  Excellent SQL skills required, as well as working knowledge of Python, shell scripting, and Bash.  Prior experience with Hadoop data warehouse/data lake implementations is essential. Onsite position in Santa Clara.",
        "industries": [
            "Data Engineering",
            "Data Analytics",
            "Software Development",
            "Technology"
        ],
        "soft_skills": [
            "Problem Solving",
            "Communication",
            "Teamwork",
            "Analytical Skills"
        ],
        "hard_skills": [
            "Hadoop",
            "Cloudera Hadoop",
            "Hive",
            "Impala",
            "Spark",
            "PySpark",
            "SQL",
            "Python",
            "Shell Scripting",
            "Bash"
        ],
        "tech_stack": [
            "Hadoop",
            "Cloudera Hadoop",
            "Hive",
            "Impala",
            "Spark",
            "PySpark"
        ],
        "programming_languages": [
            "SQL",
            "Python",
            "Bash",
            "Pyspark"
        ],
        "experience": 8,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Data Science",
                "Engineering"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Diego, CA",
        "job_id": 3804452253,
        "company": "NauWork",
        "title": "Data Engineer",
        "created_on": 1720635469.5825524,
        "description": "A NauWork client is seeking a Data Engineer to join their team. The position is fully remote or hybrid based in San Diego, California . This client is a leading medical staffing agency with a mission to help others live better by helping healthcare professionals and the patients they serve. They’ve received multiple awards and accolades for “Best Places to Work” from companies like Glassdoor and Modern Healthcare. As a Data Engineer II with a specialization in MS Power BI, you will lead the development and maintenance of data-driven solutions, creating compelling visualizations, and ensuring data integrity. Your deep understanding of data infrastructure and data visualization will help drive strategic decisions, streamline operations, and empower our team with actionable insights. Responsibilities: Data Pipeline Development & Management: Design, construct, install, and maintain large-scale processing systems and other infrastructure. Manage and optimize data pipelines, ensuring data availability, accuracy, and optimal performance. MS Power BI Development & Management: Develop, maintain, and optimize Power BI dashboards and reports tailored to business needs. Collaborate with stakeholders to identify opportunities for data-driven decision-making and to define metrics and KPIs. Ensure consistency and integrity of data visualizations across all Power BI reports. Data Analysis & Optimization: Work with cross-functional teams to gather requirements, understand business challenges, and provide data-driven solutions. Continuously analyze data processes and tools for improvement and scalability. Data Governance & Integrity: Collaborate with data governance teams to ensure data quality, compliance, and consistency. Develop and maintain documentation on data pipelines, data models, and data dictionaries. Team Collaboration & Leadership: Collaborate with IT, analytics, and business teams to ensure seamless integration of systems and tools. Required Experience: 5+ years of experience in data engineering with a strong emphasis on data visualization. Bachelor’s degree in computer science, engineering, information systems, or a related field. Proven expertise in MS Power BI development, including DAX, data modeling, and performance tuning. Strong experience in SQL, ETL processes, and data warehouse design. Familiarity with cloud platforms like Azure or AWS. Preferred Experience: Experience in the healthcare or recruiting industry. Familiarity with data governance principles and practices. Excellent communication skills, both written and verbal. Strong analytical and problem-solving skills with a keen attention to detail. To Learn More: 503-388-9585 833-NAU-WORK nauwork.com/careers Category: Technology - System Software Position: Data Engineer Location: [Remote] San Diego, California Job Type: Direct-Hire, Full-Time",
        "url": "https://www.linkedin.com/jobs/view/3804452253",
        "summary": "A leading medical staffing agency seeks a Data Engineer II with expertise in MS Power BI to develop and maintain data-driven solutions, create compelling visualizations, and ensure data integrity. This role involves managing data pipelines, optimizing Power BI dashboards, collaborating with stakeholders, and analyzing data processes. Strong experience in data visualization, MS Power BI development (including DAX), SQL, ETL processes, and data warehouse design is required.",
        "industries": [
            "Healthcare",
            "Medical Staffing",
            "Recruiting"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem-Solving",
            "Analytical",
            "Attention to Detail"
        ],
        "hard_skills": [
            "Data Engineering",
            "Data Visualization",
            "MS Power BI",
            "DAX",
            "Data Modeling",
            "Performance Tuning",
            "SQL",
            "ETL",
            "Data Warehouse Design",
            "Cloud Platforms",
            "Azure",
            "AWS",
            "Data Governance"
        ],
        "tech_stack": [
            "MS Power BI",
            "DAX",
            "SQL",
            "ETL",
            "Azure",
            "AWS"
        ],
        "programming_languages": [
            "SQL"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Computer Science",
                "Engineering",
                "Information Systems"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Alameda, CA",
        "job_id": 3961551750,
        "company": "Technogen, Inc.",
        "title": "AWS Data Infrastructure Engineer",
        "created_on": 1720635471.248187,
        "description": "TECHNOGEN, Inc. is a Proven Leader in providing full IT Services, Software Development and Solutions for 15 years. TECHNOGEN is a Small & Woman Owned Minority Business with GSA Advantage Certification. We have offices in VA; MD & Offshore development centers in India. We have successfully executed 100+ projects for clients ranging from small business and non-profits to Fortune 50 companies and federal, state and local agencies. Role: AWS Data Infrastructure Engineer/AWS Data Platform Infrastructure Engineer Full Time / FTE Location: Alameda, CA 94502 (Onsite) Salary – Market/Flexible In the process of adopting ‘ Data Mesh’ concepts to build a data platform which can support federated data products with governance and security. Data Platform provides all the data infrastructure needs of data engineers in developing data products. Key Responsibilities Architect, design and implement unified data access for various personas like data engineer, data analyst, data steward by simplified access control mechanism which supports governance on AWS and non-AWS assets Design, build & optimize CI/CD pipelines for data engineers to deploy and push code across multiple environments Develop and maintain infrastructure as code (IaaC) with Terraform for reproducible and scalable deployments. Design & implementation of AWS Cloud infrastructure using best practices and industry standards by working closely with internal & external stakeholders like Information Security, Cloud Infrastructure, Data Engineering teams, etc. Work closely with data governance teams in implementing data governance policies, periodic review, and optimization of repository from security & governance perspective Continuous improvement of Cloud Data platform to enhance speed, agility, and cost efficiency Skills & Experience Expertise in reducing costs and increasing speed and efficiency in large scale data platform deployments Proficiency in Terraform, Python Scripting along with GitHub Actions for deployment of infrastructure as code (IaC) Deep understanding of data governance policies, implementation best practices and hands-on experience in implementing Policies as Code Architecture, design & implementation of unified data access using AWS Lake formation, AWS Data Zone, IAM, OKTA and others Experience in architecture & security of AWS EC2, EBS, S3, EKS, Athena, Redshift, RDS, Kafka, Glue, and other data management tools Experience in in design and implementation of granular access to data without losing the speed & agility of delivering the access provisioning Expertise in CI/CD process and deployment technologies using tools like GitHub, GitHub Actions Expertise in logging, monitoring, reliability engineering Strong knowledge of networking concepts and experience managing network-related issues Thanks & Regards Tajuddin M mateenuddin.s@technogeninc.com | 571-934-3415",
        "url": "https://www.linkedin.com/jobs/view/3961551750",
        "summary": "TECHNOGEN, Inc. is seeking an AWS Data Infrastructure Engineer/AWS Data Platform Infrastructure Engineer to design, implement, and manage a data platform that supports federated data products with governance and security. The ideal candidate will have expertise in AWS services like Lake Formation, Data Zone, IAM, OKTA, EC2, EBS, S3, EKS, Athena, Redshift, RDS, Kafka, and Glue, as well as experience with Terraform, Python, GitHub Actions, and CI/CD processes.",
        "industries": [
            "Information Technology",
            "Software Development",
            "Data Management",
            "Cloud Computing",
            "Security",
            "Governance",
            "Consulting"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Critical Thinking",
            "Analytical Skills",
            "Time Management",
            "Organizational Skills",
            "Leadership"
        ],
        "hard_skills": [
            "AWS",
            "Lake Formation",
            "Data Zone",
            "IAM",
            "OKTA",
            "EC2",
            "EBS",
            "S3",
            "EKS",
            "Athena",
            "Redshift",
            "RDS",
            "Kafka",
            "Glue",
            "Terraform",
            "Python",
            "GitHub Actions",
            "CI/CD",
            "Logging",
            "Monitoring",
            "Reliability Engineering",
            "Networking",
            "Data Governance",
            "Security"
        ],
        "tech_stack": [
            "AWS",
            "Terraform",
            "Python",
            "GitHub Actions",
            "CI/CD"
        ],
        "programming_languages": [
            "Python"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Sunnyvale, CA",
        "job_id": 3892858959,
        "company": "E-IT",
        "title": "Senior Data Engineer with GCP",
        "created_on": 1720635474.4587853,
        "description": "Senior Data Engineer Sunnyvale, CA Onsite role Only local profiles accepted. Linkedin is mandatory Must Have Skills – Spark – 8+ Yrs of Exp Scala – 8+ Yrs of Exp GCP –5+ Yrs of Exp Hive– 8+Yrs of Exp SQL - 8+ Yrs of Exp ETL Process / Data Pipeline - 8+ Years of experience Responsibilities As a Senior Data Engineer, you will Design and develop big data applications using the latest open source technologies. Desired working in offshore model and Managed outcome Develop logical and physical data models for big data platforms. Automate workflows using Apache Airflow. Create data pipelines using Apache Hive, Apache Spark, Scala, Apache Kafka. Provide ongoing maintenance and enhancements to existing systems and participate in rotational on-call support. Learn our business domain and technology infrastructure quickly and share your knowledge freely and actively with others in the team. Mentor junior engineers on the team Lead daily standups and design reviews Groom and prioritize backlog using JIRA Act as the point of contact for your assigned business domain Requirements 8+ years of hands-on experience with developing data warehouse solutions and data products. 4+ years of hands-on experience developing a distributed data processing platform with Hadoop, Hive,Scala, Airflow or a workflow orchestration solution are required 4 + years of experience in GCP,GCS Data proc, BIG Query 2+ years of hands-on experience in modeling(Erwin) and designing schema for data lakes or for RDBMS platforms. Experience with programming languages: Python, Java, Scala, etc. Experience with scripting languages: Perl, Shell, etc. Practice working with, processing, and managing large data sets (multi TB/PB scale). Exposure to test driven development and automated testing frameworks. Background in Scrum/Agile development methodologies. Capable of delivering on multiple competing priorities with little supervision. Excellent verbal and written communication skills. Bachelor's Degree in computer science or equivalent experience. The most successful candidates will also have experience in the following: Gitflow Atlassian products – BitBucket, JIRA, Confluence etc. Continuous Integration tools such as Bamboo, Jenkins, or TFS",
        "url": "https://www.linkedin.com/jobs/view/3892858959",
        "summary": "Senior Data Engineer role in Sunnyvale, CA. Requires 8+ years of experience with Spark, Scala, GCP, Hive, and SQL. Experience with ETL processes, data pipelines, and workflow automation with Apache Airflow is essential. Responsibilities include designing and developing big data applications, data modeling, and providing ongoing maintenance and support. Ideal candidate will have experience with Hadoop, Hive, Scala, Airflow, GCP, GCS Data proc, BIG Query, modeling, scripting languages, test driven development, Scrum/Agile methodologies, Gitflow, Atlassian products, and CI tools.",
        "industries": [
            "Data Engineering",
            "Software Development",
            "Technology"
        ],
        "soft_skills": [
            "Communication",
            "Problem Solving",
            "Teamwork",
            "Leadership",
            "Mentoring",
            "Organization",
            "Prioritization",
            "Self-Management",
            "Collaboration",
            "Adaptability",
            "Time Management",
            "Analytical Thinking"
        ],
        "hard_skills": [
            "Spark",
            "Scala",
            "GCP",
            "Hive",
            "SQL",
            "ETL",
            "Data Pipelines",
            "Apache Airflow",
            "Hadoop",
            "Data Modeling",
            "Erwin",
            "RDBMS",
            "Python",
            "Java",
            "Perl",
            "Shell",
            "Gitflow",
            "Atlassian Products",
            "BitBucket",
            "JIRA",
            "Confluence",
            "Continuous Integration",
            "Bamboo",
            "Jenkins",
            "TFS"
        ],
        "tech_stack": [
            "Spark",
            "Scala",
            "GCP",
            "Hive",
            "SQL",
            "Apache Airflow",
            "Hadoop",
            "BigQuery",
            "GCS Data proc",
            "Erwin",
            "Python",
            "Java",
            "Perl",
            "Shell",
            "Gitflow",
            "BitBucket",
            "JIRA",
            "Confluence",
            "Bamboo",
            "Jenkins",
            "TFS"
        ],
        "programming_languages": [
            "Scala",
            "Python",
            "Java"
        ],
        "experience": 8,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Mountain View, CA",
        "job_id": 3916220106,
        "company": "Syntricate Technologies",
        "title": "GCP Data Engineer",
        "created_on": 1720635478.4546514,
        "description": "Job Title : GCP Data Engineer Location : Mountain View, CA [Needs to be onsite for 1 week once in a quarter on your own expenses] Job Type : W2 Contract Must Have 4 years of professional experience in Stream/Batch Processing systems at scale. Strong Programming skills in Java, Python. Experience in Public Cloud is a must. Experience with GCP and GCP managed services is a strongly required. i. Experience in Messaging/Stream Processing systems on Cloud such as Pub/Sub, Kafka, Kinesis, DataFlow, Flink etc, and/Or ii. Experience in Batch Processing systems such as Hadoop, Pig, Hive, Spark. Experience with Dataproc is a strong plus. Knowledge of DevOps principles and tools (e.g. CI/CD, IaC/Terraform). Strong understanding of Containerization technologies (e.g., Docker, Kubernetes). Strong problem solving and critical thinking skills. Strong",
        "url": "https://www.linkedin.com/jobs/view/3916220106",
        "summary": "GCP Data Engineer role requiring 4+ years of experience in Stream/Batch Processing systems at scale. Strong programming skills in Java and Python are essential, along with experience in Public Cloud, GCP, and GCP managed services. Experience with Messaging/Stream Processing systems like Pub/Sub, Kafka, Kinesis, DataFlow, Flink, or Batch Processing systems such as Hadoop, Pig, Hive, Spark is required. Knowledge of DevOps principles, tools like CI/CD, IaC/Terraform, Containerization technologies (Docker, Kubernetes), and strong problem-solving skills are also necessary.",
        "industries": [
            "Technology",
            "Data & Analytics",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Problem Solving",
            "Critical Thinking"
        ],
        "hard_skills": [
            "Java",
            "Python",
            "Stream Processing",
            "Batch Processing",
            "Public Cloud",
            "GCP",
            "Pub/Sub",
            "Kafka",
            "Kinesis",
            "DataFlow",
            "Flink",
            "Hadoop",
            "Pig",
            "Hive",
            "Spark",
            "Dataproc",
            "DevOps",
            "CI/CD",
            "IaC",
            "Terraform",
            "Docker",
            "Kubernetes"
        ],
        "tech_stack": [
            "GCP",
            "Pub/Sub",
            "Kafka",
            "Kinesis",
            "DataFlow",
            "Flink",
            "Hadoop",
            "Pig",
            "Hive",
            "Spark",
            "Dataproc",
            "Terraform",
            "Docker",
            "Kubernetes"
        ],
        "programming_languages": [
            "Java",
            "Python"
        ],
        "experience": 4,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Los Angeles, CA",
        "job_id": 3914850573,
        "company": "Gunderson Dettmer",
        "title": "Data Engineer",
        "created_on": 1720635480.2423072,
        "description": "Gunderson Dettme r is the only business law firm of its kind - exclusively serving the global venture capital and emerging technology marketplace. With 400 attorneys in eleven offices - from Silicon Valley to Singapore - we innovate for innovators, accelerate entrepreneurship, and help build companies at every stage of the growth lifecycle. We are committed to being the employer of choice by working together to create an environment, in which each of our people can grow, take initiative, and develop a fun, fulfilling and financially rewarding career. Benefits In addition to offering competitive salaries, we also offer an excellent benefit package, which includes full medical, dental and vision coverage; 401(k) Profit Sharing Plan; Flexible Spending Account and Paid Time Off. Job Description Join Gunderson Dettmer, the preeminent international law firm with an exclusive focus on the innovation economy, as a full time Data Engineer to work at the intersection of technology, law, data and innovation. We’re at the forefront of legal innovation and are actively developing and refining the law firm tech stack of the future. This unique position offers the opportunity to help architect the modern law firm’s data infrastructure, power key applications to solve problems for legal practitioners, and craft the integration layer between these bespoke tools and our data warehouse. We seek a talented individual who thrives in a collaborative, cross-functional environment and embodies a commitment to precision, repeatability, and quality. Responsibilities: Design, build and manage robust, end-to-end ETL processes with performance monitoring Create data integrations between various cloud-based software solutions and platforms (including our enterprise data warehouse) using iPaaS tools Manage and optimize enterprise data warehouse on RDBMS with a complex Dim/Fact data model for analytics Manage and optimize cloud infrastructure to ensure scalability, reliability, and cost-effectiveness Work with cross-functional teams—including legal engineers, data visualization specialists, business solutions and product specialists and subject-matter experts—to ensure efficient and coherent data processes and delivery of high-quality data products Improve and maintain quality control, implementing best practices to ensure data products meet our standards of reliability, usability, and performance Proactively identify opportunities to improve data quality, tooling, and version controls Requirements: 5+ years of relevant experience designing, building and managing ETL processes with a variety of tools Significant experience with: iPaaS tools like Apache AirFlow API technologies such as REST and GraphQL and transfering files using SFTP Data transformation tools like dbt RDBMS platforms like Snowflake noSQL databases like Mongo Dim/Fact data models for analytics Proficiency writing Python scripts and SQL queries (with experience writing Mongo Atlas aggregation queries considered a plus) Exceptional problem-solving skills, ability to work independently, and a collaborative spirit Excellent communication skills, underpinning effective collaboration with various teams and subject-matter experts Nice-to-haves: Experience with Google Cloud Platform (particularly IAM permissions, as well as log analytics, queries and dashboards) Experience with other Snowflake platform features like Snowsight and Cortex Experience in a financial services, professional services or law firm environment Experience in, or familiarity with, the venture-backed company or venture capital space, especially including familiarity with private company equity data schemas and transaction modeling Location: Work out of any of our firm’s U.S. offices or remotely. Our Offer: Join us as a Data Engineer and be at the forefront of innovation and change in the legal industry. We offer a dynamic, supportive, and collaborative work environment, where our contributions will shape the future of legal service delivery. Be part of our journey to optimize the way we serve our clients and make a lasting impact on the legal landscape. The expected starting salary for this position is $100,00 - $200,000 annually, dependent upon qualifications, experience and location. Gunderson Dettmer is an Equal Opportunity Employer and does not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class. By applying to this job you acknowledge that you have read the California Consumer Privacy Act Applicant Notice View Powered by JazzHR YfNUcpGxai",
        "url": "https://www.linkedin.com/jobs/view/3914850573",
        "summary": "Gunderson Dettmer, a leading law firm specializing in the innovation economy, seeks a Data Engineer to join their team. This role involves designing, building, and managing data pipelines, integrating cloud-based software solutions, and optimizing the firm's data warehouse. The ideal candidate will have strong ETL experience, proficiency in iPaaS tools, API technologies, and RDBMS platforms, as well as experience in Python and SQL.",
        "industries": [
            "Law",
            "Legal",
            "Technology",
            "Venture Capital",
            "Emerging Technology",
            "Financial Services",
            "Professional Services"
        ],
        "soft_skills": [
            "Collaboration",
            "Problem-Solving",
            "Communication",
            "Independent Work",
            "Precision",
            "Repeatability",
            "Quality"
        ],
        "hard_skills": [
            "ETL",
            "iPaaS",
            "Apache Airflow",
            "REST API",
            "GraphQL",
            "SFTP",
            "Data Transformation",
            "dbt",
            "RDBMS",
            "Snowflake",
            "NoSQL",
            "Mongo",
            "Python",
            "SQL",
            "Mongo Atlas Aggregation Queries",
            "Google Cloud Platform",
            "IAM Permissions",
            "Log Analytics",
            "Snowsight",
            "Cortex",
            "Private Company Equity Data Schemas",
            "Transaction Modeling"
        ],
        "tech_stack": [
            "Apache Airflow",
            "REST API",
            "GraphQL",
            "SFTP",
            "dbt",
            "Snowflake",
            "Mongo",
            "Google Cloud Platform",
            "IAM",
            "Log Analytics",
            "Snowsight",
            "Cortex"
        ],
        "programming_languages": [
            "Python",
            "SQL"
        ],
        "experience": 5,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 200000,
            "min": 100000
        },
        "benefits": [
            "Competitive Salaries",
            "Full Medical Coverage",
            "Dental Coverage",
            "Vision Coverage",
            "401(k) Profit Sharing Plan",
            "Flexible Spending Account",
            "Paid Time Off"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Culver City, CA",
        "job_id": 3946624587,
        "company": "Amazon",
        "title": "Data Engineer , GMAC Economics",
        "created_on": 1720635482.082052,
        "description": "Description Cutting edge big data technology with Spark, EMR, Glue, SageMaker, and Airflow? Check. Deep involvement with business strategy decisions? Check. Work across one of the world's largest and most complex data environments? Check. Are you excited about the idea to work with big data resources and technologies? Are you up to the challenge of working with top PhD economics and data science researchers to bring front tier economic theories to production? Are you ready to build data pipelines and integrate AWS services that inform billion dollar business decisions across Amazon? If your answer is yes, come join us! We are a hybrid research + engineering team that brings disruptive econometric models to life in the media entertainment and advertising domains. As an experienced Software Engineer, you will collaborate with economists, data scientists, and developers across the company to develop, test and deploy services that implement a wide range of econometric models. This requires the use of sophisticated distributed systems, application of advanced statistical techniques and the processing of big data. A successful candidate will have a passion for innovation, interest in cutting-edge technology, and excitement about working with data in a high-impact business domain. This is an opportunity to create and build software for advanced data analytics to support and enable data-driven decision making inside Amazon’s fast growing digital media businesses. Key job responsibilities As a Data Engineer, You Will/may Design, implement, and automate deployment of our distributed system for collecting and processing log events from multiple sources Design data schema and operate internal data warehouses and SQL/NoSQL database systems Own the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions Monitor and troubleshoot operational or data issues in the data pipelines Drive architectural plans and implementation for future data storage, reporting, and analytic solutions Work collaboratively with Business Analysts, Data Scientists, and other internal partners to identify opportunities/problems Provide assistance to the team with troubleshooting, researching the root cause, and thoroughly resolving defects in the event of a problem A day in the life You'll spend most the day coding, possibly writing or testing Spark jobs, updating our reusable python libraries, or tweaking an Apache Airflow graph. You'll join a daily standup and sync with your team mates, and maybe perform a code review or two. Depending on the project phase you may be finishing up your design document for review with your fellow engineers or scientists. Finally you'll probably join a couple of meetings, either related to your project, Sprint Planning, or maybe even a fun team event! About The Team We're a pretty atypical team at Amazon, with engineers outnumbered by scientists 3 to 1. Our mission is to productionize science prototypes into well tuned, scaled systems and support and improve data infrastructure for scientists to quickly prototype new ideas and models. You'll be working closely on your projects not just with other data or software engineers, but with Business Intelligence Engineers, Economists, and Data Scientists. Your engineering background and experience is critical to building reliable, cost effective products for our business partners and Amazon customers. Basic Qualifications 3+ years of data engineering experience Experience with data modeling, warehousing and building ETL pipelines Experience with SQL Experience with Spark Preferred Qualifications Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions Experience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases) Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us. Our compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $118,900/year in our lowest geographic market up to $205,600/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. For more information, please visit https://www.aboutamazon.com/workplace/employee-benefits. This position will remain posted until filled. Applicants should apply via our internal or external career site. Company - Amazon.com Services LLC Job ID: A2667897",
        "url": "https://www.linkedin.com/jobs/view/3946624587",
        "summary": "Join a hybrid research and engineering team at Amazon that focuses on implementing econometric models for media, entertainment, and advertising. This role involves designing, developing, and deploying data pipelines and integrating AWS services to inform billion-dollar business decisions.",
        "industries": [
            "Media",
            "Entertainment",
            "Advertising",
            "Data Science",
            "Research"
        ],
        "soft_skills": [
            "Collaboration",
            "Communication",
            "Problem Solving",
            "Analytical Thinking",
            "Innovation",
            "Passion for Technology"
        ],
        "hard_skills": [
            "Data Modeling",
            "ETL Pipelines",
            "SQL",
            "Spark",
            "AWS Services",
            "Redshift",
            "S3",
            "AWS Glue",
            "EMR",
            "Kinesis",
            "FireHose",
            "Lambda",
            "IAM",
            "Non-Relational Databases",
            "Object Storage",
            "Document or Key-Value Stores",
            "Graph Databases",
            "Column-Family Databases"
        ],
        "tech_stack": [
            "Spark",
            "EMR",
            "Glue",
            "SageMaker",
            "Airflow",
            "Redshift",
            "S3",
            "Kinesis",
            "FireHose",
            "Lambda",
            "IAM"
        ],
        "programming_languages": [
            "SQL",
            "Python"
        ],
        "experience": 3,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 205600,
            "min": 118900
        },
        "benefits": [
            "Medical",
            "Financial",
            "Equity",
            "Sign-on Payments"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3913910225,
        "company": "Unreal Staffing, Inc",
        "title": "Data Systems Engineer",
        "created_on": 1720635483.839618,
        "description": "Company Overview: Welcome to the cutting-edge of data systems engineering! At our company, we're passionate about harnessing the potential of data to drive innovation and transformation. Our mission is to develop robust and scalable data systems that empower organizations to extract insights and make data-driven decisions. Join us and be part of a dynamic team committed to shaping the future of data systems engineering. Position Overview: As a Data Systems Engineer, you'll be at the forefront of designing, building, and optimizing our data systems infrastructure. Working closely with cross-functional teams of data scientists, software engineers, and business analysts, you'll ensure the reliability, scalability, and efficiency of our data systems. If you're passionate about data engineering and eager to tackle complex challenges in data infrastructure, we want you on our team. Requirements Key Responsibilities: Data Systems Architecture: Design and architect scalable and resilient data systems infrastructure to meet business requirements and support data processing and analytics needs Data Pipeline Development: Develop and maintain data pipelines to ingest, process, and transform large volumes of data from various sources into usable formats for analysis and reporting Data Storage and Management: Implement and manage data storage solutions, including databases, data lakes, and distributed file systems, ensuring optimal performance and reliability Data Integration: Integrate data from diverse sources, including internal databases, external APIs, and third-party data providers, ensuring data consistency and integrity Performance Optimization: Optimize data systems performance for speed, scalability, and cost-effectiveness, leveraging caching, indexing, partitioning, and other techniques Data Security and Governance: Implement data security controls and governance policies to protect sensitive data and ensure compliance with regulatory requirements Monitoring and Maintenance: Monitor data systems for performance, availability, and reliability, proactively identifying and addressing issues to minimize downtime and ensure data integrity Documentation and Collaboration: Document data systems architecture, processes, and best practices, and collaborate with cross-functional teams to ensure alignment and transparency Qualifications: Bachelor's degree or higher in Computer Science, Engineering, or related field Strong background in data engineering, with hands-on experience in designing, building, and optimizing data systems infrastructure Proficiency in programming languages such as Python, Java, or Scala, and experience with data processing frameworks such as Apache Spark, Apache Flink, or Apache Beam Experience with distributed computing technologies and concepts, including Hadoop, MapReduce, and distributed file systems (e.g., HDFS) Familiarity with database technologies such as SQL, NoSQL, and NewSQL databases, as well as cloud-based data storage solutions Strong problem-solving abilities and analytical thinking, with a keen attention to detail and a passion for tackling complex technical challenges Excellent communication and collaboration skills, with the ability to work effectively in cross-functional teams and communicate technical concepts to non-technical stakeholders Benefits Competitive salary: The industry standard salary for Data Systems Engineers typically ranges from $140,000 to $210,000 per year, depending on experience and qualifications. Exceptional candidates may be eligible for higher compensation packages Comprehensive health, dental, and vision insurance plans Flexible work hours and remote work options Generous vacation and paid time off Professional development opportunities, including access to training programs, conferences, and workshops State-of-the-art technology environment with access to cutting-edge tools and resources Vibrant and inclusive company culture with team-building activities and social events Opportunities for career growth and advancement within the company Exciting projects with real-world impact across diverse industries Chance to work alongside top talent and industry experts in the field of data systems engineering Join Us: Ready to shape the future of data systems engineering? Apply now to join our team and be part of an exciting journey of innovation and discovery!",
        "url": "https://www.linkedin.com/jobs/view/3913910225",
        "summary": "This role involves designing, building, and optimizing data systems infrastructure for a company focused on data-driven innovation. As a Data Systems Engineer, you'll collaborate with data scientists, software engineers, and business analysts, ensuring the reliability, scalability, and efficiency of data systems. You'll work with data pipelines, storage solutions, data integration, performance optimization, security and governance, monitoring, and documentation.",
        "industries": [
            "Data Engineering",
            "Data Systems",
            "Software Engineering",
            "Business Analysis",
            "Technology",
            "Information Technology",
            "Data Analytics",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Problem-solving",
            "Analytical Thinking",
            "Attention to Detail",
            "Communication",
            "Collaboration",
            "Teamwork",
            "Technical Communication",
            "Stakeholder Management"
        ],
        "hard_skills": [
            "Python",
            "Java",
            "Scala",
            "Apache Spark",
            "Apache Flink",
            "Apache Beam",
            "Hadoop",
            "MapReduce",
            "HDFS",
            "SQL",
            "NoSQL",
            "NewSQL",
            "Cloud Storage",
            "Data Pipelines",
            "Data Storage Solutions",
            "Data Integration",
            "Performance Optimization",
            "Data Security",
            "Data Governance",
            "Monitoring",
            "Documentation"
        ],
        "tech_stack": [
            "Apache Spark",
            "Apache Flink",
            "Apache Beam",
            "Hadoop",
            "MapReduce",
            "HDFS",
            "SQL",
            "NoSQL",
            "NewSQL",
            "Cloud Storage",
            "Python",
            "Java",
            "Scala"
        ],
        "programming_languages": [
            "Python",
            "Java",
            "Scala"
        ],
        "experience": 0,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Computer Science",
                "Engineering",
                "Related Field"
            ]
        },
        "salary": {
            "max": 210000,
            "min": 140000
        },
        "benefits": [
            "Competitive Salary",
            "Health Insurance",
            "Dental Insurance",
            "Vision Insurance",
            "Flexible Work Hours",
            "Remote Work Options",
            "Vacation",
            "Paid Time Off",
            "Professional Development",
            "Training Programs",
            "Conferences",
            "Workshops",
            "State-of-the-Art Technology",
            "Team-Building Activities",
            "Social Events",
            "Career Growth",
            "Advancement Opportunities",
            "Real-World Impact",
            "Work with Top Talent",
            "Industry Experts"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3958004670,
        "company": "Fay",
        "title": "Software Engineer",
        "created_on": 1720635485.6100883,
        "description": "Can you help us build out a digital private practice for medical professionals that powers high quality care while handling complex payments and insurance billing behind the scenes? Our platform is already supporting a thriving business, but we need help keeping up with the rapid pace of growth and building features that will drive better care for some of the most common chronic conditions in the U.S. We're looking for a generalist full stack engineer who thrives in an early-stage startup environment and is willing to learn whatever it takes to get the job done. What You'll be Doing: Coding : You'll be focused on building — writing code up and down the stack Discovering : As a small team, we don't have all of the knowhow for what we're going to build (e.g., how to put a hold on a credit card) and need someone who's unafraid to figure it out Sleuthing : We need a clever code detective to root cause and fix the range of issues that arise in real-world software systems Idea-surfacing : You'll be close to the code and will have good ideas for what we should be focusing our engineering efforts on — we want you to speak up! Standard-setting : As an early employee and engineer at Fay, you'll be helping establish team culture and engineering standards The best companies are made of the best people. There's no shortage of work ahead, but we stay balanced and look forward to celebrating our wins as a team. See our careers page here to learn more about working on our our team.",
        "url": "https://www.linkedin.com/jobs/view/3958004670",
        "summary": "We're looking for a full stack engineer to join a growing startup building a digital platform for medical professionals. You'll be responsible for building and maintaining the platform, from front-end to back-end, while also researching new solutions and collaborating with the team to build a strong engineering culture.",
        "industries": [
            "Healthcare",
            "Software",
            "Technology",
            "Fintech"
        ],
        "soft_skills": [
            "Problem-solving",
            "Communication",
            "Teamwork",
            "Collaboration",
            "Proactive",
            "Adaptability",
            "Learning",
            "Idea generation",
            "Critical thinking"
        ],
        "hard_skills": [
            "Full stack development",
            "Front-end development",
            "Back-end development",
            "Coding",
            "Debugging",
            "Troubleshooting",
            "Research",
            "Payment processing",
            "Insurance billing"
        ],
        "tech_stack": [],
        "programming_languages": [],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Sunnyvale, CA",
        "job_id": 3945825023,
        "company": "ClifyX",
        "title": "Big Data Engineer",
        "created_on": 1720635487.321671,
        "description": "Big Data Engineer (PySpark, SQL and Scala/Python) - Apple - Day 1 onsite hybrid (Austin, TX /Sunnyvale, CA) Looking for Big Data Engineer profiles who is strong and having good hands-on experience in Spark/PySpark, Scala/Python and SQL Number of open positions: 5 Preferred Location: Austin/Sunnyvale Rate: MSA (Can go up to $84 max if the candidate is strong in technical & communication skills and diversified profile) JD: In the role of Lead Data Engineer, you will interface with key stakeholders and apply your technical proficiency across different stages of the Software Development Life Cycle including Requirements Elicitation, Application Architecture definition and Design. You will play an important role in creating the high-level design artifacts. You will also deliver high quality code deliverables for a module, lead validation for all types of testing and support activities related to implementation, transition and warranty. You will be part of a learning culture, where teamwork and collaboration are encouraged, excellence is rewarded, and diversity is respected and valued. Required Qualifications: Bachelor's degree or foreign equivalent required from an accredited institution. Will also consider three years of progressive experience in the specialty in lieu of every year of education At least 6 years of experience in Information Technology. At least 3 years of hands on experience with Hadoop distributed frameworks while handling large amount of big data using Spark and Hadoop Ecosystems. At Least 3 Years Of Experience With Spark/PySpark Required. At least 2 years of experience with Scala required. At least 3 years of experience with SQL with any RDBMS. Preferred Qualifications: At least 1 years of AWS development experience is preferred Ability to work within deadlines and effectively prioritize and execute on tasks. Strong communication skills (verbal and written) with ability to communicate across teams, internal and external at all levels. Experience in Drive automations DevOps Knowledge is an added advantage.",
        "url": "https://www.linkedin.com/jobs/view/3945825023",
        "summary": "Lead Data Engineer at Apple responsible for designing, developing, and implementing big data solutions using Spark/PySpark, Scala, and SQL. You will collaborate with stakeholders, create high-level designs, and ensure the delivery of high-quality code. The position requires 6+ years of IT experience, 3+ years of hands-on experience with Hadoop and Spark, and strong communication skills. Preference for AWS experience and DevOps knowledge.",
        "industries": [
            "Technology",
            "Software Development",
            "Data Engineering",
            "Big Data"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Teamwork",
            "Problem-solving",
            "Time management",
            "Prioritization"
        ],
        "hard_skills": [
            "Spark",
            "PySpark",
            "Scala",
            "SQL",
            "Hadoop",
            "AWS",
            "DevOps"
        ],
        "tech_stack": [
            "Spark",
            "PySpark",
            "Scala",
            "SQL",
            "Hadoop",
            "AWS"
        ],
        "programming_languages": [
            "Scala",
            "Python",
            "SQL"
        ],
        "experience": 6,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Information Technology",
                "Computer Science",
                "Data Science",
                "Engineering"
            ]
        },
        "salary": {
            "max": 84,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3925946229,
        "company": "VARITE INC",
        "title": "Data Engineer - II",
        "created_on": 1720635489.2543924,
        "description": "Pay rate range: $65-75/hr. ***Cloud Data Engineer*** Location: San Francisco Position type: Hybrid (2+ days in office) Position Description: • Designs, develops, modifies, tests, and automates the data warehouse and business intelligence applications solutions. This includes design, development, architecture recommendations, quality management, metadata and repository creation, trouble-shooting problems, and tuning warehouse applications. • Develops transition and implementation plans. Recommends changes in development, maintenance, and standards. • Advanced analytical ability and technical skill as well as the ability to provide innovative solutions to technical needs and business requirements. • Ability to exercise independent judgment in making complex business decisions. • Acute attention to detail with a high level of data integrity and accuracy • Excellent oral and written communication, with interpersonal skills to work with people at all levels of the organization. • Ability to translate highly technical information into non-technical terms. • Excellent computer skills including Microsoft Office along with various other software applications as needed for the role. • Broad knowledge of the programming tools, concepts, practices, and principles including design, implementation, and testing • Position requires continuous visual concentration and manual dexterity to operate PC • Requires prolonged sitting and minimal standing/walking. • May require on-call status. • Rare domestic travel including overnight stays may be necessary. Technical Skills: • Expert in developing and analyzing complex SQL on a variety of RDBMS (Microsoft SQL Server, Oracle) • Expert knowledge of data modeling and understanding of different data structures and their benefits and limitations under particular use cases • Experience with ETL tools (Informatica) • Ability to create quality ERD's (entity-relationship diagrams) • Excellent writing skills for writing user and system documentation • AWS Cloud Data Warehousing Technologies o Experience using core AWS services to build and support data warehouse solutions leveraging AWS architecture best practices (S3, DMS, Glue, Lambda) o Development/modeling experience with Amazon Redshift o Experience using the AWS service APIs, AWS CLI, and SDKs to build applications. o Proficiency in developing, deploying, and debugging cloud-based applications using AWS. o Ability to use a CI/CD pipeline to deploy applications on AWS (GitLab, Terraform, DBMaestro) o Ability to apply a basic understanding of cloud-native applications to write code. o Proficiency writing code for serverless applications. o Ability to write code using AWS security best practices (e.g., not using secret and access keys in the code, instead using IAM roles) o Ability to author, maintain, and debug code modules on AWS. • Experience with visualization tools (Tableau) • Experience creating scripts with Python. • Experience working on an Agile team. • Understanding of application lifecycle management • Understanding of the use of containers in the development process. Responsibilities: • Contributes to the design, development, testing, implementation, and review of complex data warehouse and business intelligence solutions. • Develops all or part of complex data warehouse applications, develops software from established requirements, builds reports and dashboards, plans and coordinates work with fellow programmers to meet delivery commitments, creates prototypes; offers insight on the feasibility of system designs. • Contributes to the design of technology infrastructure and configurations, recommends process improvements. • Reviews complex patches and new versions of data warehouse applications. Implements complex software packages and deploys code. • Key participant in cross-functional team initiatives and process improvement projects. Qualifications: • 5 to 9+ years of experience in data engineering, data science, and software engineering. • Bachelor's degree in computer science, Information Systems, or another related field • Must be a US Citizen or a Green Card holder.",
        "url": "https://www.linkedin.com/jobs/view/3925946229",
        "summary": "This is a hybrid role for a Cloud Data Engineer in San Francisco. The ideal candidate will have 5+ years of experience in data engineering, data science, and software engineering. The role involves designing, developing, testing, and automating data warehouse and business intelligence solutions. This includes working with various AWS services like S3, DMS, Glue, Lambda, Redshift, and using CI/CD pipeline with GitLab, Terraform, and DBMaestro.  The candidate should be proficient in SQL, data modeling, ETL tools like Informatica, and visualization tools like Tableau. Experience with Python scripting and Agile methodologies is preferred.  ",
        "industries": [
            "Data Engineering",
            "Software Engineering",
            "Business Intelligence",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Communication",
            "Interpersonal Skills",
            "Analytical Ability",
            "Problem-Solving",
            "Detail-Oriented",
            "Teamwork",
            "Independent Judgment",
            "Adaptability",
            "Process Improvement",
            "Technical Communication"
        ],
        "hard_skills": [
            "SQL",
            "RDBMS",
            "Microsoft SQL Server",
            "Oracle",
            "Data Modeling",
            "ETL",
            "Informatica",
            "ERD",
            "AWS",
            "S3",
            "DMS",
            "Glue",
            "Lambda",
            "Redshift",
            "AWS CLI",
            "AWS SDK",
            "CI/CD",
            "GitLab",
            "Terraform",
            "DBMaestro",
            "Python",
            "Tableau",
            "Agile",
            "Application Lifecycle Management",
            "Containers"
        ],
        "tech_stack": [
            "AWS",
            "S3",
            "DMS",
            "Glue",
            "Lambda",
            "Redshift",
            "GitLab",
            "Terraform",
            "DBMaestro",
            "Informatica",
            "Tableau"
        ],
        "programming_languages": [
            "SQL",
            "Python"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Computer Science",
                "Information Systems"
            ]
        },
        "salary": {
            "max": 75,
            "min": 65
        },
        "benefits": [
            "On-Call Status",
            "Domestic Travel"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Jose, CA",
        "job_id": 3851074712,
        "company": "TikTok",
        "title": "Data Engineer, Cloud and System",
        "created_on": 1720635491.2906318,
        "description": "Responsibilities TikTok is the leading destination for short-form mobile video. At TikTok, our mission is to inspire creativity and bring joy. TikTok's global headquarters are in Los Angeles and Singapore, and its offices include New York, London, Dublin, Paris, Berlin, Dubai, Jakarta, Seoul, and Tokyo. Why Join Us Creation is the core of TikTok's purpose. Our platform is built to help imaginations thrive. This is doubly true of the teams that make TikTok possible. Together, we inspire creativity and bring joy - a mission we all believe in and aim towards achieving every day. To us, every challenge, no matter how difficult, is an opportunity; to learn, to innovate, and to grow as one team. Status quo? Never. Courage? Always. At TikTok, we create together and grow together. That's how we drive impact - for ourselves, our company, and the communities we serve. Join us. Our Infrastructure Engineering team supports the company's fast growth by building and operating hyper-scale datacenters, managing the life cycle of server fleet, providing cloud solutions, and developing various infrastructure services and making sure they are scalable and are reliable. Responsibilities - What You'll Do - Identify data sources with different types and categories, and collect large structured and unstructured datasets, files, logs, and variables. - Building and maintaining complex data ETL pipelines, and using and utilizing algorithms and models for mining data to support petabytes of real-time or offline data. - Validate, improve, and clean data by applying models/algorithms, like error analysis, uniform data, and accuracy validation. - Analyze data based on trends and patterns, and interpret data with clear objectives in mind to help solve problems in multiple domains, like performance, cost, and bug digging. - Visualize data and define multiple perspectives of data view for multiple business purposes. - Establish solid engineering practice while collaborating with Data Engineers, Scientists, Software Engineers, and SREs. Qualifications Required skills and qualifications - 4-5 years of experience in data science. - Proficiency with building and optimizing Big Data pipelines and architectures. - Advanced with mathematics, and statistical analysis. - Experience with Big Data technologies(Hadoop, Hive, Spark, Kafka, ClickHouse, Flink etc.) - Ability to think critically and to formulate solutions to problems in a clear and concise way Preferred skills and qualifications - Bachelor’s degree (or equivalent) in statistics, applied mathematics, or related discipline. - Experience with Cloud Platforms, Sys, and Network Engineering is a plus. - Natural curiosity and a creative mind. - Mathematical and statistical expertise. - Professional certification. TikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too. TikTok is committed to providing reasonable accommodations in our recruitment processes for candidates with disabilities, pregnancy, sincerely held religious beliefs or other reasons protected by applicable laws. If you need assistance or a reasonable accommodation, please reach out to us at https://shorturl.at/cdpT2 Job Information: 【For Pay Transparency】Compensation Description (annually) The base salary range for this position in the selected city is $145000 - $355000 annually. ​ Compensation may vary outside of this range depending on a number of factors, including a candidate’s qualifications, skills, competencies and experience, and location. Base pay is one part of the Total Package that is provided to compensate and recognize employees for their work, and this role may be eligible for additional discretionary bonuses/incentives, and restricted stock units. ​ Our company benefits are designed to convey company culture and values, to create an efficient and inspiring work environment, and to support our employees to give their best in both work and life. We offer the following benefits to eligible employees: ​ We cover 100% premium coverage for employee medical insurance, approximately 75% premium coverage for dependents and offer a Health Savings Account(HSA) with a company match. As well as Dental, Vision, Short/Long term Disability, Basic Life, Voluntary Life and AD&D insurance plans. In addition to Flexible Spending Account(FSA) Options like Health Care, Limited Purpose and Dependent Care. ​ Our time off and leave plans are: 10 paid holidays per year plus 17 days of Paid Personal Time Off (PPTO) (prorated upon hire and increased by tenure) and 10 paid sick days per year as well as 12 weeks of paid Parental leave and 8 weeks of paid Supplemental Disability. ​ We also provide generous benefits like mental and emotional health benefits through our EAP and Lyra. A 401K company match, gym and cellphone service reimbursements. The Company reserves the right to modify or change these benefits programs at any time, with or without notice. ​",
        "url": "https://www.linkedin.com/jobs/view/3851074712",
        "summary": "TikTok is looking for a Data Scientist to join their Infrastructure Engineering team. The role involves collecting, cleaning, and analyzing large datasets to support the company's fast growth.  Responsibilities include building data pipelines, using algorithms for data mining, and visualizing data to inform various business decisions. The ideal candidate will have 4-5 years of experience in data science, proficiency in Big Data technologies, and a strong understanding of statistics and mathematics.",
        "industries": [
            "Technology",
            "Social Media",
            "Data Science",
            "Big Data",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Critical thinking",
            "Problem-solving",
            "Communication",
            "Collaboration"
        ],
        "hard_skills": [
            "Data Science",
            "Big Data Pipelines",
            "Big Data Architectures",
            "Mathematics",
            "Statistical Analysis",
            "Hadoop",
            "Hive",
            "Spark",
            "Kafka",
            "ClickHouse",
            "Flink",
            "Error Analysis",
            "Uniform Data",
            "Accuracy Validation",
            "Trend Analysis",
            "Pattern Analysis",
            "Data Visualization",
            "Cloud Platforms",
            "Sys",
            "Network Engineering"
        ],
        "tech_stack": [
            "Hadoop",
            "Hive",
            "Spark",
            "Kafka",
            "ClickHouse",
            "Flink",
            "Cloud Platforms"
        ],
        "programming_languages": [],
        "experience": 4,
        "education": {
            "min_degree": "Bachelor’s degree",
            "fields": [
                "Statistics",
                "Applied Mathematics",
                "Related discipline"
            ]
        },
        "salary": {
            "max": 355000,
            "min": 145000
        },
        "benefits": [
            "Medical Insurance",
            "Dental Insurance",
            "Vision Insurance",
            "Short/Long term Disability",
            "Basic Life",
            "Voluntary Life",
            "AD&D insurance",
            "Flexible Spending Account (FSA)",
            "Paid Time Off (PPTO)",
            "Paid Sick Days",
            "Paid Parental Leave",
            "Paid Supplemental Disability",
            "Employee Assistance Program (EAP)",
            "Lyra",
            "401K Company Match",
            "Gym Reimbursements",
            "Cellphone Service Reimbursements"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Jose, CA",
        "job_id": 3655228688,
        "company": "TikTok",
        "title": "Data Engineer, Data Application",
        "created_on": 1720635493.1966667,
        "description": "Responsibilities TikTok is the leading destination for short-form mobile video. Our mission is to inspire creativity and bring joy. TikTok has global offices including Los Angeles, New York, London, Paris, Berlin, Dubai, Singapore, Jakarta, Seoul and Tokyo. Why Join Us Creation is the core of TikTok's purpose. Our platform is built to help imaginations thrive. This is doubly true of the teams that make TikTok possible. Together, we inspire creativity and bring joy - a mission we all believe in and aim towards achieving every day. To us, every challenge, no matter how difficult, is an opportunity; to learn, to innovate, and to grow as one team. Status quo? Never. Courage? Always. At TikTok, we create together and grow together. That's how we drive impact - for ourselves, our company, and the communities we serve. Join us. TikTok's immersive experience, global presence, and high engagement makes it the ideal marketing destination for business, big and small, to showcase their unique brand identity, connect with their consumers, and build strong lasting relationships over time. The Ads Data Team builds and manages the petabyte scale data infrastructure, batch/realtime pipelines and services to support Tiktok's global Ads business. We are committed to building a robust data foundation and scalable data applications, unblocking the full potential of advertising data to optimize advertiser experience, boost business growth, empower strategy execution. We are looking for passionate Data Engineers that have strong problem solving skills to join forces with talented cross functional partners (business operation, data science, engineering and product management) to solve some of the most interesting data challenges with efficiency and quality. In this role, you will contribute to the company's core business across innovative advertising products, campaign management and measurement solutions. You will see a direct impact from your day-to-day work to customer satisfaction and company growth. Responsibilities: 1. Work closely with Product Managers, Data Scientists/Analysts, and Software/Machine Learning Engineers and other stakeholders to understand data requirements and deliver data solutions that meet business needs. 2. Evaluate, implement and maintain data infrastructure tools and technologies to support efficient data processing, storage and query. 3. Design, build and optimize scalable data pipelines to ingest, process and transform large volumes of data. 4. Design and implement robust data models and visualization to support complex analytical queries and reporting requirements. 5. Ensure the data integrity, accuracy and consistency of data by implementing data quality checks, validation processes and monitoring mechanisms. 6. Continously optimize data pipelines, queries and processes to improve performance, reduce latency and enhance scalability. 7. Provide rapid response to SLA oncall support to business critical data pipelines. 8. Create and maintain good documentation for data assets and promote best practices for data governance within the data user community. Qualifications Qualifications: 1. Bachelor's degree in Computer Science, Engineering, or a related field. 2. Proven 1~3 years' experience as a Data Engineer or similar role in supporting data-centric business. 3. Strong knowledge of SQL and experience working with relational and non-relational databases. 4. Proficiency in programming languages such as Python, Java, Go etc. 5. Solid understanding of data modeling and data warehousing concepts, data integration and ETL/ELT techniques. 6. Effective communication skills and ability to collaborate effectively with cross-functional teams. 7. Excellent problem-solving skills, attention to detail, and ability to thrive in a fast-paced environment. Preferred Qualifications: 1. Experience with big data technologies(e.g. Apache Hadoop, Spark, Kafka, Flink) and working with terabyte to petabyte scale data. 2. Experience with cloud data warehouses(eg. Snowflake, Databricks, BigQuery) and modern business intelligence/data stack. 3. Experience with data governance, data privacy and compliance. 4. Experience in the advertising, e-commerce or gaming industry. TikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too. TikTok is committed to providing reasonable accommodations in our recruitment processes for candidates with disabilities, pregnancy, sincerely held religious beliefs or other reasons protected by applicable laws. If you need assistance or a reasonable accommodation, please reach out to us at https://shorturl.at/cdpT2 Job Information: 【For Pay Transparency】Compensation Description (annually) The base salary range for this position in the selected city is $136800 - $205000 annually. ​ Compensation may vary outside of this range depending on a number of factors, including a candidate’s qualifications, skills, competencies and experience, and location. Base pay is one part of the Total Package that is provided to compensate and recognize employees for their work, and this role may be eligible for additional discretionary bonuses/incentives, and restricted stock units. ​ Our company benefits are designed to convey company culture and values, to create an efficient and inspiring work environment, and to support our employees to give their best in both work and life. We offer the following benefits to eligible employees: ​ We cover 100% premium coverage for employee medical insurance, approximately 75% premium coverage for dependents and offer a Health Savings Account(HSA) with a company match. As well as Dental, Vision, Short/Long term Disability, Basic Life, Voluntary Life and AD&D insurance plans. In addition to Flexible Spending Account(FSA) Options like Health Care, Limited Purpose and Dependent Care. ​ Our time off and leave plans are: 10 paid holidays per year plus 17 days of Paid Personal Time Off (PPTO) (prorated upon hire and increased by tenure) and 10 paid sick days per year as well as 12 weeks of paid Parental leave and 8 weeks of paid Supplemental Disability. ​ We also provide generous benefits like mental and emotional health benefits through our EAP and Lyra. A 401K company match, gym and cellphone service reimbursements. The Company reserves the right to modify or change these benefits programs at any time, with or without notice. ​",
        "url": "https://www.linkedin.com/jobs/view/3655228688",
        "summary": "TikTok is seeking a Data Engineer to join its Ads Data Team and build petabyte-scale data infrastructure to support its global advertising business. This role involves working with cross-functional teams to understand data requirements, design and implement data pipelines, ensure data integrity, optimize data processes, and provide on-call support for critical pipelines. The ideal candidate will have strong SQL and programming skills, experience with big data technologies, cloud data warehouses, and data governance.",
        "industries": [
            "Technology",
            "Social Media",
            "Marketing",
            "Advertising",
            "Data Engineering"
        ],
        "soft_skills": [
            "Problem Solving",
            "Communication",
            "Collaboration",
            "Attention to Detail",
            "Adaptability"
        ],
        "hard_skills": [
            "SQL",
            "Python",
            "Java",
            "Go",
            "Data Modeling",
            "Data Warehousing",
            "ETL/ELT",
            "Apache Hadoop",
            "Spark",
            "Kafka",
            "Flink",
            "Snowflake",
            "Databricks",
            "BigQuery",
            "Data Governance",
            "Data Privacy",
            "Compliance"
        ],
        "tech_stack": [
            "Apache Hadoop",
            "Spark",
            "Kafka",
            "Flink",
            "Snowflake",
            "Databricks",
            "BigQuery"
        ],
        "programming_languages": [
            "SQL",
            "Python",
            "Java",
            "Go"
        ],
        "experience": 1,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Engineering",
                "Related Fields"
            ]
        },
        "salary": {
            "max": 205000,
            "min": 136800
        },
        "benefits": [
            "Medical Insurance",
            "Dental Insurance",
            "Vision Insurance",
            "Short/Long Term Disability",
            "Basic Life Insurance",
            "Voluntary Life Insurance",
            "AD&D Insurance",
            "Flexible Spending Account",
            "Paid Time Off",
            "Paid Sick Days",
            "Parental Leave",
            "Supplemental Disability",
            "Mental Health Benefits",
            "401K Match",
            "Gym Reimbursement",
            "Cellphone Reimbursement"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3887689892,
        "company": "HireIO, Inc.",
        "title": "Senior Software Development Engineer - Big Data",
        "created_on": 1720635495.1972451,
        "description": "Team Introduction The Data Platform team works on building data infrastructures and data products to support business engineering teams. As a Software Development Engineer in the data platform team, you will have the opportunity to build, optimize and grow one of the largest data platforms in the world. You'll have the opportunity to gain hands-on experience on all kinds of systems in the data platform ecosystem. Your work will have a direct and huge impact on the company's core products as well as hundreds of millions of users. Responsibilities - What You'll Do Design and build data transformations efficiently and reliably for different purposes (e.g. reporting, growth analysis, multi-dimensional analysis) Design and implement reliable, scalable, robust and extensible big data systems that support core products and business Establish solid design and best engineering practice for engineers as well as non-technical people Requirements BS/MS from a quantitative field of study (CS, STEM, etc) Experience in API, backend, and data services development Experience in Big Data stack(Hadoop, M/R, Hive, Spark, Metastore, Presto, Flume, Kafka, ClickHouse, Flink etc.) Experience with ETL (Extraction, Transformation & Loading) or ELT, and architecting data systems Ability to ship code in Java, Python and SQL Solid communication and collaboration skills",
        "url": "https://www.linkedin.com/jobs/view/3887689892",
        "summary": "Software Development Engineer for the Data Platform team responsible for building, optimizing, and growing a large-scale data platform. Responsibilities include designing and implementing data transformations and big data systems, establishing engineering best practices, and collaborating with engineers and non-technical stakeholders.",
        "industries": [
            "Technology",
            "Data Science",
            "Software Development",
            "Big Data"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration"
        ],
        "hard_skills": [
            "API Development",
            "Backend Development",
            "Data Services Development",
            "Hadoop",
            "MapReduce",
            "Hive",
            "Spark",
            "Metastore",
            "Presto",
            "Flume",
            "Kafka",
            "ClickHouse",
            "Flink",
            "ETL",
            "ELT",
            "Java",
            "Python",
            "SQL"
        ],
        "tech_stack": [
            "Hadoop",
            "MapReduce",
            "Hive",
            "Spark",
            "Metastore",
            "Presto",
            "Flume",
            "Kafka",
            "ClickHouse",
            "Flink"
        ],
        "programming_languages": [
            "Java",
            "Python",
            "SQL"
        ],
        "experience": 0,
        "education": {
            "min_degree": "BS",
            "fields": [
                "CS",
                "STEM"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "California, United States",
        "job_id": 3970386503,
        "company": "Netflix",
        "title": "Software Engineer (L5), Privacy Engineering",
        "created_on": 1720635497.1081972,
        "description": "Netflix is revolutionizing entertainment by producing, distributing, and streaming content at a massive scale. Over 260 million members around the world are currently enjoying more than two billion hours per month of original series, documentaries and feature films. Visit our Long-Term View to learn more. The Team The Privacy Engineering organization is dedicated to ensuring safe & efficient data use across Netflix, and the Privacy Infrastructure Engineering team plays a key role in this mission. We work closely with our stunning colleagues in privacy engineering designing and building privacy and data protection solutions for the entire company. We promote privacy-enhancing designs, set engineering standards and build systems to ensure Netflix is managing data correctly. We are a small and agile team with tremendous visibility and impact, and with Netflix’s culture of “People Over Process,” you’ll decide how to best use your talents and time. The Role We are looking for a Senior Software Engineer to build privacy engineering solutions across the entire Netflix infrastructure. In this role, you will partner closely with other engineers, data scientists, and TPMs to conceptualize, prototype, develop, optimize, and refine data cataloging, detection, and metadata systems at scale. This role provides a unique opportunity to work across diverse environments, including distributed systems, data streaming, and data warehouse solutions. You will have the freedom to innovate, solve interesting problems, and impact the business in a meaningful way. Desired Background You are passionate about privacy and data protection! You are a highly skilled software engineer with strong experience in system design, building, and developing scalable and sustainable solutions. You are proficient in Java or Python, with experience in Scala is a plus. You excel in using SQL (any variant) with large data sets and have built data pipelines on technologies such as Spark, Presto, Flink, or Kafka. You have a bias towards action and are comfortable navigating through ambiguous environments. You’re able to communicate complex technical topics to diverse audiences with varying technical and subject matter backgrounds. Stunning colleagues come from different education levels and backgrounds. The Netflix culture determines who we hire, how we work together, and how we make decisions. You will be surrounded by stunning colleagues and empowered to produce your best work. Our compensation structure consists solely of an annual salary; we do not have bonuses. You choose each year how much of your compensation you want in salary versus stock options. To determine your personal top of market compensation, we rely on market indicators and consider your specific job family, background, skills, and experience to determine your compensation in the market range. The range for this role is $100,000 - $720,000. Netflix provides comprehensive benefits including Health Plans, Mental Health support, a 401(k) Retirement Plan with employer match, Stock Option Program, Disability Programs, Health Savings and Flexible Spending Accounts, Family-forming benefits, and Life and Serious Injury Benefits. We also offer paid leave of absence programs. Full-time hourly employees accrue 35 days annually for paid time off to be used for vacation, holidays, and sick paid time off. Full-time salaried employees are immediately entitled to flexible time off. See more details about our Benefits here. Netflix is a unique culture and environment. Learn more here. We are an equal-opportunity employer and celebrate diversity, recognizing that diversity of thought and background builds stronger teams. We approach diversity and inclusion seriously and thoughtfully. We do not discriminate on the basis of race, religion, color, ancestry, national origin, caste, sex, sexual orientation, gender, gender identity or expression, age, disability, medical condition, pregnancy, genetic makeup, marital status, or military service.",
        "url": "https://www.linkedin.com/jobs/view/3970386503",
        "summary": "Netflix is seeking a Senior Software Engineer to build privacy engineering solutions across its infrastructure. This role involves partnering with engineers, data scientists, and TPMs to develop and refine data cataloging, detection, and metadata systems at scale. The ideal candidate has strong experience in system design, development, and data engineering with expertise in Java, Python, SQL, and technologies like Spark, Presto, Flink, or Kafka.  This position offers the opportunity to work in a diverse environment, innovate, and make a meaningful impact on the business.",
        "industries": [
            "Technology",
            "Entertainment",
            "Streaming",
            "Media",
            "Data"
        ],
        "soft_skills": [
            "Passionate about privacy and data protection",
            "Strong communication skills",
            "Ability to work in ambiguous environments",
            "Collaboration",
            "Problem-solving",
            "Innovation"
        ],
        "hard_skills": [
            "System Design",
            "Software Development",
            "Scalability",
            "Sustainability",
            "Java",
            "Python",
            "Scala",
            "SQL",
            "Spark",
            "Presto",
            "Flink",
            "Kafka",
            "Data Pipelines",
            "Data Cataloguing",
            "Data Detection",
            "Metadata Systems"
        ],
        "tech_stack": [
            "Java",
            "Python",
            "Scala",
            "SQL",
            "Spark",
            "Presto",
            "Flink",
            "Kafka"
        ],
        "programming_languages": [
            "Java",
            "Python",
            "Scala"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 720000,
            "min": 100000
        },
        "benefits": [
            "Health Plans",
            "Mental Health Support",
            "401(k) Retirement Plan with employer match",
            "Stock Option Program",
            "Disability Programs",
            "Health Savings and Flexible Spending Accounts",
            "Family-forming benefits",
            "Life and Serious Injury Benefits",
            "Paid Leave of Absence",
            "Paid Time Off",
            "Flexible Time Off"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3948447566,
        "company": "DoorDash",
        "title": "Software Engineer, Data Mobility",
        "created_on": 1720635498.6780853,
        "description": "About the Team Come help us build the world's most reliable on-demand, logistics engine for delivery! We're bringing on experienced engineers to help us further our 24x7, global infrastructure system that powers DoorDash's three-sided marketplace of consumers, merchants, and dashers. About the Team The Data Ingestion team at DoorDash is at the forefront of managing the seamless movement of trillions of telemetry and transaction data points from diverse sources to our data lakehouse in real-time. By integrating this data with our online systems, we empower multiple business lines, drive critical machine learning models, and fuel fast-paced experimentation. Our team leverages cutting-edge open-source technologies such as Apache Spark, Flink, Kafka, Airflow, Delta Lake, and Iceberg to build and maintain a scalable, high-quality data ingestion framework. As a key player in this innovative and dynamic team, you will help evolve our systems to support DoorDash's expanding international footprint and ensure the highest standards of reliability and flexibility. This hybrid role requires you to be located in the Bay Area or Seattle. You're excited about this opportunity because you will… High Impact: Contribute to powering multiple business lines with high-quality, low-latency data directly integrated into online systems, driving billions in revenue. Cutting-Edge Technology: Work with advanced open-source technologies such as Apache Spark, Flink, Kafka, Airflow, Delta Lake, and Iceberg. Scalability: Play a crucial role in evolving our systems to accommodate a 10x scale increase, supporting DoorDash's expanding international footprint. Innovation and Excellence: Be part of a team that drives innovation and maintains high standards of reliability and flexibility in our data infrastructure. Cross-Functional Collaboration: Collaborate closely with cross-functional teams in Analytics, Product, and Engineering to ensure stakeholder satisfaction with the data platform's roadmap. Career Growth: Join a dynamic and growing company where your contributions are recognized and valued, providing excellent visibility and opportunities for professional development. We're excited about you because… B.S., M.S., or PhD. in Computer Science or equivalent 2+ years of experience with CS fundamental concepts and experience with at least one of the programming languages of Scala, Java, and Python You are located or are willing to locate to the Bay Area or Seattle Prior technical experience in Big Data solutions - you've built meaningful pieces of data infrastructure. Bonus if those were open-sourced big data processing frameworks using technologies like Spark, Airflow, Kafka, Flink, Iceberg, Deltalake Experience improving efficiency, scalability, and stability of data platforms Compensation The location-specific base salary range for this position is listed below. Compensation in other geographies may vary. Actual compensation within the pay range will be decided based on factors including, but not limited to, skills, prior relevant experience, and specific work location. For roles that are available to be filled remotely, base salary is localized according to employee work location. Please discuss your intended work location with your recruiter for more information. DoorDash cares about you and your overall well-being, and that's why we offer a comprehensive benefits package, for full-time employees, that includes healthcare benefits, a 401(k) plan including an employer match, short-term and long-term disability coverage, basic life insurance, wellbeing benefits, paid time off, paid parental leave, and several paid holidays, among others. In addition to base salary, the compensation package for this role also includes opportunities for equity grants. We use Covey as part of our hiring and / or promotional process for jobs in NYC and certain features may qualify it as an AEDT. As part of the evaluation process we provide Covey with job requirements and candidate submitted applications. We began using Covey Scout for Inbound on August 21, 2023. Please see the independent bias audit report covering our use of Covey here. California Pay Range: $140,100—$210,100 USD Washington Pay Range: $140,100—$210,100 USD About DoorDash At DoorDash, our mission to empower local economies shapes how our team members move quickly, learn, and reiterate in order to make impactful decisions that display empathy for our range of users—from Dashers to merchant partners to consumers. We are a technology and logistics company that started with door-to-door delivery, and we are looking for team members who can help us go from a company that is known for delivering food to a company that people turn to for any and all goods. DoorDash is growing rapidly and changing constantly, which gives our team members the opportunity to share their unique perspectives, solve new challenges, and own their careers. We're committed to supporting employees' happiness, healthiness, and overall well-being by providing comprehensive benefits and perks including premium healthcare, wellness expense reimbursement, paid parental leave and more. Our Commitment to Diversity and Inclusion We're committed to growing and empowering a more inclusive community within our company, industry, and cities. That's why we hire and cultivate diverse teams of people from all backgrounds, experiences, and perspectives. We believe that true innovation happens when everyone has room at the table and the tools, resources, and opportunity to excel. Statement of Non-Discrimination: In keeping with our beliefs and goals, no employee or applicant will face discrimination or harassment based on: race, color, ancestry, national origin, religion, age, gender, marital/domestic partner status, sexual orientation, gender identity or expression, disability status, or veteran status. Above and beyond discrimination and harassment based on \"protected categories,\" we also strive to prevent other subtler forms of inappropriate behavior (i.e., stereotyping) from ever gaining a foothold in our office. Whether blatant or hidden, barriers to success have no place at DoorDash. We value a diverse workforce – people who identify as women, non-binary or gender non-conforming, LGBTQIA+, American Indian or Native Alaskan, Black or African American, Hispanic or Latinx, Native Hawaiian or Other Pacific Islander, differently-abled, caretakers and parents, and veterans are strongly encouraged to apply. Thank you to the Level Playing Field Institute for this statement of non-discrimination. Pursuant to the San Francisco Fair Chance Ordinance, Los Angeles Fair Chance Initiative for Hiring Ordinance, and any other state or local hiring regulations, we will consider for employment any qualified applicant, including those with arrest and conviction records, in a manner consistent with the applicable regulation. If you need any accommodations, please inform your recruiting contact upon initial connection.",
        "url": "https://www.linkedin.com/jobs/view/3948447566",
        "summary": "DoorDash is seeking a Data Ingestion Engineer to help build and maintain a scalable, high-quality data ingestion framework. The ideal candidate will have 2+ years of experience with CS fundamentals, at least one programming language from Scala, Java, and Python, and prior technical experience in Big Data solutions. This role involves working with cutting-edge open-source technologies such as Apache Spark, Flink, Kafka, Airflow, Delta Lake, and Iceberg, and contributing to the company's international expansion.",
        "industries": [
            "Technology",
            "Logistics",
            "E-commerce",
            "Food Delivery"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem-solving",
            "Analytical thinking",
            "Teamwork",
            "Innovation",
            "Adaptability",
            "Reliability",
            "Flexibility"
        ],
        "hard_skills": [
            "Scala",
            "Java",
            "Python",
            "Apache Spark",
            "Flink",
            "Kafka",
            "Airflow",
            "Delta Lake",
            "Iceberg",
            "Big Data",
            "Data Infrastructure",
            "Data Ingestion",
            "Data Pipelines",
            "Data Processing",
            "Data Engineering",
            "Data Management"
        ],
        "tech_stack": [
            "Apache Spark",
            "Flink",
            "Kafka",
            "Airflow",
            "Delta Lake",
            "Iceberg"
        ],
        "programming_languages": [
            "Scala",
            "Java",
            "Python"
        ],
        "experience": 2,
        "education": {
            "min_degree": "B.S.",
            "fields": [
                "Computer Science"
            ]
        },
        "salary": {
            "max": 210100,
            "min": 140100
        },
        "benefits": [
            "Healthcare benefits",
            "401(k) plan",
            "Employer match",
            "Short-term and long-term disability coverage",
            "Basic life insurance",
            "Wellbeing benefits",
            "Paid time off",
            "Paid parental leave",
            "Paid holidays",
            "Equity grants"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Santa Clara, CA",
        "job_id": 3965960110,
        "company": "VisitorsCoverage Inc.",
        "title": "Data Engineer - Marketing (Contractor)",
        "created_on": 1720635500.477278,
        "description": "Come join VisitorsCoverage, one of Silicon Valley's most successful InsurTech companies, certified as a Great Place to Work®! We are looking for a Marketing Data Engineer to join our team. This contract role will offer the right candidate first-hand experience in a booming industry. A successful candidate will be responsible for streamlining reporting and data aggregation with GA4 and other 3rd party marketing tools. You'll lead and assist in building visualizations, dashboards, running SQL queries, and creating and managing tags in Google Tag Manager. What We Do: VisitorsCoverage is an Insurtech company, located in the heart of Silicon Valley, revolutionizing the way travelers search, compare, purchase, and manage their travel insurance. Imagine a place where buying travel insurance is as easy as ordering an item from your favorite online retailer. You know exactly what the benefits are and what each word on the coverage document means, and you are able to zip through the checkout process. We are obsessed with simplifying Travel Insurance! We wake up everyday thinking of new ways to meet the same expectations that users have from their online retailers and delivery or streaming services. We are a team of people who counter the thought that insurance is boring and love the challenge of delighting our users at every step of their decision-making process. If this sounds like the perfect role and workplace for you, we encourage you to apply for this position! VisitorsCoverage is on a mission to hire only the best, and we are committed to providing exceptional employee experiences with meaningful work and true work/life balance. Location This is a hybrid position. Candidate must be able to commute to the Santa Clara office 4 days a week. Weekly duration: 20-40 hours a week Compensation This is a contract position. Rate is $74-$100 per hour, based on experience. Requirements Responsibilities: Aggregate traffic and revenue data accurately in GA4 from the following sources: Google Ads, Microsoft Ads, Meta, Reddit and additional online paid/organic sources. Fix any data discrepancies due to Google Tag Manager Audit Google Tag Manager's triggers and tags for accuracy and it's data feed to other platforms Work with the team to QA tags and data streams for accuracy Work with Engineering and Marketing teams to align on tracking KPIs Experience & Qualifications: 3 years of experience in tracking Support and Management using Google Tag Manager 3 years of experience integrating data from key marketing platforms, including Meta, Google Ads, and TikTok Ads, to guarantee accurate data flow into Google Analytics 4 (GA4), and Tableau 1 year experience working with Tableau (Or similar online reporting/CRM platform) 1 year experience with BigQuery 1+ Year of experience building Data Extraction pipelines in Python 1+ Year of experience in SQL Significant experience in management of Google Analytics (GA4) architecture and delivery of web analytics insights and reporting from Google Analytics (required) - Especially tracking ‘key events' and ‘revenue' tracking Significant understanding of attribution models and how this functions for tracking website sales Proven history of working on technical site integrations or projects Ability to work independently and collaboratively as part of a team Strong attention to detail and ability to meet deadlines Preferred Qualifications: Experience with Looker Studio, Funnel IO Experience with Google Sheets Experience with A/B testing and experimentation Excellent communication and presentation skills What do we need in the next 60-90 days: Get data to a clean and accurate state in a singular location Fix discrepancies between various marketing tools' data for revenue and sales tracking across below platforms GA4 Meta Google Ads Reddit Bing Clean up Google Tag Manager data/metadata Export aggregated data to Google Sheets",
        "url": "https://www.linkedin.com/jobs/view/3965960110",
        "summary": "VisitorsCoverage, an InsurTech company in Silicon Valley, is seeking a Marketing Data Engineer to join their team on a contract basis. The role involves streamlining reporting and data aggregation with GA4 and other marketing tools, building visualizations, dashboards, running SQL queries, and managing tags in Google Tag Manager. The ideal candidate will have 3+ years of experience with Google Tag Manager, integrating data from marketing platforms (Meta, Google Ads, TikTok), working with Tableau, and experience with BigQuery, Python, and SQL. ",
        "industries": [
            "Insurance",
            "Technology",
            "FinTech",
            "Marketing"
        ],
        "soft_skills": [
            "Communication",
            "Presentation",
            "Attention to Detail",
            "Teamwork",
            "Problem Solving",
            "Data Analysis",
            "Organization"
        ],
        "hard_skills": [
            "Google Tag Manager",
            "GA4",
            "Tableau",
            "BigQuery",
            "Python",
            "SQL",
            "Google Ads",
            "Meta",
            "TikTok Ads",
            "Reddit",
            "Bing"
        ],
        "tech_stack": [
            "GA4",
            "Google Tag Manager",
            "Tableau",
            "BigQuery",
            "Python",
            "SQL"
        ],
        "programming_languages": [
            "Python",
            "SQL"
        ],
        "experience": 3,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 100,
            "min": 74
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Los Gatos, CA",
        "job_id": 3847934349,
        "company": "Netflix",
        "title": "Technical Support Engineer (L5) - Data Platform, Big Data / Analytics",
        "created_on": 1720635502.2927017,
        "description": "Netflix is the world’s leading streaming entertainment service with 250 million paid memberships in over 190 countries, enjoying TV series, documentaries, and feature films across a wide variety of genres and languages. Members can watch as much as they want, anytime, anywhere, on any internet-connected screen. Members can play, pause and resume watching, all without commercials or commitments. About The Engineering Support Organization The aim of the Engineering Support Organization is to enable Productivity Engineering to effectively and sustainably scale the support they provide to their customers. The team is the frontline resource for the engineering support needs of our customers (i.e., our engineering workforce) - handling, troubleshooting, and resolving customer requests and issues. In addition, the team will focus on ways of working, customer advocacy, support tooling, platform product offerings, documentation, and developer education. Our Mission Deliver an excellent support experience to Netflix’s developer community. To advocate for our customers, follow through on issues and resolve them in a reasonable time. If blockers prevent immediate resolution, we communicate status and ensure there is visibility into why there is a delay. Provide insights, feedback and champion customer sentiment about the tools we support to our partners across Productivity Engineering. Partner with PM and Engineering to track and maintain visibility into ongoing issues and communicate customer needs to ensure improving in these areas is prioritized. Drive collaboration efforts to reduce product friction and increase usability so that Productivity Engineering can build, deploy and deliver highly functional solutions for the Developer Community. Our culture is unique, and we tend to live by our values, allowing you to do your best work and grow. To learn more about Productivity Engineering, feel free to listen to this podcast. The Role We are looking for a Technical Support Engineer with a passion for productivity infrastructure and tooling, customer service, and automation. You will be responsible for monitoring and handling our customers’ requests, troubleshooting, solving issues, automating support needs, developing runbooks, improving and maintaining support tools, understanding our product offerings, and continuously looking for ways to improve the engineering support experience. Our ideal team member has first-hand experience working in customer-facing, engineering support roles, writing and building a comprehensive self-service knowledge base and has knowledge of infrastructure, internal tooling, platforms, and cloud computing. You are excellent at understanding and solving complex and ambiguous problems and constantly seek improvement. As an Engineer in this role, we need a candidate who can understand our complex offerings on a technical level, be hands-on in the development of our support automation tooling, and recommend product and operational improvements based on customer interactions. Location Los Gatos, CA or US Remote only. What you’ll need to be successful You are skilled in providing superior customer support across a complex organization, ideally as part of a central team You are passionate about customer experience, striving to be an excellent customer advocate You are highly adaptable and comfortable taking on diverse roles and responsibilities from end to end in an investigation You are a data-driven and evidence-based decision-maker You have excellent communication skills and a proven track record of meaningful enhancements toward comprehensive documentation You are proficient in at least one programming language, ideally Python and/or Java, enabling you to contribute to codebases across related domains Prior experience supporting platforms built using open-source technologies such as Apache Kafka, Spark, and Hadoop You have worked with big data warehouse storage systems (e.g. Iceberg or Hive) You have experience developing data and/or ETL pipelines using Apache Spark framework or technologies such as Flink and Kafka Ability to read and write SQL queries to pull required complex data to support any reported issues/product defects Experience with cloud infrastructure and/or container orchestration platforms is a plus You have the desire and aptitude to learn how the pieces of big data platform work together You thrive in fast-paced environments and seek to improve on operational efficiencies by leaning into automation and visualization tooling Our compensation structure consists solely of an annual salary; we do not have bonuses. You choose each year how much of your compensation you want in salary versus stock options. To determine your personal top of market compensation, we rely on market indicators and consider your specific job family, background, skills, and experience to determine your compensation in the market range. The range for this role is $150,000- $370,000. Netflix provides comprehensive benefits including Health Plans, Mental Health support, a 401(k) Retirement Plan with employer match, Stock Option Program, Disability Programs, Health Savings and Flexible Spending Accounts, Family-forming benefits, and Life and Serious Injury Benefits. We also offer paid leave of absence programs. Full-time hourly employees accrue 35 days annually for paid time off to be used for vacation, holidays, and sick paid time off. Full-time salaried employees are immediately entitled to flexible time off. See more detail about our Benefits here. Netflix is a unique culture and environment. Learn more here. We are an equal-opportunity employer and celebrate diversity, recognizing that diversity of thought and background builds stronger teams. We approach diversity and inclusion seriously and thoughtfully. We do not discriminate on the basis of race, religion, color, ancestry, national origin, caste, sex, sexual orientation, gender, gender identity or expression, age, disability, medical condition, pregnancy, genetic makeup, marital status, or military service.",
        "url": "https://www.linkedin.com/jobs/view/3847934349",
        "summary": "Netflix is seeking a Technical Support Engineer to join their Engineering Support Organization. This role will involve providing support to the engineering workforce, troubleshooting issues, automating support needs, and improving the overall engineering support experience. The ideal candidate will have experience in customer-facing roles, knowledge of infrastructure and tooling, and strong technical skills in areas like big data, cloud computing, and automation. The role offers a competitive salary range of $150,000-$370,000 and a comprehensive benefits package.",
        "industries": [
            "Technology",
            "Streaming",
            "Entertainment",
            "Software Development"
        ],
        "soft_skills": [
            "Customer service",
            "Communication",
            "Problem-solving",
            "Collaboration",
            "Adaptability",
            "Data-driven decision making",
            "Customer advocacy"
        ],
        "hard_skills": [
            "Python",
            "Java",
            "Apache Kafka",
            "Spark",
            "Hadoop",
            "Iceberg",
            "Hive",
            "Flink",
            "SQL",
            "Cloud Infrastructure",
            "Container Orchestration",
            "Automation",
            "Visualization tooling"
        ],
        "tech_stack": [
            "Apache Kafka",
            "Spark",
            "Hadoop",
            "Iceberg",
            "Hive",
            "Flink",
            "Cloud Infrastructure",
            "Container Orchestration",
            "Python",
            "Java"
        ],
        "programming_languages": [
            "Python",
            "Java"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 370000,
            "min": 150000
        },
        "benefits": [
            "Health Plans",
            "Mental Health support",
            "401(k) Retirement Plan",
            "Stock Option Program",
            "Disability Programs",
            "Health Savings and Flexible Spending Accounts",
            "Family-forming benefits",
            "Life and Serious Injury Benefits",
            "Paid leave of absence",
            "Paid time off",
            "Flexible time off"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3961092714,
        "company": "Fieldguide",
        "title": "Software Engineer",
        "created_on": 1720635504.0182002,
        "description": "About Us: Fieldguide is establishing a new state of trust for global commerce and capital markets through automating and streamlining the work of assurance and audit practitioners specifically within cybersecurity, privacy, and ESG (Environmental, Social, Governance). Put simply, we build software for the people who enable trust between businesses. We’re based in San Francisco, CA, but built as a remote-first company that enables you to do your best work from anywhere. We're backed by top investors including Bessemer Venture Partners, 8VC, Floodgate, Y Combinator, DNX Ventures, Global Founders Capital, Justin Kan, Elad Gil, and more. We value diversity — in backgrounds and in experiences. We need people from all backgrounds and walks of life to help build the future of audit and advisory. Fieldguide’s team is inclusive, driven, humble and supportive. We are deliberate and self-reflective about the kind of team and culture that we are building, seeking teammates that are not only strong in their own aptitudes but care deeply about supporting each other's growth. As an early stage start-up employee, you’ll have the opportunity to build out the future of business trust. We make audit practitioners’ lives easier by eliminating up to 50% of their work and giving them better work-life balance. If you share our values and enthusiasm for building a great culture and product, you will find a home at Fieldguide. About the role: As a Software Engineer at Fieldguide, you’ll be an early member of the team, taking a front-row seat as we build both the company and the engineering organization to tackle the massive and archaic audit and advisory industry. Responsibilities Be a core technical contributor at a Series A-stage company as it scales Contribute to the end-to-end development of features Bring a mindset of continuous improvement to your work You’ll Need To Have Experience With The Following Modern web tech stacks consisting of several of the following: TypeScript, React, GraphQL, NodeJS, Hasura, Postgres, and AWS Test and automation tools like Jest and Cypress DevOps and continuous integration/delivery best practices Information security best practices Collaborating on all aspects of product strategy and UX Shaping a young tech stack, product, and engineering organization More about Fieldguide: Fieldguide is a values-based company. Our values are: Fearless - Inspire & break down seemingly impossible walls. Fast - Launch fast with excellence, iterate to perfection. Lovable - Deliver happiness & 11 star experiences. Owners - Execute & run the business with ownership. Win-win - Create mutual value & earn trust for life. Inclusive - Scale the best ideas with inclusive teams. Some of our benefits include: Competitive compensation packages with meaningful ownership Unlimited PTO Wellness benefits, including a bundle of free therapy sessions 401k Technology & Work from Home reimbursement Flexible work schedules Compensation Range: $125K - $167K",
        "url": "https://www.linkedin.com/jobs/view/3961092714",
        "summary": "Fieldguide is a Series A startup building software for cybersecurity, privacy, and ESG audit and advisory professionals. They're a remote-first company looking for a Software Engineer to contribute to the end-to-end development of features, participate in continuous improvement, and help shape the company's tech stack, product, and engineering organization.",
        "industries": [
            "Software",
            "Technology",
            "Cybersecurity",
            "Privacy",
            "ESG",
            "Audit",
            "Advisory"
        ],
        "soft_skills": [
            "Continuous improvement",
            "Collaboration",
            "Product strategy",
            "UX",
            "Ownership",
            "Inclusiveness"
        ],
        "hard_skills": [
            "TypeScript",
            "React",
            "GraphQL",
            "NodeJS",
            "Hasura",
            "Postgres",
            "AWS",
            "Jest",
            "Cypress",
            "DevOps",
            "Continuous integration/delivery",
            "Information security"
        ],
        "tech_stack": [
            "TypeScript",
            "React",
            "GraphQL",
            "NodeJS",
            "Hasura",
            "Postgres",
            "AWS",
            "Jest",
            "Cypress"
        ],
        "programming_languages": [
            "TypeScript",
            "JavaScript",
            "SQL"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 167000,
            "min": 125000
        },
        "benefits": [
            "Competitive compensation",
            "Meaningful ownership",
            "Unlimited PTO",
            "Wellness benefits",
            "Therapy sessions",
            "401k",
            "Technology & Work from Home reimbursement",
            "Flexible work schedules"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Diego, CA",
        "job_id": 3779266849,
        "company": "Sky Consulting Inc.",
        "title": "Senior Data Platform Engineer",
        "created_on": 1720635505.7326746,
        "description": "Title: Senior Data Platform Engineer Location - San Diego, CA (Need local or near by candidates open for Hybrid.) Type of Role: Full time No sponsorship is available now or in the future Bachelor's or Master's degree in Computer Science, Information Systems, or a related field 5+ years of experience in data platform engineering with coding expertise in Scala, Java, and Python Experience with technologies such as Spark, Airflow, Kafka, MySQL, Cassandra, Delta Lake, and Data Bricks Experience with distributed systems, messaging queues, NoSQL and SQL databases, API design, microservice architecture, and streaming architecture Experience leading and mentoring other engineers Strong problem-solving and analytical skills Excellent communication and collaboration skills Nice to have: healthcare experience and familiarity with healthcare tech standards like x12 EDI, HL7 FHIR, CCDA, and V2 messaging",
        "url": "https://www.linkedin.com/jobs/view/3779266849",
        "summary": "Senior Data Platform Engineer with 5+ years of experience in data platform engineering, coding expertise in Scala, Java, and Python. Experience with Spark, Airflow, Kafka, MySQL, Cassandra, Delta Lake, and Data Bricks. Experience with distributed systems, messaging queues, NoSQL and SQL databases, API design, microservice architecture, and streaming architecture.  Strong problem-solving and analytical skills, excellent communication and collaboration skills. Nice to have: healthcare experience and familiarity with healthcare tech standards like x12 EDI, HL7 FHIR, CCDA, and V2 messaging.",
        "industries": [
            "Healthcare",
            "Technology"
        ],
        "soft_skills": [
            "Problem-solving",
            "Analytical",
            "Communication",
            "Collaboration",
            "Leadership",
            "Mentoring"
        ],
        "hard_skills": [
            "Scala",
            "Java",
            "Python",
            "Spark",
            "Airflow",
            "Kafka",
            "MySQL",
            "Cassandra",
            "Delta Lake",
            "Data Bricks",
            "Distributed Systems",
            "Messaging Queues",
            "NoSQL",
            "SQL",
            "API Design",
            "Microservice Architecture",
            "Streaming Architecture"
        ],
        "tech_stack": [
            "Spark",
            "Airflow",
            "Kafka",
            "MySQL",
            "Cassandra",
            "Delta Lake",
            "Data Bricks"
        ],
        "programming_languages": [
            "Scala",
            "Java",
            "Python"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Computer Science",
                "Information Systems"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3956362505,
        "company": "LHH",
        "title": "Senior Data Engineer",
        "created_on": 1720635507.3904688,
        "description": "Our client is headquartered in San Francisco, who is a premier private medical information solutions company. Our client's data-driven performance measurement and improvement solutions align health plans and care delivery organizations, ensuring the best care and experience for every patient across all settings. Responsibilities: Design and build robust infrastructure for data extraction, transformation, loading, and cleaning from diverse sources using APIs, SQL, and AWS big data technologies. Implement CI/CD pipelines for seamless deployment of data models and applications. Gather and process raw data at scale to support both ad-hoc requests and core pipeline development. Develop and optimize our analytics engine, creating intuitive tools and solutions for stakeholders to consume and understand data. Collaborate with various stakeholders and functional teams to drive business results through data-based insights. Identify inefficiencies, optimize processes and data flows, and recommend improvements. Mentor junior engineers, fostering a collaborative and innovative team environment. Provide technical insights and recommendations to the management team regarding data processing infrastructure and strategies. Qualifications: Bachelor’s or master’s degree in CF Computer Science, IT Management, or equivalent. Authorization to work in the USA. Minimum of 8 years of engineering experience, with at least 6 years in data engineering. Experience with data ingestion, normalization, and quality, particularly in healthcare claims and clinical data sets. Proficiency in SQL, Python, Java, and experience with FHIR and other healthcare data formats. Demonstrated experience with cloud-based data warehousing solutions, preferably Snowflake. Familiarity with HIPAA compliance and security standards. Preferred Skills: Expertise in Snowflake SQL, SAS analytics, and ETL DataOps. Experience with HITRUST certification, CPT, ICD-10, and business intelligence tools. Proficiency in DBT, Terraform, and airflow. Strong project management and leadership abilities, with a talent for coordinating cross-functional teams and achieving project objectives and deadlines. Ability to identify and address issues or roadblocks to project success and enhance continuous quality improvement functions. Personal Qualities: Strong problem-solving, quantitative, and qualitative reasoning skills. Excellent written and oral communication skills. Commitment to modeling compliance with company policies and procedures. Supportive of company mission, values, and standards of ethics and integrity. Ability to thrive in a high-accountability, quality-focused, and deadline-oriented organization.",
        "url": "https://www.linkedin.com/jobs/view/3956362505",
        "summary": "This role is a Data Engineer at a leading medical information solutions company. You will build and optimize data infrastructure, develop analytics tools, and collaborate with stakeholders to drive business results. Experience with healthcare data (claims, clinical data), cloud-based data warehousing (Snowflake), and FHIR is required.",
        "industries": [
            "Healthcare",
            "Information Technology",
            "Data Analytics",
            "Software",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Problem-solving",
            "Communication",
            "Collaboration",
            "Leadership",
            "Project Management",
            "Teamwork",
            "Analytical",
            "Quantitative Reasoning",
            "Qualitative Reasoning",
            "Quality Focus"
        ],
        "hard_skills": [
            "SQL",
            "Python",
            "Java",
            "FHIR",
            "AWS",
            "Snowflake",
            "HIPAA",
            "ETL",
            "CI/CD",
            "DataOps",
            "SAS Analytics",
            "DBT",
            "Terraform",
            "Airflow",
            "HITRUST Certification",
            "CPT",
            "ICD-10",
            "Business Intelligence"
        ],
        "tech_stack": [
            "AWS",
            "Snowflake",
            "FHIR",
            "SQL",
            "Python",
            "Java",
            "SAS Analytics",
            "ETL",
            "CI/CD",
            "DataOps",
            "DBT",
            "Terraform",
            "Airflow"
        ],
        "programming_languages": [
            "SQL",
            "Python",
            "Java"
        ],
        "experience": 8,
        "education": {
            "min_degree": "Bachelor’s or master’s degree",
            "fields": [
                "Computer Science",
                "IT Management"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3835343266,
        "company": "Patreon",
        "title": "Senior Data Engineer, Platform",
        "created_on": 1720635509.4685295,
        "description": "Patreon is the best place for creators to fire up their fandoms, share exclusive work, and turn their passions into lasting creative businesses. Over 250,000 podcasters, writers, musicians, artists, and other creative people use Patreon to reach their biggest fans directly and earn an income for the value they provide. Creators can offer paid memberships that unlock access to exclusive work and community, or sell individual digital items from their own Patreon shops. Ultimately, our goal is simple: fund the creative class. And we’re leaders in that space, having sent over $3.5 billion to creators since our founding. We’re continuing to invest heavily in building the best creator tools with the best team in the creator economy, and are looking for a Data Engineer to support our mission. This role is available to those wishing to work in our San Francisco and New York offices on a hybrid work model or those wishing to be fully remote in the United States. About The Role Work on a tight-knit team of highly motivated and experienced data engineers with frequent collaboration with data scientists, product managers and product engineers. Work on both “data analytics” and “data infrastructure” type projects in a fast-paced, high-growth startup environment. Build core data sets and metrics to power analytics, reports and experimentation. Write real-time and batch data pipelines to support a wide range of projects and features including our creator-facing analytics product, executive reporting, FP&A, marketing initiatives, model training, data science analytics, A/B testing, etc. Help manage and build out our data platform and suite of data tools. Be a driver of a data-centric culture at Patreon. Work autonomously on large green field initiatives and help define data best practices at the company. About You Expert in SQL, Spark and Python or Scala Significant experience modeling data and developing core data sets and metrics to support analytics, reports and experimentation. Solid understanding of how to use data to inform the product roadmap. Enjoy collaborating with Data Scientists, Product Managers and Product Engineers. Comfortable playing the role of a Project Manager in order to drive results. Have previously built real-time and batch data pipelines using tooling such as Airflow, Spark, Kafka, S3, Fivetran, Census, etc. Experience working with event tracking frameworks, data observability frameworks and experimentation frameworks. Experience managing and working with Data Warehouses and Data Lakes such as Redshift, Big Query, Snowflake, Delta Lake, etc. Highly motivated self-starter that is keen to make an impact and is unafraid of tackling large, complicated problems and putting in the work to ensure high craft deliverables. About Patreon Patreon powers creators to do what they love and get paid by the people who love what they do. Our team is passionate about making this mission and our core values come to life every day in our work. Through this work, our Patronauts: Put Creators First | They’re the reason we’re here. When creators win, we win.**** Build with Craft | We sign our name to every deliverable, just like the creators we serve. Make it Happen | We don’t quit. We learn and deliver. Win Together | We grow as individuals. We win as a team. We hire talented and passionate people from different backgrounds across the organization. If you’re excited about a role but your past experience doesn’t match with every bullet point outlined above, we strongly encourage you to apply anyway. If you’re a creator at heart, are energized by our mission, and share our company values, we’d love to hear from you. Patreon is proud to be an equal-opportunity employer. We provide employment opportunities without regard to age, race, color, ancestry, national origin, religion, disability, sex, gender identity or expression, sexual orientation, veteran status, or any other protected class. Patreon offers a competitive benefits package including and not limited to salary, equity plans, healthcare, unlimited paid time off, company holidays and recharge days, commuter benefits, lifestyle stipends, learning and development stipends, patronage, parental leave, and 401k plan with matching.",
        "url": "https://www.linkedin.com/jobs/view/3835343266",
        "summary": "Patreon is seeking a Data Engineer to support its mission of funding the creative class. This role will involve building core data sets and metrics, writing real-time and batch data pipelines, managing the data platform, and driving a data-centric culture. The ideal candidate will have expertise in SQL, Spark, Python or Scala, and experience with data warehousing, data lakes, and various data engineering tools.",
        "industries": [
            "Technology",
            "Media & Entertainment",
            "Arts & Culture",
            "FinTech"
        ],
        "soft_skills": [
            "Highly motivated",
            "self-starter",
            "collaborative",
            "communication",
            "problem-solving",
            "project management"
        ],
        "hard_skills": [
            "SQL",
            "Spark",
            "Python",
            "Scala",
            "Airflow",
            "Kafka",
            "S3",
            "Fivetran",
            "Census",
            "Redshift",
            "Big Query",
            "Snowflake",
            "Delta Lake"
        ],
        "tech_stack": [
            "Spark",
            "Airflow",
            "Kafka",
            "S3",
            "Fivetran",
            "Census",
            "Redshift",
            "Big Query",
            "Snowflake",
            "Delta Lake"
        ],
        "programming_languages": [
            "SQL",
            "Python",
            "Scala"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Salary",
            "Equity plans",
            "Healthcare",
            "Unlimited paid time off",
            "Company holidays",
            "Recharge days",
            "Commuter benefits",
            "Lifestyle stipends",
            "Learning and development stipends",
            "Patronage",
            "Parental leave",
            "401k plan with matching"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Sunnyvale, CA",
        "job_id": 3967000518,
        "company": "Compunnel Inc.",
        "title": "Data Engineer",
        "created_on": 1720635511.2380533,
        "description": "Description Designs, develops, and implements Hadoop eco-system based applications to support business requirements. Follows approved life cycle methodologies, creates design documents, and performs program coding and testing. Resolves technical issues through debugging, research, and investigation. Experience/Skills Required Bachelor's degree in Computer Science, Information Technology, or related field and 5 years experience in computer programming, software development or related 3+ years of solid Java and 2+ years experience in design, implementation, and support of solutions big data solution in Hadoop using Hive, Spark, Drill, Impala, HBase Hands on experience with Unix, Teradata and other relational databases. Experience with @Scale a plus. Strong communication and problem-solving skills Candidates must have 5+ years of data experience, 2+ years of experience with Google Cloud platform, and hands on experience process large scale distributed data. Education: Bachelors Degree",
        "url": "https://www.linkedin.com/jobs/view/3967000518",
        "summary": "Designs, develops, and implements Hadoop eco-system based applications for business requirements. Follows SDLC methodologies, creates design documents, codes, and tests. Resolves technical issues through debugging, research, and investigation.",
        "industries": [
            "Technology",
            "Data Science",
            "Software Development",
            "Big Data",
            "Analytics"
        ],
        "soft_skills": [
            "Communication",
            "Problem-solving"
        ],
        "hard_skills": [
            "Java",
            "Hadoop",
            "Hive",
            "Spark",
            "Drill",
            "Impala",
            "HBase",
            "Unix",
            "Teradata",
            "Relational Databases",
            "Google Cloud Platform"
        ],
        "tech_stack": [
            "Hadoop",
            "Hive",
            "Spark",
            "Drill",
            "Impala",
            "HBase",
            "Unix",
            "Teradata",
            "Google Cloud Platform"
        ],
        "programming_languages": [
            "Java"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Information Technology"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Fremont, CA",
        "job_id": 3818366185,
        "company": "Info Way Solutions",
        "title": "Data�Engineer",
        "created_on": 1720635513.1225932,
        "description": "Hi Professionals, Hope you are doing good Job Description This is Sangeetha from Info Way Solutions, LLC We have job opening for Data Engineer and the detailed Job description is given below: Kindly check the JD and share your views Data Engineer Sunnyvale CA / Austin , TX \" A background in computer science, engineering, mathematics, or similar quantitative field with a minimum of 2 years professional experience \" Experience in implementing data pipelines using python \" Experience with workflow scheduling / orchestration such as Kubernetes, Airflow or Oozie \" Extract Transform Load (ETL) experience using Spark, Kafka, Hadoop, or similar technologies \" Experience with query APIs using JSON, Protocol Buffers, or XML \" Experience with Unix-based command line interface and Bash scripts Thanks & Regards Sangeetha| Infowaygroup.com | US IT Recruiter, sangeetha @Infowaygroup.com Cell:(925)241-4886 https://www.linkedin.com/in/sangeetha-kannan-291636206/ Info Way Solutions LLC | 46520 Fremont Blvd, Suite 614 | Fremont, CA 9453",
        "url": "https://www.linkedin.com/jobs/view/3818366185",
        "summary": "Data Engineer position at Info Way Solutions, LLC in Sunnyvale, CA or Austin, TX. Requires a minimum of 2 years of professional experience in computer science, engineering, mathematics, or a similar quantitative field. Experience in data pipelines using Python, workflow scheduling/orchestration tools like Kubernetes, Airflow or Oozie, ETL experience with Spark, Kafka, Hadoop, query APIs using JSON, Protocol Buffers, or XML, and Unix-based command line interface and Bash scripts are essential.",
        "industries": [
            "Data Engineering",
            "Technology",
            "Software Development",
            "Information Technology"
        ],
        "soft_skills": [
            "Problem-solving",
            "Analytical thinking",
            "Communication",
            "Teamwork",
            "Collaboration"
        ],
        "hard_skills": [
            "Python",
            "Kubernetes",
            "Airflow",
            "Oozie",
            "Spark",
            "Kafka",
            "Hadoop",
            "JSON",
            "Protocol Buffers",
            "XML",
            "Unix",
            "Bash"
        ],
        "tech_stack": [
            "Python",
            "Kubernetes",
            "Airflow",
            "Oozie",
            "Spark",
            "Kafka",
            "Hadoop",
            "JSON",
            "Protocol Buffers",
            "XML",
            "Unix",
            "Bash"
        ],
        "programming_languages": [
            "Python"
        ],
        "experience": 2,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Computer Science",
                "Engineering",
                "Mathematics",
                "Quantitative"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Anaheim, CA",
        "job_id": 3945750467,
        "company": "Sibitalent Corp",
        "title": "Urgent Hiring || Data Engineer || Location|| Anaheim, CA",
        "created_on": 1720635515.0031471,
        "description": "Hello My Name is Aftab and I am a Staffing Specialist at SIBITALENT CORP . I am reaching out to you on an exciting job opportunity with one of our clients. Job Title : Data Engineer Location : Anaheim, CA Duration : 6+ Months Visa : USC/GC Interview : Video Client : Wescom Credit Union (W2 or Self-Corp only) General JD – Project Tasks And Responsibilities Primary Skills* Minimum 7 years of experience in a data warehouse/data engineering role for a mid to large size organization; Health Care industry experience highly desired * Minimum 4 years of hands-on experience with ETL/Data Integration, BI, data mining and modeling * Ability to comply with standards and procedures such as standard of communication, work management, change management, version control, implementation and/or consistency of coding; Recognizes code, process and/or standard inefficiencies and suggests new standards and opportunities for improvement * Ability to build and integrate a data-driven intelligent solution into our business processes * Keep big picture concepts in mind when designing solutions; Fully understanding business needs * Strong experience on database technologies, data warehouse, data validation, data quality, metadata management, and data governance* Able to provide proactive technical oversight and advice that foster re-use, scalability, stability, and operational efficiency of data/analytical solutions Knowledge of and experienced in rolling out best practices in all facets of DW architecture, data flow strategy, data modeling, metadata, and master * Data management experience * Strong TSQL and MS SQL development skills * Needs to have highly advanced technical skills and overall knowledge of IT stack * Clear communication skills * Able to self-learn, reverse engineer, and support existing processes * Proficient in 3rd normal form (TNF) DW Architecture, data flow best practices, data modeling, metadata, and master data management * Expertise in SQL and RDBMS systems such as Microsoft SQL Server * Able to stay up to date with established and industry emerging data technologies * Demonstrate ability to produce high quality technical documentation * Experience in implementing a data architecture with separation between storage and compute preferred Thanks & Regards Aftab Technical Recruiter Sibi talent Corp. Phone : 19728466531 Email : aftab@sibitalent.com URL : www.sibitalent.com",
        "url": "https://www.linkedin.com/jobs/view/3945750467",
        "summary": "Data Engineer with 7+ years of experience in data warehousing and engineering, with strong skills in ETL, data integration, BI, data mining, and modeling. Healthcare industry experience preferred. This role requires expertise in TSQL, MS SQL Server, 3rd normal form DW Architecture, and data flow best practices. Candidate should have experience in data architecture with separation between storage and compute.",
        "industries": [
            "Healthcare",
            "Financial Services",
            "Technology"
        ],
        "soft_skills": [
            "Communication",
            "Problem Solving",
            "Analytical Skills",
            "Teamwork",
            "Self-Learning"
        ],
        "hard_skills": [
            "ETL",
            "Data Integration",
            "BI",
            "Data Mining",
            "Data Modeling",
            "TSQL",
            "MS SQL Server",
            "Data Warehousing",
            "Data Validation",
            "Data Quality",
            "Metadata Management",
            "Data Governance",
            "DW Architecture",
            "Data Flow Strategy",
            "Data Modeling",
            "Metadata",
            "Master Data Management",
            "RDBMS"
        ],
        "tech_stack": [
            "ETL",
            "Data Integration",
            "BI",
            "Data Mining",
            "Data Modeling",
            "TSQL",
            "MS SQL Server",
            "Data Warehousing",
            "Data Validation",
            "Data Quality",
            "Metadata Management",
            "Data Governance",
            "DW Architecture",
            "Data Flow Strategy",
            "Data Modeling",
            "Metadata",
            "Master Data Management",
            "RDBMS"
        ],
        "programming_languages": [
            "TSQL"
        ],
        "experience": 7,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Mountain View, CA",
        "job_id": 3947272256,
        "company": "Calypso Way",
        "title": "Sr. Database Engineer",
        "created_on": 1720635516.4944444,
        "description": "Title: Database Engineer 10+ years’ experience Local Pref. (Mountain View, CA) Duration: 6 months+ Requirement Proven experience working with Amazon Aurora and/or PostgreSQL in a production environment. (This has to be super strong) Strong SQL skills and experience with SQL tuning techniques. à ( Need to do hands-on check ) (This has to be super strong) Familiarity with database security concepts and best practices. (This has to be super strong) Hands-on experience with scripting languages (e.g., Python, Bash) for automation. Bachelor’s degree in computer science, Engineering, or related field. Proficiency in AWS services such as EC2, Route 53, VPC, IAM, and CloudFormation. Excellent problem-solving skills and attention to detail. (Development) Strong communication and collaboration skills, with the ability to work effectively in a team environment. Preferred Qualifications AWS Certification Experience with other AWS database services such as RDS. Knowledge of containerization technologies (e.g., Docker, Kubernetes). Experience with DevOps practices and tools (e.g., CI/CD pipelines, Git). Calypso Way is a California technology staffing service, that delivers a competitive advantage for its customers through software, solutions, and services. Established in 2019. Calypso Way is headquartered in San Ramon, California. We work with technology giants in Silicon Valley California.",
        "url": "https://www.linkedin.com/jobs/view/3947272256",
        "summary": "Database Engineer with 10+ years' experience needed for a 6+ month contract in Mountain View, CA. Must have strong experience with Amazon Aurora and/or PostgreSQL, SQL skills, database security concepts, and scripting languages like Python or Bash. AWS experience with EC2, Route 53, VPC, IAM, and CloudFormation is required.  Preferred qualifications include AWS certification, experience with other AWS database services, knowledge of containerization technologies, and DevOps practices. ",
        "industries": [
            "Technology",
            "Software",
            "Staffing",
            "Cloud Computing",
            "Database"
        ],
        "soft_skills": [
            "Problem-solving",
            "Attention to Detail",
            "Communication",
            "Collaboration",
            "Teamwork"
        ],
        "hard_skills": [
            "Amazon Aurora",
            "PostgreSQL",
            "SQL",
            "SQL Tuning",
            "Database Security",
            "Python",
            "Bash",
            "AWS",
            "EC2",
            "Route 53",
            "VPC",
            "IAM",
            "CloudFormation",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Git"
        ],
        "tech_stack": [
            "Amazon Aurora",
            "PostgreSQL",
            "SQL",
            "Python",
            "Bash",
            "AWS",
            "EC2",
            "Route 53",
            "VPC",
            "IAM",
            "CloudFormation",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Git"
        ],
        "programming_languages": [
            "Python",
            "Bash"
        ],
        "experience": 10,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Computer Science",
                "Engineering"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Hawthorne, CA",
        "job_id": 3877287689,
        "company": "Amazon",
        "title": "Data Engineer II, Ring Data Warehouse",
        "created_on": 1720635518.498527,
        "description": "Description Ring is seeking a Data Engineer with strong analytical, communication, and project management skills to join our team. This role will work closely with software development engineers, scientist and business stakeholders across various verticals. You will design, evangelize, and implement state-of-the-art solutions that help us provide a great customer experience. You will work with a complicated data environment, employ the right architecture to handle data, and support various analytics use cases including business reporting, production data pipeline, machine learning, optimization models, statistical models, and simulations. Key job responsibilities This role will be responsible for building and maintaining efficient, scalable, and privacy/security complaint data pipelines to support our business stakeholders. These pipelines will be built using both tools available in Native AWS as well as Amazon internal tools and technologies. This role will need to be able to work closely with stakeholders to understand their needs and work alongside them to ensure data being ingested meets the business user's needs and will well modeled and organized to promote scalable usage and good data hygiene. This role is expected to be able to learn and adapt quickly new technologies and business needs, building and improving frameworks and processes, incorporating new patterns and technologies where appropriate. A day in the life This Role Will Collect and Discuss requirements from Business Stakeholders across verticals such as Subscriptions, Sales, Reverse Logistics, Finance, Product, etc. Build new data ingestions using a combination of Native AWS services and/or internal Amazon tools. Maintaining/Improving existing data ingestions and ensuring they meet evolving standards and corporate mandates. Building/Improving/Maintaining frameworks or tools for internal team and external stakeholder usage to manage data. Performing Code Reviews and ensuring best practices are followed for ETL and Data Hygiene by team members and supporting teams. About The Team Ring Data Warehouse team manages the centralized data repository for Ring. We bring in and publish data from numerous internal and external sources, which are used by various stakeholders such as Ring Product engineering, Finance, Supply Chain, Marketing, Customer Support, and Sales information. Basic Qualifications 3+ years of data engineering experience Experience with data modeling, warehousing and building ETL pipelines Experience with SQL Preferred Qualifications Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions Experience in at least one modern scripting or programming language, such as Python, Java, Scala, or NodeJS Experience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases) Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us. Our compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $118,900/year in our lowest geographic market up to $205,600/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. For more information, please visit https://www.aboutamazon.com/workplace/employee-benefits. This position will remain posted until filled. Applicants should apply via our internal or external career site. Company - Amazon.com Services LLC Job ID: A2533818",
        "url": "https://www.linkedin.com/jobs/view/3877287689",
        "summary": "Ring is looking for a Data Engineer with strong analytical, communication, and project management skills to join their team. This role will design, evangelize, and implement state-of-the-art solutions that help Ring provide a great customer experience. This role will involve working with a complicated data environment, employing the right architecture to handle data, and supporting various analytics use cases including business reporting, production data pipeline, machine learning, optimization models, statistical models, and simulations.",
        "industries": [
            "Technology",
            "E-commerce",
            "Security",
            "Data",
            "Analytics"
        ],
        "soft_skills": [
            "Analytical",
            "Communication",
            "Project Management",
            "Problem Solving",
            "Collaboration",
            "Adaptability",
            "Learning"
        ],
        "hard_skills": [
            "Data Engineering",
            "Data Modeling",
            "Data Warehousing",
            "ETL Pipelines",
            "SQL",
            "AWS Technologies",
            "Redshift",
            "S3",
            "AWS Glue",
            "EMR",
            "Kinesis",
            "FireHose",
            "Lambda",
            "IAM Roles and Permissions",
            "Scripting",
            "Python",
            "Java",
            "Scala",
            "NodeJS",
            "Non-relational Databases",
            "Object Storage",
            "Document or Key-value Stores",
            "Graph Databases",
            "Column-Family Databases"
        ],
        "tech_stack": [
            "AWS",
            "Redshift",
            "S3",
            "AWS Glue",
            "EMR",
            "Kinesis",
            "FireHose",
            "Lambda",
            "IAM",
            "Python",
            "Java",
            "Scala",
            "NodeJS",
            "SQL"
        ],
        "programming_languages": [
            "Python",
            "Java",
            "Scala",
            "NodeJS",
            "SQL"
        ],
        "experience": 3,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 205600,
            "min": 118900
        },
        "benefits": [
            "Medical",
            "Financial",
            "Equity",
            "Sign-on Payments"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Palo Alto, CA",
        "job_id": 3943915471,
        "company": "AppLovin",
        "title": "Data Engineer",
        "created_on": 1720635520.1804554,
        "description": "About AppLovin AppLovin makes technologies that help businesses of every size connect to their ideal customers. The company provides end-to-end software and AI solutions for businesses to reach, monetize and grow their global audiences. For more information about AppLovin, visit: www.applovin.com. To deliver on this mission, our global team is composed of team members with life experiences, backgrounds, and perspectives that mirror our developers and customers around the world. At AppLovin, we are intentional about the team and culture we are building, seeking candidates who are outstanding in their own right and also demonstrate their support of others. Fortune recognizes AppLovin as one of the Best Workplaces in the Bay Area, and the company has been a Certified Great Place to Work for the last four years (2021-2024). Check out the rest of our awards HERE. A Day in the Life As a member of the Platform team, you will take ownership of projects and work with large-scale data processing systems. We are seeking a motivated engineer to join the team responsible for AppLovin's core products which process over 6PB of data and reach 1B users daily. We are responsible for scaling a platform that produces hundreds of billions of unique events consumed trillions of times throughout our geographically distributed data centers every day. The technical stack includes Java, Scala, Spark, Airflow, GCP and working with a variety of databases. The Impact You’ll Make Design, develop, and maintain large-scale distributed systems Collaborate with various engineering teams to meet a wide range of technological challenges Influence and inspire team members Who You Are Minimum 2 years of meaningful professional experience Have a Bachelor’s and/or Master’s Degree in Computer Science or a related field Have used Java or Scala in a professional environment for at least 1 year Strong algorithms experience Have some experience with big data systems, like Apache Spark, big data processing, big data processing, big data pipelines, HDFS, etc. Have a desire to solve large, complex problems. You look beyond the surface to understand root causes so that you can build long-term solutions for the whole ecosystem Additional Strengths Knowledge of Airflow Practical experience working with big data systems (Apache Spark, SparkSQL, HDFS) Practical experience with broker systems (Apache Kafka, RabbitMQ, etc) Practical experience working in the cloud (GCP, AWS, etc) or with kubernetes We use Covey as part of our hiring and / or promotional process for jobs in NYC and certain features may qualify it as an AEDT. As part of the evaluation process we provide Covey with job requirements and candidate submitted applications. We began using Covey Scout for Inbound on March 12, 2024. Please see the independent bias audit report covering our use of Covey here. AppLovin is proud to be an equal opportunity employer that is committed to inclusion and diversity. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status, or other legally protected characteristics. Learn more about EEO rights as an applicant here. If you need assistance and/or a reasonable accommodation due to a disability during the application or recruiting process, please send us a request at accommodations@applovin.com. AppLovin will consider for employment all qualified applicants with criminal histories in a manner consistent with applicable law. If you’re applying for a position in California, learn more here. Please read our Global Applicant Privacy Notice to learn more about how AppLovin processes your personal information.",
        "url": "https://www.linkedin.com/jobs/view/3943915471",
        "summary": "AppLovin seeks a motivated engineer to join their Platform team, responsible for scaling their core products that process over 6PB of data daily. The ideal candidate has 2+ years of experience with Java or Scala, strong algorithms knowledge, and big data systems expertise, particularly in Apache Spark, big data processing, and HDFS.",
        "industries": [
            "Software",
            "Technology",
            "Artificial Intelligence",
            "Data Processing",
            "Mobile"
        ],
        "soft_skills": [
            "Motivated",
            "Collaborative",
            "Influential",
            "Problem-Solving",
            "Desire to solve complex problems"
        ],
        "hard_skills": [
            "Java",
            "Scala",
            "Spark",
            "Airflow",
            "GCP",
            "Algorithms",
            "Apache Spark",
            "SparkSQL",
            "HDFS",
            "Apache Kafka",
            "RabbitMQ",
            "Kubernetes"
        ],
        "tech_stack": [
            "Java",
            "Scala",
            "Spark",
            "Airflow",
            "GCP",
            "Apache Spark",
            "SparkSQL",
            "HDFS",
            "Apache Kafka",
            "RabbitMQ",
            "Kubernetes"
        ],
        "programming_languages": [
            "Java",
            "Scala"
        ],
        "experience": 2,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Computer Science",
                "Related field"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Campbell, CA",
        "job_id": 3813094331,
        "company": "1872 Consulting",
        "title": "Data Engineer",
        "created_on": 1720635521.8879936,
        "description": "Data Engineer Portland, OR - Hybrid onsite 2-3 days per week Description This position is available with one of the world's most experienced platform firmware security companies. They are seeking a Senior Database engineer to implement a remote management system that can pass/receive commands and data to/from client devices. Responsibilities Maintaining and enhancing the functionality and performance of existing database programs. Monitoring and enhancing databases and related systems to optimize performance. Proactively addressing scalability and performance issues. Ensuring data quality and integrity while supporting large data sets. Debugging and resolving database reliability, integrity, and efficiency. Documenting processes related to the design, configuration, and deployment to new instances. Collaborate with Product, Engineering and Data Science to ensure data quality and integrity for data reporting, exploration, and analysis. Follow agile development methodologies to deliver solutions and product features by following DevOps practices. Requirements: 5+ years of Data Engineering experience Experience with AWS or Azure Cloud Services (AWS preferred) Experience with Cloud-based Data Warehousing design MUST have at least some PostgreSQL experience Nice to have: Experience with Python or similar language for data automation Experience with Hadoop DevOps Experience: Docker, Jenkins, GitHub, etc. Tableau, Domo or Power BI",
        "url": "https://www.linkedin.com/jobs/view/3813094331",
        "summary": "This is a Senior Database Engineer role at a global platform firmware security company. The position involves implementing a remote management system to handle command and data exchange between devices. Key responsibilities include maintaining and optimizing database systems, ensuring data quality and integrity, troubleshooting issues, collaborating with cross-functional teams, and following agile development practices.",
        "industries": [
            "Security",
            "Software",
            "Technology"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem-solving",
            "Troubleshooting",
            "Data Analysis",
            "Teamwork",
            "Agile",
            "DevOps"
        ],
        "hard_skills": [
            "Database Administration",
            "Database Performance Tuning",
            "Data Quality",
            "Data Integrity",
            "Data Warehousing",
            "PostgreSQL",
            "AWS",
            "Azure",
            "Python",
            "Hadoop",
            "Docker",
            "Jenkins",
            "GitHub",
            "Tableau",
            "Domo",
            "Power BI"
        ],
        "tech_stack": [
            "PostgreSQL",
            "AWS",
            "Azure",
            "Python",
            "Hadoop",
            "Docker",
            "Jenkins",
            "GitHub",
            "Tableau",
            "Domo",
            "Power BI"
        ],
        "programming_languages": [
            "Python"
        ],
        "experience": 5,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Fremont, CA",
        "job_id": 3965494929,
        "company": "BayOne Solutions",
        "title": "Senior Azure / Data Engineer with (ETL/ Data warehouse background)",
        "created_on": 1720635523.611555,
        "description": "Role: Senior Azure / Data Engineer with (ETL/ Data warehouse background) Location: Fremont, CA, Austin, TX and Tualatin, OR Duration: Long Term Contract It’s a Hybrid Role and needs to be Onsite 2 Days a Week Need with 10+ years of experience Must have Skills : Min 5 years of experience in modern data engineering/data warehousing/data lakes technologies on cloud platforms like Azure, AWS, GCP, Data Bricks, etc. Azure experience is preferred over other cloud platforms. 10 + years of proven experience with SQL, schema design, and dimensional data modeling Solid knowledge of data warehouse best practices, development standards, and methodologies Experience with ETL/ELT tools like ADF, Informatica, Talend, etc., and data warehousing technologies like Azure Synapse, Azure SQL, Amazon Redshift, Snowflake, Google Big Query, etc.. Strong experience with big data tools(Databricks, Spark, etc..) and programming skills in PySpark and Spark SQL. Be an independent self-learner with a “let’s get this done” approach and the ability to work in Fast paced and Dynamic environment. Excellent communication and teamwork abilities. Nice-to-Have Skills: Event Hub, IOT Hub, Azure Stream Analytics, Azure Analysis Service, Cosmo DB knowledge. SAP ECC /S/4 and Hana knowledge. Intermediate knowledge on Power BI Azure DevOps and CI/CD deployments, Cloud migration methodologies and processes Best Regards, Santosh Cherukuri Email: scherukuri@bayonesolutions.com",
        "url": "https://www.linkedin.com/jobs/view/3965494929",
        "summary": "Seeking a Senior Azure Data Engineer with 10+ years of experience in data warehousing and ETL/ELT processes.  Strong experience with Azure technologies is required, including Azure Synapse, Azure SQL, Azure Data Factory, and Databricks.  Strong SQL, schema design, and dimensional modeling skills are essential. Experience with big data tools like Spark and PySpark is a must.  The role is a hybrid position requiring 2 days per week on-site in Fremont, CA, Austin, TX, or Tualatin, OR.  ",
        "industries": [
            "Technology",
            "Data Analytics",
            "Cloud Computing",
            "Software Development"
        ],
        "soft_skills": [
            "Self-Learner",
            "Independent",
            "Teamwork",
            "Communication"
        ],
        "hard_skills": [
            "SQL",
            "Schema Design",
            "Dimensional Data Modeling",
            "ETL",
            "ELT",
            "Data Warehousing",
            "Azure Synapse",
            "Azure SQL",
            "Amazon Redshift",
            "Snowflake",
            "Google BigQuery",
            "Databricks",
            "Spark",
            "PySpark",
            "Spark SQL",
            "Azure Data Factory",
            "Informatica",
            "Talend",
            "Event Hub",
            "IOT Hub",
            "Azure Stream Analytics",
            "Azure Analysis Service",
            "Cosmos DB",
            "Power BI",
            "Azure DevOps",
            "CI/CD",
            "Cloud Migration"
        ],
        "tech_stack": [
            "Azure",
            "Azure Synapse",
            "Azure SQL",
            "Azure Data Factory",
            "Databricks",
            "Spark",
            "PySpark",
            "Spark SQL",
            "Event Hub",
            "IOT Hub",
            "Azure Stream Analytics",
            "Azure Analysis Service",
            "Cosmos DB",
            "Power BI",
            "Azure DevOps",
            "CI/CD"
        ],
        "programming_languages": [
            "SQL",
            "PySpark",
            "Spark SQL"
        ],
        "experience": 10,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Jose, CA",
        "job_id": 3907284672,
        "company": "TikTok",
        "title": "Data Engineer, Data Services",
        "created_on": 1720635525.5073373,
        "description": "Responsibilities TikTok is the leading destination for short-form mobile video. Our mission is to inspire creativity and bring joy. TikTok has global offices including Los Angeles, New York, London, Paris, Berlin, Dubai, Singapore, Jakarta, Seoul and Tokyo. Why Join Us Creation is the core of TikTok's purpose. Our platform is built to help imaginations thrive. This is doubly true of the teams that make TikTok possible. Together, we inspire creativity and bring joy - a mission we all believe in and aim towards achieving every day. To us, every challenge, no matter how difficult, is an opportunity; to learn, to innovate, and to grow as one team. Status quo? Never. Courage? Always. At TikTok, we create together and grow together. That's how we drive impact - for ourselves, our company, and the communities we serve. Join us. Our ads data platform team work closely with our product managers and other Engineers by building state of the art streaming and batch data processing solution. The entire data pipeline is supporting both the Tiktok ads platform and our internal business intelligence platform. In this role, you will see a direct link between your work, and the company's business success. You will have opportunities to deal with Petabyte-level data warehouse. Some of the world's most challenging technical and business problems are waiting for you to solve. Responsibilities: 1. Work closely with engineering, product managers, and business leaders to make the data-first product design 2. Interface with engineers, and product managers to understand real data needs 3. Design, build, and run large-scale data service framework, and real-time/batch data pipelines. Resolve reliability/scalability challenges Qualifications Qualifications: 1. Have a deep understanding of computer principles, and have a good data structure and algorithm foundation 2. Expertise in developing backend systems and data product design with big data technologies (Flink/Spark/Kafka/Hive/ES/Clickhouse/Doris) 3. 2+ years of hands-on experience in software development, particularly with Java 4. Good at communication, sensitive to business, able to quickly understand business background, and have excellent ability to integrate technology and business Preferred Qualification: 1. Experience in large-scale streaming data computing and data warehouses is preferred but is not required TikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too. TikTok is committed to providing reasonable accommodations in our recruitment processes for candidates with disabilities, pregnancy, sincerely held religious beliefs or other reasons protected by applicable laws. If you need assistance or a reasonable accommodation, please reach out to us at https://shorturl.at/cdpT2 Job Information: 【For Pay Transparency】Compensation Description (annually) The base salary range for this position in the selected city is $136800 - $205000 annually. ​ Compensation may vary outside of this range depending on a number of factors, including a candidate’s qualifications, skills, competencies and experience, and location. Base pay is one part of the Total Package that is provided to compensate and recognize employees for their work, and this role may be eligible for additional discretionary bonuses/incentives, and restricted stock units. ​ Our company benefits are designed to convey company culture and values, to create an efficient and inspiring work environment, and to support our employees to give their best in both work and life. We offer the following benefits to eligible employees: ​ We cover 100% premium coverage for employee medical insurance, approximately 75% premium coverage for dependents and offer a Health Savings Account(HSA) with a company match. As well as Dental, Vision, Short/Long term Disability, Basic Life, Voluntary Life and AD&D insurance plans. In addition to Flexible Spending Account(FSA) Options like Health Care, Limited Purpose and Dependent Care. ​ Our time off and leave plans are: 10 paid holidays per year plus 17 days of Paid Personal Time Off (PPTO) (prorated upon hire and increased by tenure) and 10 paid sick days per year as well as 12 weeks of paid Parental leave and 8 weeks of paid Supplemental Disability. ​ We also provide generous benefits like mental and emotional health benefits through our EAP and Lyra. A 401K company match, gym and cellphone service reimbursements. The Company reserves the right to modify or change these benefits programs at any time, with or without notice. ​",
        "url": "https://www.linkedin.com/jobs/view/3907284672",
        "summary": "TikTok is seeking a Data Engineer to design, build, and run large-scale data service frameworks and real-time/batch data pipelines. The ideal candidate will have a deep understanding of computer principles, expertise in big data technologies (Flink, Spark, Kafka, Hive, ES, Clickhouse, Doris), and 2+ years of hands-on Java development experience. The role offers opportunities to work with petabyte-level data warehouses and solve challenging technical and business problems. This position is based in the US and offers a base salary range of $136,800 - $205,000 annually, along with comprehensive benefits such as health insurance, dental, vision, paid time off, parental leave, and a 401K company match.",
        "industries": [
            "Technology",
            "Social Media",
            "Data Engineering"
        ],
        "soft_skills": [
            "Communication",
            "Business Acumen",
            "Problem Solving",
            "Collaboration",
            "Teamwork"
        ],
        "hard_skills": [
            "Java",
            "Flink",
            "Spark",
            "Kafka",
            "Hive",
            "ES",
            "Clickhouse",
            "Doris",
            "Data Structures",
            "Algorithms",
            "Backend Systems",
            "Data Product Design"
        ],
        "tech_stack": [
            "Flink",
            "Spark",
            "Kafka",
            "Hive",
            "ES",
            "Clickhouse",
            "Doris"
        ],
        "programming_languages": [
            "Java"
        ],
        "experience": 2,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 205000,
            "min": 136800
        },
        "benefits": [
            "Medical Insurance",
            "Dental Insurance",
            "Vision Insurance",
            "Health Savings Account (HSA)",
            "Short/Long Term Disability",
            "Life Insurance",
            "AD&D Insurance",
            "Flexible Spending Account (FSA)",
            "Paid Time Off",
            "Paid Sick Days",
            "Parental Leave",
            "Supplemental Disability",
            "Employee Assistance Program (EAP)",
            "401K Match",
            "Gym Reimbursement",
            "Cellphone Reimbursement"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Palo Alto, CA",
        "job_id": 3920748324,
        "company": "Rhombus Power Inc.",
        "title": "Data Engineer, Palo Alto",
        "created_on": 1720635527.2514815,
        "description": "Rhombus Power is purposefully transforming defense and global security enterprises with Guardian, our Artificial Intelligence platform for strategic, operational, and tactical decision-making at the speed of relevance. We provide relevant, actionable, and AI-powered insights at each step in the defense decision-making cycle. Equipped with Guardian's AI-powered tools-- from infrastructure to data to insights -- our clients are able to solve their most complex, interconnected challenges and achieve decision and operational superiority. Come join our cross-disciplinary and world-class team that is delivering game-changing solutions to transform global security. Learn more about Rhombus and watch a demonstration of Guardian, our AI Platform here -- https://youtube.com/watch?v=3PxY6su1Q-Q See the following articles to learn more about what we do: https://foreignpolicy.com/2023/06/19/ai-artificial-intelligence-national-security-foreign-policy-threats-prediction/ https://federalnewsnetwork.com/air-force/2023/12/new-decision-advantage-tool-will-change-how-air-force-makes-investment-decisions/ https://apnews.com/article/us-intelligence-services-ai-models-9471e8c5703306eb29f6c971b6923187 Location Palo Alto, CA Job Description Rhombus' Data Engineers are instrumental in designing and implementing data engineering activities on topical, mission-driven projects that inform our national security agenda. They analyze, build, and maintain our organization's data infrastructure and processing systems. As a Data Engineer, you'll perform various code development and query needs for integration with new and existing applications, dashboards and machine learning pipelines. You'll work cross functionally with other departments and utilize your skills at all stages of the analytics pipeline to create results that drive business values and customer success! You'll be asked to travel to client sites where you'll engage and collaborate directly with the client to discuss data pipelines, solutioning, and to ensure a strong client connection. This role will require travel to client sites in Honolulu, HI; Omaha, NE and/or Colorado Springs, CO. Frequency will depend on business needs and may vary at times but can be upwards of twice per month. Last minute notice to travel to client sites may be given at times. Responsibilities: Develop code using various programming and scripting languages to automate data ingestion and improve data management processes. Architect data repositories, stand up data platforms and develop data pipelines for ingestion, transformation, and aggregation. Review existing architecture, data strategy, and improve processes for data governance, data quality, and metadata management. Extract and analyze raw data from multiple data sources via APIs, SQL Stored Procedures, or Python scripts. Ability to develop scripts and programs for converting various types of data into usable formats and support project teams to scale, monitor and operate data platforms. Collaborate with a multi-disciplinary team of analysts, data scientists, data engineers, developers, and data consumers in a fast-paced, agile environment. You'll use your experience in analytical exploration and data examination while you manage the assessment, design, building, and maintenance of scalable platforms for clients. Communicate project status and results to various levels of leadership. Qualifications: A Bachelor's degree in Data Analytics, Computer Science, Computer Engineering, Information Systems/Sciences, or other relevant area (or equivalent experience) and at least 1 year of professional experience, or a Master's degree with strong academic project experience. Experience with 1 or more programming and scripting languages, with a strong focus on Python, Pandas and Numpy. Other languages include: Shell, PERL, Java, C/C++/C#, Scala, etc. Experience with 1 or more of the following relational, noSQL and/or file based storage (e.g. MYSQL, MongoDB, SQL Server, Oracle, Postgres, Hbase, DynamoDB, etc.) Experience building and maintaining ETL data pipelines Experience with software development life cycle including testing, documenting, delivery and support Working knowledge of AWS/cloud technologies Experience using query optimization as well as data modeling techniques. Familiarity with machine learning frameworks (such as TensorFlow, Scikit-Learn, etc.) The ability to obtain and maintain a US security clearance. U.S. citizenship is required as only U.S. citizens are eligible for a security clearance Must be eligible for Secret clearance and for TS/SCI Personal Qualities: Ability to contextualize data as it relates to Rhombus' vision. We want someone who can understand the \"why\" and \"so what\", and not just the \"how\". Our Data Engineers help us transmit the enthusiasm we have around the data work to the individuals doing that work. Must have strong problem-solving skills and demonstrated excellent oral and written communication skills. We're seeking folks with strong intellectual curiosity and creativity, with an ability and willingness to learn new skills and apply novel solutions to problems. Comfort in a fast-paced start-up environment is crucial, with the ability to consistently revise your approach in response to new information, and to be flexible and adaptive. Benefits Full medical, dental, vision coverage for employee and dependents 401k matching program PTO and Holidays Bonus and other incentive programs Access to mental health program Access to Flexible Spending Accounts for Health Care, Dependent and Commuter About Rhombus Rhombus Power Inc. (Rhombus) is a startup located in the heart of Silicon Valley at Stanford Research Park in Palo Alto.  We use cutting-edge cross-disciplinary approaches to solve pressing Big Data and Sensing problems in security, energy, and healthcare. Our advisory board includes two Nobel Laureates and a Draper Prize winner. Rhombus compensates, motivates, and develops employees, who are trusted, empowered, and involved. Employees have clear roles and expectations – and their roles are flexible enough to move at the speed of innovation in order to meet and exceed client expectations. We have a unique culture of global purpose, rooted in the innovation and progress of Silicon Valley. Rhombus knows that diversity is a condition for success. We are committed to hiring and retaining a diverse workforce. We are proud to be an Equal Opportunity/Affirmative Action Employer.",
        "url": "https://www.linkedin.com/jobs/view/3920748324",
        "summary": "Rhombus Power is seeking a Data Engineer to join their team and design, implement, and maintain data engineering activities for mission-driven projects in national security. The ideal candidate will have experience with Python, Pandas, and Numpy, as well as various data storage technologies, ETL pipelines, and AWS/cloud technologies. They will also have strong problem-solving skills and a passion for data analysis and machine learning.",
        "industries": [
            "Defense",
            "Security",
            "Artificial Intelligence",
            "National Security",
            "Data Analytics",
            "Data Engineering"
        ],
        "soft_skills": [
            "Problem-solving",
            "Communication",
            "Creativity",
            "Adaptability",
            "Collaboration",
            "Teamwork",
            "Curiosity"
        ],
        "hard_skills": [
            "Python",
            "Pandas",
            "Numpy",
            "Shell",
            "PERL",
            "Java",
            "C",
            "C++",
            "C#",
            "Scala",
            "MYSQL",
            "MongoDB",
            "SQL Server",
            "Oracle",
            "Postgres",
            "Hbase",
            "DynamoDB",
            "ETL",
            "AWS",
            "Cloud technologies",
            "Query Optimization",
            "Data Modeling",
            "TensorFlow",
            "Scikit-Learn"
        ],
        "tech_stack": [
            "Python",
            "Pandas",
            "Numpy",
            "MYSQL",
            "MongoDB",
            "SQL Server",
            "Oracle",
            "Postgres",
            "Hbase",
            "DynamoDB",
            "AWS",
            "TensorFlow",
            "Scikit-Learn"
        ],
        "programming_languages": [
            "Python",
            "Shell",
            "PERL",
            "Java",
            "C",
            "C++",
            "C#",
            "Scala"
        ],
        "experience": 1,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Data Analytics",
                "Computer Science",
                "Computer Engineering",
                "Information Systems/Sciences"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Medical",
            "Dental",
            "Vision",
            "401k matching",
            "PTO",
            "Holidays",
            "Bonus",
            "Mental health program",
            "Flexible Spending Accounts"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Jose, CA",
        "job_id": 3955649540,
        "company": "Steneral Consulting",
        "title": "Looking Independent Data Engineer :: HANA",
        "created_on": 1720635528.8302,
        "description": "Job title: Data Engineer - HANA Workplace type: Hybrid (1-2 days onsite) - Locals Only Worksite location: 345 Park Ave, San Jose, California, 95110-2704, United States Duration: 1-year contract Top Skills: HANA, SQL, Data modeling, HANA modeling Top Skills' Details **Please note that the position is hybrid onsite in San Jose, CA. Candidates must be already local, or be onsite on day 1.** Strong demonstrated skill working with SQL programming Experience in HANA database Performance Tuning of Database Schemas, Databases, SQL, ETL Jobs, and related scripts Demonstrated skills in data modeling (HANA data modeling), SQL Stored Procedures Functional knowledge of SAP S/4 and/or ECC (specifically SD) a big plus Experience with troubleshooting/ production support Experience with Big Data (Azure, Databricks) desired",
        "url": "https://www.linkedin.com/jobs/view/3955649540",
        "summary": "This is a 1-year contract for a Data Engineer specializing in HANA database. The position is hybrid (1-2 days onsite) in San Jose, CA. Candidates must be local or onsite on day 1. The role requires strong SQL skills, HANA database experience, performance tuning abilities, data modeling skills (HANA data modeling), and knowledge of SQL Stored Procedures. Familiarity with SAP S/4 and/or ECC (particularly SD) is beneficial. Experience with troubleshooting, production support, and Big Data (Azure, Databricks) is desired.",
        "industries": [
            "Software",
            "Technology",
            "Data Engineering",
            "Database Management",
            "SAP"
        ],
        "soft_skills": [
            "Troubleshooting",
            "Problem Solving",
            "Communication",
            "Collaboration",
            "Teamwork"
        ],
        "hard_skills": [
            "SQL",
            "HANA",
            "Data Modeling",
            "HANA Modeling",
            "Performance Tuning",
            "ETL",
            "SQL Stored Procedures",
            "SAP S/4HANA",
            "ECC",
            "SD",
            "Azure",
            "Databricks",
            "Big Data"
        ],
        "tech_stack": [
            "HANA",
            "SQL",
            "SAP S/4HANA",
            "ECC",
            "Azure",
            "Databricks"
        ],
        "programming_languages": [
            "SQL"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3787337439,
        "company": "Lever Middleware Test Company 2",
        "title": "Software Engineer",
        "created_on": 1720635530.4408069,
        "description": "Work at Derby Consulting Lever builds software for teams to source, interview, and hire top talent. Our team strives to set a new bar for enterprise software with modern, well-designed, real-time apps. As a Software Engineer, you’ll help us build out our core product by developing high-impact, user-facing features. In our engineering organization, you’ll be a driver for positive change in our engineering culture, processes and technology. You will be a strong voice in product planning, drive the implementation and release of major features, and be a champion of best practices for writing well-tested, well-organized code. You’ll become familiar with all parts of our stack. You will exercise judgment in making tradeoffs between design and feasibility. You’ll engineer your features to be scalable and resilient in a complex, single-page application. We believe that user-centric design ultimately leads to the best products, so we listen closely to our users, both external and internal. As an engineer on our close-knit, cross-functional team, you’ll be an active voice in shaping our product. We are constantly rolling out high-demand features and tackling ever greater challenges of scale. You’ll join a team where everyone—including you—is knowledgeable about development patterns and cares about the product development process. The Derby Story We participated in Y Combinator in summer 2012, and since then have raised $40 million. This year, we’re doubling the team in size, and we’re looking forward to supporting more great companies like Netflix, Eventbrite, and Lyft. Interested in learning more? Take an inside look at Lever! Derby Consulting is proud to be an equal opportunity workplace dedicated to pursuing and hiring a diverse workforce.",
        "url": "https://www.linkedin.com/jobs/view/3787337439",
        "summary": "Derby Consulting is seeking a Software Engineer to help build out their core product by developing high-impact, user-facing features. The ideal candidate will have strong experience with modern development practices and be able to work in a fast-paced environment. Responsibilities include developing features, driving positive change in engineering culture and processes, and championing best practices for writing well-tested code.",
        "industries": [
            "Software",
            "Technology",
            "Human Resources",
            "Recruitment"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem-solving",
            "Decision-making",
            "Leadership",
            "Teamwork",
            "User-centricity"
        ],
        "hard_skills": [
            "Software Development",
            "Feature Development",
            "Engineering Culture",
            "Engineering Processes",
            "Code Quality",
            "Testing",
            "Scalability",
            "Resilience",
            "Single-Page Applications",
            "Product Development",
            "Development Patterns"
        ],
        "tech_stack": [
            "React",
            "Redux",
            "Node.js",
            "AWS",
            "Postgres",
            "Redis"
        ],
        "programming_languages": [
            "JavaScript",
            "SQL"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Sunnyvale, CA",
        "job_id": 3948118834,
        "company": "Walmart Data Ventures",
        "title": "Senior, Data Engineer - Data Ventures",
        "created_on": 1720635532.1682158,
        "description": "Position Summary... What you'll do... What you'll do... As a \"Senior Data Engineer\", you should be able to technically help and assist team to steer through correct technical directions following the best practices. You will have deeper understanding of Data Engineering approaches along with hands on experience in building highly scalable solutions. About Team: Data Ventures Our team creates reusable technologies to help with customer acquisition, onboarding, and empowering merchants, while ensuring a seamless experience for both stakeholders. We also optimize tariffs and assortment in accordance with Walmart's Everyday Low-Cost philosophy. We not only create affordability, but we also deliver customized experiences for customers across all channels - in-store, mobile app, and websites. Our team is responsible for providing support to US Marketplace sellers. We focus on providing immediate solutions to the cases/tickets created by sellers. We interact with multiple teams across the company to provide excellent seller experience. What You'll Do You will lead the work of other small groups of three to five engineers, including offshore associates, for assigned Engineering projects by providing pertinent. documents, direction, and examples; identifying short- and long- term solutions and timeline; reviewing and approving proposed solutions. You will drive the execution of multiple business plans and projects by identifying customer and operational needs, developing, and communicating business. Participates in medium- to large-scale, complex, cross-functional projects by reviewing project requirements, translating requirements into technical solutions; gathering requested information (for example, design documents, product requirements, wire frames); writing and developing. code; conducting unit testing; communicating status and issues to team members and stakeholders; collaborating with project team and cross. functional teams; troubleshooting open issues and bug-fixes; enhancing design to prevent re-occurrences of defects; ensuring on-time delivery and hand-offs: interacting with project manager to provide input on project plan; and providing leadership to the project team. plans and priorities; removing barriers and obstacles that impact performance; providing resources; identifying performance standards; measuring. progress and adjusting performance; accordingly, developing contingency plans; and demonstrating adaptability and supporting continuous learning. Implementing new architectural patterns; and performing design and code reviews of changes. Promotes and supports company policies, procedures, mission, values, and standards of ethics and integrity by training and providing direction to others in their use and application; ensuring compliance with them; and utilizing and supporting the Open Door Policy. Ensures business needs are being met by evaluating the ongoing effectiveness of current plans, programs, and initiatives, consulting with business. partners, managers, co-workers, or other key stakeholders; soliciting, evaluating, and applying suggestions for improving efficiency and cost effectiveness. and participating in and supporting community outreach events. What You'll Bring Must have Strong hands-on experience with Hadoop, Hive, Apache Spark . Experience in building highly scalable Big Data solutions and ETL ecosystems. Proven track record coding with at least one programming language e.g., Scala (preferred), Python. Expert in cloud computing platforms and offerings from Google Cloud Platform (GCP) (preferred), Microsoft Azure. Evangelize an extremely high standard of code quality, system reliability, and performance. Experience with the integration tools like Automic, Airflow Skilled in data modeling & data migration protocols Working knowledge of CI/CD pipelines. Ability to write designs for data architecture of data warehouse or data lake solutions or end to end pipelines. Expert in data architecture principles, distributed computing Intake prioritization, cost/benefit analysis, decision making of what to pursue across a wide base of users/stakeholders and across products, databases, and services, Should be able to communicate complex technical solutions and ideas to both technical and non-technical team members. Ability to lead our technical relationship with partners and mentor senior software developers in multiple initiatives. Nice to have Knowledge of Databricks is an added advantage. Experience with ThoughtSpot, Druid, Big Query and ClickHouse is added advantage. Hands on knowledge in NoSQL like Cosmos DB along with RDBMS like MySQL, Postgres is plus. Hands on working experience in any messaging platform like Kafka is preferred. Increase the efficiency of the team by setting right Processes of Software Development, Requirement Intake, Effort Estimation Demonstrating creative, critical thinking & troubleshooting skills. About Walmart Global Tech Imagine working in an environment where one line of code can make life easier for hundreds of millions of people. That's what we do at Walmart Global Tech. We're a team of software engineers, data scientists, cybersecurity expert's and service professionals within the world's leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered. We train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail. Flexible, Hybrid Work We use a hybrid way of working that is primarily in office coupled with virtual when not onsite. Our campuses serve as a hub to enhance collaboration, bring us together for purpose and deliver on business needs. This approach helps us make quicker decisions, remove location barriers across our global team and be more flexible in our personal lives. Benefits Benefits: Beyond our great compensation package, you can receive incentive awards for your performance. Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and much more. Equal Opportunity Employer Walmart, Inc. is an Equal Opportunity Employer - By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing diversity- unique styles, experiences, identities, ideas and opinions - while being inclusive of all people. The above information has been designed to indicate the general nature and level of work performed in the role. It is not designed to contain or be interpreted as a comprehensive inventory of all responsibilities and qualifications required of employees assigned to this job. The full Job Description can be made available as part of the hiring process. At Walmart, we offer competitive pay as well as performance-based bonus awards and other great benefits for a happier mind, body, and wallet. Health benefits include medical, vision and dental coverage. Financial benefits include 401(k), stock purchase and company-paid life insurance. Paid time off benefits include PTO (including sick leave), parental leave, family care leave, bereavement, jury duty, and voting. Other benefits include short-term and long-term disability, company discounts, Military Leave Pay, adoption and surrogacy expense reimbursement, and more. ‎ ‎ ‎ You will also receive PTO and/or PPTO that can be used for vacation, sick leave, holidays, or other purposes. The amount you receive depends on your job classification and length of employment. It will meet or exceed the requirements of paid sick leave laws, where applicable. ‎ For information about PTO, see https://one.walmart.com/notices . ‎ ‎ Live Better U is a Walmart-paid education benefit program for full-time and part-time associates in Walmart and Sam's Club facilities. Programs range from high school completion to bachelor's degrees, including English Language Learning and short-form certificates. Tuition, books, and fees are completely paid for by Walmart. ‎ Eligibility requirements apply to some benefits and may depend on your job classification and length of employment. Benefits are subject to change and may be subject to a specific plan or program terms. ‎ For Information About Benefits And Eligibility, See One.Walmart . ‎ Sunnyvale, California US-04396:The annual salary range for this position is $117,000.00-$234,000.00 ‎ Bentonville, Arkansas US-09050:The annual salary range for this position is $90,000.00-$180,000.00 ‎ ‎ ‎ ‎ ‎ ‎ ‎ ‎ ‎ ‎ Additional Compensation Includes Annual Or Quarterly Performance Bonuses. ‎ Additional Compensation For Certain Positions May Also Include ‎ ‎ Stock ‎ ‎ Minimum Qualifications... Outlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications. Option 1: Bachelor's degree in Computer Science and 3 years' experience in software engineering or related field. Option 2: 5 years' experience in software engineering or related field. Option 3: Master's degree in Computer Science and 1 year's experience in software engineering or related field. 2 years' experience in data engineering, database engineering, business intelligence, or business analytics. Preferred Qualifications... Outlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications. Data engineering, database engineering, business intelligence, or business analytics, ETL tools and working with large data sets in the cloud, Master's degree in Computer Science or related field and 3 years' experience in software engineering, We value candidates with a background in creating inclusive digital experiences, demonstrating knowledge in implementing Web Content Accessibility Guidelines (WCAG) 2.2 AA standards, assistive technologies, and integrating digital accessibility seamlessly. The ideal candidate would have knowledge of accessibility best practices and join us as we continue to create accessible products and services following Walmart's accessibility standards and guidelines for supporting an inclusive culture. Primary Location... 840 W California Ave, Sunnyvale, CA 94086-4828, United States of America",
        "url": "https://www.linkedin.com/jobs/view/3948118834",
        "summary": "Senior Data Engineer role at Walmart Global Tech focusing on building highly scalable big data solutions and ETL ecosystems, leading teams, and driving the execution of business plans. Requires strong experience with Hadoop, Hive, Spark, Scala/Python, cloud platforms (GCP preferred), and CI/CD pipelines.",
        "industries": [
            "Retail",
            "E-commerce",
            "Data Analytics",
            "Software Development",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Leadership",
            "Communication",
            "Problem Solving",
            "Decision Making",
            "Teamwork",
            "Technical Communication",
            "Relationship Building",
            "Mentorship"
        ],
        "hard_skills": [
            "Hadoop",
            "Hive",
            "Apache Spark",
            "Scala",
            "Python",
            "Google Cloud Platform",
            "Microsoft Azure",
            "Automic",
            "Airflow",
            "Data Modeling",
            "Data Migration",
            "CI/CD Pipelines",
            "Data Architecture",
            "Distributed Computing",
            "Databricks",
            "ThoughtSpot",
            "Druid",
            "BigQuery",
            "ClickHouse",
            "Cosmos DB",
            "MySQL",
            "Postgres",
            "Kafka"
        ],
        "tech_stack": [
            "Hadoop",
            "Hive",
            "Apache Spark",
            "Scala",
            "Python",
            "Google Cloud Platform",
            "Microsoft Azure",
            "Automic",
            "Airflow",
            "Databricks",
            "ThoughtSpot",
            "Druid",
            "BigQuery",
            "ClickHouse",
            "Cosmos DB",
            "MySQL",
            "Postgres",
            "Kafka"
        ],
        "programming_languages": [
            "Scala",
            "Python"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Computer Science",
                "Software Engineering",
                "Data Engineering",
                "Database Engineering",
                "Business Intelligence",
                "Business Analytics"
            ]
        },
        "salary": {
            "max": 234000,
            "min": 90000
        },
        "benefits": [
            "Competitive pay",
            "Performance-based bonuses",
            "401(k) match",
            "Stock purchase plan",
            "Paid maternity and parental leave",
            "PTO",
            "Multiple health plans",
            "Medical",
            "Vision",
            "Dental",
            "Life insurance",
            "Short-term and long-term disability",
            "Company discounts",
            "Military Leave Pay",
            "Adoption and surrogacy expense reimbursement",
            "Live Better U education benefit program"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3842800900,
        "company": "Envoy",
        "title": "Analytics Engineer",
        "created_on": 1720635533.7600021,
        "description": "About Envoy Envoy’s workplace platform has redefined how companies welcome visitors, improve the onsite experience, book desks and meeting rooms, manage deliveries, and access accurate and unified workplace data in 16,000 locations around the globe by designing products that solve common workplace problems. Envoy provides a simple way to manage your complex safety, security, and compliance needs across all your workplace locations—wherever you need to bring people together. Rely on smart, automated solutions to common workplace problems, like freeing up unused space and eliminating repetitive tasks. Not only does this allow you to make the most efficient use of your space and resources, it frees up your team’s time to focus on the work that matters. With Envoy’s intuitive technology that employees actually enjoy using, you can create a great workplace experience that fosters community and togetherness by making it easy for teams to coordinate working onsite. Unlike companies that offer disconnected workplace solutions and disparate (and often imprecise) data sources, Envoy’s platform provides accurate, comprehensive, and unified workplace data so you can make informed business decisions. Envoy’s integrated solutions pull data from multiple sources to ensure that you always have the most accurate data available. For more information, visit Envoy.com . About The Role We are a top-notch data organization with a great culture and have the same high standards with our code, systems, practices, and people. We value learning and growth and hire diverse, well-rounded, communicative people. Pipeline Management: You’ll build and maintain robust and scalable data pipelines, monitoring and optimizing data pipeline performance Data Modeling: Be responsible for designing, implementing and maintaining scalable and efficient models for both structured and unstructured data Collaboration: Partner with product, engineering, and Go-to-Market stakeholders to ensure we’re collecting and processing the right data to drive decision making ETL Development: Develop and optimize ETL processes to load data into our data stores, then implement efficient testing and validation to ensure the accuracy and integrity of the data Dataset Evaluation: You’ll be the decision maker on how the data is consumed for our internal and external stakeholders Our main technology stack currently includes Redshift, Spark, AWS Glue, Databricks, dbt, Airflow, Looker, Segment and Jupyter Notebooks. If you’re looking to challenge the status quo and build the Office OS, come join us. This is a hybrid position that requires at least 3 days a week (Tuesday - Thursday) in our San Francisco HQ office. You will Expand our data warehouse with clean data ready for analysis Help to define and improve our internal standards for style, maintainability, and best practices for high-scale data infrastructure Apply software engineering best practices like version control, naming conventions, and continuous integration to the analytics code base Maintain our orchestration infrastructure (we use Airflow) as well as build high quality data models (we use dbt) to answer key business questions Push the boundaries of our data stack by implementing new tools and technologies that increase the efficiency and productivity of our team Lead efforts to design data models and build curated data sets that are the foundation of reporting and analytics at Envoy Mentor junior members of the team You have 5+ years in data engineering or analytics engineering, with experience at a high-growth startup preferred Advanced SQL skills with experience standardizing queries and building data infrastructure involving large-scale relational datasets Experience using Python to parse, structure, and transform data Significant hands-on experience with modern data stack tooling. Our stack includes Redshift, Spark, AWS Glue, Databricks, dbt, Airflow, Looker, and Segment Experience working with databases that power APIs for front-end applications Hands-on experience with maintaining and scaling dbt Experience working with CI/CD pipelines You are Someone with extremely high standards that obsesses over data accuracy. You’re practical and know perfect is the enemy of good, but you aspire for us to be great. You care deeply about the quality of your work and our data. You are passionate about finding the insights and “truth” that data can give. A big picture, systems thinker. You think about how the whole is a sum of many parts and how we can properly measure them end to end such that we have the per context for insights. An owner. You feel personally accountable and responsible and know seeing the problem is less than half of it. You look for problems and inefficiencies and find elegant solutions to them before they become major issues. Fast-paced. You love the speed of and impact you have in startups. Conversely you never, ever want to be a tiny cog in a gigantic machine. An open-minded learner. You live to learn new things, like staying up to date on new technologies, tools, and techniques. You are inspired by what people inside and outside Envoy know and are eager to incorporate the world's knowledge into your work. Emotionally mature & humble. You care about being effective over being right. Ideally you’ve made major changes & decisions in your life and learned from the results. Communicative & empathetic. You are happy when helping others succeed, particularly your team and partners across the company. You'll get A high degree of trust in your ideas and execution An opportunity to partner and collaborate with other talented people An inclusive community where you feel welcomed and cared for as a person The ability to make an immediate impact helping customers create a great workplace experience Support for your personal and professional growth Compensation Description Envoy's compensation package includes market competitive salary, equity for all full time roles, and great benefits. If you are located in San Francisco Bay Area , our expected cash compensation for this role is $152,000 - $179,000 (Annually). We are hiring for multiple levels and backgrounds, so final offers may vary within the range provided based on experience, expertise, and other factors. If you have any questions related to compensation, please contact Recruiting after you apply. By applying for this position, you acknowledge that you have fully read and understand the job requirements and received the Envoy Privacy Notice for applicants, which is linked here . Completing this application requires you to provide personal data, such as your name and contact information, which is mandatory for Envoy to process your application. Envoy is an EEO Employer and does not discriminate on the basis of any characteristic protected by local, state or federal law.",
        "url": "https://www.linkedin.com/jobs/view/3842800900",
        "summary": "Envoy is seeking a Data Engineer to join their team in San Francisco. The role will focus on building and maintaining data pipelines, designing and implementing data models, and collaborating with stakeholders to ensure data accuracy and integrity.  This position involves working with a variety of technologies, including Redshift, Spark, AWS Glue, Databricks, dbt, Airflow, Looker, and Segment, and requires 5+ years of experience in data engineering or analytics engineering.",
        "industries": [
            "Technology",
            "Software",
            "Workplace Management",
            "Data Engineering"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem-solving",
            "Learning",
            "Accountability",
            "Ownership",
            "Empathy",
            "Emotional Maturity",
            "Humility"
        ],
        "hard_skills": [
            "SQL",
            "Python",
            "Redshift",
            "Spark",
            "AWS Glue",
            "Databricks",
            "dbt",
            "Airflow",
            "Looker",
            "Segment",
            "Jupyter Notebooks",
            "ETL",
            "Data Modeling",
            "Data Pipelines",
            "Data Warehousing",
            "CI/CD",
            "Version Control",
            "Data Infrastructure",
            "Data Accuracy",
            "Data Analysis",
            "API"
        ],
        "tech_stack": [
            "Redshift",
            "Spark",
            "AWS Glue",
            "Databricks",
            "dbt",
            "Airflow",
            "Looker",
            "Segment",
            "Jupyter Notebooks"
        ],
        "programming_languages": [
            "SQL",
            "Python"
        ],
        "experience": 5,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 179000,
            "min": 152000
        },
        "benefits": [
            "Equity",
            "Competitive Salary",
            "Benefits"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Jose, CA",
        "job_id": 3797979951,
        "company": "TikTok",
        "title": "Data Engineer, Global Live",
        "created_on": 1720635541.17603,
        "description": "Responsibilities TikTok is the leading destination for short-form mobile video. Our mission is to inspire creativity and bring joy. TikTok has global offices including Los Angeles, New York, London, Paris, Berlin, Dubai, Singapore, Jakarta, Seoul and Tokyo. Why Join Us Creation is the core of TikTok's purpose. Our platform is built to help imaginations thrive. This is doubly true of the teams that make TikTok possible. Together, we inspire creativity and bring joy - a mission we all believe in and aim towards achieving every day. To us, every challenge, no matter how difficult, is an opportunity; to learn, to innovate, and to grow as one team. Status quo? Never. Courage? Always. At TikTok, we create together and grow together. That's how we drive impact - for ourselves, our company, and the communities we serve. Join us. The Data Platform Global Live team is dedicated to empowering the growth of TikTok LIVE business through big data. We support our businesses in achieving their missions by building high quality real-time and offline data warehouses, creating various forms of efficient and data-friendly data assets, and exploring and implementing business oriented data solutions. We provide stable and reliable data capabilities for daily operations, analyses, decision-making of TikTok LIVE features, in addition to robust data support to enhance live performance for streamers. As a data engineer in the Global Live team, you will have the opportunity to build, optimize and grow one of the largest data platforms in the world. You'll have the opportunity to gain hands-on experience on all kinds of systems in the data platform ecosystem. Your work will have a direct and huge impact on the company's core products as well as hundreds of millions of users. Responsibilities - What You'll Do • Design and build data transformations efficiently and reliably for different purposes (e.g. reporting, growth analysis, multi-dimensional analysis); • Design and implement reliable, scalable, robust and extensible big data systems that support core products and business; • Establish solid design and best engineering practice for engineers as well as non-technical people. Qualifications • BS or MS degree in Computer Science or related technical field or equivalent practical experience; • Experience in the Big Data technologies(Hadoop, M/R, Hive, Spark, Metastore, Presto, Flume, Kafka, ClickHouse, Flink etc.); • Experience with performing data analysis, data ingestion and data integration; • Experience with ETL(Extraction, Transformation & Loading) and architecting data systems; • Experience with schema design, data modeling and SQL queries; • Passionate and self-motivated about technologies in the Big Data area. TikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too. TikTok is committed to providing reasonable accommodations in our recruitment processes for candidates with disabilities, pregnancy, sincerely held religious beliefs or other reasons protected by applicable laws. If you need assistance or a reasonable accommodation, please reach out to us at dataecommerce.accommodations@tiktok.com. Job Information: 【For Pay Transparency】Compensation Description (annually) The base salary range for this position in the selected city is $136000 - $280000 annually. ​ Compensation may vary outside of this range depending on a number of factors, including a candidate’s qualifications, skills, competencies and experience, and location. Base pay is one part of the Total Package that is provided to compensate and recognize employees for their work, and this role may be eligible for additional discretionary bonuses/incentives, and restricted stock units. ​ Our company benefits are designed to convey company culture and values, to create an efficient and inspiring work environment, and to support our employees to give their best in both work and life. We offer the following benefits to eligible employees: ​ We cover 100% premium coverage for employee medical insurance, approximately 75% premium coverage for dependents and offer a Health Savings Account(HSA) with a company match. As well as Dental, Vision, Short/Long term Disability, Basic Life, Voluntary Life and AD&D insurance plans. In addition to Flexible Spending Account(FSA) Options like Health Care, Limited Purpose and Dependent Care. ​ Our time off and leave plans are: 10 paid holidays per year plus 17 days of Paid Personal Time Off (PPTO) (prorated upon hire and increased by tenure) and 10 paid sick days per year as well as 12 weeks of paid Parental leave and 8 weeks of paid Supplemental Disability. ​ We also provide generous benefits like mental and emotional health benefits through our EAP and Lyra. A 401K company match, gym and cellphone service reimbursements. The Company reserves the right to modify or change these benefits programs at any time, with or without notice. ​",
        "url": "https://www.linkedin.com/jobs/view/3797979951",
        "summary": "TikTok is seeking a Data Engineer to join their Global Live team. This role will involve building, optimizing, and scaling a large-scale data platform for TikTok LIVE, working with technologies like Hadoop, Spark, Kafka, and ClickHouse. You'll design data transformations, implement robust data systems, and contribute to best practices for data engineering.",
        "industries": [
            "Technology",
            "Social Media",
            "Entertainment",
            "Data Engineering",
            "Big Data",
            "Streaming",
            "Analytics"
        ],
        "soft_skills": [
            "Passionate",
            "Self-motivated",
            "Problem-solving",
            "Communication",
            "Teamwork",
            "Analytical",
            "Detail-oriented"
        ],
        "hard_skills": [
            "Hadoop",
            "M/R",
            "Hive",
            "Spark",
            "Metastore",
            "Presto",
            "Flume",
            "Kafka",
            "ClickHouse",
            "Flink",
            "Data Analysis",
            "Data Ingestion",
            "Data Integration",
            "ETL",
            "Data Modeling",
            "Schema Design",
            "SQL"
        ],
        "tech_stack": [
            "Hadoop",
            "Spark",
            "Kafka",
            "ClickHouse",
            "Flink",
            "Hive",
            "Presto",
            "Flume",
            "Metastore"
        ],
        "programming_languages": [
            "SQL"
        ],
        "experience": 0,
        "education": {
            "min_degree": "BS",
            "fields": [
                "Computer Science",
                "Related Technical Field"
            ]
        },
        "salary": {
            "max": 280000,
            "min": 136000
        },
        "benefits": [
            "Medical Insurance",
            "Dental Insurance",
            "Vision Insurance",
            "Short/Long Term Disability",
            "Life Insurance",
            "AD&D Insurance",
            "Flexible Spending Account (FSA)",
            "Health Savings Account (HSA)",
            "Paid Time Off (PTO)",
            "Sick Leave",
            "Parental Leave",
            "Supplemental Disability Leave",
            "Employee Assistance Program (EAP)",
            "Mental and Emotional Health Benefits",
            "401K Match",
            "Gym Reimbursements",
            "Cellphone Reimbursements"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Sunnyvale, CA",
        "job_id": 3939142654,
        "company": "Walmart Data Ventures",
        "title": "Senior, Data Engineer - Data Ventures",
        "created_on": 1720635542.7874146,
        "description": "Position Summary... What you'll do... As a \"Senior Data Engineer\", you should be able to technically help and assist team to steer through correct technical directions following the best practices. You will have deeper understanding of Data Engineering approaches along with hands on experience in building highly scalable solutions. About Team: Data Ventures Our team creates reusable technologies to help with customer acquisition, onboarding, and empowering merchants, while ensuring a seamless experience for both stakeholders. We also optimize tariffs and assortment in accordance with Walmart's Everyday Low-Cost philosophy. We not only create affordability, but we also deliver customized experiences for customers across all channels - in-store, mobile app, and websites. Our team is responsible for providing support to US Marketplace sellers. We focus on providing immediate solutions to the cases/tickets created by sellers. We interact with multiple teams across the company to provide excellent seller experience. What You'll Do You will lead the work of other small groups of three to five engineers, including offshore associates, for assigned Engineering projects by providing pertinent. documents, direction, and examples; identifying short- and long- term solutions and timeline; reviewing and approving proposed solutions. You will drive the execution of multiple business plans and projects by identifying customer and operational needs, developing, and communicating business. Participates in medium- to large-scale, complex, cross-functional projects by reviewing project requirements, translating requirements into technical solutions; gathering requested information (for example, design documents, product requirements, wire frames); writing and developing. code; conducting unit testing; communicating status and issues to team members and stakeholders; collaborating with project team and cross. functional teams; troubleshooting open issues and bug-fixes; enhancing design to prevent re-occurrences of defects; ensuring on-time delivery and hand-offs: interacting with project manager to provide input on project plan; and providing leadership to the project team. plans and priorities; removing barriers and obstacles that impact performance; providing resources; identifying performance standards; measuring. progress and adjusting performance; accordingly, developing contingency plans; and demonstrating adaptability and supporting continuous learning. Implementing new architectural patterns; and performing design and code reviews of changes. Promotes and supports company policies, procedures, mission, values, and standards of ethics and integrity by training and providing direction to others in their use and application; ensuring compliance with them; and utilizing and supporting the Open Door Policy. Ensures business needs are being met by evaluating the ongoing effectiveness of current plans, programs, and initiatives, consulting with business. partners, managers, co-workers, or other key stakeholders; soliciting, evaluating, and applying suggestions for improving efficiency and cost effectiveness. and participating in and supporting community outreach events. What You'll Bring Must have Strong hands-on experience with Hadoop, Hive, Apache Spark . Experience in building highly scalable Big Data solutions and ETL ecosystems. Proven track record coding with at least one programming language e.g., Scala (preferred), Python. Expert in cloud computing platforms and offerings from Google Cloud Platform (GCP) (preferred), Microsoft Azure. Evangelize an extremely high standard of code quality, system reliability, and performance. Experience with the integration tools like Automic, Airflow Skilled in data modeling & data migration protocols Working knowledge of CI/CD pipelines. Ability to write designs for data architecture of data warehouse or data lake solutions or end to end pipelines. Expert in data architecture principles, distributed computing Intake prioritization, cost/benefit analysis, decision making of what to pursue across a wide base of users/stakeholders and across products, databases, and services, Should be able to communicate complex technical solutions and ideas to both technical and non-technical team members. Ability to lead our technical relationship with partners and mentor senior software developers in multiple initiatives. Nice to have Knowledge of Databricks is an added advantage. Experience with ThoughtSpot, Druid, Big Query and ClickHouse is added advantage. Hands on knowledge in NoSQL like Cosmos DB along with RDBMS like MySQL, Postgres is plus. Hands on working experience in any messaging platform like Kafka is preferred. Increase the efficiency of the team by setting right Processes of Software Development, Requirement Intake, Effort Estimation Demonstrating creative, critical thinking & troubleshooting skills. About Walmart Global Tech Imagine working in an environment where one line of code can make life easier for hundreds of millions of people. That's what we do at Walmart Global Tech. We're a team of software engineers, data scientists, cybersecurity expert's and service professionals within the world's leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered. We train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail. Flexible, Hybrid Work We use a hybrid way of working that is primarily in office coupled with virtual when not onsite. Our campuses serve as a hub to enhance collaboration, bring us together for purpose and deliver on business needs. This approach helps us make quicker decisions, remove location barriers across our global team and be more flexible in our personal lives. Benefits Benefits: Beyond our great compensation package, you can receive incentive awards for your performance. Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and much more. Equal Opportunity Employer Walmart, Inc. is an Equal Opportunity Employer - By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing diversity- unique styles, experiences, identities, ideas and opinions - while being inclusive of all people. The above information has been designed to indicate the general nature and level of work performed in the role. It is not designed to contain or be interpreted as a comprehensive inventory of all responsibilities and qualifications required of employees assigned to this job. The full Job Description can be made available as part of the hiring process. At Walmart, we offer competitive pay as well as performance-based bonus awards and other great benefits for a happier mind, body, and wallet. Health benefits include medical, vision and dental coverage. Financial benefits include 401(k), stock purchase and company-paid life insurance. Paid time off benefits include PTO (including sick leave), parental leave, family care leave, bereavement, jury duty, and voting. Other benefits include short-term and long-term disability, company discounts, Military Leave Pay, adoption and surrogacy expense reimbursement, and more. ‎ ‎ ‎ You will also receive PTO and/or PPTO that can be used for vacation, sick leave, holidays, or other purposes. The amount you receive depends on your job classification and length of employment. It will meet or exceed the requirements of paid sick leave laws, where applicable. ‎ For information about PTO, see https://one.walmart.com/notices . ‎ ‎ Live Better U is a Walmart-paid education benefit program for full-time and part-time associates in Walmart and Sam's Club facilities. Programs range from high school completion to bachelor's degrees, including English Language Learning and short-form certificates. Tuition, books, and fees are completely paid for by Walmart. ‎ Eligibility requirements apply to some benefits and may depend on your job classification and length of employment. Benefits are subject to change and may be subject to a specific plan or program terms. ‎ For Information About Benefits And Eligibility, See One.Walmart . ‎ Sunnyvale, California US-08479:The annual salary range for this position is $117,000.00-$234,000.00 ‎ Bentonville, Arkansas US-09050:The annual salary range for this position is $90,000.00-$180,000.00 ‎ ‎ ‎ ‎ ‎ ‎ ‎ ‎ ‎ ‎ Additional Compensation Includes Annual Or Quarterly Performance Bonuses. ‎ Additional Compensation For Certain Positions May Also Include ‎ ‎ Stock ‎ ‎ Minimum Qualifications... Outlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications. Option 1: Bachelor's degree in Computer Science and 3 years' experience in software engineering or related field. Option 2: 5 years' experience in software engineering or related field. Option 3: Master's degree in Computer Science and 1 year's experience in software engineering or related field. 2 years' experience in data engineering, database engineering, business intelligence, or business analytics. Preferred Qualifications... Outlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications. Data engineering, database engineering, business intelligence, or business analytics, ETL tools and working with large data sets in the cloud, Master's degree in Computer Science or related field and 3 years' experience in software engineering, We value candidates with a background in creating inclusive digital experiences, demonstrating knowledge in implementing Web Content Accessibility Guidelines (WCAG) 2.2 AA standards, assistive technologies, and integrating digital accessibility seamlessly. The ideal candidate would have knowledge of accessibility best practices and join us as we continue to create accessible products and services following Walmart's accessibility standards and guidelines for supporting an inclusive culture. Primary Location... 640 W California Avenue, Sunnyvale, CA 94086-4828, United States of America",
        "url": "https://www.linkedin.com/jobs/view/3939142654",
        "summary": "Senior Data Engineer leads engineering projects, builds highly scalable Big Data solutions, and works with cloud computing platforms like GCP. Requires strong experience with Hadoop, Hive, Apache Spark, Scala or Python, and data modeling.",
        "industries": [
            "Retail",
            "E-commerce",
            "Technology",
            "Data Engineering",
            "Big Data"
        ],
        "soft_skills": [
            "Leadership",
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Technical Expertise",
            "Decision Making",
            "Mentoring",
            "Analytical Thinking",
            "Critical Thinking",
            "Troubleshooting"
        ],
        "hard_skills": [
            "Hadoop",
            "Hive",
            "Apache Spark",
            "Scala",
            "Python",
            "Cloud Computing",
            "GCP",
            "Azure",
            "Automic",
            "Airflow",
            "Data Modeling",
            "Data Migration",
            "CI/CD",
            "Data Architecture",
            "Distributed Computing",
            "NoSQL",
            "Cosmos DB",
            "RDBMS",
            "MySQL",
            "Postgres",
            "Kafka",
            "Databricks",
            "ThoughtSpot",
            "Druid",
            "Big Query",
            "ClickHouse"
        ],
        "tech_stack": [
            "Hadoop",
            "Hive",
            "Apache Spark",
            "Scala",
            "Python",
            "GCP",
            "Azure",
            "Automic",
            "Airflow",
            "Databricks",
            "ThoughtSpot",
            "Druid",
            "Big Query",
            "ClickHouse",
            "Cosmos DB",
            "MySQL",
            "Postgres",
            "Kafka"
        ],
        "programming_languages": [
            "Scala",
            "Python"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Computer Science"
            ]
        },
        "salary": {
            "max": 234000,
            "min": 90000
        },
        "benefits": [
            "401(k) match",
            "Stock purchase plan",
            "Paid maternity and parental leave",
            "PTO",
            "Multiple health plans",
            "Performance-based bonus awards",
            "Company-paid life insurance",
            "Short-term and long-term disability",
            "Company discounts",
            "Military Leave Pay",
            "Adoption and surrogacy expense reimbursement",
            "Live Better U education benefit program"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "California City, CA",
        "job_id": 3941634186,
        "company": "Software Technology Inc.",
        "title": "Junior Software Engineer",
        "created_on": 1720635544.338976,
        "description": "Hello Everyone, Job Description. Understand software implementation, system interface requirements and interface management. Participate in software code reviews and create and/or update software based on problem reports. Review software engineering documentation to ensure specifications meet system needs and are accurate. Update documentation, as required Atlassian tool suite expertise (JIRA, Confluence) Microsoft Office Suite (Word, Excel, PowerPoint) Embedded C/C++ Experience developing code on various platforms (ARM, x86, PowerPC, SPARC, 8051, etc.) Experience with hardware interfaces (PCI, 1553, SPI, I2C, CAN, RS-422/232, etc.) Experience with developing RTOS applications and device drivers",
        "url": "https://www.linkedin.com/jobs/view/3941634186",
        "summary": "This job requires a software engineer with expertise in embedded systems, hardware interfaces, and RTOS applications. Responsibilities include understanding software implementation, reviewing code, updating documentation, and working with Atlassian tools.",
        "industries": [
            "Software Development",
            "Embedded Systems",
            "Aerospace & Defense",
            "Automotive",
            "Telecommunications"
        ],
        "soft_skills": [
            "Problem Solving",
            "Communication",
            "Teamwork",
            "Documentation"
        ],
        "hard_skills": [
            "C",
            "C++",
            "JIRA",
            "Confluence",
            "Word",
            "Excel",
            "PowerPoint",
            "ARM",
            "x86",
            "PowerPC",
            "SPARC",
            "8051",
            "PCI",
            "1553",
            "SPI",
            "I2C",
            "CAN",
            "RS-422",
            "RS-232",
            "RTOS",
            "Device Drivers"
        ],
        "tech_stack": [
            "Atlassian Tool Suite (JIRA, Confluence)",
            "Microsoft Office Suite (Word, Excel, PowerPoint)",
            "Embedded C/C++",
            "ARM",
            "x86",
            "PowerPC",
            "SPARC",
            "8051",
            "PCI",
            "1553",
            "SPI",
            "I2C",
            "CAN",
            "RS-422/232",
            "RTOS"
        ],
        "programming_languages": [
            "C",
            "C++"
        ],
        "experience": 0,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Electrical Engineering",
                "Software Engineering"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Irvine, CA",
        "job_id": 3786117682,
        "company": "Kaygen, Inc.",
        "title": "PySpark Data Engineer/ETL Developer",
        "created_on": 1720635546.0360744,
        "description": "KAYGEN is an emerging leader in providing top talent for technology based staffing services. We specialize in providing high-volume contingent staffing, direct hire staffing and project based solutions to companies worldwide ranging from startups to Fortune 500 and Managed Service Providers (MSP) across a wide variety of industries Job Description: We are seeking a highly skilled and motivated Data Engineer/ETL Developer to join our dynamic team. The ideal candidate will have expertise in utilizing technologies such as Python, PySpark (Main Technology), Airflow, AWS, S3, QTest, GitHub, and SQL to design, develop, and maintain efficient data pipelines and ETL processes. Responsibilities: ETL Development: Design, develop, and implement scalable and efficient ETL processes using Python and PySpark. Collaborate with cross-functional teams to gather and understand data requirements. Ensure the quality, reliability, and performance of data pipelines. Workflow Automation: Utilize Airflow to create and manage workflow automation for scheduling, monitoring, and orchestrating data tasks. Implement best practices for scheduling and dependencies within data workflows. Cloud Integration: Work with AWS services, particularly S3, to store and retrieve large volumes of data. Implement and optimize data storage solutions on the cloud platform. Testing and Quality Assurance: Collaborate with the testing team to ensure data quality through the use of QTest and other testing tools. Develop and execute test plans for ETL processes. Version Control and Collaboration: Use GitHub for version control, collaborating with other developers to manage code repositories effectively. Participate in code reviews to ensure code quality and adherence to coding standards. Database Management: Write complex SQL queries and optimize database performance. Work with various databases to extract, transform, and load data efficiently. Qualifications: Bachelor’s degree in Computer Science, Information Technology, or related field. Proven experience in developing ETL processes and data pipelines. Proficiency in Python, PySpark, Airflow, AWS (especially S3), QTest, GitHub, and SQL. Strong understanding of data modeling and database design concepts. Experience with version control and collaborative development workflows. Excellent problem-solving and troubleshooting skills. Strong communication skills with the ability to work in a collaborative team environment. Preferred Skills: Familiarity with big data technologies and frameworks. Experience with data warehousing and business intelligence tools. Knowledge of best practices in data security and compliance. At KAYGEN, we are always looking for dynamic, talented and experienced individuals. We invite you to join our team of talented IT professionals, consulting at client locations across the globe. Our culture is team-orientated; we strive to stand by our core values of respect, honesty and integrity. Our team of experienced staffing experts will work with you to find you the best opportunity. For more information please visit us at www.kaygen.com. Benefits: Healthcare Insurance Vision and Dental Insurance 401(k) Retirement Plan Free Life Insurance Vacation Time Off Sick Time Off Family Medical Leave (FMLA) Achieve your Kaizen by clicking here. A unique and exclusive talent community supported by Kaygen, that includes programs like: Certifications Mentorship Program Referrals Family and Wellness benefits Continuous Growth and Career Development",
        "url": "https://www.linkedin.com/jobs/view/3786117682",
        "summary": "KAYGEN is seeking a Data Engineer/ETL Developer with experience in Python, PySpark, Airflow, AWS, S3, QTest, GitHub, and SQL to design, develop, and maintain efficient data pipelines and ETL processes. Responsibilities include ETL development, workflow automation, cloud integration, testing and quality assurance, version control and collaboration, and database management. The ideal candidate will have a Bachelor's degree in Computer Science or related field, proven experience in developing ETL processes, proficiency in the listed technologies, strong understanding of data modeling and database design, experience with version control and collaborative development workflows, excellent problem-solving and troubleshooting skills, and strong communication skills.",
        "industries": [
            "Information Technology",
            "Staffing",
            "Technology",
            "Data Engineering",
            "Data Analytics",
            "Cloud Computing",
            "Big Data"
        ],
        "soft_skills": [
            "Communication",
            "Teamwork",
            "Problem Solving",
            "Troubleshooting",
            "Collaboration"
        ],
        "hard_skills": [
            "Python",
            "PySpark",
            "Airflow",
            "AWS",
            "S3",
            "QTest",
            "GitHub",
            "SQL",
            "Data Modeling",
            "Database Design",
            "Version Control",
            "Data Security",
            "Data Compliance"
        ],
        "tech_stack": [
            "Python",
            "PySpark",
            "Airflow",
            "AWS",
            "S3",
            "QTest",
            "GitHub",
            "SQL"
        ],
        "programming_languages": [
            "Python",
            "SQL"
        ],
        "experience": 0,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Information Technology"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Healthcare Insurance",
            "Vision and Dental Insurance",
            "401(k) Retirement Plan",
            "Free Life Insurance",
            "Vacation Time Off",
            "Sick Time Off",
            "Family Medical Leave (FMLA)",
            "Certifications",
            "Mentorship Program",
            "Referrals",
            "Family and Wellness benefits",
            "Continuous Growth and Career Development"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Los Angeles, CA",
        "job_id": 3944029482,
        "company": "Cynet Systems",
        "title": "Data Engineer",
        "created_on": 1720635547.5885148,
        "description": "Job Description: Pay Range $50hr - $55hr Experience: 6 - 8 years delivering production data pipelines. Must Haves: Python, JSON data, Relational Data. Nice To Haves: Node/JavaScript.",
        "url": "https://www.linkedin.com/jobs/view/3944029482",
        "summary": "Seeking an experienced Data Engineer with 6-8 years of experience in building production data pipelines. Strong proficiency in Python, JSON data, and relational databases is required. Node/JavaScript experience is a plus.",
        "industries": [
            "Data Engineering",
            "Software Development"
        ],
        "soft_skills": [],
        "hard_skills": [
            "Python",
            "JSON",
            "Relational Databases",
            "Node.js",
            "JavaScript"
        ],
        "tech_stack": [
            "Python",
            "JSON",
            "Relational Databases",
            "Node.js"
        ],
        "programming_languages": [
            "Python",
            "JavaScript"
        ],
        "experience": 6,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 55,
            "min": 50
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3885775424,
        "company": "Unity",
        "title": "Principal Data Engineer",
        "created_on": 1720635549.2081776,
        "description": "The Opportunity We are looking for an experienced Principal Data Engineer with a strong background in machine learning and a proven track record in the AdTech sector. The ideal candidate will lead our data engineering efforts, focusing on developing and optimizing our data pipelines for machine learning models to enhance our advertising technology solutions. You will lead the effort in driving our data strategy, to enable the next generations of Machine Learning models to drive our Ad Business What you’ll be doing: Lead the design, implementation, and continuous improvement of data pipeline for machine learning that drive our AdTech platform. Collaborate with cross-functional teams to understand business needs and translate them into complex data models and algorithms. Manage the collection, storage, and processing of large-scale data sets to ensure they are optimized for machine learning. Develop and maintain a scalable and reliable data infrastructure that supports real-time and batch data processing. Stay abreast of industry trends and advancements in AdTech and machine learning to identify and implement best practices. Mentor senior data engineers and scientists, fostering a culture of technical excellence and innovation. Ensure data privacy and compliance standards are met in all data engineering and machine learning initiatives. What we’re looking for: Bachelor's or Master's degree in Computer Science, Engineering, Mathematics, or a related field. Minimum of 7 years of experience in data engineering with a focus on machine learning models, preferably in the AdTech industry. Proficient in programming languages such as Python, Scala, or Java. Extensive experience with big data technologies (e.g., Flink, Spark) and cloud platforms (e.g., GCP preferred, AWS). Strong knowledge of machine learning frameworks (e.g., TensorFlow, PyTorch) and experience with deployment of ML models at scale. Demonstrated ability to lead projects and collaborate effectively with cross-functional teams. Excellent problem-solving skills and ability to work in a fast-paced, dynamic environment. Life at Unity Unity [NYSE: U] is the world's leading platform of tools for creators to build and grow real-time games, apps, and experiences across multiple platforms. Creators, ranging from game developers to artists, architects, automotive designers, infrastructure experts, filmmakers, and more, use Unity to bring their imaginations to life across multiple platforms, from mobile, PC, and console, to spatial computing. As of the fourth quarter of 2023, more than 69% of the top 1,000 mobile games are made with Unity as derived from a blended number of the top 1,000 games in the Google Play Store and iOS App Store. In 2023, Made with Unity applications had an average of 3.7 billion downloads per month. For more information, please visit www.unity.com. Unity is a proud equal opportunity employer. We are committed to fostering an inclusive, innovative environment and celebrate our employees across age, race, color, ancestry, national origin, religion, disability, sex, gender identity or expression, sexual orientation, or any other protected status in accordance with applicable law. Our differences are strengths that enable us to support the growing and evolving needs of our customers, partners, and collaborators. If there are preparations or accommodations we can make to help ensure you have a comfortable and positive interview experience, please fill out this form to let us know. Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records. This position requires the incumbent to have a sufficient knowledge of English to have professional verbal and written exchanges in this language since the performance of the duties related to this position requires frequent and regular communication with colleagues and partners located worldwide and whose common language is English. Headhunters and recruitment agencies may not submit resumes/CVs through this Web site or directly to managers. Unity does not accept unsolicited headhunter and agency resumes. Unity will not pay fees to any third-party agency or company that does not have a signed agreement with Unity. Your privacy is important to us. Please take a moment to review our Prospect and Applicant Privacy Policies. Should you have any concerns about your privacy, please contact us at DPO@unity.com. #SEN Note: Certain locations require a good faith disclosure of the base pay for the role. The actual base pay for the successful candidate may differ based on location and level. Gross pay salary $190,700—$258,000 USD",
        "url": "https://www.linkedin.com/jobs/view/3885775424",
        "summary": "Unity is looking for an experienced Principal Data Engineer with expertise in machine learning and AdTech to lead data engineering efforts, optimize data pipelines for machine learning models, and drive the company's data strategy.",
        "industries": [
            "Technology",
            "Software",
            "Gaming",
            "Advertising",
            "Data Science",
            "Machine Learning",
            "AdTech"
        ],
        "soft_skills": [
            "Leadership",
            "Collaboration",
            "Communication",
            "Problem-solving",
            "Project Management",
            "Technical Excellence",
            "Innovation",
            "Mentorship"
        ],
        "hard_skills": [
            "Python",
            "Scala",
            "Java",
            "Flink",
            "Spark",
            "GCP",
            "AWS",
            "TensorFlow",
            "PyTorch",
            "Machine Learning",
            "Data Pipelines",
            "Data Modeling",
            "Data Engineering"
        ],
        "tech_stack": [
            "Flink",
            "Spark",
            "GCP",
            "AWS",
            "TensorFlow",
            "PyTorch"
        ],
        "programming_languages": [
            "Python",
            "Scala",
            "Java"
        ],
        "experience": 7,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Computer Science",
                "Engineering",
                "Mathematics"
            ]
        },
        "salary": {
            "max": 258000,
            "min": 190700
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Irvine, CA",
        "job_id": 3835797612,
        "company": "Essex Property Trust",
        "title": "Data Engineer",
        "created_on": 1720635550.8624046,
        "description": "City Irvine State California Job Location Irvine Regional Office (Derian) Position Type Regular About Us The Data Platform team is a tight group of technologists and data scientists whose mission is to spread insight, information, and data driven solutions throughout the organization. We have complex data pipelines, large macroeconomic and market-based data sets, as well as our proprietary in-house data to develop with. Role Description We are seeking a highly skilled and experienced Data Engineer to join our growing team. You will be responsible for designing, developing, and maintaining our data infrastructure and architecture. You will work closely with our data architect, data engineers and data analysts to ensure the availability and reliability of our data pipelines and systems. The ideal candidate has a strong background in data engineering and is passionate about leveraging data to drive business insights and decisions. Please note that this job position entails in-person office requirements for a minimum of 3 days per week: Mondays, Tuesdays, and Wednesdays, located at Essex's corporate offices in Irvine, Woodland Hills, San Mateo, and Bellevue. Responsibilities Design, build, and maintain scalable and high-performance data pipelines and infrastructure. Collaborate with stakeholders to understand requirements and develop efficient data assets. Implement data integration, transformation, and cleanse pipelines using industry best practices. Design and implement data models and schemas to support business reporting and analytics. Identifying and resolving performance bottlenecks and data quality issues. Managing and monitoring data storage, backup, and recovery systems. Performing data validation, testing, and ongoing data quality assurance. Qualification And Work Experience Requirements Bachelor's degree in computer science, engineering, or related field. 2 - 5 years of demonstrable professional experience working as a Data Engineer. Expert knowledge of data processing and transformation techniques. In-depth understanding of data warehousing concepts and technologies. Strong programming skills in Python and SQL. Experience with cloud-based data platforms such as AWS, Google Cloud or Azure. Proficiency in designing and implementing data models and schemas. Knowledge of working with SSAS cubes using MDX and SSRS. Familiarity with data integration and ETL tools such as Informatica, dbt, Matillion or Talend. Practical knowledge of DevOps best practices. Strong problem-solving skills and attention to detail. Excellent communication and collaboration skills. Ability to work effectively in a fast-paced and dynamic environment. Effective communication and collaboration abilities. Prior multi-family real estate experience a plus. Good meme selection and desire to have fun. Preferred Experience Experience building data models Experience with Azure SQL or SQL Server Experience with Snowflake Experience with dbt Experience with Matillion Knowledge of Yardi All full-time regular associates are offered competitive salaries, experience career growth, and are eligible for benefit packages that include medical, dental, vision, paid parental leave, 401k employer match, excellence rewards, wellness programs, and much more. With our Sunday property operations office closures, 10 paid holidays, and 15 PTO days, work/life balance is a priority! Additionally, most positions are eligible for a housing discount of 20%. Essex provides great communities in which to live, work and invest. We are a purpose-driven company, and we pride ourselves on promoting an internal culture of growth and opportunity by engaging, enabling, and empowering our teams. Working at Essex is not a destination. It is a journey where you can confidently build your career. The salary range for this position is $101,000.00 - $151,000.00 per year. New hires generally start between $101,000.00 - $127,000.00 per year. The final salary offer will be determined after reviewing relevant factors, including but not limited to skill sets; relevant experience; internal equity; and other business and organizational needs. This role is also eligible to participate in Essex’s discretionary Annual Bonus program that is commensurate with the level of the position.",
        "url": "https://www.linkedin.com/jobs/view/3835797612",
        "summary": "Essex is seeking a Data Engineer with 2-5 years of experience to design, develop, and maintain data infrastructure and pipelines. The role involves working with stakeholders to understand requirements, implementing data integration and transformation, designing data models, resolving performance issues, and ensuring data quality. The ideal candidate has strong Python, SQL, and cloud platform skills, along with experience in data warehousing, ETL tools, and DevOps practices.",
        "industries": [
            "Real Estate",
            "Technology",
            "Data Science",
            "Data Engineering"
        ],
        "soft_skills": [
            "Collaboration",
            "Communication",
            "Problem-solving",
            "Attention to Detail",
            "Teamwork",
            "Dynamic Environment",
            "Effective Communication"
        ],
        "hard_skills": [
            "Data Processing",
            "Data Transformation",
            "Data Warehousing",
            "Python",
            "SQL",
            "AWS",
            "Google Cloud",
            "Azure",
            "Data Modeling",
            "MDX",
            "SSRS",
            "Informatica",
            "dbt",
            "Matillion",
            "Talend",
            "DevOps",
            "Azure SQL",
            "SQL Server",
            "Snowflake",
            "Yardi"
        ],
        "tech_stack": [
            "AWS",
            "Google Cloud",
            "Azure",
            "Informatica",
            "dbt",
            "Matillion",
            "Talend",
            "Azure SQL",
            "SQL Server",
            "Snowflake",
            "Yardi"
        ],
        "programming_languages": [
            "Python",
            "SQL",
            "MDX"
        ],
        "experience": 2,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Computer Science",
                "Engineering",
                "Related Field"
            ]
        },
        "salary": {
            "max": 151000,
            "min": 101000
        },
        "benefits": [
            "Medical",
            "Dental",
            "Vision",
            "Paid Parental Leave",
            "401k Employer Match",
            "Excellence Rewards",
            "Wellness Programs",
            "Sunday Property Operations Office Closures",
            "Paid Holidays",
            "PTO",
            "Housing Discount"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Thousand Oaks, CA",
        "job_id": 3926633820,
        "company": "Syntricate Technologies",
        "title": "Data Engineer with Pharma Domain Exp",
        "created_on": 1720635552.4989057,
        "description": "Required Skills Required Skills: 3+ years of experience in the data warehouse space 3+ years of experience with one or more programming languages, Python, Scala, or Java. 5+ Experience architecting and building ETL pipelines; Hands-on experience with SQL Experience with Semantic Layer technologies Experience with data modeling, performance tuning, and experience on relational and graph databases. Experience working with Apache Spark, Apache Airflow Hands-on development experience with Databricks Experience with Software engineering best-practices, including but not limited to version control, CI/CD, automated testing Experience with AWS services: EC2, S3, EMR, RDS, Redshift/Spectrum, Lambda, Glue, Athena, API gateway, and design patterns (Containers, Serverless, Docker, etc.) Domain expertise Pharmacy experience Responsible for product architecture, manage dependencies, feature prioritization, reviews and suggestion for solution Implementation.",
        "url": "https://www.linkedin.com/jobs/view/3926633820",
        "summary": "Data Warehouse Engineer with 3+ years experience in data warehousing, ETL pipeline architecture, and SQL.  Requires expertise in Semantic Layer technologies, data modeling, performance tuning, and relational/graph databases.  Experience with Apache Spark, Airflow, Databricks, and AWS services is essential.  Pharmacy domain expertise is a plus.",
        "industries": [
            "Data Warehousing",
            "Data Engineering",
            "Software Engineering",
            "Pharmacy"
        ],
        "soft_skills": [
            "Problem Solving",
            "Communication",
            "Collaboration",
            "Teamwork",
            "Critical Thinking",
            "Analytical Skills",
            "Decision Making",
            "Leadership"
        ],
        "hard_skills": [
            "SQL",
            "ETL",
            "Data Modeling",
            "Performance Tuning",
            "Relational Databases",
            "Graph Databases",
            "Apache Spark",
            "Apache Airflow",
            "Databricks",
            "AWS",
            "EC2",
            "S3",
            "EMR",
            "RDS",
            "Redshift",
            "Spectrum",
            "Lambda",
            "Glue",
            "Athena",
            "API Gateway",
            "Containers",
            "Serverless",
            "Docker"
        ],
        "tech_stack": [
            "AWS",
            "EC2",
            "S3",
            "EMR",
            "RDS",
            "Redshift",
            "Spectrum",
            "Lambda",
            "Glue",
            "Athena",
            "API Gateway",
            "Docker",
            "Apache Spark",
            "Apache Airflow",
            "Databricks",
            "SQL"
        ],
        "programming_languages": [
            "Python",
            "Scala",
            "Java"
        ],
        "experience": 3,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Data Science",
                "Engineering",
                "Pharmacy"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Thousand Oaks, CA",
        "job_id": 3925996788,
        "company": "Syntricate Technologies",
        "title": "Data Engineer with Pharma Domain",
        "created_on": 1720635554.1305337,
        "description": "Thousand Oaks, CA Contract 3+ years of experience in the data warehouse space \" 3+ years of experience with one or more programming languages, Python, Scala, or Java. \" 5+ Experience architecting and building ETL pipelines; Hands-on experience with SQL \" Experience with Semantic Layer technologies \" Experience with data modelling, performance tuning, and experience on relational and graph databases. \" Experience working with Apache Spark, Apache Airflow \" Hands-on development experience with Databricks \" Experience with Software engineering best-practices, including but not limited to version control, CI/CD, automated testing \" Experience with AWS services: EC2, S3, EMR, RDS, Redshift/Spectrum, Lambda, Glue, Athena, API gateway, and design patterns (Containers, Serverless, Docker, etc.) Regards, Ashutosh Pasbola Assistant Manager | Syntricate Technologies Inc. Direct: (781)-552-4332| Fax: 781-649-0786 Email: ashutosh@syntricatetechnologies.com | Web: www.syntricatetechnologies.com We're hiring! connect with us on LinkedIn and visit our Jobs Portal Minority Business Enterprise (MBE) Certified | E-Verified Corporation | Equal Employment Opportunity (EEO) Employer This e-mail message may contain confidential or legally privileged information and is intended only for the use of the intended recipient(s). Any unauthorized disclosure, dissemination, distribution, copying or the taking of any action in reliance on the information herein is prohibited. Please notify the sender immediately by email if you have received this email by mistake and delete this e-mail from your system. You have received this email as we have your email address shared by you or from one of our data sources or from our member(s) or subscriber(s) list. If you do not want to receive any further emails or updates, please reply and request to unsubscribe .",
        "url": "https://www.linkedin.com/jobs/view/3925996788",
        "summary": "Syntricate Technologies is looking for a Data Engineer with 3+ years of experience in data warehousing. Responsibilities include architecting and building ETL pipelines, working with Semantic Layer technologies, data modeling, performance tuning, and experience on relational and graph databases. The ideal candidate will have experience with Apache Spark, Apache Airflow, Databricks, and AWS services like EC2, S3, EMR, RDS, Redshift/Spectrum, Lambda, Glue, Athena, and API gateway. Software engineering best practices, including version control, CI/CD, and automated testing are also required.",
        "industries": [
            "Data Engineering",
            "Software Engineering",
            "Cloud Computing",
            "Analytics",
            "Data Warehousing"
        ],
        "soft_skills": [
            "Problem-solving",
            "Analytical",
            "Communication",
            "Collaboration",
            "Teamwork",
            "Self-motivated",
            "Organized"
        ],
        "hard_skills": [
            "Python",
            "Scala",
            "Java",
            "SQL",
            "ETL",
            "Semantic Layer",
            "Data Modeling",
            "Performance Tuning",
            "Relational Databases",
            "Graph Databases",
            "Apache Spark",
            "Apache Airflow",
            "Databricks",
            "Version Control",
            "CI/CD",
            "Automated Testing",
            "AWS",
            "EC2",
            "S3",
            "EMR",
            "RDS",
            "Redshift",
            "Spectrum",
            "Lambda",
            "Glue",
            "Athena",
            "API Gateway",
            "Docker",
            "Containers",
            "Serverless"
        ],
        "tech_stack": [
            "Python",
            "Scala",
            "Java",
            "SQL",
            "Apache Spark",
            "Apache Airflow",
            "Databricks",
            "AWS",
            "EC2",
            "S3",
            "EMR",
            "RDS",
            "Redshift",
            "Spectrum",
            "Lambda",
            "Glue",
            "Athena",
            "API Gateway",
            "Docker",
            "Containers",
            "Serverless"
        ],
        "programming_languages": [
            "Python",
            "Scala",
            "Java",
            "SQL"
        ],
        "experience": 3,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Jose, CA",
        "job_id": 3944426749,
        "company": "iSpace, Inc.",
        "title": "BI Data Engineer (with Databricks)",
        "created_on": 1720635555.949059,
        "description": "Title: BI Data Engineer (with Databricks) Location: REMOTE Duration: 3-6 Months Contract to PERM Remote role PST and CST time zones. As a member of the Concert Business Systems development team the BI Data engineer performs a wide range of data modeling, engineering, architecture and data management activities. Job Responsibilities Follow best practices in areas of data modeling, data interoperability, metadata management Work to streamline existing ETL processes, migrate to new platforms and improve data processing while building functional data lake for the business Apply best practices in data engineering processes, data architecture, data security, documentation. Deep understanding of agile methodologies is required Gather, analyze and communicate requirements for the platform functionality based on needs off implementing ETL processes. Actively engage and lead efforts of modernizing legacy systems, streamlining traditional ETL processes and documenting all systems and processes. Work with operations team to ensure that maintenance responsibilities are not part of data engineering day to day, including delivering easy to maintain, self-healing, scalable data pipelines Actively participate in building a data driven culture, contributing to the data community and supporting data enablement team in efforts related to making data easily accessible, ready in time, discoverable, usable and easy to understand Join continuous innovation efforts, bring and implement ideas to advance data engineering team into the center of excellence. Technical Skills/Competencies Demonstrated data modelling and engineering skills for scalability, data streaming, self-healing and scalable process design, data partitioning, distributed data processing, metadata management. Advanced programming skills in Python, PySpark API, advanced Spark SQL. Hands on experience with Databricks Delta Lake architectures. Strong understanding of data warehousing techniques. Deep understanding of query plans, ETL best practices for timeseries reporting, advanced dimensional modeling Experience writing SQL queries for customer applications and troubleshooting. Hands on experience using BI tools such as Tableau, Business objects, Cognos, Power BI etc. Expertise in understanding complex business needs, analyzing, designing and developing solutions. Experience with technical stack which includes Data Lake technologies like S3, AWS Data Lake, Databricks Delta lake; Data warehousing tech like Snowflake, Teradata, Oracle EDW; Deep understanding of operationalizing ETL processes using Spark and modern streaming ETL technologies like Spark Streaming, StreamSets, Kafka Connect Bachelor's degree in computer science or related field plus six years demonstrated work experience. Ideal candidates are highly motivated, resourceful, and capable of coming up to speed quickly.",
        "url": "https://www.linkedin.com/jobs/view/3944426749",
        "summary": "This role is for a BI Data Engineer responsible for data modeling, engineering, architecture, and management. The role involves streamlining ETL processes, migrating to new platforms, building a data lake, and modernizing legacy systems. The ideal candidate has advanced programming skills in Python and PySpark, experience with Databricks Delta Lake, and strong understanding of data warehousing techniques. They will also have experience with various BI tools, ETL technologies, and data lake technologies.",
        "industries": [
            "Data Engineering",
            "Data Analytics",
            "Software Development"
        ],
        "soft_skills": [
            "Communication",
            "Problem Solving",
            "Teamwork",
            "Leadership",
            "Analytical Thinking",
            "Resourcefulness",
            "Motivation"
        ],
        "hard_skills": [
            "Data Modeling",
            "Data Engineering",
            "Data Architecture",
            "Data Management",
            "ETL",
            "Data Interoperability",
            "Metadata Management",
            "Agile Methodologies",
            "Data Security",
            "Documentation",
            "Python",
            "PySpark",
            "Spark SQL",
            "Databricks Delta Lake",
            "Data Warehousing",
            "Query Plans",
            "Dimensional Modeling",
            "SQL",
            "Tableau",
            "Business Objects",
            "Cognos",
            "Power BI",
            "S3",
            "AWS Data Lake",
            "Snowflake",
            "Teradata",
            "Oracle EDW",
            "Spark Streaming",
            "StreamSets",
            "Kafka Connect"
        ],
        "tech_stack": [
            "Databricks",
            "PySpark",
            "Spark SQL",
            "Databricks Delta Lake",
            "S3",
            "AWS Data Lake",
            "Snowflake",
            "Teradata",
            "Oracle EDW",
            "Spark Streaming",
            "StreamSets",
            "Kafka Connect",
            "Tableau",
            "Business Objects",
            "Cognos",
            "Power BI"
        ],
        "programming_languages": [
            "Python",
            "SQL"
        ],
        "experience": 6,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Computer Science",
                "Related Field"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3860732166,
        "company": "Magic",
        "title": "Software Engineer - Pretraining Data",
        "created_on": 1720635559.3926542,
        "description": "Magic’s mission is to build safe AGI that accelerates humanity’s progress on the world’s most important problems. We believe the most promising path to safe AGI lies in automating research and code generation to improve models and solve alignment more reliably than humans can alone. Our approach combines frontier-scale pre-training, domain-specific RL, ultra-long context, and test-time compute to achieve this goal. About the role: As a Software Engineer working on our pretraining data, you write efficient and robust pipelines for giant, multimodal datasets. You will develop and optimize web scraping techniques to harvest and maintain data at internet-scale. What you might work on: Design & implement multimodal (video, audio, text etc) web crawlers for scraping and indexing petabytes of data Create large scale data processing pipelines using tools like Ray, Apache Spark, Apache Flink, Google BigQuery etc. Implement and scale deduplication techniques across modalities and apply heuristic and model-based techniques for parsing and filtering crawled data Identify new data sources for inclusion in pre/post-training datasets What we’re looking for: Strong proficiency in distributed computing and parallel processing techniques Obsession with details, reliability, and good testing to ensure data quality and integrity Experience with designing and maintaining high-performance, scalable data architectures Ability to design, develop and operate an LLM data pipeline from web scraping to data loading Magic strives to be the place where high-potential individuals can do their best work. We value quick learning and grit just as much as skill and experience. Our culture: Integrity. Words and actions should be aligned Hands-on. At Magic, everyone is building Teamwork. We move as one team, not N individuals Focus. Safely deploy AGI. Everything else is noise Quality. Magic should feel like magic Compensation, benefits and perks (US): Annual salary range: $100K - $1M Equity is a significant part of total compensation, in addition to salary 401(k) plan with 6% salary matching Generous health, dental and vision insurance for you and your dependants Unlimited paid time off Option to work in-person in SF or remotely Visa sponsorship and relocation stipend to bring you to SF A small, fast-paced, highly focused team",
        "url": "https://www.linkedin.com/jobs/view/3860732166",
        "summary": "Magic is looking for a Software Engineer to develop and maintain efficient, robust data pipelines for pre-training large, multimodal datasets. You will focus on web scraping, data processing, deduplication, data filtering, and identifying new data sources for pre/post-training.  This role requires strong distributed computing and parallel processing skills with a focus on data quality and integrity. You will design and operate data pipelines from web scraping to data loading.",
        "industries": [
            "Artificial Intelligence",
            "Machine Learning",
            "Data Science",
            "Software Development"
        ],
        "soft_skills": [
            "Obsession with details",
            "Reliability",
            "Good Testing",
            "Quick Learning",
            "Grit",
            "Integrity",
            "Teamwork",
            "Focus"
        ],
        "hard_skills": [
            "Distributed Computing",
            "Parallel Processing",
            "Web Scraping",
            "Data Processing",
            "Deduplication",
            "Data Filtering",
            "Data Pipelines",
            "Ray",
            "Apache Spark",
            "Apache Flink",
            "Google BigQuery",
            "LLM Data Pipelines",
            "Data Loading"
        ],
        "tech_stack": [
            "Ray",
            "Apache Spark",
            "Apache Flink",
            "Google BigQuery"
        ],
        "programming_languages": [],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 1000000,
            "min": 100000
        },
        "benefits": [
            "Equity",
            "401(k) plan with 6% salary matching",
            "Health, Dental and Vision Insurance",
            "Unlimited Paid Time Off",
            "Remote Work",
            "Visa Sponsorship",
            "Relocation Stipend"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Aliso Viejo, CA",
        "job_id": 3961812220,
        "company": "ScaleneWorks INC",
        "title": "Senior Data Engineer",
        "created_on": 1720635561.2616897,
        "description": "Summary Of Qualifications 4+ years of experience in developing data-centric applications. Proficiency in code refactoring, data format conversion, and data movement from raw to curated. Skilled in creating data transformation and validation logic. Strong understanding of best practices in streaming and batch data processing. Advanced knowledge of data formats (e.g., JSON, XML, Parquet) and data modeling techniques. Experience in data modeling for third normal form and dimensional models. Expertise in structuring and organizing Data Lake file systems for handling large data volumes. Collaboration Job Responsibilities: Work with a team of professionals to implement proposed designs and drive project completion. Ensure successful integration of computer programs to meet business needs and client requirements. Technical Skills Develop error-free code independently while upholding high-quality standards. Provide guidance to other developers and support Lead 1 - Software Engineering. Understand and strictly adhere to client needs for effective project delivery. Note: This job description outlines typical responsibilities. Additional essential functions may also be assigned. Employment Type: Full-Time",
        "url": "https://www.linkedin.com/jobs/view/3961812220",
        "summary": "This role requires a data engineer with 4+ years of experience in developing data-centric applications. The ideal candidate will have a strong understanding of data processing, data modeling, and data formats, including expertise in structuring and organizing Data Lake file systems. They will also possess excellent coding skills, including code refactoring, data format conversion, and data movement.",
        "industries": [
            "Data Engineering",
            "Software Development",
            "Information Technology"
        ],
        "soft_skills": [
            "Collaboration",
            "Problem Solving",
            "Communication",
            "Teamwork",
            "Client Focus"
        ],
        "hard_skills": [
            "Data Engineering",
            "Data Modeling",
            "Data Processing",
            "Code Refactoring",
            "Data Format Conversion",
            "Data Movement",
            "Data Transformation",
            "Data Validation",
            "Streaming Data Processing",
            "Batch Data Processing",
            "JSON",
            "XML",
            "Parquet",
            "Third Normal Form",
            "Dimensional Modeling",
            "Data Lake",
            "File Systems",
            "Software Development",
            "Code Development",
            "Quality Assurance"
        ],
        "tech_stack": [
            "Data Lake",
            "JSON",
            "XML",
            "Parquet"
        ],
        "programming_languages": [],
        "experience": 4,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Mountain View, CA",
        "job_id": 3947935930,
        "company": "Intuit",
        "title": "Distinguished Data Engineer (Mailchimp)",
        "created_on": 1720635562.881033,
        "description": "Overview Intuit is looking for a Distinguished Data Engineer who will be responsible for Mailchimp’s data technology strategy, architecture and our approach to leverage data to drive growth for the business. As a Distinguished Engineer, you will play a critical role in shaping the future of Intuit Mailchimp by providing architectural and thought leadership across multiple engineering teams, architects, systems, and offerings. You will drive the technology vision, direction, and strategy within your respective scope, ensuring that our technology is aligned with business outcomes and our customers' needs. What you'll bring 10+ years’ experience designing and leading architectural efforts for web, software or mobile applications. Experience with cloud/SaaS, big data, or analytics and Machine Learning. Boundaryless leadership experience: Applying the nuances of influencing based on the situation, knowing when to push and when to ease, driving through designs, ideas, and roadmaps. Seasoned and Ambidextrous leadership - able to balance short and long, inspire and coach. Excellent communication skills: Demonstrated ability to present to all levels of leadership, including executives. Experience in conceptualizing, launching, and driving business and product strategies and multi-year roadmaps. You question the status quo and always expand through others by mastering the art of galvanizing people, teams, organization . (Technology/Customer & Business problems) You are a deep platform thinker that breaks down the problem into self contained functionalities that work together. Expertise with modern technology stacks, API strategies, microservices, public cloud and programming languages. Experience or familiarity with MarTech and/or Fintech industries would be a plus Expertise with Agile Development, SCRUM, or Extreme Programming methodologies. BS/MS in computer science or equivalent work experience. Significant design/architecture experience. How you will lead Lead the data technology strategy and architecture for Mailchimp’s top initiatives (including our customer data platform (CDP)). Bring executive level domain expertise to lead strategic initiatives and architecture including the CDP and how to leverage data effectively for AI/ML models. You will identify gaps/opportunities and establish data technology vision, direction, and strategy within respective scopes. Lead Mailchimp’s strategy and approach to data architecture with the Intuit Data Strategy. Partner closely with Intuit data technology leadership to drive Mailchimp outcomes. Represent Mailchimp’s CDP, AI/ML Engine, Data Capabilities efforts internally and externally. Be a talent steward across Intuit, role modeling diversity and inclusion.",
        "url": "https://www.linkedin.com/jobs/view/3947935930",
        "summary": "Intuit Mailchimp seeks a Distinguished Data Engineer to lead their data technology strategy and architecture, including their Customer Data Platform (CDP). This role requires 10+ years of experience in web, software, or mobile application architecture, expertise in cloud/SaaS, big data, analytics, Machine Learning, and modern technology stacks. The engineer will be responsible for driving the vision, direction, and strategy of Mailchimp's data technologies, aligning them with business outcomes and customer needs. They will also collaborate with Intuit data technology leadership to achieve Mailchimp's goals and represent Mailchimp's data capabilities internally and externally.",
        "industries": [
            "Marketing",
            "Technology",
            "Software",
            "Data",
            "Analytics",
            "Machine Learning",
            "Fintech",
            "MarTech"
        ],
        "soft_skills": [
            "Leadership",
            "Communication",
            "Influence",
            "Strategy",
            "Collaboration",
            "Problem Solving",
            "Creativity",
            "Ambidextrous",
            "Presentation",
            "Visionary",
            "Talent Stewardship",
            "Diversity and Inclusion"
        ],
        "hard_skills": [
            "Cloud/SaaS",
            "Big Data",
            "Analytics",
            "Machine Learning",
            "API Strategies",
            "Microservices",
            "Public Cloud",
            "Agile Development",
            "SCRUM",
            "Extreme Programming"
        ],
        "tech_stack": [
            "Customer Data Platform (CDP)",
            "AI/ML Models",
            "Modern Technology Stacks",
            "Data Architecture",
            "Data Strategy"
        ],
        "programming_languages": [],
        "experience": 10,
        "education": {
            "min_degree": "BS/MS",
            "fields": [
                "Computer Science"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Costa Mesa, CA",
        "job_id": 3946912457,
        "company": "Mantek Solutions Inc.",
        "title": "Senior Cloud Data Platform Engineer",
        "created_on": 1720635564.4351792,
        "description": "Sr Cloud Data Platform Engineer - Hybrid Remote, Costa Mesa, CA If you are passionate about making an impact utilizing cutting-edge data technology and eager to learn and grow with talented data & analytics teams, we believe you would be a great fit and help in innovation and solving challenges. Responsibilities Design, deploy, and maintain scalable and reliable cloud infrastructure on AWS and GCP. Build scalability, fault-tolerance, security, and performance into our data platforms to meet our growing data & analytics needs Configure and optimize networking components, to ensure high performance, availability, and security. Work with our data engineering teams, data science teams, and analytical business users to review and ensure best practices are followed and high-quality code gets implemented Partner with our Information Security, Application Security, Data Center services, and Cloud Infrastructure Services teams to ensure data is secure both at rest and in-flight Ensure data platforms remain current to take advantage of the latest features and support Communicate the status of assigned work to management and follow agile practices, standard procedures & policies Seek guidance when direction is needed and speak up about technology risks identified You have worked previously with an Agile team or understand these concepts. You expect to participate in daily standup meetings, you’ll complete your projects or stories during our sprints, and you’ll be ready to meet frequent deployment deadlines. Requirements Extensive hands-on expertise in troubleshooting logs and optimizing system performance. Deep understanding of Cloud and data security in AWS or Google Cloud Good experience with networking, Linux and Unix OS, and shell scripting. Demonstrated aptitude for identifying the underlying issues in Cloud services such as Glue, Lambda, Big Query, and Cloud Functions Good experience with Cloud platforms AWS and/or Google Cloud in supporting Data pipelines, and integrations Experience with Cloud Automation using Cloud Formation or Terraform Good understanding of network infrastructures and protocols such as TCP/IP, TLS Familiarity with data integration from different sources into Big Data systems is preferable. Familiarity with SQL and programming languages such as Python, Spark, YAML, and Bash. Cloud Certification in Dev Ops in AWS or Google Cloud a plus If qualified and interested in this opportunity, please apply with an updated resume and annual salary requirements. No Corp to Corp / No Sponsorship / W2 Only / No third party candidates considered for this position. JPC-6945",
        "url": "https://www.linkedin.com/jobs/view/3946912457",
        "summary": "This is a job for a Senior Cloud Data Platform Engineer who will design, deploy, and maintain scalable and reliable cloud infrastructure on AWS and GCP. The role involves building data platforms, configuring networking components, ensuring data security, working with data engineering and science teams, and keeping data platforms up to date. The ideal candidate has extensive hands-on expertise in troubleshooting logs and optimizing system performance, a deep understanding of cloud and data security in AWS or Google Cloud, and experience with networking, Linux/Unix OS, shell scripting, and Cloud platforms like AWS and Google Cloud. Familiarity with data integration, SQL, and programming languages like Python, Spark, YAML, and Bash is preferred. Cloud Certification in Dev Ops in AWS or Google Cloud is a plus.",
        "industries": [
            "Information Technology",
            "Data Analytics",
            "Software Development",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Communication",
            "Problem Solving",
            "Teamwork",
            "Collaboration",
            "Time Management",
            "Organizational Skills",
            "Attention to Detail",
            "Self-Motivation",
            "Adaptability",
            "Leadership"
        ],
        "hard_skills": [
            "Troubleshooting",
            "System Performance Optimization",
            "Cloud Security",
            "AWS",
            "Google Cloud",
            "Networking",
            "Linux",
            "Unix",
            "Shell Scripting",
            "Glue",
            "Lambda",
            "Big Query",
            "Cloud Functions",
            "Data Pipelines",
            "Cloud Automation",
            "Cloud Formation",
            "Terraform",
            "TCP/IP",
            "TLS",
            "Data Integration",
            "SQL",
            "Python",
            "Spark",
            "YAML",
            "Bash",
            "DevOps"
        ],
        "tech_stack": [
            "AWS",
            "GCP",
            "Glue",
            "Lambda",
            "Big Query",
            "Cloud Functions",
            "Cloud Formation",
            "Terraform",
            "TCP/IP",
            "TLS",
            "SQL",
            "Python",
            "Spark",
            "YAML",
            "Bash"
        ],
        "programming_languages": [
            "Python",
            "Spark",
            "YAML",
            "Bash"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3897978610,
        "company": "Unreal Staffing, Inc",
        "title": "Senior Cloud Data Engineer",
        "created_on": 1720635565.989893,
        "description": "Company Overview: Welcome to the forefront of data-driven innovation! Our company is dedicated to harnessing the power of data to drive transformative change and solve complex problems across industries. We're committed to building cutting-edge cloud data solutions that leverage the scalability, flexibility, and power of cloud platforms. Join us and be part of a dynamic team shaping the future of cloud data engineering. Position Overview: As a Senior Cloud Data Engineer, you'll play a crucial role in designing, building, and optimizing our cloud data solutions. You'll work on challenging projects, from data ingestion and storage to processing and analysis, to support the needs of our data-driven organization. If you're a seasoned engineer with expertise in cloud data technologies and a passion for building scalable and reliable data systems, we want you on our team. Requirements Key Responsibilities: Cloud Data Architecture: Design and implement cloud data architectures on platforms such as AWS, Azure, or Google Cloud Platform (GCP), leveraging cloud-native services to build scalable, reliable, and cost-effective data solutions Data Ingestion: Develop and maintain data ingestion pipelines to collect data from various sources, including databases, APIs, files, and streaming sources, ensuring efficient and reliable data ingestion into cloud storage Data Storage: Design and implement data storage solutions using cloud-native storage services such as Amazon S3, Azure Data Lake Storage, or Google Cloud Storage, optimizing data storage for performance, cost, and scalability Data Processing: Develop and maintain data processing pipelines using cloud-native data processing frameworks such as Apache Spark, Apache Flink, or Google Dataflow, enabling real-time and batch data processing and analysis Data Integration: Integrate data from diverse sources and systems into cloud data solutions, ensuring seamless data flow and interoperability across different data sources and formats Data Governance: Implement data governance policies and procedures to ensure data quality, security, and compliance with regulatory requirements in cloud data environments Performance Optimization: Optimize data pipelines and processing workflows for performance and scalability, leveraging cloud-native features and techniques to maximize efficiency and minimize costs Monitoring and Alerting: Implement monitoring and alerting solutions to track data pipeline performance and health, proactively identifying and resolving issues to minimize downtime and data loss Documentation and Best Practices: Document cloud data architectures, pipelines, and best practices, providing clear and comprehensive documentation to facilitate understanding and collaboration among team members Collaboration: Collaborate with cross-functional teams, including data engineers, data scientists, and business analysts, to understand requirements and deliver cloud data solutions that meet business needs Mentorship and Development: Mentor junior engineers, sharing expertise and best practices in cloud data engineering, and facilitate knowledge sharing sessions within the team Qualifications: Bachelor's degree or higher in Computer Science, Engineering, Mathematics, or related field 5+ years of experience in data engineering, with a focus on cloud data technologies Proficiency in cloud platforms such as AWS, Azure, or GCP, and services like Amazon S3, Azure Data Lake Storage, or Google Cloud Storage Strong programming skills in languages such as Python, Java, or Scala, with experience in data processing frameworks like Apache Spark or Apache Flink Experience with cloud-native data processing services such as AWS Glue, Azure Data Factory, or Google Dataflow Strong understanding of data integration concepts and techniques, with experience integrating data from diverse sources and systems in cloud environments Strong problem-solving skills and analytical thinking, with the ability to design and troubleshoot complex cloud data solutions Excellent communication and collaboration skills, with the ability to work effectively in cross-functional teams and communicate technical concepts to non-technical stakeholders Benefits Competitive salary: The industry standard salary for Senior Cloud Data Engineers typically ranges from $170,000 to $230,000 per year, depending on experience and qualifications Comprehensive health, dental, and vision insurance plans Flexible work hours and remote work options Generous vacation and paid time off Professional development opportunities, including access to training programs, conferences, and workshops State-of-the-art technology environment with access to cutting-edge tools and resources Vibrant and inclusive company culture with opportunities for growth and advancement Exciting projects with real-world impact at the forefront of data-driven innovation Join Us: Ready to shape the future of cloud data engineering? Apply now to join our team and be part of the data revolution!",
        "url": "https://www.linkedin.com/jobs/view/3897978610",
        "summary": "We are seeking a Senior Cloud Data Engineer with 5+ years of experience to design, build, and optimize cloud data solutions on platforms like AWS, Azure, or GCP. You will be responsible for data ingestion, storage, processing, integration, governance, performance optimization, monitoring, and documentation. Strong programming skills in Python, Java, or Scala, and experience with cloud-native data processing services are required.",
        "industries": [
            "Cloud Computing",
            "Data Engineering",
            "Software Development",
            "Technology"
        ],
        "soft_skills": [
            "Problem-solving",
            "Analytical Thinking",
            "Communication",
            "Collaboration",
            "Mentorship"
        ],
        "hard_skills": [
            "AWS",
            "Azure",
            "GCP",
            "Amazon S3",
            "Azure Data Lake Storage",
            "Google Cloud Storage",
            "Python",
            "Java",
            "Scala",
            "Apache Spark",
            "Apache Flink",
            "AWS Glue",
            "Azure Data Factory",
            "Google Dataflow",
            "Data Integration",
            "Data Governance",
            "Performance Optimization",
            "Monitoring",
            "Documentation"
        ],
        "tech_stack": [
            "AWS",
            "Azure",
            "GCP",
            "Amazon S3",
            "Azure Data Lake Storage",
            "Google Cloud Storage",
            "Apache Spark",
            "Apache Flink",
            "AWS Glue",
            "Azure Data Factory",
            "Google Dataflow"
        ],
        "programming_languages": [
            "Python",
            "Java",
            "Scala"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Computer Science",
                "Engineering",
                "Mathematics"
            ]
        },
        "salary": {
            "max": 230000,
            "min": 170000
        },
        "benefits": [
            "Competitive salary",
            "Health, dental, and vision insurance",
            "Flexible work hours",
            "Remote work options",
            "Vacation and paid time off",
            "Professional development opportunities",
            "State-of-the-art technology environment",
            "Vibrant and inclusive company culture",
            "Growth and advancement opportunities"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Diego, CA",
        "job_id": 3954888068,
        "company": "San Diego Community Power",
        "title": "Data Engineer",
        "created_on": 1720635567.7423134,
        "description": "About the role : The San Diego Community Power (SDCP) is seeking a seasoned Data Engineer to join our growing team of analytics experts who will be responsible for designing, maintaining, expanding, and optimizing our data infrastructure for data collection, management, transformation, and access. A key priority of this role will be to assist in SDCP’s development of centralizing its data eco-system to allow for creation of pipelines that convert raw data into usable formats for data analysts and other data consumers to utilize. The Data Engineer will handle the core data aspects of software engineering and data science and utilize software engineering principles to develop algorithms that automate the data flow process. They will collaborate with data and system analysts to build machine learning and analytics infrastructure from testing to deployment. WHO IS SAN DIEGO COMMUNITY POWER? San Diego Community Power is a community-owned organization that provides affordable clean energy and invests in the community to create an equitable and sustainable future for the San Diego region. We aim to be a global leader, inspiring innovative solutions to climate change by powering our communities with 100% clean affordable energy while prioritizing equity, sustainability, and high-quality jobs. We are a values-led, mission driven organization grounded in Justice/Equity/Diversity/Inclusion (JEDI), Impact, Integrity, Innovation, Servant Leadership, and Togetherness. Our culture is built on open communication, accountability, and curiosity. We are a growing team whose key premise is trust, collaboration, and connection with each other and the communities we serve. We are responsive and work smart to achieve high goals. ESSENTIAL DUTIES AND PRIMARY RESPONSIBILITES Building and maintaining a centralized cloud-based data infrastructure for optimal extraction, transformation, and loading of data from a wide variety of sources. Assemble large, complex data sets that meet functional/non-functional business requirements. Developing data tools and APIs for data analysis. Deploying and monitoring machine learning algorithms and statistical methods in production environments to solve organizational needs. Ensuring data accessibility and security and implementing company data policies regarding data privacy and confidentiality. Improving data systems reliability, speed, and performance. Build analytics tools that utilize the data pipeline to provide actionable insights into customer trends, operational efficiency, and other key business performance metrics. Collaborating with other internal teams, data analysts, and other stakeholders to understand and optimize how data can be leveraged to meet business needs. Assist in drafting data and analytics’ solicitations and in the selection of consultants. Assist with drafting staff reports and presentations for Board and Committee meetings. Performs other related duties and responsibilities as required. MINIMUM KNOWLEDGE, SKILLS, AND ABILITES Advance working knowledge of SQL, experience working with relational databases, query authoring (SQL) and working familiarity with various databases. Demonstrated experience in developing API and integrations to support analytics in cloud environment. Experience building and optimizing ‘big data’ data pipelines, architectures, and data sets. Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. Strong analytic skills related to working with unstructured datasets. Build processes supporting data transformation, data structures, metadata, dependency, and workload management. A successful history of manipulating, processing, and extracting value from large, disconnected datasets. Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores. Good understanding of data architecture, data design, data warehousing and data modeling concepts. Strong project management and organizational skills. Experience supporting and working with cross-functional teams in a dynamic environment. PREFERRED KNOWLEDGE, SKILLS, AND ABILITES Experience in the energy sector and/or supporting the implementation of programs funded by California state agencies (e.g., California Public Utilities Commission, California Energy Commission, California Air Resources Board). Ability to strongly represent the organization in various professional engagement settings from local to national convenings (conferences, workshops, executive meetings, etc.). PREFERRED QUALIFICATIONS, EDUCATION AND EXPERIENCE Minimum of five (5) years of professional experience in a Data Engineer role, preferably in the energy industry with a graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. The candidate should also have experience using the following software/tools: Experience with big data tools: Hadoop, Spark, PowerBI, Kafka, etc. Experience with AWS, Azure and Google Cloud services. Experience with relational SQL and NoSQL databases. Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc. Experience with open-source technologies: Python, Pytorch, Flask, Tensorflow or Keras Experience with stream-processing systems: Storm, Spark-Streaming, etc. Experience with object-oriented/object function scripting languages: Java, C++ etc. Experience with batch, micro-batching and real-time data ingestion methodologies. Experience with GitHub or similar code repositories COMPENSATION: Salary Range: The position salary range is: $110,400 to $135,800; with exact compensation to be determined by SDCP, depending upon experience. Benefits : Standard benefits package including but not limited to: Insurance : SDCP offers group health benefits, including medical, vision, and dental insurance, for eligible FT employees. Also provided is a $100,000 Life & AD&D policy, STD and LTD coverage that is 100% paid by SDCP. Retirement : SDCP offers a 457(b) plan for employee contributions and contributes 10% of eligible compensation to the employee’s Money Purchase Plan. Paid Time Off : 11 holidays per year + paid winter holiday (between 12/24-12/31), 160 hours of accrued paid time off per year (increases with time in service), and 96 hours per year of accrued paid sick leave. This job description may not be inclusive of all assigned duties, responsibilities, or aspects of the job described, and may be amended at the discretion of SDCP as needed.",
        "url": "https://www.linkedin.com/jobs/view/3954888068",
        "summary": "San Diego Community Power is hiring a Data Engineer to design, maintain, and optimize their data infrastructure. The role involves building a centralized cloud-based data infrastructure, developing data tools and APIs, deploying machine learning algorithms, ensuring data security, and collaborating with data analysts to leverage data for business insights.",
        "industries": [
            "Energy",
            "Clean Energy",
            "Sustainability",
            "Data Analytics",
            "Machine Learning"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Project Management",
            "Organizational Skills",
            "Problem Solving",
            "Analytical Skills"
        ],
        "hard_skills": [
            "SQL",
            "Relational Databases",
            "API Development",
            "Big Data Pipelines",
            "Data Architecture",
            "Data Warehousing",
            "Data Modeling",
            "Root Cause Analysis",
            "Message Queuing",
            "Stream Processing",
            "Data Transformation",
            "Data Structures",
            "Metadata Management",
            "Workload Management",
            "Python",
            "Pytorch",
            "Flask",
            "Tensorflow",
            "Keras",
            "Java",
            "C++",
            "GitHub"
        ],
        "tech_stack": [
            "Hadoop",
            "Spark",
            "PowerBI",
            "Kafka",
            "AWS",
            "Azure",
            "Google Cloud",
            "Azkaban",
            "Luigi",
            "Airflow",
            "Storm",
            "Spark-Streaming",
            "Python",
            "Pytorch",
            "Flask",
            "Tensorflow",
            "Keras",
            "Java",
            "C++",
            "GitHub"
        ],
        "programming_languages": [
            "SQL",
            "Python",
            "Java",
            "C++"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Graduate Degree",
            "fields": [
                "Computer Science",
                "Statistics",
                "Informatics",
                "Information Systems",
                "Quantitative Fields"
            ]
        },
        "salary": {
            "max": 135800,
            "min": 110400
        },
        "benefits": [
            "Group Health Insurance",
            "Vision Insurance",
            "Dental Insurance",
            "Life Insurance",
            "Short-Term Disability",
            "Long-Term Disability",
            "457(b) Plan",
            "Money Purchase Plan",
            "Paid Time Off",
            "Paid Sick Leave",
            "Holidays"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Irvine, CA",
        "job_id": 3919786437,
        "company": "Open Systems Inc.",
        "title": "Software Engineer 1",
        "created_on": 1720635569.318105,
        "description": "Summary: The main function of a software engineer is to apply the principles of computer science and mathematical analysis to the design, development, testing, and evaluation of the software and systems that make computers work. A typical software engineer researches, designs, develops and tests operating systems-level software, compilers, and network distribution software for medical, industrial, military, communications, aerospace, business, scientific and general computing applications. Job Responsibilities: Modify existing databases and database management systems. Write and code logical and physical database descriptions and specify identifiers of database to management system or direct others in coding descriptions. Work as part of a project team to coordinate database development and determine project scope and limitations. Review project requests describing database user needs to estimate time and cost required to accomplish project. Skills: Verbal and written communication skills, problem solving skills, customer service and interpersonal skills. Basic ability to work independently and manage ones time. Basic knowledge of the full software development lifecycle: from business/systems analysis, through requirements gathering and functional specification authoring, to development, testing and delivery. Basic ability to troubleshoot issues and make system changes as needed to resolve issue. Basic knowledge of computer hardware and software. Knowledge of computer development software as it relates to systems, such as SQL, Visual Basic, etc. Education/Experience: Bachelor's degree in computer science, software engineering or relevant field required. 0-2 years experience required. Who We Are Open Systems Inc. (OSI) was founded in 1994 to provide information technology solutions and staffing services to large and mid-size companies across the U.S. Our corporate office is located at 6495 Shiloh Road, Ste 310 Alpharetta, GA 30005. We provide a full range of staffing services including contract, contract-to-hire, and direct hire solutions. Our technical recruiting experts are experienced in technical screening, candidate sourcing, and behavioral interviewing techniques. They focus on providing candidates who match your technical requirements and fit seamlessly into your company culture. Contact Open Systems, Inc. anytime by website, phone or email. We look forward to hearing from you!!",
        "url": "https://www.linkedin.com/jobs/view/3919786437",
        "summary": "Software engineer responsible for designing, developing, testing, and evaluating software and systems. This role involves working on operating systems, compilers, and network distribution software for various industries.",
        "industries": [
            "Medical",
            "Industrial",
            "Military",
            "Communications",
            "Aerospace",
            "Business",
            "Scientific",
            "General Computing"
        ],
        "soft_skills": [
            "Verbal and written communication",
            "Problem solving",
            "Customer service",
            "Interpersonal skills",
            "Time management"
        ],
        "hard_skills": [
            "SQL",
            "Visual Basic",
            "Database design",
            "Database management systems",
            "Software development lifecycle",
            "Troubleshooting",
            "Computer hardware and software"
        ],
        "tech_stack": [
            "SQL",
            "Visual Basic"
        ],
        "programming_languages": [
            "SQL",
            "Visual Basic"
        ],
        "experience": 2,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Computer science",
                "Software engineering",
                "Relevant field"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Sunnyvale, CA",
        "job_id": 3939147014,
        "company": "Walmart Data Ventures",
        "title": "Data Engineer III - Data Ventures",
        "created_on": 1720635570.9523494,
        "description": "Position Summary... What you'll do... Do you have boundless energy and passion for engineering data used to solve dynamic problems that will shape the future of retail? With the sheer scale of Walmart's environment comes the biggest of big data sets. As a Walmart Data Engineer, you will dig into our mammoth scale of data to help unleash the power of retail data science by imagining, developing, and maintaining data pipelines that our Data Scientists and Analysts can rely on. You will be responsible for contributing to an orchestration layer of complex data transformations, refining raw data from source into targeted, valuable data assets for consumption in a governed way. You will partner with Data Scientists, Analysts, other engineers, and business stakeholders to solve complex and exciting challenges so that we can build out capabilities that evolve the retail business model while making a positive impact on our customers' lives. MUST HAVE TO BE CONSIDERED: Scala or Python, Hadoop, Spark, Python or PySpark, GCP or Azure, Java, Kafka, Automic or Airflow About Team: Data Ventures Our team creates reusable technologies to help with customer acquisition, onboarding, and empowering merchants, while ensuring a seamless experience for both stakeholders. We also optimize tariffs and assortment in accordance with Walmart's Everyday Low-Cost philosophy. We not only create affordability, but we also deliver customized experiences for customers across all channels - in-store, mobile app, and websites. Our team is responsible for providing support to US Marketplace sellers. We focus on providing immediate solutions to the cases/tickets created by sellers. We interact with multiple teams across the company to provide excellent seller experience. What You'll Do Data Transformation and Integration: Extracts data from identified databases. Creates data pipelines and transform data to a structure that is relevant to the problem by selecting appropriate techniques. Develops knowledge of current analytics trends. Data Source Identification: Supports the understanding of the priority order of requirements and service level agreements. Helps identify the most suitable source for data that is fit for purpose. Performs initial data quality checks on extracted data. Data Modeling: Analyses complex data elements, systems, data flows, dependencies, and relationships to contribute to conceptual, physical, and logical data models. Develops the Logical Data Model and Physical Data Models including data warehouse and data mart designs. Defines relational tables, primary and foreign keys, and stored procedures to create a data model structure. Evaluates existing data models and physical databases for variances and discrepancies. Develops efficient data flows. Analyses data-related system integration challenges and proposes appropriate solutions. Code Development and Testing: Writes code to develop the required solution and application features by determining the appropriate programming language and leveraging business, technical and data requirements. Creates test cases to review and validate the proposed solution design. Creates proofs of concept. Tests the code using the appropriate testing approach. Deploys software to production servers. Contributes code documentation, maintains playbook, and provides timely progress updates. Applied Business Acumen: Provides recommendations to business stakeholders to solve complex business issues. Develops business cases for projects with a projected return on investment or cost savings. Translates business requirements into projects, activities, and tasks and aligns to overall business strategy. Serves as an interpreter and conduit to connect business needs with tangible solutions and results. Recommends new processes and ways of working. Data Governance: Establishes, modifies, and documents data governance projects and recommendations. Implements data governance practices in partnership with business stakeholders and peers. Interprets company and regulatory policies on data. Educates others on data governance processes, practices, policies, and guidelines. Provides recommendations on needed updates or inputs into data governance policies, practices, or guidelines. Demonstrates up-to-date expertise and applies this to the development, execution, and improvement of action plans by providing expert advice and guidance to others. Supporting and aligning efforts to meet customer and business needs and building commitment for perspectives and rationales. Provides and supports the implementation of business solutions by building relationships and partnerships with key stakeholders. Identifying business needs, determining, and carrying out necessary processes and practices. Promotes and supports company policies, procedures, mission, values, and standards of ethics and integrity by training and providing direction to others in their use and application, ensuring compliance with them. Ensures business needs are being met by evaluating the ongoing effectiveness of current plans, programs, and initiatives. Applying suggestions for improving efficiency and cost effectiveness; and participating in and supporting community outreach events. Creates training documentation and trains end-users on data modeling. Oversees the tasks of less experienced programmers and stipulates system troubleshooting supports. What You'll Bring Must Have Well versed with Hadoop, Spark, Cloud, Python/PySpark and Java, Streaming, Kafka, Backend. You have a proven track record coding with at least one programming language (e.g., Scala, Python) You're experienced in one of cloud computing platforms (e.g., GCP, Azure) You're skilled in data modeling & data migration protocols. Experience with GCP, Data warehousing, BI preferred. Experience with the integration tools like Automic, Airflow Experience in building highly scalable Big Data solutions and ETL ecosystems. Nice to have Knowledge of Databricks is an added advantage. Hands on knowledge in NoSQL like Cosmos DB along with RDBMS like MySQL, Postgres is plus. Hands on working experience in any messaging platform like Kafka is preferred. Increase the efficiency of the team by setting right Processes of Software Development, Requirement Intake, Effort Estimation Demonstrating creative, critical thinking & troubleshooting skills. About Walmart Global Tech Imagine working in an environment where one line of code can make life easier for hundreds of millions of people. That's what we do at Walmart Global Tech. We're a team of software engineers, data scientists, cybersecurity expert's and service professionals within the world's leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered. We train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail. Flexible, Hybrid Work We use a hybrid way of working that is primarily in office coupled with virtual when not onsite. Our campuses serve as a hub to enhance collaboration, bring us together for purpose and deliver on business needs. This approach helps us make quicker decisions, remove location barriers across our global team and be more flexible in our personal lives. Benefits Benefits: Beyond our great compensation package, you can receive incentive awards for your performance. Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and much more. Equal Opportunity Employer Walmart, Inc. is an Equal Opportunity Employer - By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing diversity- unique styles, experiences, identities, ideas and opinions - while being inclusive of all people. At Walmart, we offer competitive pay as well as performance-based bonus awards and other great benefits for a happier mind, body, and wallet. Health benefits include medical, vision and dental coverage. Financial benefits include 401(k), stock purchase and company-paid life insurance. Paid time off benefits include PTO (including sick leave), parental leave, family care leave, bereavement, jury duty, and voting. Other benefits include short-term and long-term disability, company discounts, Military Leave Pay, adoption and surrogacy expense reimbursement, and more. ‎ ‎ ‎ You will also receive PTO and/or PPTO that can be used for vacation, sick leave, holidays, or other purposes. The amount you receive depends on your job classification and length of employment. It will meet or exceed the requirements of paid sick leave laws, where applicable. ‎ For information about PTO, see https://one.walmart.com/notices . ‎ ‎ Live Better U is a Walmart-paid education benefit program for full-time and part-time associates in Walmart and Sam's Club facilities. Programs range from high school completion to bachelor's degrees, including English Language Learning and short-form certificates. Tuition, books, and fees are completely paid for by Walmart. ‎ Eligibility requirements apply to some benefits and may depend on your job classification and length of employment. Benefits are subject to change and may be subject to a specific plan or program terms. ‎ For Information About Benefits And Eligibility, See One.Walmart . ‎ Sunnyvale, California US-08479:The annual salary range for this position is $117,000.00-$234,000.00 ‎ Bentonville, Arkansas US-09050:The annual salary range for this position is $90,000.00-$180,000.00 ‎ ‎ ‎ ‎ ‎ ‎ ‎ ‎ ‎ ‎ Additional Compensation Includes Annual Or Quarterly Performance Bonuses. ‎ Additional Compensation For Certain Positions May Also Include ‎ ‎ Stock ‎ ‎ Minimum Qualifications... Outlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications. Option 1: Bachelor's degree in Computer Science and 2 years' experience in software engineering or related field. Option 2: 4 years' experience in software engineering or related field. Option 3: Master's degree in Computer Science. Preferred Qualifications... Outlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications. Data engineering, database engineering, business intelligence, or business analytics, Master's degree in Computer Science or related field and 2 years' experience in software engineering or related field, We value candidates with a background in creating inclusive digital experiences, demonstrating knowledge in implementing Web Content Accessibility Guidelines (WCAG) 2.2 AA standards, assistive technologies, and integrating digital accessibility seamlessly. The ideal candidate would have knowledge of accessibility best practices and join us as we continue to create accessible products and services following Walmart's accessibility standards and guidelines for supporting an inclusive culture. Primary Location... 640 W California Avenue, Sunnyvale, CA 94086-4828, United States of America",
        "url": "https://www.linkedin.com/jobs/view/3939147014",
        "summary": "Walmart Data Engineer is responsible for developing and maintaining data pipelines for retail data science. This role involves extracting, transforming, and integrating data from various sources, developing data models, and building scalable big data solutions. The engineer will work closely with data scientists, analysts, and business stakeholders to solve complex challenges and enhance retail business models.",
        "industries": [
            "Retail",
            "Data Science",
            "E-commerce",
            "Technology"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Analytical Thinking",
            "Critical Thinking",
            "Troubleshooting",
            "Creativity",
            "Teamwork",
            "Leadership",
            "Decision Making",
            "Time Management"
        ],
        "hard_skills": [
            "Scala",
            "Python",
            "Hadoop",
            "Spark",
            "PySpark",
            "GCP",
            "Azure",
            "Java",
            "Kafka",
            "Automic",
            "Airflow",
            "Data Modeling",
            "Data Migration",
            "Data Warehousing",
            "BI",
            "ETL",
            "Databricks",
            "NoSQL",
            "Cosmos DB",
            "RDBMS",
            "MySQL",
            "Postgres",
            "Messaging Platforms"
        ],
        "tech_stack": [
            "Hadoop",
            "Spark",
            "Cloud",
            "Python/PySpark",
            "Java",
            "Streaming",
            "Kafka",
            "Backend",
            "GCP",
            "Azure",
            "Automic",
            "Airflow",
            "Databricks",
            "Cosmos DB",
            "MySQL",
            "Postgres"
        ],
        "programming_languages": [
            "Scala",
            "Python",
            "Java"
        ],
        "experience": 2,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Computer Science",
                "Software Engineering",
                "Related Field"
            ]
        },
        "salary": {
            "max": 234000,
            "min": 90000
        },
        "benefits": [
            "401(k) match",
            "Stock purchase plan",
            "Paid maternity and parental leave",
            "PTO",
            "Health plans",
            "Performance-based bonus awards",
            "Company-paid life insurance",
            "Short-term and long-term disability",
            "Company discounts",
            "Military Leave Pay",
            "Adoption and surrogacy expense reimbursement",
            "Live Better U education benefit program"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Sunnyvale, CA",
        "job_id": 3910743791,
        "company": "Walmart",
        "title": "(USA) Staff, Data Engineer",
        "created_on": 1720635572.5677836,
        "description": "Position Summary... What you'll do... For R-1719400 (USA) Staff, Data Engineer As a \"Staff Data Engineer\", you should be able to technically help and assist team to steer through correct technical directions following the best practices. You will have deeper understanding of Data Engineering approaches along with hands on experience in building highly scalable solutions. About Team: Data Ventures Our team creates reusable technologies to help with customer acquisition, onboarding, and empowering merchants, while ensuring a seamless experience for both stakeholders. We also optimize tariffs and assortment in accordance with Walmart's Everyday Low-Cost philosophy. We not only create affordability, but we also deliver customized experiences for customers across all channels - in-store, mobile app, and websites. Our team is responsible for providing support to US Marketplace sellers. We focus on providing immediate solutions to the cases/tickets created by sellers. We interact with multiple teams across the company to provide excellent seller experience. What You'll Do You will lead the work of other small groups of ten to twelve engineers, including offshore associates, for assigned Engineering projects by providing pertinent. documents, direction, and examples; identifying short- and long- term solutions and timeline; reviewing and approving proposed solutions. You will drive the execution of multiple business plans and projects by identifying customer and operational needs, developing, and communicating business. Leads and participates in medium- to large-scale, complex, cross-functional projects by reviewing project requirements, translating requirements into technical solutions; gathering requested information (for example, design documents, product requirements, wire frames); writing and developing. code; conducting unit testing; communicating status and issues to team members and stakeholders; collaborating with project team and cross. functional teams; troubleshooting open issues and bug-fixes; enhancing design to prevent re-occurrences of defects; ensuring on-time delivery and hand-offs: interacting with project manager to provide input on project plan; and providing leadership to the project team. plans and priorities; removing barriers and obstacles that impact performance; providing resources; identifying performance standards; measuring. progress and adjusting performance; accordingly, developing contingency plans; and demonstrating adaptability and supporting continuous learning. Implementing new architectural patterns; and performing design and code reviews of changes. Promotes and supports company policies, procedures, mission, values, and standards of ethics and integrity by training and providing direction to others in their use and application; ensuring compliance with them; and utilizing and supporting the Open Door Policy. Ensures business needs are being met by evaluating the ongoing effectiveness of current plans, programs, and initiatives, consulting with business. partners, managers, co-workers, or other key stakeholders; soliciting, evaluating, and applying suggestions for improving efficiency and cost effectiveness. and participating in and supporting community outreach events. Must have Seasoned Big Data Engineer with 8-12 years of Industry experience Strong hands-on experience with Hadoop, Hive, Apache Spark . Experience in building highly scalable Big Data solutions and ETL ecosystems. Proven track record coding with at least one programming language e.g., Scala (preferred), Python. Expert in cloud computing platforms and offerings from Google Cloud Platform (GCP) (preferred), Microsoft Azure. Evangelize an extremely high standard of code quality, system reliability, and performance. Experience with the integration tools like Automic, Airflow Skilled in data modeling & data migration protocols Working knowledge of CI/CD pipelines. Ability to write designs for data architecture of data warehouse or data lake solutions or end to end pipelines. Expert in data architecture principles, distributed computing Intake prioritization, cost/benefit analysis, decision making of what to pursue across a wide base of users/stakeholders and across products, databases, and services, Should be able to communicate complex technical solutions and ideas to both technical and non-technical team members. Ability to lead our technical relationship with partners and mentor senior software developers in multiple initiatives. Nice To Have Knowledge of Databricks, Snowflake is an added advantage. Experience with ThoughtSpot, Druid, Big Query and ClickHouse is added advantage. Hands on knowledge in NoSQL like Cosmos DB along with RDBMS like MySQL, Postgres is plus. Hands on working experience in any messaging platform like Kafka is preferred. Increase the efficiency of the team by setting right Processes of Software Development, Requirement Intake, Effort Estimation Demonstrating creative, critical thinking & troubleshooting skills. Drives the execution of multiple business plans and projects by identifying customer and operational needs; developing and communicating business plans and priorities; removing barriers and obstacles that impact performance; providing resources; identifying performance standards; measuring progress and adjusting performance accordingly; developing contingency plans; and demonstrating adaptability and supporting continuous learning. Provides supervision and development opportunities for associates by selecting and training; mentoring; assigning duties; building a team-based work environment; establishing performance expectations and conducting regular performance evaluations; providing recognition and rewards; coaching for success and improvement; and ensuring diversity awareness. Promotes and supports company policies, procedures, mission, values, and standards of ethics and integrity by training and providing direction to others in their use and application; ensuring compliance with them; and utilizing and supporting the Open Door Policy. Ensures business needs are being met by evaluating the ongoing effectiveness of current plans, programs, and initiatives; consulting with business partners, managers, co-workers, or other key stakeholders; soliciting, evaluating, and applying suggestions for improving efficiency and cost-effectiveness; and participating in and supporting community outreach events. Live our ValuesCulture Champion Models the Walmart values to foster our culture; holds oneself and others accountable; and supports Walmart's commitment to communities, social justice, corporate social responsibility, and sustainability; maintains and promotes the highest standards of integrity, ethics and compliance.Servant Leadership Acts as an altruistic servant leader and is consistently humble, self-aware, honest, and transparent.Embrace ChangeCuriosity & Courage Demonstrates curiosity and a growth mindset; fosters an environment that supports learning, innovation, and intelligent risk-taking; and exhibits resilience in the face of setbacks.Digital Transformation & Change Seeks and implements continuous improvements and encourages the team to leverage new digital tools and ways of working.Deliver for the CustomerCustomer Focus Delivers expected business results while putting the customer first and consistently applying an omni-merchant mindset and the EDLP and EDLC business models to all plans.Strategic Thinking Adopts a holistic perspective that considers data, analytics, customer insights, and different parts of the business when making plans and shaping the team's strategy.Focus on our AssociatesDiversity, Equity & Inclusion Identifies, attracts, and retains diverse and inclusive team members; builds a high-performing team; embraces diversity in all its forms; and actively supports diversity goal programs.Collaboration & Influence Builds strong and trusting relationships with team members and business partners; works collaboratively and cross-functionally to achieve objectives; and communicates with energy and positivity to motivate, influence, and inspire commitment and action.Talent Management Creates a discipline and focus around developing talent, promotes an environment allowing everyone to bring their best selves to work, empowers associates and partners to act in the best interest of the customer and company, and regularly recognizes others' contributions and accomplishments. At Walmart, we offer competitive pay as well as performance-based bonus awards and other great benefits for a happier mind, body, and wallet. Health benefits include medical, vision and dental coverage. Financial benefits include 401(k), stock purchase and company-paid life insurance. Paid time off benefits include PTO (including sick leave), parental leave, family care leave, bereavement, jury duty, and voting. Other benefits include short-term and long-term disability, company discounts, Military Leave Pay, adoption and surrogacy expense reimbursement, and more. ‎ ‎ ‎ You will also receive PTO and/or PPTO that can be used for vacation, sick leave, holidays, or other purposes. The amount you receive depends on your job classification and length of employment. It will meet or exceed the requirements of paid sick leave laws, where applicable. ‎ For information about PTO, see https://one.walmart.com/notices . ‎ ‎ Live Better U is a Walmart-paid education benefit program for full-time and part-time associates in Walmart and Sam's Club facilities. Programs range from high school completion to bachelor's degrees, including English Language Learning and short-form certificates. Tuition, books, and fees are completely paid for by Walmart. ‎ Eligibility requirements apply to some benefits and may depend on your job classification and length of employment. Benefits are subject to change and may be subject to a specific plan or program terms. ‎ For Information About Benefits And Eligibility, See One.Walmart . ‎ The annual salary range for this position is $143,000.00-$286,000.00 ‎ Additional Compensation Includes Annual Or Quarterly Performance Bonuses. ‎ Additional Compensation For Certain Positions May Also Include ‎ ‎ Stock ‎ ‎ Minimum Qualifications... Outlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications. Option 1: Bachelor's degree in Computer Science and 4 years' experience in software engineering or related field. Option 2: 6 years' experience in software engineering or related field. Option 3: Master's degree in Computer Science and 2 years' experience in software engineering or related field. 3 years' experience in data engineering, database engineering, business intelligence, or business analytics. Preferred Qualifications... Outlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications. Data engineering, database engineering, business intelligence, or business analytics, ETL tools and working with large data sets in the cloud, Master's degree in Computer Science or related field and 4 years' experience in software engineering or related field, We value candidates with a background in creating inclusive digital experiences, demonstrating knowledge in implementing Web Content Accessibility Guidelines (WCAG) 2.2 AA standards, assistive technologies, and integrating digital accessibility seamlessly. The ideal candidate would have knowledge of accessibility best practices and join us as we continue to create accessible products and services following Walmart's accessibility standards and guidelines for supporting an inclusive culture. Primary Location... 840 W California Ave, Sunnyvale, CA 94086-4828, United States of America",
        "url": "https://www.linkedin.com/jobs/view/3910743791",
        "summary": "This role requires a seasoned Big Data Engineer with 8-12 years of industry experience to lead a team of engineers in building highly scalable Big Data solutions and ETL ecosystems. The engineer will be responsible for designing and implementing data warehouse and data lake solutions, integrating tools like Automic and Airflow, and working with cloud platforms like Google Cloud Platform (GCP) and Microsoft Azure. Strong communication skills and the ability to mentor junior developers are essential.",
        "industries": [
            "Retail",
            "E-commerce",
            "Data Engineering",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Leadership",
            "Communication",
            "Problem Solving",
            "Teamwork",
            "Mentoring",
            "Decision Making",
            "Technical Communication",
            "Critical Thinking"
        ],
        "hard_skills": [
            "Hadoop",
            "Hive",
            "Apache Spark",
            "Scala",
            "Python",
            "Google Cloud Platform (GCP)",
            "Microsoft Azure",
            "Automic",
            "Airflow",
            "Data Modeling",
            "Data Migration",
            "CI/CD Pipelines",
            "Data Architecture",
            "Distributed Computing",
            "Databricks",
            "Snowflake",
            "ThoughtSpot",
            "Druid",
            "Big Query",
            "ClickHouse",
            "Cosmos DB",
            "MySQL",
            "Postgres",
            "Kafka"
        ],
        "tech_stack": [
            "Hadoop",
            "Hive",
            "Apache Spark",
            "Scala",
            "Python",
            "Google Cloud Platform (GCP)",
            "Microsoft Azure",
            "Automic",
            "Airflow",
            "Databricks",
            "Snowflake",
            "ThoughtSpot",
            "Druid",
            "Big Query",
            "ClickHouse",
            "Cosmos DB",
            "MySQL",
            "Postgres",
            "Kafka"
        ],
        "programming_languages": [
            "Scala",
            "Python"
        ],
        "experience": 8,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Computer Science",
                "Software Engineering"
            ]
        },
        "salary": {
            "max": 286000,
            "min": 143000
        },
        "benefits": [
            "Medical",
            "Vision",
            "Dental",
            "401(k)",
            "Stock Purchase",
            "Life Insurance",
            "Paid Time Off (PTO)",
            "Parental Leave",
            "Family Care Leave",
            "Bereavement",
            "Jury Duty",
            "Voting",
            "Short-Term Disability",
            "Long-Term Disability",
            "Company Discounts",
            "Military Leave Pay",
            "Adoption and Surrogacy Expense Reimbursement",
            "Performance-Based Bonuses",
            "Live Better U Education Benefit Program"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3840709411,
        "company": "Arine",
        "title": "Backend Software Engineer (Python)",
        "created_on": 1720635574.476969,
        "description": "Based in San Francisco, Arine is a rapidly growing healthcare technology and clinical services company with a mission to ensure individuals receive the safest and most effective treatments for their unique and evolving healthcare needs. Frequently, medications cause more harm than good. Incorrect drugs and doses costs the US healthcare system over $528 billion in waste, avoidable harm, and hospitalizations each year. Arine is redefining what excellent healthcare looks like by solving these issues through our software platform (SaaS). We combine cutting edge data science, machine learning, AI, and deep clinical expertise to introduce a patient-centric view to medication management, and develop and deliver personalized care plans on a massive scale for patients and their care teams. Arine is committed to improving the lives and health of complex patients that have an outsized impact on healthcare costs and have traditionally been difficult to identify and address. These patients face numerous challenges including complicated prescribing issues across multiple medications and providers, medication challenges with many chronic diseases, and patient issues with access to care. Backed by leading healthcare investors and collaborating with top healthcare organizations and providers, we deliver recommendations and facilitate clinical interventions that lead to significant, measurable health improvements for patients and cost savings for customers. Why is Arine a Great Place to Work?: Market Opportunity - Arine is backed by leading healthcare investors and was founded to tackle one of the largest healthcare problems today. Non-optimized medications therapies which cost the US 275,000 lives and $528 billion annually. Dramatic Growth - Arine is managing more than 18 million lives across prominent health plans after only 4 years in the market. Outstanding Team and Culture - Our shared mission unites and motivates us to do our best work. We have a relentless passion and commitment to the innovation required to be the market leader in medication intelligence. Making a Proven Difference in Healthcare - We are saving patient lives, and enabling individuals to experience improved health outcomes, including significant reductions in hospitalizations and cost of care. The Role: As Backend Software Engineer, you will have the opportunity to work in close collaboration with technical stakeholders to enhance and optimize Arine's backend web APIs, patient data processing, data management systems, and cloud infrastructure. You will support, maintain, and develop software using a variety of different tools including but not limited to: Python, AWS, and SQL and NoSQL databases. What You'll be Doing: Participate in all aspects of Arine's backend / API based platform, including the following: Write production-level Python to implement web APIs, CLIs, libraries, and scripts Work significantly AWS services to build/deploy new features, troubleshoot reported bugs, and monitor mission-critical infrastructure and microservices Write unit and integration tests, and perform a modest amount of QA Contribute to paying down technical debt and improving existing systems Collaborate with Frontend and Data Engineers to collectively support Arine's internal Clinical Operations team and external customers/users Our Ideal Candidate Possesses: Proficient to fluent in Python (can build and deploy web APIs, CLIs, libraries, and scripts) Proficient to Power-User in AWS Services (Lambda, API Gateway, DynamoDB, S3, IAM, VPC, RDS, CloudFormation, SQS, SNS, EventBridge, Kinesis, etc.) Proven track record in architecting multi-component distributed systems Solid understanding of synchronous vs. asynchronous design, SQL vs NoSQL databases, and REST API design (WebSockets is preferred) Comfortable writing unit and integration tests (familiarity with TDD is preferred) Proficient with Git/GitHub, branching, and pull requests Meticulous attention to detail with the ability to own a project end-to-end - gather and translate requirements, communicate and collaborate with Stakeholders, and mentor junior engineers Comfortable working in a fast-paced environment with a strong ability to adapt to changing priorities Results-oriented and can deliver against strict deadlines A team player who can also work independently Nice-to-Haves: Experience with CI/CD tools and platforms such as Jenkins, Docker, Kubernetes, and GitHub Actions Experience with event-driven and serverless architectures Experience optimizing database design, e.g. indexes, queries, and aggregations Experience working with healthcare data or EHR systems Travel and Other Information: For candidates local to the San Francisco Bay Area, the ability to commute to our office on Market Street in San Francisco at least 2-3 days/week Ability to travel to Arine's San Francisco office at least twice per year Remote Work Requirements: An established private work area that ensures information privacy A stable high-speed internet connection for remote work Perks: Joining Arine offers you a dynamic role and the opportunity to contribute to the company's growth and shape its future. You'll have unparalleled learning and growth prospects, collaborating closely with experienced Clinicians, Engineers, Software Architects, and Digital Health Entrepreneurs. The posted range represents the expected salary for this position and does not include any other potential components of the compensation package, benefits, and perks. Ultimately, the final pay decision will consider factors such as your experience, job level, location, and other relevant job-related criteria. The salary range for this position is: $120,000-160,000/year. Job Requirements: Ability to pass a background check Must live in and be eligible to work in the United States Information Security Roles and Responsibilities: All staff at Arine are expected to be part of its Information Security Management Program and undergo periodic training on Information Security Awareness and HIPAA guidelines. Each user is responsible to maintain a secure working environment and follow all policies and procedures. Upon hire, each person is assigned and must complete trainings before access is granted for their specific role within Arine. Arine is an equal opportunity employer. We are committed to creating a diverse and inclusive workplace where all employees are treated with fairness and respect. We do not discriminate on the basis of race, ethnicity, color, religion, gender, sexual orientation, age, disability, or any other legally protected status. Our hiring decisions and employment practices are based solely on qualifications, merit, and business needs. We encourage individuals from all backgrounds to apply and join us in our mission. Check our website at https://www.arine.io . This is a unique opportunity to join a growing start-up revolutionizing the healthcare industry! Note to Recruitment Agencies: We appreciate your interest in finding talent for Arine, but please be advised that we do not accept unsolicited resumes from recruitment agencies. All resumes submitted to Arine without a prior written agreement in place will be considered property of Arine, and no fee will be paid in the event of a hire. Thank you for your understanding.",
        "url": "https://www.linkedin.com/jobs/view/3840709411",
        "summary": "Arine is a healthcare technology company focused on medication management, using data science, AI, and clinical expertise to develop personalized care plans for patients. They are seeking a Backend Software Engineer to build and maintain their backend web APIs, data processing systems, and cloud infrastructure using Python, AWS, and various databases. The role involves working closely with technical stakeholders, writing unit tests, collaborating with other engineers, and contributing to the company's growth. The ideal candidate has strong Python, AWS, and database experience, along with experience in architecting distributed systems, testing, and working in a fast-paced environment.",
        "industries": [
            "Healthcare",
            "Technology",
            "Software",
            "Data Science",
            "Machine Learning",
            "Artificial Intelligence",
            "Clinical Services",
            "SaaS"
        ],
        "soft_skills": [
            "Collaboration",
            "Communication",
            "Problem Solving",
            "Teamwork",
            "Adaptability",
            "Time Management",
            "Detail Oriented",
            "Leadership",
            "Mentorship"
        ],
        "hard_skills": [
            "Python",
            "AWS",
            "SQL",
            "NoSQL",
            "REST API",
            "WebSockets",
            "Git",
            "GitHub",
            "Jenkins",
            "Docker",
            "Kubernetes",
            "GitHub Actions",
            "CI/CD",
            "TDD"
        ],
        "tech_stack": [
            "Python",
            "AWS",
            "Lambda",
            "API Gateway",
            "DynamoDB",
            "S3",
            "IAM",
            "VPC",
            "RDS",
            "CloudFormation",
            "SQS",
            "SNS",
            "EventBridge",
            "Kinesis",
            "Jenkins",
            "Docker",
            "Kubernetes",
            "GitHub Actions",
            "SQL",
            "NoSQL"
        ],
        "programming_languages": [
            "Python"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 160000,
            "min": 120000
        },
        "benefits": [
            "Dynamic Role",
            "Growth Opportunities",
            "Learning and Development",
            "Collaboration with Experienced Clinicians and Engineers"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Foster City, CA",
        "job_id": 3892757666,
        "company": "Visa",
        "title": "Sr. Data Engineer (multiple openings)",
        "created_on": 1720635576.061841,
        "description": "Company Description Visa is a world leader in payments and technology, with over 259 billion payments transactions flowing safely between consumers, merchants, financial institutions, and government entities in more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable, and secure payments network, enabling individuals, businesses, and economies to thrive while driven by a common purpose – to uplift everyone, everywhere by being the best way to pay and be paid. Make an impact with a purpose-driven industry leader. Join us today and experience Life at Visa. Job Description Visa Technology & Operations LLC, a Visa Inc. company, needs a Sr. Data Engineer (multiple openings) in Foster City, CA to: Work with user and application teams to design, enhance, and build solution dealing with batch and real-time data in a fast-paced environment. Manage thousands of active users and hundreds of application teams using big data services. Handle and support large scale production big data deployments. Follow established customer success guidelines and support structure. Provide cross-team collaboration, and build and maintain relationships with the customer teams, the user community, architects, and engineering teams. Provide effective root cause analysis of major production incidents and develop learning documentation. Solve customer cases through a variety of customer contact channels which include telephone, email, and web/live chat. Apply advanced troubleshooting techniques to provide tailored solutions for customers and drive customer interactions by thoughtfully working with customers to dive deep into the root cause of an issue. Position reports to the Foster City, California office and may allow for partial telecommuting. Qualifications Bachelor’s degree, or foreign equivalent, in Computer Science, Engineering or related field, and 3 years of experience in the job offered or in a data engineering related occupation Shell and Python programming for automation Debugging in Python, Java, Scala and SQL Apache Hadoop, Apache Spark, Apache Hive, Apache Kafka, Apache HBase and Presto System administration with Linux (RHEL/CentOS), including Microsoft Active Directory and LDAP integration Troubleshooting Kerberos Authentication problems Network troubleshooting Additional Information Worksite: This is a hybrid position. Hybrid employees can alternate time between both remote and office. Employees in hybrid roles are expected to work from the office 2-3 set days a week (determined by leadership/site), with a general guidepost of being in the office 50% or more of the time based on business needs. Travel Requirements: This position does not require travel. Mental/Physical Requirements: This position will be performed in an office setting.  The position will require the incumbent to sit and stand at a desk, communicate in person and by telephone, frequently operate standard office equipment, such as telephones and computers. Visa is an EEO Employer.  Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status.  Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law. Visa will consider for employment qualified applicants with criminal histories in a manner consistent with applicable local law, including the requirements of Article 49 of the San Francisco Police Code. U.S. APPLICANTS ONLY: The estimated salary range for a new hire into this position is $147,368.00 USD to $194,500.00 USD per year, which may include potential sales incentive payments (if applicable). Salary may vary depending on job-related factors which may include knowledge, skills, experience, and location. In addition, this position may be eligible for bonus and equity. Visa has a comprehensive benefits package for which this position may be eligible that includes Medical, Dental, Vision, 401 (k), FSA/HSA, Life Insurance, Paid Time Off, and Wellness Program.",
        "url": "https://www.linkedin.com/jobs/view/3892757666",
        "summary": "Visa is seeking a Senior Data Engineer to join their Technology & Operations team in Foster City, CA. The role involves designing, building, and managing big data solutions for thousands of users and hundreds of application teams. Responsibilities include working with user and application teams, handling large-scale production deployments, providing customer support, performing root cause analysis, and collaborating cross-functionally.  The position offers a hybrid work model and a competitive salary and benefits package.",
        "industries": [
            "Technology",
            "Financial Services",
            "Payments"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem-solving",
            "Customer service",
            "Troubleshooting",
            "Teamwork",
            "Analytical",
            "Relationship building",
            "Root Cause Analysis",
            "Documentation"
        ],
        "hard_skills": [
            "Python",
            "Shell Scripting",
            "Java",
            "Scala",
            "SQL",
            "Apache Hadoop",
            "Apache Spark",
            "Apache Hive",
            "Apache Kafka",
            "Apache HBase",
            "Presto",
            "Linux (RHEL/CentOS)",
            "Microsoft Active Directory",
            "LDAP",
            "Kerberos Authentication",
            "Network troubleshooting"
        ],
        "tech_stack": [
            "Apache Hadoop",
            "Apache Spark",
            "Apache Hive",
            "Apache Kafka",
            "Apache HBase",
            "Presto",
            "Linux (RHEL/CentOS)",
            "Microsoft Active Directory",
            "LDAP",
            "Kerberos Authentication"
        ],
        "programming_languages": [
            "Python",
            "Shell",
            "Java",
            "Scala",
            "SQL"
        ],
        "experience": 3,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Computer Science",
                "Engineering",
                "Related field"
            ]
        },
        "salary": {
            "max": 194500,
            "min": 147368
        },
        "benefits": [
            "Medical",
            "Dental",
            "Vision",
            "401(k)",
            "FSA/HSA",
            "Life Insurance",
            "Paid Time Off",
            "Wellness Program"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Newport Beach, CA",
        "job_id": 3909461791,
        "company": "Kaygen, Inc.",
        "title": "Data Engineer",
        "created_on": 1720635580.3527195,
        "description": "KAYGEN is an emerging leader in providing top talent for technology based staffing services. We specialize in providing high-volume contingent staffing, direct hire staffing and project based solutions to companies worldwide ranging from startups to Fortune 500 and Managed Service Providers (MSP) across a wide variety of industries. Job Title: Data Engineer Location: Newport Beach, CA Duration: FTE Job Description Are you someone looking for a challenge to work in a startup environment and innovate in a global environment to help build a SaaS Service to scale? Are you willing to roll up your sleeves and do what it takes to deliver value to Customers? As we expand our Engineering, we are looking for a Sr. Data Engineer to join us and help build robust data pipelines that is an integral part of our SaaS platform The role would offer an opportunity to work with cross functional teams in developing and evolving Cloud services powering innovative technology solutions used in the real estate industry. We are seeking a skilled and experienced Data Engineer to join our dynamic team. The Data Engineer will be responsible for designing, implementing, and maintaining our data infrastructure, pipelines, and systems. This role involves collaborating with cross-functional teams to ensure efficient data flow, data quality, and optimal performance. The ideal candidate should have a strong background in software engineering, database management, and data architecture, along with proficiency in relevant technologies and tools. Responsibilities Lead the design and implementation of robust data pipelines for efficient extraction, transformation, and loading (ETL) of large datasets from diverse sources. Collaborate with stakeholders to understand business requirements, translating them into scalable data solutions tailored to end-user needs. Enhance and maintain our data architecture, ensuring scalability, reliability, and performance. Investigate and resolve data anomalies within ETL pipelines, ensuring data integrity and accuracy. Develop and maintain documentation for data processes, ensuring clarity and sustainability. Utilize AWS platform capabilities to architect and operate scalable production solutions. Implement data security practices and compliance measures to protect sensitive information. Build and manage data warehouses and data lakes to store and organize large datasets. Implement data quality checks and procedures to ensure data accuracy and consistency. Automate data workflows and processes using scripting languages and data pipeline orchestration tools. Collaborate with business analysts, onboarding teams, and business stakeholders to understand data needs and translate them into technical solutions. Monitor and troubleshoot data pipelines, identify bottlenecks, and optimize performance of data infrastructure to ensure scalability and efficiency. Troubleshoot issues with production data and/or inconsistencies and propose sustainable solutions to address them Document data pipelines and processes for maintainability and knowledge sharing. Stay updated with emerging technologies and best practices in data engineering, and propose innovative solutions. Document system architectures, processes, and procedures for knowledge sharing and future reference. Provide technical guidance and support to junior team members, and contribute to their professional development Skills And Qualifications Bachelor's degree in Computer Science, Information Technology, or a related field (or equivalent experience). A minimum of 5 years of experience as a Data Engineer or similar role. Strong proficiency in SQL and experience with relational databases (e.g., MySQL, PostgreSQL). Experience with cloud-based data platforms (e.g., AWS, Azure, GCP). Experience with data warehousing solutions (e.g., Snowflake, Redshift, BigQuery). Experience with data ingestion tools (e.g., Fivetran, Stitch, Kafka). Programming experience in Python, Java, or similar languages. Experience with scripting languages (e.g., Bash, Shell). Experience with data workflow management tools (e.g., Airflow, Luigi). Excellent problem-solving and analytical skills. Strong communication and collaboration skills. Ability to work independently and as part of a team. At KAYGEN, we are always looking for dynamic, talented and experienced individuals. We invite you to join our team of talented IT professionals, consulting at client locations across the globe. Our culture is team-orientated; we strive to stand by our core values of respect, honesty and integrity. Our team of experienced staffing experts will work with you to find you the best opportunity. For more information please visit us at www.kaygen.com. Benefits Free Healthcare Insurance Vision and Dental Insurance 401(k) Retirement Plan Free Life Insurance Sick Time Off Achieve your Kaizen by clicking here. A unique and exclusive talent community supported by Kaygen, that includes programs like: Certifications Mentorship Program Referrals Family and Wellness benefits Continuous Growth and Career Development",
        "url": "https://www.linkedin.com/jobs/view/3909461791",
        "summary": "KAYGEN is seeking a Sr. Data Engineer to join their growing engineering team and help build robust data pipelines for their SaaS platform. The ideal candidate will have 5+ years of experience in data engineering and a strong proficiency in SQL, cloud platforms (AWS, Azure, GCP), data warehousing solutions, and data ingestion tools. They will be responsible for designing, implementing, and maintaining data infrastructure, pipelines, and systems, as well as collaborating with cross-functional teams to ensure efficient data flow, data quality, and optimal performance.",
        "industries": [
            "Technology",
            "Software",
            "SaaS",
            "Real Estate"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem-solving",
            "Analytical",
            "Teamwork",
            "Independent",
            "Documentation",
            "Leadership",
            "Technical guidance",
            "Mentorship"
        ],
        "hard_skills": [
            "SQL",
            "MySQL",
            "PostgreSQL",
            "AWS",
            "Azure",
            "GCP",
            "Snowflake",
            "Redshift",
            "BigQuery",
            "Fivetran",
            "Stitch",
            "Kafka",
            "Python",
            "Java",
            "Bash",
            "Shell",
            "Airflow",
            "Luigi"
        ],
        "tech_stack": [
            "AWS",
            "Azure",
            "GCP",
            "Snowflake",
            "Redshift",
            "BigQuery",
            "Fivetran",
            "Stitch",
            "Kafka",
            "Airflow",
            "Luigi"
        ],
        "programming_languages": [
            "SQL",
            "Python",
            "Java"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Computer Science",
                "Information Technology"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Free Healthcare Insurance",
            "Vision and Dental Insurance",
            "401(k) Retirement Plan",
            "Free Life Insurance",
            "Sick Time Off",
            "Certifications",
            "Mentorship Program",
            "Referrals",
            "Family and Wellness benefits",
            "Continuous Growth and Career Development"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Foster City, CA",
        "job_id": 3966736229,
        "company": "Zoox",
        "title": "Data Engineer - Collision Avoidance System",
        "created_on": 1720635581.922002,
        "description": "The Collision Avoidance System (CAS) is responsible for detecting and reacting to imminent collision situations in support of our vehicle’s overall safety goals. CAS Perception is responsible for processing raw sensor data from our vehicle’s world-class sensor suite using a combination of geometric, interpretable algorithms and deep learning to detect near-collisions with obstacles along our intended driving path, in the most challenging dense urban environments and under tight compute resource constraints. Overall CAS is parallel and complementary to our Main AI autonomy stack, and has a close relationship with our vehicle hardware and safety teams in order to architect redundancy into our overall driving system. As a Data Engineer on the Collision Avoidance system, you will play a crucial role in driving dat-driven decision making by ensuring the availability of high-quality, reliable data and metrics for the team and Zoox as a whole. By doing so, you will contribute significantly to making autonomous vehicles safer for all passengers. In This Role, You Will Design the data pipelines to support Zoox’s machine learning systems and data mining at scale Develop and maintain ETL (Extract, Transform, Load) processes for data ingestion and transformation to make data readily available for ML models training and validation. Maintain high quality data pipelines implementing data quality checks, ensuring data consistency and accuracy and adherence to data engineering best practices. Design and implement data models and data storage solutions for fast, reliable and user-friendly data querying. Build self-serve data dashboards for quick fact checking and ongoing reporting purposes. Collaborate with data scientists, software engineers, and other stakeholders to ensure that the data pipelines meet the requirements for machine learning models and make recommendations for changes or upgrades. Collaborate with legal, infrastructure, platform teams to develop effective solutions that aligns with data access, retention, privacy protection policies and regulations. Qualifications BS/MS degree in a technical field Experience designing and building complex data infrastructure at scale Advanced Structure Query Language (SQL) and data warehousing experience Experience operating a workflow manager such as Airflow Experience with large scale streaming platforms (e.g. Kafka, Kinesis), processing frameworks (e.g. Spark, Hadoop) and storage engines (e.g. HDFS, HBase) Bonus Qualifications Exceptional Python or Scala skills Basic fluency in C++ Familiarity with or exposure to experimentation platforms A strong DataOps mindset and opinions on next-generation warehousing tools Compensation There are three major components to compensation for this position: salary, Amazon Restricted Stock Units (RSUs), and Zoox Stock Appreciation Rights. The salary will range from $164,000 - $265,000. A sign-on bonus may be part of a compensation package. Compensation will vary based on geographic location, job-related knowledge, skills, and experience. Zoox also offers a comprehensive package of benefits including paid time off (e.g. sick leave, vacation, bereavement), unpaid time off, Zoox Stock Appreciation Rights, Amazon RSUs, health insurance, long-term care insurance, long-term and short-term disability insurance, and life insurance.",
        "url": "https://www.linkedin.com/jobs/view/3966736229",
        "summary": "Zoox is searching for a Data Engineer to contribute to its Collision Avoidance System (CAS) by designing, building, and maintaining data pipelines that support machine learning and data mining at scale. The ideal candidate will have experience in SQL, data warehousing, workflow management tools (e.g., Airflow), and large-scale streaming and processing platforms (e.g., Kafka, Kinesis, Spark, Hadoop).  The role will involve collaborating with data scientists, engineers, legal, and infrastructure teams to ensure data quality, consistency, and adherence to privacy regulations.",
        "industries": [
            "Autonomous Vehicles",
            "Machine Learning",
            "Data Engineering",
            "Software Development",
            "Artificial Intelligence"
        ],
        "soft_skills": [
            "Problem Solving",
            "Communication",
            "Collaboration",
            "Teamwork",
            "Critical Thinking",
            "Data-Driven Decision Making",
            "Attention to Detail",
            "Organization",
            "Time Management"
        ],
        "hard_skills": [
            "SQL",
            "Data Warehousing",
            "ETL",
            "Data Pipelines",
            "Data Modeling",
            "Data Storage",
            "Data Quality",
            "Data Consistency",
            "Data Accuracy",
            "Data Engineering Best Practices",
            "Airflow",
            "Kafka",
            "Kinesis",
            "Spark",
            "Hadoop",
            "HDFS",
            "HBase",
            "Python",
            "Scala",
            "C++",
            "Experimentation Platforms",
            "DataOps"
        ],
        "tech_stack": [
            "SQL",
            "Airflow",
            "Kafka",
            "Kinesis",
            "Spark",
            "Hadoop",
            "HDFS",
            "HBase",
            "Python",
            "Scala",
            "C++"
        ],
        "programming_languages": [
            "SQL",
            "Python",
            "Scala",
            "C++"
        ],
        "experience": 5,
        "education": {
            "min_degree": "BS",
            "fields": [
                "Technical Field"
            ]
        },
        "salary": {
            "max": 265000,
            "min": 164000
        },
        "benefits": [
            "Paid Time Off",
            "Unpaid Time Off",
            "Zoox Stock Appreciation Rights",
            "Amazon RSUs",
            "Health Insurance",
            "Long-Term Care Insurance",
            "Long-Term Disability Insurance",
            "Short-Term Disability Insurance",
            "Life Insurance"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Palo Alto, CA",
        "job_id": 3946603771,
        "company": "Alldus",
        "title": "Data Engineer - Palo Alto",
        "created_on": 1720635583.6943283,
        "description": "Data Engineer - Palo Alto, Hybrid Our client is a highly innovative health tech company operating in stealth mode, focused on revolutionizing healthcare delivery through cutting-edge AI. As they enter an exciting phase of growth and transformation, they are seeking a skilled and experienced Data Engineer to contribute to the company's future success. You will play a critical role in building and managing data pipelines that pull and process data from various hospitals to support applications in healthcare. Responsibilities: Developing data pipelines to pull data from hospital systems Managing data adhering to standards and regulations Using orchestration tools to manage data flow and processing at scale Qualifications: 4 years’ experience in Data Engineering Experience working in Healthcare data – FHIR, HL7 Experience working on EHR systems Azure experience – beneficial Python programming skills Excellent communication and problem-solving skills If you are interested, please apply or send a resume to kieran@alldus.com",
        "url": "https://www.linkedin.com/jobs/view/3946603771",
        "summary": "A health tech company is looking for a Data Engineer to build and manage data pipelines for healthcare applications. The role involves pulling data from hospital systems, managing data adhering to standards, and using orchestration tools for data flow processing at scale. Experience with FHIR, HL7, EHR systems, Azure, and Python is required.",
        "industries": [
            "Healthcare",
            "Health Tech",
            "Data Science",
            "Software Development",
            "Technology"
        ],
        "soft_skills": [
            "Communication",
            "Problem Solving"
        ],
        "hard_skills": [
            "Data Engineering",
            "Data Pipelines",
            "Data Management",
            "Data Processing",
            "Orchestration Tools",
            "FHIR",
            "HL7",
            "EHR Systems",
            "Azure",
            "Python"
        ],
        "tech_stack": [
            "Azure",
            "Python",
            "FHIR",
            "HL7",
            "EHR"
        ],
        "programming_languages": [
            "Python"
        ],
        "experience": 4,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Palo Alto, CA",
        "job_id": 3966741675,
        "company": "Akraya, Inc.",
        "title": "Data Engineer V: 24-01837",
        "created_on": 1720635585.4062521,
        "description": "Primary Skills: Data Engineer, Python, SQL, Airflow, Cloud Data Warehouse Contract Type: W2 Duration: 12 months Contract Location: California (Remote) Pay Range: $93.90 - $97.90 Per Hour. #IND1 Job Responsibilities Experienced Senior Data Engineer to join our Enterprise Data Platform team. The successful candidate will be integral in developing robust data pipelines for a cloud data warehouse, leveraging python and cloud technologies. This role requires collaboration with cross-functional stakeholders to gather data requirements and use technical expertise to implement efficient data solutions. Job Requeriments Collaborate with cross-functional teams to gather and understand data requirements. Develop and maintain data pipelines using Python for snowflake integration. Extract data from Rest APIs and ingest it into a cloud data warehouse. Utilize S3 buckets effectively for data storage and management. Write and manage DAGs on Airflow to automate and monitor data workflows. Must-Have Skills 5+ Years of experience building data pippeline using Python. SQL is a must have. Experience in building data pipelines and working with cloud data warehouses. Familiarity with Rest APIs, S3, and Airflow for data extraction, storage, and workflow automation. ABOUT AKRAYA Akraya is an award-winning IT staffing firm consistently recognized for our commitment to excellence and a positive work environment. Voted the #1 Best Place to Work in Silicon Valley (2023) and a Glassdoor Best Places to Work (2023 & 2022), Akraya prioritizes a culture of inclusivity and fosters a sense of belonging for all team members. We are staffing solutions providers for Fortune 100 companies, and our industry recognitions solidify our leadership position in the IT staffing space. Let us lead you to your dream career, join Akraya today!",
        "url": "https://www.linkedin.com/jobs/view/3966741675",
        "summary": "This is a 12-month W2 contract position for a Senior Data Engineer to join an Enterprise Data Platform team. The role involves developing data pipelines for a cloud data warehouse using Python, Snowflake, Rest APIs, S3, and Airflow. The candidate will work with cross-functional stakeholders to gather data requirements and implement efficient data solutions.",
        "industries": [
            "Data Engineering",
            "Technology",
            "Cloud Computing",
            "Software Development"
        ],
        "soft_skills": [
            "Collaboration",
            "Communication",
            "Problem Solving",
            "Data Analysis",
            "Technical Expertise"
        ],
        "hard_skills": [
            "Python",
            "SQL",
            "Airflow",
            "Snowflake",
            "Cloud Data Warehousing",
            "Rest APIs",
            "S3",
            "Data Pipelines"
        ],
        "tech_stack": [
            "Python",
            "SQL",
            "Airflow",
            "Snowflake",
            "S3",
            "Rest APIs"
        ],
        "programming_languages": [
            "Python",
            "SQL"
        ],
        "experience": 5,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 9790,
            "min": 9390
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Santa Clara, CA",
        "job_id": 3955028561,
        "company": "Palo Alto Networks",
        "title": "Senior Software Engineer (Threat Data Platform)",
        "created_on": 1720635587.3069046,
        "description": "Company Description Our Mission At Palo Alto Networks® everything starts and ends with our mission: Being the cybersecurity partner of choice, protecting our digital way of life. Our vision is a world where each day is safer and more secure than the one before. We are a company built on the foundation of challenging and disrupting the way things are done, and we’re looking for innovators who are as committed to shaping the future of cybersecurity as we are. Our Approach to Work We lead with flexibility and choice in all of our people programs. We have disrupted the traditional view that all employees have the same needs and wants. We offer personalization and offer our employees the opportunity to choose what works best for them as often as possible - from your wellbeing support to your growth and development, and beyond! At Palo Alto Networks, we believe in the power of collaboration and value in-person interactions. This is why our employees generally work from the office three days per week, leaving two days for choice and flexibility to work where you feel most effective. This setup fosters casual conversations, problem-solving, and trusted relationships. While details may evolve, our goal is to create an environment where innovation thrives, with office-based teams coming together three days a week to collaborate and thrive, together! Job Description Your Career The Threat Data Platform team specializes in providing tools enabling threat researchers and incident response consultants to work more efficiently across the vast amounts of data Palo Alto Networks has at its disposal. Our experts bridge the gap between application development using modern best practices/technology and the world-class research teams. This role will challenge you to become a true force-multiplier, assisting the security research team through designing and building threat centric application workflows, process automations, and the creation of intelligent data architectures for threat intelligence and telemetry collection at scale. Your Impact Design and develop features & integrations to enable scalable threat data collection, analysis, and countermeasure creation Work with existing backend systems to drive REST APIs and workflows via python (flask or fastAPI), relying on a variety of datasources - SQL, NoSQL, and document stores containing more data than one can imagine Contribute to existing and new frameworks that allow threat researchers to focus on the threats and engineers (us) focus on the proper tooling to make it all work at scale Partner with other development and business teams to manage dependencies and communicate technical specifications Work with existing deployment infrastructure to regularly deploy releases using modern tools such as git and CI/CD (Gitlab CI, Docker, Spinnaker, Kubernetes) Qualifications Your Experience Solid understanding and usage of Python, SQL, and working knowledge of one or more non-relational databases such as ElasticSearch, Cassandra, BigQuery, etc. Experience in security Experience developing container based applications using tools such as Docker, Kubernetes, and/or Helm Demonstrated full stack programming capability and desire to mature an idea or feature from pseudo-code to a minimum viable product Creation of, and integration with REST APIs Experience working with queuing technologies such as RabbitMQ & Pub/Sub to build scalable solutions Excellent written and verbal communication skills, and experience working on remote teams Experience with front-end frameworks such as React is a plus Experience with cloud environments such as AWS or GCP is a plus Additional Information The Team Our engineering team is at the core of our products and connected directly to the mission of preventing cyberattacks. We are constantly innovating — challenging the way we, and the industry, think about cybersecurity. Our engineers don’t shy away from building products to solve problems no one has pursued before. We define the industry instead of waiting for directions. We need individuals who feel comfortable in ambiguity, excited by the prospect of a challenge, and empowered by the unknown risks facing our everyday lives that are only enabled by a secure digital environment. Our Commitment We’re trailblazers that dream big, take risks, and challenge cybersecurity’s status quo. It’s simple: we can’t accomplish our mission without diverse teams innovating, together. We are committed to providing reasonable accommodations for all qualified individuals with a disability. If you require assistance or accommodation due to a disability or special need, please contact us at accommodations@paloaltonetworks.com. Palo Alto Networks is an equal opportunity employer. We celebrate diversity in our workplace, and all qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or other legally protected characteristics. All your information will be kept confidential according to EEO guidelines. The compensation offered for this position will depend on your degree or progress toward degree , qualifications, experience, and work location. For candidates who receive an offer, the starting rate is expected to be $175,000/yr for the specific role. Is role eligible for Immigration Sponsorship?: No. Please note that we will not sponsor applicants for work visas for this position.",
        "url": "https://www.linkedin.com/jobs/view/3955028561",
        "summary": "Palo Alto Networks seeks a Threat Data Platform Engineer to develop and maintain tools for threat researchers and incident response consultants. This role involves designing and building threat-centric application workflows, process automations, and intelligent data architectures for threat intelligence and telemetry collection. The ideal candidate will have strong Python, SQL, and non-relational database experience, as well as experience in security, containerized applications, REST APIs, queuing technologies, and full-stack programming. Bonus points for experience with front-end frameworks, cloud environments, and remote teams.",
        "industries": [
            "Cybersecurity",
            "Information Technology",
            "Software Development"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Decision Making",
            "Time Management",
            "Teamwork",
            "Adaptability",
            "Creativity",
            "Analytical Thinking",
            "Organization",
            "Attention to Detail"
        ],
        "hard_skills": [
            "Python",
            "SQL",
            "ElasticSearch",
            "Cassandra",
            "BigQuery",
            "Docker",
            "Kubernetes",
            "Helm",
            "REST APIs",
            "RabbitMQ",
            "Pub/Sub",
            "Git",
            "CI/CD",
            "Gitlab CI",
            "Spinnaker",
            "React",
            "AWS",
            "GCP"
        ],
        "tech_stack": [
            "Python",
            "Flask",
            "FastAPI",
            "SQL",
            "NoSQL",
            "ElasticSearch",
            "Cassandra",
            "BigQuery",
            "Docker",
            "Kubernetes",
            "Helm",
            "Git",
            "Gitlab CI",
            "Spinnaker",
            "React",
            "AWS",
            "GCP",
            "RabbitMQ",
            "Pub/Sub"
        ],
        "programming_languages": [
            "Python"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 175000,
            "min": 175000
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3891670476,
        "company": "Unreal Staffing, Inc",
        "title": "AI Software Engineer",
        "created_on": 1720635588.5018294,
        "description": "Embark on the AI Odyssey Imagine a place where your work doesn't just change the game—it transforms the very fabric of reality. [Your Company Name] is that place. As an AI Software Engineer, you'll join a team of explorers, tasked with navigating uncharted territories of artificial intelligence and machine learning. Our mission? To solve the unsolvable and create technology that dreams are made of. Your Expedition Awaits Design and refine algorithms that will make self-learning machines a staple of everyday life Forge new paths in AI research and development, pushing boundaries and challenging the status quo Work on interdisciplinary projects that combine AI with other fields, like robotics, quantum computing, or bioinformatics, to create innovations that the world has never seen Harness big data, turning it into actionable insights and transforming industries from healthcare to finance, and beyond Be part of a team where your ideas fuel our collective journey, leading to groundbreaking achievements Why: Be at the forefront of AI innovation, where your work makes a difference on a global scale Enjoy a competitive salary, comprehensive benefits, and options for equity Embrace a flexible work environment that values balance, wellness, and productivity Continuous learning opportunities, including workshops, courses, and conferences, to keep you at the cutting edge of your field Join a culture that celebrates diversity, inclusion, and the power of creative thinking How to Chart Your Course to Us: Please send your resume, a portfolio of your projects (GitHub links encouraged) we're not just building AI; we're building a brighter future. Join us on this journey, and let's create something extraordinary together. Requirements Who Will Thrive Here: Someone with a degree in Computer Science, Artificial Intelligence, Machine Learning, or a related field, who views the unknown as a playground Proficiency in programming languages such as Python, Java, or C++, and experience with AI frameworks like TensorFlow or PyTorch A visionary who understands that AI is not just about technology—it's about creating a future where technology enhances human potential A collaborator who believes that the best solutions arise from the melding of diverse perspectives and disciplines An innovator who is as excited about learning and growing as they are about developing new AI solutions Benefits Unlimited Paid Time Off (PTO): Encourages work-life balance and shows trust in your employees to manage their time effectively Professional Development Fund: Allocate a certain budget for each employee to attend conferences, workshops, or courses of their choice, fostering continuous learning and growth Remote Work Options: Even post-pandemic, the flexibility to work from anywhere can be a huge draw for talent who value the ability to travel or prefer not to relocate Wellness Programs: Offer subscriptions to mental health apps, fitness memberships, or even in-office wellness activities like yoga and meditation sessions Pet-Friendly Workplace: Allow employees to bring their pets to work, reducing stress and promoting a more relaxed atmosphere Tech Stipend: Provide a budget for employees to set up or upgrade their home office setup, ensuring they have the tools they need to succeed Four-Day Work Week: Implement a condensed workweek to promote productivity and work-life balance Parental Support: Offer generous parental leave policies, childcare assistance, or even on-site childcare services Sabbatical Leave: Allow employees to take an extended leave of absence after a certain period of employment to travel, pursue personal projects, or simply recharge Equity or Stock Options: Give employees a stake in the company's success, aligning their interests with the company's long-term goals Transportation and Parking Benefits: Provide subsidies for public transportation, electric vehicle charging stations, or free parking to ease the commute Customized Career Pathing: Work with employees to design personalized career development plans, acknowledging their unique strengths and ambitions Health and Safety: Offer comprehensive health insurance, including mental health coverage, and ergonomic workplace assessments to ensure physical well-being Food and Snacks: Provide free meals, a well-stocked kitchen, or food allowances, especially for teams working late or in crunch times Team Retreats and Offsites: Organize annual retreats in exciting locales to build team cohesion and reward hard work",
        "url": "https://www.linkedin.com/jobs/view/3891670476",
        "summary": "This job posting seeks an AI Software Engineer to design and refine algorithms for self-learning machines, explore AI research and development, collaborate on interdisciplinary projects, and harness big data for actionable insights across industries. The ideal candidate will have a degree in Computer Science, AI, or Machine Learning, strong programming skills, a vision for AI's impact on human potential, and a collaborative spirit. This role offers a competitive salary, comprehensive benefits, and a flexible work environment.",
        "industries": [
            "Artificial Intelligence",
            "Machine Learning",
            "Robotics",
            "Quantum Computing",
            "Bioinformatics",
            "Healthcare",
            "Finance"
        ],
        "soft_skills": [
            "Visionary",
            "Collaborative",
            "Innovator",
            "Problem-solving",
            "Communication",
            "Teamwork",
            "Critical thinking",
            "Adaptability",
            "Learning agility"
        ],
        "hard_skills": [
            "Python",
            "Java",
            "C++",
            "TensorFlow",
            "PyTorch"
        ],
        "tech_stack": [
            "TensorFlow",
            "PyTorch"
        ],
        "programming_languages": [
            "Python",
            "Java",
            "C++"
        ],
        "experience": 0,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Computer Science",
                "Artificial Intelligence",
                "Machine Learning"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Unlimited Paid Time Off",
            "Professional Development Fund",
            "Remote Work Options",
            "Wellness Programs",
            "Pet-Friendly Workplace",
            "Tech Stipend",
            "Four-Day Work Week",
            "Parental Support",
            "Sabbatical Leave",
            "Equity or Stock Options",
            "Transportation and Parking Benefits",
            "Customized Career Pathing",
            "Health and Safety",
            "Food and Snacks",
            "Team Retreats and Offsites"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3966694931,
        "company": "Hayden AI",
        "title": "Software Engineer",
        "created_on": 1720635590.2024045,
        "description": "About Us At Hayden AI, we are on a mission to harness the power of artificial intelligence and machine learning to transform the way governments and businesses address real-world challenges. From optimizing bus lane and bus stop enforcement to pioneering digital twin modeling and beyond, our innovative mobile perception system empowers our clients to accelerate transit, enhance street safety, and drive forward a sustainable future. COMPANY NAME: Hayden AI Technologies, Inc. POSITION TITLE: Software Engineer POSITION DUTIES: Design, develop, test and document internal tools and scripts to streamline and facilitate current processes adopted by different teams. Develop backend microservices for cloud team to add additional functionality to current company system. Program in GoLang and python. Use monitoring tools such as Grafana to identify and fix problems with current remote systems/devices. Debug existing company software systems and report areas of potential improvement. Use Grafana to create multiple dashboards and panels to visualize data queried through different databases - influxDB and Postgres. Create alerts to notify the engineering team in case certain thresholds are crossed for each query. Work with AWS s3, EC2, and other cloud tools to facilitate company and code development tasks. Test, deploy, maintain and document microservices in Golang and Python. DEGREE REQUIREMENTS: Bachelor of Science or Arts degree or the foreign equivalent in Computer Science, Computer Engineering, Electrical Engineering or a related field. EXPERIENCE REQUIREMENT: Six (6) months of experience in the position offered, as a Junior Software Engineering Intern, or a related software engineering role. OTHER SPECIAL REQUIREMENTS:Six (6) months of experience with all of the following: Python programming and software design skills; C++; and automated data annotation. RATE OF PAY: $132,870.00 to 135,000.00 per year LOCATION OF POSITION AND INTERVIEW: Hayden AI Technologies, Inc. 460 Bryant Street San Francisco, CA 94107 APPLICANTS SHOULD SUBMIT RESUMES TO: people@hayden.ai Hayden AI Technologies, Inc. 460 Bryant Street San Francisco, CA 94107 Benefits and Perks There are endless learning and development opportunities from a highly diverse and talented peer group, including experts in a wide range of fields (AI, Computer Vision, Government Contracting, Systems & Device Engineering, Operations, Communications, and more!) Options for 100% company paid medical, dental, and vision coverage for employees and dependents (for US employees) Flexible Spending Account (FSA) and Dependent Care Flexible Spending Account (DCFSA) Life, AD&D, Short and Long Term Disability Insurance Aflac Critical Illness, Accident Insurance & Hospital Indemnity Insurance MetLife Legal Plan(s) & Pet Insurance Farmers GroupSelect Auto & Home Insurance 401(k) with 3% company matching Professional development reimbursement Unlimited PTO Hybrid work opportunities Daily catered lunches in our San Francisco office Hayden AI is committed to creating a diverse and inclusive environment that fosters learning from each other. We celebrate people of diverse backgrounds, experiences, abilities, and perspectives. We are an equal opportunity employer and are committed to providing a work environment free of harassment and discrimination. Hayden AI is also committed to working with and providing reasonable accommodations to individuals with disabilities. Please let your recruiter know if you need an accommodation at any point during the interview process. To all recruitment agencies: Hayden AI does not accept agency resumes. Please do not forward resumes to our jobs alias, Hayden AI employees or any other company location. Hayden AI is not responsible for any fees related to unsolicited resumes. Compensation Range: $132.9K - $135K",
        "url": "https://www.linkedin.com/jobs/view/3966694931",
        "summary": "Hayden AI is looking for a Software Engineer to design, develop, test, and document internal tools and scripts for their cloud team. The role will involve backend microservice development using Golang and Python, monitoring systems using Grafana, debugging existing software systems, and working with AWS tools. The ideal candidate will have 6 months of experience with Python, C++, and automated data annotation.",
        "industries": [
            "Artificial Intelligence",
            "Machine Learning",
            "Software Development",
            "Cloud Computing",
            "Government",
            "Transportation",
            "Technology"
        ],
        "soft_skills": [
            "Communication",
            "Problem Solving",
            "Teamwork",
            "Time Management",
            "Organization",
            "Analytical Skills",
            "Attention to Detail"
        ],
        "hard_skills": [
            "Python",
            "GoLang",
            "C++",
            "Grafana",
            "InfluxDB",
            "Postgres",
            "AWS",
            "S3",
            "EC2",
            "Automated Data Annotation",
            "Microservices"
        ],
        "tech_stack": [
            "Golang",
            "Python",
            "Grafana",
            "InfluxDB",
            "Postgres",
            "AWS",
            "S3",
            "EC2"
        ],
        "programming_languages": [
            "Python",
            "GoLang",
            "C++"
        ],
        "experience": 6,
        "education": {
            "min_degree": "Bachelor of Science or Arts",
            "fields": [
                "Computer Science",
                "Computer Engineering",
                "Electrical Engineering"
            ]
        },
        "salary": {
            "max": 135000,
            "min": 132870
        },
        "benefits": [
            "Medical, Dental, and Vision Insurance",
            "Flexible Spending Account",
            "Dependent Care Flexible Spending Account",
            "Life Insurance",
            "AD&D Insurance",
            "Short and Long Term Disability Insurance",
            "Critical Illness Insurance",
            "Accident Insurance",
            "Hospital Indemnity Insurance",
            "Legal Plan",
            "Pet Insurance",
            "Auto and Home Insurance",
            "401(k) with Company Matching",
            "Professional Development Reimbursement",
            "Unlimited PTO",
            "Hybrid Work Opportunities",
            "Catered Lunches"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3897981473,
        "company": "Unreal Staffing, Inc",
        "title": "Senior Data Systems Engineer",
        "created_on": 1720635591.813322,
        "description": "Company Overview: Welcome to the forefront of data-driven innovation! Our company is dedicated to harnessing the power of data to drive transformative change and solve complex problems across industries. We're committed to building cutting-edge data systems that enable efficient data management, processing, and analysis. Join us and be part of a dynamic team shaping the future of data systems engineering. Position Overview: As a Senior Data Systems Engineer, you'll play a critical role in designing, building, and optimizing our data systems infrastructure. You'll work on challenging projects, from architecting data storage solutions to implementing data processing frameworks, to support the needs of our data-driven organization. If you're a seasoned engineer with expertise in data systems technologies and a passion for building scalable and reliable data systems, we want you on our team. Requirements Key Responsibilities: Data Systems Architecture: Design and implement scalable and reliable data systems architecture to support the organization's data needs, including data storage, processing, and analytics Data Storage Solutions: Architect and implement data storage solutions, including relational databases, NoSQL databases, data warehouses, and data lakes, ensuring optimal performance, reliability, and scalability Data Processing Frameworks: Implement and optimize data processing frameworks and technologies, such as Apache Hadoop, Apache Spark, and Apache Flink, to enable efficient data processing and analysis Data Pipeline Development: Develop and maintain data pipeline solutions to ingest, transform, and deliver data from various sources to target systems, ensuring seamless data flow and interoperability Data Integration: Integrate data from diverse sources and systems into data systems infrastructure, ensuring data consistency, integrity, and security Data Governance: Establish and enforce data governance policies and procedures to ensure data quality, security, and compliance with regulatory requirements Performance Optimization: Optimize data systems performance through indexing, partitioning, and other techniques, ensuring scalability and responsiveness for analytical and reporting needs Monitoring and Alerting: Implement monitoring and alerting systems to track data systems performance and health, proactively identifying and resolving issues to minimize downtime and data loss Documentation and Best Practices: Document data systems designs, processes, and best practices, providing clear and comprehensive documentation to facilitate understanding and collaboration among team members Collaboration: Collaborate with cross-functional teams, including data engineers, data scientists, and business analysts, to understand requirements and deliver data systems solutions that meet business needs Mentorship and Development: Mentor and coach junior engineers, providing guidance, support, and opportunities for skill development and career growth Qualifications: Bachelor's degree or higher in Computer Science, Engineering, Mathematics, or related field 5+ years of experience in data engineering or systems engineering, with a focus on data systems technologies Proficiency in data storage technologies such as relational databases (e.g., PostgreSQL, MySQL), NoSQL databases (e.g., MongoDB, Cassandra), data warehouses (e.g., Snowflake, Redshift), and data lakes (e.g., Amazon S3, Azure Data Lake Storage) Strong programming skills in languages such as Python, Java, or Scala, with experience in data processing frameworks like Apache Spark or Apache Flink Experience with cloud platforms such as AWS, Azure, or Google Cloud Platform, and services like AWS Glue, Azure Data Factory, or Google Dataflow Strong understanding of data integration concepts and techniques, with experience integrating data from diverse sources and systems Strong problem-solving skills and analytical thinking, with the ability to design and troubleshoot complex data systems issues Excellent communication and collaboration skills, with the ability to work effectively in cross-functional teams and communicate technical concepts to non-technical stakeholders Benefits Competitive salary: The industry standard salary for Senior Data Systems Engineers typically ranges from $170,000 to $230,000 per year, depending on experience and qualifications Comprehensive health, dental, and vision insurance plans Flexible work hours and remote work options Generous vacation and paid time off Professional development opportunities, including access to training programs, conferences, and workshops State-of-the-art technology environment with access to cutting-edge tools and resources Vibrant and inclusive company culture with opportunities for growth and advancement Exciting projects with real-world impact at the forefront of data-driven innovation Join Us: Ready to shape the future of data systems engineering? Apply now to join our team and be part of the data revolution!",
        "url": "https://www.linkedin.com/jobs/view/3897981473",
        "summary": "This job description seeks a Senior Data Systems Engineer with 5+ years of experience to design, build, and optimize data systems infrastructure. This involves architecting data storage solutions, implementing data processing frameworks, and developing data pipelines. The ideal candidate will have strong expertise in data systems technologies, cloud platforms (AWS, Azure, GCP), and programming languages (Python, Java, Scala).",
        "industries": [
            "Data Engineering",
            "Software Engineering",
            "Technology",
            "Data Science",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Problem-solving",
            "Analytical Thinking",
            "Communication",
            "Collaboration",
            "Mentorship",
            "Leadership",
            "Teamwork"
        ],
        "hard_skills": [
            "Data Systems Architecture",
            "Data Storage Solutions",
            "Relational Databases",
            "NoSQL Databases",
            "Data Warehouses",
            "Data Lakes",
            "Data Processing Frameworks",
            "Apache Hadoop",
            "Apache Spark",
            "Apache Flink",
            "Data Pipeline Development",
            "Data Integration",
            "Data Governance",
            "Performance Optimization",
            "Monitoring and Alerting",
            "Documentation",
            "Python",
            "Java",
            "Scala",
            "AWS",
            "Azure",
            "GCP",
            "AWS Glue",
            "Azure Data Factory",
            "Google Dataflow"
        ],
        "tech_stack": [
            "PostgreSQL",
            "MySQL",
            "MongoDB",
            "Cassandra",
            "Snowflake",
            "Redshift",
            "Amazon S3",
            "Azure Data Lake Storage",
            "Apache Hadoop",
            "Apache Spark",
            "Apache Flink",
            "AWS Glue",
            "Azure Data Factory",
            "Google Dataflow",
            "Python",
            "Java",
            "Scala",
            "AWS",
            "Azure",
            "GCP"
        ],
        "programming_languages": [
            "Python",
            "Java",
            "Scala"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Computer Science",
                "Engineering",
                "Mathematics"
            ]
        },
        "salary": {
            "max": 230000,
            "min": 170000
        },
        "benefits": [
            "Competitive salary",
            "Health insurance",
            "Dental insurance",
            "Vision insurance",
            "Flexible work hours",
            "Remote work options",
            "Vacation",
            "Paid time off",
            "Professional development",
            "Training programs",
            "Conferences",
            "Workshops",
            "Cutting-edge technology",
            "Vibrant company culture",
            "Growth opportunities",
            "Advancement opportunities",
            "Exciting projects"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3952439068,
        "company": "Prelim",
        "title": "Software Engineer (SF Remote)",
        "created_on": 1720635595.3035834,
        "description": "About Prelim Prelim is a rapidly expanding, remote-only startup based in the U.S., with team members spread across the country. We're revolutionizing the way banks onboard customers, providing a seamless platform for both online and in-branch experiences. Our mission is to enhance access to banking, one financial institution at a time. If you're eager to shape the future of banking and thrive in a dynamic, fast-paced environment, we encourage you to apply. We're looking for driven, ambitious individuals ready to make a significant impact. Key Responsibilities: Design, develop, and maintain the technology that powers our digital account opening solutions Work closely with our product and development teams to bring new features and improvements to our platform Troubleshoot and debug customer issues, and build features to address them Collaborate with other teams to increase the scalability of our platform Help assess and recruit future engineers Qualifications: Bachelor's degree in Computer Science or related field Strong experience with modern front-end web development technologies (JavaScript, React, HTML, CSS) Experience with back-end technologies such as Node.js, Python, or Ruby Experience with databases such as PostgreSQL Strong problem-solving and analytical skills Excellent communication and teamwork abilities About Prelim: Prelim is a cutting-edge software provider that specializes in customer onboarding for banks. Our platform is designed to streamline the account opening process for both consumers and businesses, increasing speed-to-market and improving the customer experience. We pride ourselves on being a forward-thinking and innovative company, always on the lookout for new ways to improve our offerings and stay ahead of the curve in the banking industry. Our small team is made up of passionate and dedicated individuals who are committed to delivering exceptional results for our clients. As a member of the Prelim team, you will have the opportunity to work with cutting-edge technology and be at the forefront of the digital account opening industry. You will have the chance to work with a diverse range of clients, including some of the biggest names in the banking industry, and have an outsized impact. We offer a fast-paced and dynamic work environment, with ample opportunities for growth and advancement. If you're excited to be part of a team that is shaping the future of the banking industry, then we encourage you to apply to join us at Prelim. We are looking for individuals who are driven, ambitious, and excited about the opportunity to make a real impact in the digital account opening industry.",
        "url": "https://www.linkedin.com/jobs/view/3952439068",
        "summary": "Prelim is a remote-only startup revolutionizing bank customer onboarding. They're seeking a front-end developer with experience in JavaScript, React, HTML, CSS, and back-end tech like Node.js, Python, or Ruby, as well as databases like PostgreSQL. The role involves designing, developing, and maintaining digital account opening solutions, collaborating with product and development teams, troubleshooting issues, and contributing to scalability. The company offers a dynamic environment, growth opportunities, and the chance to impact the banking industry.",
        "industries": [
            "FinTech",
            "Banking",
            "Software Development",
            "Technology"
        ],
        "soft_skills": [
            "Problem-solving",
            "Analytical",
            "Communication",
            "Teamwork"
        ],
        "hard_skills": [
            "JavaScript",
            "React",
            "HTML",
            "CSS",
            "Node.js",
            "Python",
            "Ruby",
            "PostgreSQL"
        ],
        "tech_stack": [
            "JavaScript",
            "React",
            "HTML",
            "CSS",
            "Node.js",
            "Python",
            "Ruby",
            "PostgreSQL"
        ],
        "programming_languages": [
            "JavaScript",
            "Python",
            "Ruby"
        ],
        "experience": 0,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Related Field"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco Bay Area",
        "job_id": 3958516877,
        "company": "LHH",
        "title": "Senior Data Engineer",
        "created_on": 1720635596.9620738,
        "description": "About the Company: LHH is seeking a Sr. Data Engineer to join our client's team in a full time role. This is an on-site role for candidates who live within 30 miles of our client's San Francisco office, and hybrid for those with a longer commute. Responsibilities: Design and build infrastructure for extraction, transformation, loading, and cleaning of data from a wide variety of sources using APIs, SQL, and AWS 'big data' technologies Implement CI/CD pipelines for data operations to ensure smooth and efficient deployment of data models and applications Gather and process raw data at scale Service both ad-hoc requests as well as core pipeline development Develop and optimize our analytics engine, crafting new tools and solutions that enable stakeholders to consume and understand data more intuitively Work with a wide range of stakeholders and functional teams to drive business results through data-based insights Identify inefficiencies, optimize processes and data flows, and make recommendations for improvements Mentor junior engineers, providing guidance and support to foster a collaborative and innovative team environment. Offer technical insights and recommendations to the management team regarding data processing infrastructure and strategies. Qualifications: Bachelor’s degree or master’s degree in computer science or IT management or equivalent Authorization to work in the US A minimum of 8 years of engineering experience with at least 6 years of data engineering experience including data ingestion, data normalization, and data quality 4+ years of experience working with healthcare claims and clinical data sets 3+ years of experience in building data analytic engines 3+ years of experience in building applications using large data sets 2+ years of experience in Snowflake development Specific Skills and Knowledge: Proficiency in SQL, Python, Java Experience with FHIR and other healthcare data formats Demonstrated experience with cloud-based data warehousing solutions, preferably Snowflake Familiarity with HIPAA compliance and security compliance Preferred Skills: Snowflake SQL SAS analytics ETL DataOps (Database query) optimization HITRUST certification CPT ICD-10 Business intelligence (BI) tools DBT Terraform R Project Management and Leadership Abilities: Ability to coordinate cross-functional personnel, including internal personnel and customer and third-party personnel in achieving objectives and deadlines Ability to identify and address issues or roadblocks to project success Ability to enhance and improve the continuous quality improvement function for all projects Ability to leverage project management tools and practices as required to meet project objectives and deadlines",
        "url": "https://www.linkedin.com/jobs/view/3958516877",
        "summary": "Sr. Data Engineer needed for an on-site role in San Francisco.  Responsibilities include designing and building data infrastructure using AWS technologies, implementing CI/CD pipelines, gathering and processing raw data at scale, developing and optimizing analytics engines, collaborating with stakeholders, identifying inefficiencies, mentoring junior engineers, and offering technical insights to management.",
        "industries": [
            "Healthcare",
            "Information Technology",
            "Data Science",
            "Analytics"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Leadership",
            "Mentoring",
            "Project Management",
            "Time Management",
            "Analytical Thinking",
            "Critical Thinking",
            "Decision Making",
            "Adaptability"
        ],
        "hard_skills": [
            "SQL",
            "Python",
            "Java",
            "FHIR",
            "AWS",
            "Snowflake",
            "HIPAA",
            "HITRUST",
            "CPT",
            "ICD-10",
            "ETL",
            "DataOps",
            "Optimization",
            "SAS",
            "Business Intelligence",
            "DBT",
            "Terraform",
            "R"
        ],
        "tech_stack": [
            "AWS",
            "Snowflake",
            "FHIR",
            "HIPAA",
            "HITRUST",
            "CPT",
            "ICD-10",
            "ETL",
            "DataOps",
            "SAS",
            "Business Intelligence",
            "DBT",
            "Terraform"
        ],
        "programming_languages": [
            "SQL",
            "Python",
            "Java",
            "R"
        ],
        "experience": 8,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "IT Management"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Jose, CA",
        "job_id": 3815692642,
        "company": "Balbix",
        "title": "Data Engineer",
        "created_on": 1720635598.508501,
        "description": "Who We Are Balbix is the world's leading platform for cybersecurity posture automation company. The Balbix Security Cloud uses AI and automation to reinvent how the World's leading organizations reduce their cyber risk. With Balbix, security teams can accurately inventory their cloud and on-prem assests, conduct vulnerability management and quantify their cyber risk in monetary terms. Balbix counts many global 1000 companies among its rapidly growing customer base . We are backed by John Chambers (the former CEO and Chairman of Cisco) , top Silicon Valley VCs and global investors . We have been called magical , and have received raving reviews as well as customer testimonials , numerous industry awards , and recognition by Gartner as a Cool Vendor , and by Frost & Sullivan . About This Role As a senior data engineer you will work on complex data pipelines dealing with petabytes of data. Balbix platform is used as one of the critical security tools by the CIOs, CISOs, and the sec-ops teams of small, medium and large sized enterprises including Fortune 10 companies around the world. You will solve problems related to massive cybersecurity and IT data sets. You will collaborate closely with our data scientists, threat researchers and network experts to solve real-world problems plaguing cybersecurity. This role requires excellent algorithm, programming and testing skills as well as experience in large-scale data engineering projects. You Will Design and implement the features and own the modules for ingesting, storing and manipulating large data sets for a variety of cybersecurity use-cases Write code to provide backend support for data-driven UI widgets, web dashboards, workflows, search and API connectors Design and implement web services, rest APIs, and microservices Build production quality solutions that balance complexity and meet acceptance criteria of functional requirements Work with multiple-interfacing teams, including ML, UI, backend and data engineering You Are Driven to experience and learn more about design, architecture, and take on progressive roles Collaborative and comfortable working with across teams including data engineering, front end, product management, and DevOps Responsible and like to take ownership of challenging problems An effective communicator, including good documentation practices and articulating thought processes in a team setting Comfortable with working in an agile environment Curious about technology and the industry, and a constant learner You Have MS/BS +2 years in Computer Science or a related field Expert programming experience with Python, Java, or Scala Good working knowledge of SQL databases such as Postgres and NoSQL databases such as MongoDB, Cassandra, Redis Experience with search engine database such as ElasticSearch is preferred Time-series databases such as InfluxDB, Druid, Prometheus Strong computer science fundamentals: data structures, algorithms, and distributed systems Life @ Balbix At Balbix, we have built a culture that aligns to our values of ownership, customer focus, curiosity, tenacity, innovation, judgement, teamwork, communication, honesty and impact. In joining our team you’ll work with very motivated and knowledgeable people, build pioneering products and utilize cutting-edge technology. Our Balbix team members see rapid career growth opportunities stemming from our culture of alignment, bottom up innovation, our clarity of goals and unrelenting mission. Last but not least, developing the world's most advanced platform to address what the most important (and hardest) technology problem facing mankind today is exceptionally rewarding! Benefits & Perks Balbix offers comprehensive medical, dental, vision, life insurance and long-term disability coverage for you and your family. Our Flex Time Off policy encourages you to take time off when you need it because we know and value how hard you work. When it comes to our offices it’s location, location, location we’re right next door to Santana Row so you can enjoy your time in (and out) of the office! More information at https://www.balbix.com/company/careers/ Please reach out if you want a seat on our rocket-ship and are passionate about changing the cybersecurity equation. At Balbix we’re proud to be an equal opportunity workplace dedicated to equality, fairness and human kindness. APPLY FOR THIS JOB",
        "url": "https://www.linkedin.com/jobs/view/3815692642",
        "summary": "Balbix is seeking a Senior Data Engineer to design and implement complex data pipelines for their cybersecurity platform. The role involves working with petabytes of data, building backend support for data-driven UI widgets, web dashboards, workflows, search and API connectors, and collaborating with data scientists, threat researchers, and network experts. The ideal candidate will have strong programming skills in Python, Java, or Scala, experience with SQL and NoSQL databases, and a solid understanding of computer science fundamentals.",
        "industries": [
            "Cybersecurity",
            "Information Technology",
            "Software",
            "Data Science",
            "Artificial Intelligence"
        ],
        "soft_skills": [
            "Collaboration",
            "Communication",
            "Ownership",
            "Problem-solving",
            "Teamwork",
            "Curiosity",
            "Learning",
            "Documentation"
        ],
        "hard_skills": [
            "Python",
            "Java",
            "Scala",
            "SQL",
            "Postgres",
            "MongoDB",
            "Cassandra",
            "Redis",
            "ElasticSearch",
            "InfluxDB",
            "Druid",
            "Prometheus",
            "Data Structures",
            "Algorithms",
            "Distributed Systems"
        ],
        "tech_stack": [
            "Python",
            "Java",
            "Scala",
            "SQL",
            "Postgres",
            "MongoDB",
            "Cassandra",
            "Redis",
            "ElasticSearch",
            "InfluxDB",
            "Druid",
            "Prometheus",
            "AI",
            "Machine Learning",
            "Data Pipelines",
            "Web Services",
            "REST APIs",
            "Microservices"
        ],
        "programming_languages": [
            "Python",
            "Java",
            "Scala"
        ],
        "experience": 2,
        "education": {
            "min_degree": "MS/BS",
            "fields": [
                "Computer Science",
                "Related Field"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Medical",
            "Dental",
            "Vision",
            "Life Insurance",
            "Long-Term Disability",
            "Flex Time Off"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco Bay Area",
        "job_id": 3954951199,
        "company": "LHH",
        "title": "Senior Data Engineer",
        "created_on": 1720635600.1583,
        "description": "LHH Recruitment Solutions is looking for a Senior Data Engineer for a medical solutions company located in San Francisco, CA. Position is Hybrid, Direct Hire/W2, and it is Full Time. Apologies, this client isn't able to offer VISA or any other kind of sponsorship/transfer. Responsibilities: Handle the collection and processing of large-scale raw data. Develop and enhance our analytics engine by creating new tools and solutions that facilitate intuitive data consumption for stakeholders. Construct and improve infrastructure for data extraction, transformation, loading, and cleaning from diverse sources using APIs, SQL, and AWS ‘big data’ technologies. Implement continuous integration and continuous deployment (CI/CD) pipelines for data operations, ensuring efficient and smooth deployment of data models and applications. Address both ad-hoc data requests and core pipeline development tasks. Work closely with a range of stakeholders and functional teams to leverage data insights for driving business results. Detect inefficiencies, optimize processes and data flows, and suggest improvements. Provide mentorship to junior engineers, promoting a collaborative and innovative team atmosphere. Offer technical insights and strategic recommendations regarding data processing infrastructure to the management team. Qualifications: At least 7 years of experience in engineering. Minimum of 7 years in Big Data engineering, covering data ingestion, normalization, and quality assurance. 5 years of implementing CI/CD pipelines Over 2 years of experience handling healthcare claims and clinical data sets. More than 2 years developing data analytics engines and building applications using extensive data sets, preferably with healthcare data. At least 2 years of hands-on experience with Snowflake development 2 years in Python and SQL 3+ years of experience designing and building infrastructure for extraction, transformation, loading, and cleaning of data from a wide variety of sources using APIs, SQL, and AWS ‘big data’ technologies A bachelor's or master's degree in computer science, IT management, or a related field. Expertise in SAS analytics, and ETL. DBT, Terraform (managing SQL and documentation), Airflow experience. Expected salary compensation: 160k -220k standard PTO 40k standard medical, dental, and vision other company perks!",
        "url": "https://www.linkedin.com/jobs/view/3954951199",
        "summary": "LHH Recruitment Solutions is seeking a Senior Data Engineer for a medical solutions company in San Francisco, CA. This hybrid, full-time, direct-hire role requires 7+ years of engineering experience, including extensive experience in big data engineering, CI/CD pipelines, and handling healthcare claims and clinical data. Responsibilities include data collection, processing, analytics engine development, data infrastructure construction, CI/CD pipeline implementation, data requests, stakeholder collaboration, process optimization, mentorship, and strategic recommendations. Strong expertise in Snowflake, Python, SQL, AWS, SAS analytics, ETL, DBT, Terraform, and Airflow is required.  Compensation includes a salary of $160k-$220k, PTO, 401k, and standard medical, dental, and vision benefits.",
        "industries": [
            "Healthcare",
            "Medical Solutions",
            "Technology",
            "Data Engineering",
            "Analytics"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem-Solving",
            "Analytical Thinking",
            "Leadership",
            "Mentorship",
            "Strategic Thinking"
        ],
        "hard_skills": [
            "Data Engineering",
            "Big Data",
            "Data Ingestion",
            "Data Normalization",
            "Data Quality Assurance",
            "CI/CD Pipelines",
            "Healthcare Claims",
            "Clinical Data",
            "Data Analytics Engines",
            "Snowflake",
            "Python",
            "SQL",
            "AWS",
            "SAS Analytics",
            "ETL",
            "DBT",
            "Terraform",
            "Airflow"
        ],
        "tech_stack": [
            "Snowflake",
            "Python",
            "SQL",
            "AWS",
            "SAS Analytics",
            "ETL",
            "DBT",
            "Terraform",
            "Airflow"
        ],
        "programming_languages": [
            "Python",
            "SQL"
        ],
        "experience": 7,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Computer Science",
                "IT Management",
                "Related Fields"
            ]
        },
        "salary": {
            "max": 220000,
            "min": 160000
        },
        "benefits": [
            "PTO",
            "401k",
            "Medical",
            "Dental",
            "Vision",
            "Company Perks"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3813095369,
        "company": "1872 Consulting",
        "title": "Senior Data Engineer",
        "created_on": 1720635601.7712255,
        "description": "Senior Data Engineer SAN FRANCISCO, CA / ENGINEERING / FULL-TIME What will you be doing? Building fast, reliable, scalable data pipelines that process over billions of historical data points collected from tens of thousands of retail stores across the US which power our machine learning models. Integrating, analyzing, finding issues with, and solving shortcomings in the critical lifeblood of our system-the data our customers send us-that powers every aspect of our system. Maintaining, augmenting, and helping architect our data platform and data warehouses to ensure that our data and Client teams can efficiently process and access the data they need. Collaborating with an interdisciplinary team of experts in machine learning, data scientists, design, software engineering, and business process optimization What skills and experience do you need? Bachelors or Masters in Computer Science or equivalent. 3+ years of work experience. Strong programming and problem-solving skills. Expert-level knowledge of using SQL and data warehouses, data lakes, or databases. Experience with Apache Spark or other Big Data frameworks; working with cloud infrastructure; data visualization tools; and applied statistics is a big plus.",
        "url": "https://www.linkedin.com/jobs/view/3813095369",
        "summary": "As a Senior Data Engineer, you'll be responsible for building and maintaining data pipelines processing billions of data points from retail stores, integrating and analyzing data, architecting data platform and warehouses, and collaborating with a team of machine learning, data science, and software engineering experts.",
        "industries": [
            "Retail",
            "Technology",
            "Machine Learning",
            "Data Science"
        ],
        "soft_skills": [
            "Problem-solving",
            "Collaboration"
        ],
        "hard_skills": [
            "SQL",
            "Data Warehousing",
            "Data Lakes",
            "Databases",
            "Apache Spark",
            "Big Data",
            "Cloud Infrastructure",
            "Data Visualization",
            "Applied Statistics"
        ],
        "tech_stack": [
            "Apache Spark",
            "Cloud Infrastructure"
        ],
        "programming_languages": [
            "SQL"
        ],
        "experience": 3,
        "education": {
            "min_degree": "Bachelors",
            "fields": [
                "Computer Science"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3922624055,
        "company": "Midpoint Markets",
        "title": "Software Engineer",
        "created_on": 1720635603.6288714,
        "description": "About Midpoint Markets We are a small but growing financial science firm focused on solving complex challenges in cryptocurrency trading using a systematic and quantitative approach. Our team of scientists and engineers, comprising members with over 100 years experience in quantitative trading, is dedicate to delivering innovative, rigorous, and practical solutions to quickly evolving problems in a dynamic, fast-paced, and highly competitive environment. As we continue to grow, we are seeking the brightest and most enthusiastic team-oriented minds to join us. About The Role As a Software Engineer, you'll be responsible for building tools to scale out strategies to new markets. You will also help us build tools to progressively automate the management of our trading, respond to changing market dynamics, and develop the core infrastructure of our trading systems. What You'll Do Build complicated, highly performant trading systems. Work on fast, high-volume, distributed systems with many connections and interdependencies. Write code to acquire, process, and manipulate extremely large data sets. Ability to integrate low-level trading software with higher-level quantitative trading tools to facilitate trading. Build tools to monitor our trading and further improve our logging and monitoring processes Qualifications Broad experience with large complex systems Bachelor's or advanced degree in a technical or quantitative field Strong mathematical and analytical intuition and abilities Solid Python background Willingness to work in a 24/7 production environment Strong interest in trading Benefits 100% coverage of medical, dental, and vision insurance premiums Competitive 401k match Wellness benefits In-office lunches We are proud to be an equal opportunity workplace. We do not discriminate based upon race, religion, color, national origin, sex, sexual orientation, gender identity/expression, age, status as a protected veteran, status as an individual with a disability, or any other applicable legally protected characteristics.",
        "url": "https://www.linkedin.com/jobs/view/3922624055",
        "summary": "Midpoint Markets, a financial science firm specializing in cryptocurrency trading, is seeking a Software Engineer to build tools for scaling trading strategies, automating management, and developing core infrastructure. Responsibilities include building high-performance trading systems, working with large, distributed systems, manipulating massive datasets, integrating low-level trading software with quantitative tools, and enhancing monitoring and logging processes. The ideal candidate will have experience with complex systems, a strong mathematical and analytical background, solid Python skills, and a passion for trading.",
        "industries": [
            "Finance",
            "FinTech",
            "Cryptocurrency",
            "Quantitative Trading"
        ],
        "soft_skills": [
            "Team-Oriented",
            "Enthusiastic",
            "Problem-Solving",
            "Analytical",
            "Communication",
            "Collaboration",
            "Adaptable",
            "Passionate"
        ],
        "hard_skills": [
            "Software Engineering",
            "Python",
            "Distributed Systems",
            "Large Data Sets",
            "High-Performance Computing",
            "Trading Systems",
            "Monitoring",
            "Logging"
        ],
        "tech_stack": [
            "Python"
        ],
        "programming_languages": [
            "Python"
        ],
        "experience": 0,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Technical",
                "Quantitative"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Medical Insurance",
            "Dental Insurance",
            "Vision Insurance",
            "401k Match",
            "Wellness Benefits",
            "In-Office Lunches"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Los Angeles Metropolitan Area",
        "job_id": 3970907300,
        "company": "Beacon Hill",
        "title": "Big Data Sr. Software Engineer (Hybrid: LA, SF, or Seattle)",
        "created_on": 1720635605.2154934,
        "description": "Big Data Sr. Software Engineer Hybrid (LA, SF, Seattle) 18 month contract $80/hour Basic Qualifications: 3+ years of meaningful relevant work experience Strong programming (Scala/Java) and shell scripting skills Experience with real-time processing technologies like Spark/Storm/Flink Experience with Hadoop/Hive Knowledge of cloud big data services and technologies Knowledge of lambda architecture and data-warehousing concepts Preferred Qualifications: Self-motivated, collaborative, proactive, and capable of committing to results under pressure and ambiguity Experience in Software Development Lifecycle and its activities Experience with Couchbase and/or MemSQL Preferred Qualifications Required Education BS/MS in Computer Science or related field Preferred Education Job Summary: Data Platforms team is seeking a Software Developer who will be an extraordinary addition to our growing team. As a Software Developer, you will help build our next-generation data analytics platform utilizing the most advanced big data technologies. You should be willing to go to the depths to tackle complex problems and have the curiosity to explore and learn new technologies for innovative solutions. Are you someone who enjoys answering difficult data questions and enjoy the startup play hard/work hard environment? If so, then this is a great role for you! Responsibilities: Design, architect, develop, scale, and improve our data platform and its services Implement large-scale high-throughput solutions using on-premise and cloud technologies such as Kafka, Spark on Hadoop/Hive/S3, AWS Lambda, Terraform, Kubernetes, etc. Participate in novel solutions to challenging technical problems, introduce new technologies to our data platform and its services Diagnose and debug issues in development, staging and production environments Collaborate with product/program managers, software developers and analysts in an open, creative environment Beacon Hill is an Equal Opportunity Employer that values the strength diversity brings to the workplace. Individuals with Disabilities and Protected Veterans are encouraged to apply. If you would like to complete our voluntary self-identification form, please click here or copy and paste the following link into an open window in your browser: https://jobs.beaconhillstaffing.com/eeoc/ Completion of this form is voluntary and will not affect your opportunity for employment, or the terms or conditions of your employment. This form will be used for reporting purposes only and will be kept separate from all other records. Company Profile: Beacon Hill Technologies, a premier National Information Technology Staffing Group, provides world class technology talent across all industries utilizing a complete suite of staffing services.  Beacon Hill Technologies' dedicated team of recruiting and staffing experts consistently delivers quality IT professionals to solve our customers' technical and business needs. Beacon Hill Technologies covers a broad spectrum of IT positions, including Project Management and Business Analysis, Programming/Development, Database, Infrastructure, Quality Assurance, Production/Support and ERP roles. Learn more about Beacon Hill Staffing Group and our specialty divisions, Beacon Hill Associates, Beacon Hill Financial, Beacon Hill HR, Beacon Hill Legal, Beacon Hill Life Sciences and Beacon Hill Technologies by visiting www.beaconhillstaffing.com. We look forward to working with you. Beacon Hill. Employing the Future™",
        "url": "https://www.linkedin.com/jobs/view/3970907300",
        "summary": "Data Platforms team is seeking a Software Developer to build the next-generation data analytics platform utilizing advanced big data technologies. The role involves designing, architecting, developing, scaling, and improving the data platform and its services. This is a great role for individuals who enjoy answering difficult data questions and thrive in a fast-paced environment.",
        "industries": [
            "Technology",
            "Data Analytics",
            "Software Development"
        ],
        "soft_skills": [
            "Self-motivated",
            "Collaborative",
            "Proactive",
            "Results-oriented",
            "Problem-solving",
            "Communication",
            "Teamwork"
        ],
        "hard_skills": [
            "Scala",
            "Java",
            "Shell Scripting",
            "Spark",
            "Storm",
            "Flink",
            "Hadoop",
            "Hive",
            "Cloud Big Data Services",
            "Lambda Architecture",
            "Data Warehousing",
            "Kafka",
            "AWS Lambda",
            "Terraform",
            "Kubernetes",
            "Couchbase",
            "MemSQL"
        ],
        "tech_stack": [
            "Spark",
            "Storm",
            "Flink",
            "Hadoop",
            "Hive",
            "Kafka",
            "AWS Lambda",
            "Terraform",
            "Kubernetes",
            "Couchbase",
            "MemSQL"
        ],
        "programming_languages": [
            "Scala",
            "Java"
        ],
        "experience": 3,
        "education": {
            "min_degree": "BS",
            "fields": [
                "Computer Science",
                "Related Field"
            ]
        },
        "salary": {
            "max": 80,
            "min": 80
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Fernando, CA",
        "job_id": 3963961064,
        "company": "ClifyX",
        "title": "Big Data Engineer",
        "created_on": 1720635607.1413393,
        "description": "Big Data Engineer Locations AZ - Phoenix,CA - Los Angeles,CA - San Francisco,WA - Seattle JD:- The Work: Work with clients to grasp their business philosophy and IT strategy; act as a beacon of our vision that the purpose of data pipelines is to help clients make better decisions by giving them the right data at the right time. Design, build, and implement advanced data pipelines that bring together data from a range of sources. Make sure the data is accessible to data scientists, analysts, and other users via a variety of programming languages. Carry out complex Big Data projects that collect, parse, manage, analyze, and visualize large data sets, providing insights that clients can act on across their customer-facing platforms. Understand the challenges being addressed by an engagement and collaborate with team members and clients to deliver a technical solution that meets the unique needs of our clients. Describe technical solutions to appropriate audiences by creating high quality content. Mentor and lead more junior team members. Racking up those air miles will have to wait, as weekly non-essential travel to client sites is currently suspended. For now, all business travel, international and domestic, is currently restricted to client-essential sales/delivery activity only. Qualification:- Here's What You Need: Minimum of 2 years of hands-on technical experience implementing or supporting Big Data solutions utilizing Hadoop, Hive and Python Experience developing solutions utilizing at least two of the following: Kafka based streaming services R Studio Cassandra , MongoDB MapReduce, Pig, Hive Scala, Spark Knowledge on Jenkins, Chef, Puppet Bachelor's Degree or equivalent work experience of 12 years or Associate's Degree or 6 years of work experience Bonus Points if: You've got experience of full life-cycle development You have experience with delivering Big Data Solutions in the cloud with AWS or Azure You can configure and support API and Open Source integrations You have experience administering Hadoop or other Data Science and Analytics platforms using the technologies above [LC1] You're no newbie when it comes to working in a DevOps environment You're familiar with designing ingestion, low-latency, visualization clusters to sustain data loads Professional Skill Requirements Proven success in contributing to a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication (written and oral) and interpersonal skills Ability to work with senior client executives",
        "url": "https://www.linkedin.com/jobs/view/3963961064",
        "summary": "Big Data Engineer role responsible for designing, building, and implementing data pipelines, working with clients to understand their business needs, and delivering insights from large datasets. Requires experience with Hadoop, Hive, Python, and at least two of the following: Kafka, R Studio, Cassandra, MongoDB, MapReduce, Pig, Hive, Scala, Spark, Jenkins, Chef, Puppet.  Experience with cloud platforms (AWS or Azure) is a plus.",
        "industries": [
            "Data Science",
            "Analytics",
            "Software Development",
            "Information Technology",
            "Consulting"
        ],
        "soft_skills": [
            "Communication",
            "Interpersonal",
            "Problem Solving",
            "Teamwork",
            "Creativity",
            "Analytical",
            "Leadership",
            "Mentoring"
        ],
        "hard_skills": [
            "Hadoop",
            "Hive",
            "Python",
            "Kafka",
            "R Studio",
            "Cassandra",
            "MongoDB",
            "MapReduce",
            "Pig",
            "Scala",
            "Spark",
            "Jenkins",
            "Chef",
            "Puppet",
            "AWS",
            "Azure",
            "API",
            "Open Source",
            "DevOps"
        ],
        "tech_stack": [
            "Hadoop",
            "Hive",
            "Python",
            "Kafka",
            "R Studio",
            "Cassandra",
            "MongoDB",
            "MapReduce",
            "Pig",
            "Scala",
            "Spark",
            "Jenkins",
            "Chef",
            "Puppet",
            "AWS",
            "Azure",
            "API",
            "Open Source",
            "DevOps"
        ],
        "programming_languages": [
            "Python",
            "Scala",
            "R"
        ],
        "experience": 2,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Information Technology",
                "Data Science",
                "Analytics"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Diego, CA",
        "job_id": 3905546385,
        "company": "eTeam",
        "title": "Data Engineer",
        "created_on": 1720635608.752971,
        "description": "4-5 years of experience with building end-to-end Data Lake using AWS technologies (S3, Glue, Athena, RedShift) Design, develop and support data pipelines in a hybrid cloud environment to enable advanced analytics Experience with scheduling data pipelines using Apache Airflow Ability to develop new services in AWS using server-less or container-based Experience with CI/CD of data pipelines and micro-services Hands-on experience with DevOps using Jenkins and AWS CDK Strong Python programming background with advanced SQL knowledge Basic DWH knowledge along with some ETL tool exposure Hands-on experience with Docker/Kubernetes Experience working in Agile Scrum methodology",
        "url": "https://www.linkedin.com/jobs/view/3905546385",
        "summary": "Data Engineer with 4-5 years experience building data lakes on AWS, designing and developing data pipelines, implementing CI/CD, and working with DevOps using Jenkins and AWS CDK. Strong Python and SQL skills, experience with Airflow, Docker/Kubernetes, and agile methodologies.",
        "industries": [
            "Data Engineering",
            "Cloud Computing",
            "Software Development"
        ],
        "soft_skills": [
            "Problem Solving",
            "Communication",
            "Teamwork",
            "Time Management"
        ],
        "hard_skills": [
            "AWS",
            "S3",
            "Glue",
            "Athena",
            "Redshift",
            "Data Pipelines",
            "Apache Airflow",
            "Serverless Computing",
            "Containerization",
            "CI/CD",
            "Jenkins",
            "AWS CDK",
            "Python",
            "SQL",
            "DWH",
            "ETL",
            "Docker",
            "Kubernetes",
            "Agile",
            "Scrum"
        ],
        "tech_stack": [
            "AWS",
            "S3",
            "Glue",
            "Athena",
            "Redshift",
            "Apache Airflow",
            "Docker",
            "Kubernetes",
            "Jenkins",
            "AWS CDK",
            "Python"
        ],
        "programming_languages": [
            "Python",
            "SQL"
        ],
        "experience": 4,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Data Science",
                "Engineering"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Atherton, CA",
        "job_id": 3926154951,
        "company": "Backbone",
        "title": "Junior Software Engineer, Backend",
        "created_on": 1720635612.9482188,
        "description": "Backbone is looking for a talented junior backend engineer . A backend engineer is responsible for building features, tools, and new experiences for the Backbone application. You will work with other members of the backend team, the client team, data science, product, and many others to turn ideas into reality. You will be critical to delivering on initiatives that will provide the best experience to users who want to get the best mobile game controller on the market. You will get to help influence the product roadmap for the software initiatives. Use modern technologies and help drive the technical roadmap to ensure Backbone has an awesome codebase. This is a hybrid position. Join us in the office in Seattle, WA or Atherton, CA once a week ( partial in-person required, relocation offered ). Responsibilities Develop, architect, and ship new features and tools for our application and other services. Build elegant and maintainable code, writing automated tests to enable us to move fast and deliver quality. Ensure code quality and maintainability by implementing best practices, performing code reviews, and establishing automated testing frameworks. Collaborate with cross-functional teams to translate business requirements into technical specifications and ensure seamless integration of APIs for our client teams. Research new technologies and tooling to help improve our backend stack. Play a pivotal role in migrating our backend services from Node.js to Go. Qualifications 1-3 years of experience in software development Experience with Go or TypeScript Strong computer science and software engineering fundamentals Understanding of distributed systems and cloud architectures with an understanding of their strengths and weaknesses Experience with modern software development practices, version control systems, and agile methodologies Bonus points Experience building GraphQL services Direct experience with Google Cloud Platform or AWS",
        "url": "https://www.linkedin.com/jobs/view/3926154951",
        "summary": "Backbone is seeking a Junior Backend Engineer to develop features and tools for their application. The role involves building maintainable code, collaborating with cross-functional teams, and researching new technologies. Experience with Go or TypeScript and a strong understanding of distributed systems are required. This is a hybrid position with in-person requirements in Seattle, WA or Atherton, CA.",
        "industries": [
            "Technology",
            "Software Development",
            "Gaming"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Critical Thinking",
            "Time Management",
            "Organization",
            "Adaptability",
            "Teamwork"
        ],
        "hard_skills": [
            "Go",
            "TypeScript",
            "Node.js",
            "GraphQL",
            "Distributed Systems",
            "Cloud Architectures",
            "Software Development",
            "Code Reviews",
            "Automated Testing",
            "Version Control",
            "Agile Methodologies"
        ],
        "tech_stack": [
            "Go",
            "TypeScript",
            "Node.js",
            "GraphQL",
            "Google Cloud Platform",
            "AWS"
        ],
        "programming_languages": [
            "Go",
            "TypeScript",
            "Node.js"
        ],
        "experience": 1,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Software Engineering"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Relocation Offered",
            "Hybrid Work"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3600360354,
        "company": "Notion",
        "title": "Software Engineer, Growth",
        "created_on": 1720635614.5986912,
        "description": "About Us We're on a mission to make it possible for every person, team, and company to be able to tailor their software to solve any problem and take on any challenge. Computers may be our most powerful tools, but most of us can't build or modify the software we use on them every day. At Notion, we want to change this with focus, design, and craft. We've been working on this together since 2016, and have customers like Pixar, Mitsubishi, Figma, Plaid, Match Group, and thousands more on this journey with us. Today, we're growing fast and excited for new teammates to join us who are the best at what they do. We're passionate about building a company as diverse and creative as the millions of people Notion reaches worldwide. Notion is an in-person company, and currently requires its employees to come to the office for two Anchor Days (Mondays & Thursdays) and requests that employees spend the majority of their week in the office (including a third day). About The Role Notion's Growth Engineers make our mission of software ubiquity a reality. Using experiments, creativity, and feature creation, this team accelerates the value of Notion to an ever growing audience of users. We are owners of the ins and outs of our business, using a blend of data, qualitative research, and product intuition to accelerate our impact at this critical time in our company’s journey. What You'll Achieve Deliver experiments end-to-end, from ideation to analysis, with velocity and quality to move core business metrics Get to know the Notion customer to understand key points in the customer journey and uncover insights that will drive activation and engagement Plan, shape, and build new features to create a more engaging and personalized early product experience for our customers Work at all levels of the stack from AWS + Node + SQL to React + HTML + CSS Skills You'll Need To Bring Shipping polished user interfaces: You can work with others to build and maintain quality user interfaces for a big audience. You have 2+ years of experience and are comfortable using Web technologies like HTML, CSS, JavaScript, and a modern UI framework like React. If you've experienced the technical challenges of rapid growth, that's a plus. Data and experimentation: You have Growth Engineering experience at another company. You are comfortable iteratively testing product flows using A/B testing to understand customer behavior and make data-informed decisions. Thoughtful problem-solving: For you, problem-solving starts with a clear and accurate understanding of the context. You can decompose tricky problems and work towards a clean solution, by yourself or with teammates. You're comfortable asking for help when you get stuck. Pragmatism and impact-orientation: You care about business impact and prioritize projects accordingly. You're not just going after cool stuff—you understand the balance between craft, speed, and the bottom line. Putting users first: You think critically about the implications of what you're building, and how it shapes real people's lives. You understand that reach comes with responsibility for our impact—good and bad. Empathetic communication: You communicate nuanced ideas clearly, whether you're explaining technical decisions in writing or brainstorming in real time. In disagreements, you engage thoughtfully with other perspectives and compromise when needed. Team player: For you, work isn't a solo endeavor. You enjoy collaborating cross-functionally to accomplish shared goals, and you care about learning, growing, and helping others to do the same. Nice To Haves A history of strong cross-functional collaboration with marketing, sales, and/or other non-technical teams You've heard of computing pioneers like Ada Lovelace, Douglas Engelbart, Alan Kay, and others—and understand why we're big fans of their work. We hire talented and passionate people from a variety of backgrounds because we want our global employee base to represent the wide diversity of our customers. If you’re excited about a role but your past experience doesn’t align perfectly with every bullet point listed in the job description, we still encourage you to apply. If you’re a builder at heart, share our company values, and enthusiastic about making software toolmaking ubiquitous, we want to hear from you. Notion is proud to be an equal opportunity employer. We do not discriminate in hiring or any employment decision based on race, color, religion, national origin, age, sex (including pregnancy, childbirth, or related medical conditions), marital status, ancestry, physical or mental disability, genetic information, veteran status, gender identity or expression, sexual orientation, or other applicable legally protected characteristic. Notion considers qualified applicants with criminal histories, consistent with applicable federal, state and local law. Notion is also committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. If you need assistance or an accommodation made due to a disability, please let your recruiter know. Notion is committed to providing highly competitive cash compensation, equity, and benefits. The compensation offered for this role will be based on multiple factors such as location, the role’s scope and complexity, and the candidate’s experience and expertise, and may vary from the range provided below. For roles based in San Francisco and New York City, the estimated base salary range for this role is $130,000 - $250,000 per year.",
        "url": "https://www.linkedin.com/jobs/view/3600360354",
        "summary": "Notion is seeking a Growth Engineer to accelerate the value of their product to an expanding user base. This role involves designing and conducting experiments, analyzing customer behavior, developing new features to enhance user experience, and working across the entire tech stack (AWS, Node, SQL, React, HTML, CSS). Ideal candidates have experience in Growth Engineering, data analysis, A/B testing, user interface development, and problem-solving.  The role emphasizes  collaborative work, user-centricity, and a strong understanding of business impact.",
        "industries": [
            "Software",
            "Technology",
            "SaaS"
        ],
        "soft_skills": [
            "Communication",
            "Empathy",
            "Problem-Solving",
            "Teamwork",
            "Collaboration",
            "Creativity",
            "Data Analysis",
            "Critical Thinking",
            "Decision Making",
            "Pragmatism",
            "User-Centricity",
            "Impact-Orientation"
        ],
        "hard_skills": [
            "A/B Testing",
            "Experimentation",
            "Product Development",
            "User Interface Design",
            "Web Development",
            "HTML",
            "CSS",
            "JavaScript",
            "React",
            "AWS",
            "Node",
            "SQL"
        ],
        "tech_stack": [
            "AWS",
            "Node",
            "SQL",
            "React",
            "HTML",
            "CSS"
        ],
        "programming_languages": [
            "JavaScript"
        ],
        "experience": 2,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 250000,
            "min": 130000
        },
        "benefits": [
            "Competitive Cash Compensation",
            "Equity",
            "Benefits"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "California, United States",
        "job_id": 3929738691,
        "company": "Enexus Global Inc.",
        "title": "Senior Data Engineer - Remote",
        "created_on": 1720635616.425907,
        "description": "Senior Data Engineer Location - Remote Contract Type - W2/C2C/1099 Minimum Experience - 12+ Years Responsibilities Develop and enhance data-processing, orchestration, monitoring, and more by leveraging popular open-source software, AWS, and GitLab automation. Collaborate with product and technology teams to design and validate the capabilities of the data platform Identify, design, and implement process improvements: automating manual processes, optimizing for usability, re-designing for greater scalability Provide technical support and usage guidance to the users of our platform's services. Drive the creation and refinement of metrics, monitoring, and alerting mechanisms to give us the visibility we need into our production services. Qualifications Experience building and optimizing data pipelines in a distributed environment Experience supporting and working with cross-functional teams Proficiency working in Linux environment 8+ years of advanced working knowledge of SQL, Python, and PySpark 5+ years of experience with using a broad range of AWS technologies Experience using tools such as: Git/Bitbucket, Jenkins/CodeBuild, CodePipeline Experience with platform monitoring and alerts tools Thanks & Regards Sahil 510-925-0283 EXT 131",
        "url": "https://www.linkedin.com/jobs/view/3929738691",
        "summary": "Senior Data Engineer role focused on building and optimizing data pipelines in a distributed environment, leveraging AWS, GitLab automation, and open-source software. Responsibilities include collaborating with product and tech teams, identifying process improvements, providing technical support, and driving data platform monitoring.",
        "industries": [
            "Data & Analytics",
            "Software Development",
            "Technology"
        ],
        "soft_skills": [
            "Collaboration",
            "Communication",
            "Problem Solving",
            "Technical Support",
            "Process Improvement",
            "Optimization"
        ],
        "hard_skills": [
            "Data Processing",
            "Orchestration",
            "Monitoring",
            "AWS",
            "GitLab Automation",
            "Open Source Software",
            "SQL",
            "Python",
            "PySpark",
            "Linux",
            "Git",
            "Bitbucket",
            "Jenkins",
            "CodeBuild",
            "CodePipeline",
            "Platform Monitoring",
            "Alerting"
        ],
        "tech_stack": [
            "AWS",
            "GitLab",
            "Open Source Software",
            "SQL",
            "Python",
            "PySpark",
            "Linux",
            "Git",
            "Bitbucket",
            "Jenkins",
            "CodeBuild",
            "CodePipeline"
        ],
        "programming_languages": [
            "SQL",
            "Python",
            "PySpark"
        ],
        "experience": 12,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Los Gatos, CA",
        "job_id": 3970387451,
        "company": "Netflix",
        "title": "Software Engineer (L4) - Continuous Integration",
        "created_on": 1720635618.1243787,
        "description": "Netflix is one of the world's leading entertainment services, with 270 million paid memberships in over 190 countries, enjoying TV series, films, and games across a wide variety of genres and languages. Members can play, pause, and resume watching as much as they want, anytime, anywhere, and can change their plans at any time. Netflix’s source code is distributed across tens of thousands of code repositories written in numerous programming languages managed by thousands of engineers. Engineers are constantly making changes to these repositories, which are being built and tested on Netflix's continuous integration infrastructure, powered by Jenkins. Our Jenkins based infrastructure is hosted across 30+ controllers and 1000+ build agents to run over tens of thousands of build jobs a day. In addition to this mature CI platform, Netflix is currently exploring the future of CI to understand how we might address the needs of our users in the coming years. We are looking for engineers to join this team to manage and evolve our CI platform. Your Day-to-Day Driving feature work and taking ownership of services that are a part of Netflix’s Continuous Integration platform that operates and manages various aspects of the CI tooling. Operate and maintain Jenkins and its build agents while keeping plugins and version upgrades current. Collaborate with various other Platform teams to ensure their systems can integrate with our CI Platform Support cross-organization critical change campaigns across a wide variety of source code repositories Support customers with a variety of build and environment needs Work cross-functionally to build new services, tools, and data models to operationalize our workflows at scale. Participate in product reviews and team meetings, providing technical insight. Help scope, estimate, and prioritize between conflicting needs. Evaluate new technologies and approaches to streamline and improve our rapid application development tooling and conventions. Regularly learn new systems and tools as the Netflix platform and ecosystem evolve. Participate in our on-call rotation and contribute to incident reviews. You May Enjoy Working With Us If You are self-driven and highly motivated to deliver top-tier solutions with minimal guidance. You also recognize when you're wrong, learning from your mistakes and moving past them. You enjoy collaborating and pairing with engineers to understand what they want, figure out what they really need, and finally, what we should build. You strive to embrace best practices and are always searching for opportunities to continually improve. We Would Love Working With You If You are excited to be working with Java or other JVM languages You have experience running and operating continuous integration systems like Jenkins, CircleCI, GitHub Actions and others You are passionate about solving developer productivity challenges for engineers at the enterprise scale. You are adept at designing and building APIs for customers, in particular with GraphQL You excel at working with microservice architectures. You are experienced with relational and non-relational data stores Here’s Some Stuff We’ve Worked On How We Build Code at Netflix Towards True Continuous Integration Evolving Continuous Integration at Netflix How Netflix Autoscales CI Our compensation structure consists solely of an annual salary; we do not have bonuses. You choose each year how much of your compensation you want in salary versus stock options. To determine your personal top of market compensation, we rely on market indicators and consider your specific job family, background, skills, and experience to determine your compensation in the market range. The range for this role is $100,000- $720,000. Netflix provides comprehensive benefits including Health Plans, Mental Health support, a 401(k) Retirement Plan with employer match, Stock Option Program, Disability Programs, Health Savings and Flexible Spending Accounts, Family-forming benefits, and Life and Serious Injury Benefits. We also offer paid leave of absence programs. Full-time hourly employees accrue 35 days annually for paid time off to be used for vacation, holidays, and sick paid time off. Full-time salaried employees are immediately entitled to flexible time off. See more details about our Benefits here. Netflix is a unique culture and environment. Learn more here. We are an equal-opportunity employer and celebrate diversity, recognizing that diversity of thought and background builds stronger teams. We approach diversity and inclusion seriously and thoughtfully. We do not discriminate on the basis of race, religion, color, ancestry, national origin, caste, sex, sexual orientation, gender, gender identity or expression, age, disability, medical condition, pregnancy, genetic makeup, marital status, or military service.",
        "url": "https://www.linkedin.com/jobs/view/3970387451",
        "summary": "Netflix is seeking engineers to join their Continuous Integration team and manage their Jenkins-based infrastructure, which runs tens of thousands of build jobs daily. The role involves operating and maintaining Jenkins, collaborating with other platform teams, supporting change campaigns, and working cross-functionally to build new services. Ideal candidates have experience with Java/JVM languages, CI systems like Jenkins/CircleCI/GitHub Actions, and microservice architectures. They are adept at API design (especially with GraphQL) and working with relational/non-relational data stores.",
        "industries": [
            "Technology",
            "Software Development",
            "Entertainment",
            "Streaming"
        ],
        "soft_skills": [
            "Self-driven",
            "Motivated",
            "Collaborative",
            "Problem-solving",
            "Communication",
            "Analytical",
            "Detail-oriented",
            "Technical",
            "Adaptable",
            "Teamwork",
            "Problem Solving",
            "Communication",
            "Analytical",
            "Detail-oriented",
            "Technical",
            "Adaptable",
            "Teamwork"
        ],
        "hard_skills": [
            "Jenkins",
            "Java",
            "JVM languages",
            "CircleCI",
            "GitHub Actions",
            "API design",
            "GraphQL",
            "Microservice architectures",
            "Relational databases",
            "Non-relational databases"
        ],
        "tech_stack": [
            "Jenkins",
            "Java",
            "JVM languages",
            "CircleCI",
            "GitHub Actions",
            "GraphQL",
            "Microservices"
        ],
        "programming_languages": [
            "Java"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 720000,
            "min": 100000
        },
        "benefits": [
            "Health Plans",
            "Mental Health support",
            "401(k) Retirement Plan with employer match",
            "Stock Option Program",
            "Disability Programs",
            "Health Savings and Flexible Spending Accounts",
            "Family-forming benefits",
            "Life and Serious Injury Benefits",
            "Paid leave of absence programs",
            "Paid time off",
            "Flexible time off"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Mountain View, CA",
        "job_id": 3956351338,
        "company": "Google",
        "title": "Software Engineer III, Chrome",
        "created_on": 1720635619.8983016,
        "description": "Note: By applying to this position you will have an opportunity to share your preferred working location from the following: Mountain View, CA, USA; Seattle, WA, USA . Minimum qualifications: Bachelor’s degree or equivalent practical experience. 2 years of experience with software development in one or more programming languages, or 1 year of experience with an advanced degree in an industry setting. 2 years of experience with data structures or algorithms in either an academic or industry setting. Preferred qualifications: Master's degree or PhD in Computer Science or related technical fields. 2 years of experience with performance, large scale systems data analysis, visualization tools, and/or debugging. Experience developing accessible technologies. Proficiency in code and system health, diagnosis and resolution, and software test engineering. About The Job Google's software engineers develop the next-generation technologies that change how billions of users connect, explore, and interact with information and one another. Our products need to handle information at massive scale, and extend well beyond web search. We're looking for engineers who bring fresh ideas from all areas, including information retrieval, distributed computing, large-scale system design, networking and data storage, security, artificial intelligence, natural language processing, UI design and mobile; the list goes on and is growing every day. As a software engineer, you will work on a specific project critical to Google’s needs with opportunities to switch teams and projects as you and our fast-paced business grow and evolve. We need our engineers to be versatile, display leadership qualities and be enthusiastic to take on new problems across the full-stack as we continue to push technology forward. With your technical expertise you will manage project priorities, deadlines, and deliverables. You will design, develop, test, deploy, maintain, and enhance software solutions. Chrome is dedicated to building a better, more open web. We’re focused on making a better browser (on both desktop and mobile) to help users take advantage of all the web has to offer in a safe and secure way.Chrome is available across all major platforms — iOS, Android, Windows, Mac, Linux and Chrome OS. We also built Chrome as an open source project so the entire web ecosystem could benefit from the latest innovations in speed, simplicity and security. The US base salary range for this full-time position is $136,000-$200,000 + bonus + equity + benefits. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process. Please note that the compensation details listed in US role postings reflect the base salary only, and do not include bonus, equity, or benefits. Learn more about benefits at Google . Responsibilities Write product or system development code. Participate in, or lead design reviews with peers and stakeholders to decide amongst available technologies. Review code developed by other developers and provide feedback to ensure best practices (e.g., style guidelines, checking code in, accuracy, testability, and efficiency). Contribute to existing documentation or educational content and adapt content based on product/program updates and user feedback. Triage product or system issues and debug/track/resolve by analyzing the sources of issues and the impact on hardware, network, or service operations and quality. Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing our Accommodations for Applicants form .",
        "url": "https://www.linkedin.com/jobs/view/3956351338",
        "summary": "Google is seeking a Software Engineer to join their Chrome team. The engineer will contribute to the development and maintenance of the Chrome browser across various platforms, including desktop and mobile.  Responsibilities include writing product/system development code, design reviews, code reviews, documentation/educational content, issue triaging/debugging, and resolving issues related to hardware, network, or service operations. The role involves working on a specific project critical to Google's needs with opportunities to switch teams and projects as the business grows and evolves.",
        "industries": [
            "Technology",
            "Software Development",
            "Web Development",
            "Internet",
            "Browser Development"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Leadership",
            "Problem Solving",
            "Analytical Skills",
            "Critical Thinking",
            "Time Management",
            "Organization",
            "Attention to Detail"
        ],
        "hard_skills": [
            "Software Development",
            "Programming Languages",
            "Data Structures",
            "Algorithms",
            "Performance Analysis",
            "Large Scale Systems",
            "Data Analysis",
            "Visualization Tools",
            "Debugging",
            "Accessibility",
            "Code Health",
            "System Diagnosis",
            "Software Test Engineering"
        ],
        "tech_stack": [
            "Chrome",
            "iOS",
            "Android",
            "Windows",
            "Mac",
            "Linux",
            "Chrome OS"
        ],
        "programming_languages": [],
        "experience": 2,
        "education": {
            "min_degree": "Bachelor’s degree",
            "fields": [
                "Computer Science",
                "Related Technical Fields"
            ]
        },
        "salary": {
            "max": 200000,
            "min": 136000
        },
        "benefits": [
            "Bonus",
            "Equity",
            "Benefits"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Mountain View, CA",
        "job_id": 3868947359,
        "company": "LinkedIn",
        "title": "Staff Software Engineer, Systems Infrastructure",
        "created_on": 1720635621.5301573,
        "description": "Company Description LinkedIn is the world’s largest professional network, built to create economic opportunity for every member of the global workforce. Our products help people make powerful connections, discover exciting opportunities, build necessary skills, and gain valuable insights every day. We’re also committed to providing transformational opportunities for our own employees by investing in their growth. We aspire to create a culture that’s built on trust, care, inclusion, and fun – where everyone can succeed. Join us to transform the way the world works. Job Description This role can be based in Mountain View, CA, San Francisco, CA, or Bellevue, WA. At LinkedIn, we trust each other to do our best work where it works best for us and our teams. This role offers a hybrid work option, meaning you can both work from home and commute to a LinkedIn office, depending on what’s best for you and when it is important for your team to be together. As part of our world-class software engineering team, you will be charged with building the next-generation infrastructure and platforms for LinkedIn, including but not limited to: an application and service delivery platform,compute platform, massively scalable data storage and replication systems, cutting-edge search platform, best-in-class AI platform, experimentation platform, privacy and compliance platform etc. You will work and learn among the best, putting to use your passion for distributed technologies and algorithms, API design and systems-design, and your passion for writing code that performs at an extreme scale. LinkedIn has already pioneered well-known open-source infrastructure projects like Apache Kafka, Pinot, Azkaban, Samza, Venice, Datahub, Feather, etc. We also work with industry standard open source infrastructure products like Kubernetes, GRPC and GraphQL - come join our infrastructure teams and share the knowledge with a broader community while making a real impact within our company. Responsibilities - You will own the technical strategy for broad or complex requirements with insightful and forward-looking approaches that go beyond the direct team and solve large open-ended problems. - You will design, implement, and optimize the performance of large-scale distributed systems with security and compliance in mind. - You will Improve the observability and understandability of various systems with a focus on improving developer productivity and system sustenance - You will effectively communicate with the team, partners and stakeholders. - You will mentor other engineers, define our challenging technical culture, and help to build a fast-growing team - You will work closely with the open-source community to participate and influence cutting edge open-source projects (e.g., Apache Iceberg) - You will deliver incremental impact by driving innovation while iteratively building and shipping software at scale - You will diagnose technical problems, debug in production environments, and automate routine tasks Basic Qualifications - BA/BS Degree in Computer Science or related technical discipline, or related practical experience. - 4+ years of industry experience in software design, development, and algorithm related solutions. - 4+ years experience programming in object-oriented languages such as Java, C++, Python, Go, Rust, C# and/or Functional languages such as Scala or other relevant coding languages - Hands on experience developing distributed systems, large-scale systems, databases and/or Backend APIs Preferred Qualifications - BS and 8+ years of relevant work experience, MS and 7+ years of relevant work experience, or PhD and 4+ years of relevant work experience - Experience in architecting, building, and running large-scale distributed systems - Experience with industry, opensource, and/or academic research in technologies such as Hadoop, Spark, Kubernetes, Feather, GraphQL, GRPC, Apache Kafka, Pinot, Samza or Venice - Experience with open-source project management and governance Suggested Skills - Distributed systems - Backend Systems Infrastructure - Java/Golang/Rust/Python You will Benefit from our Culture We strongly believe in the well-being of our employees and their families. That is why we offer generous health and wellness programs and time away for employees of all levels. LinkedIn is committed to fair and equitable compensation practices. The pay range for this role is $156,000 - $255,000. Actual compensation packages are based on a wide array of factors unique to each candidate, including but not limited to skill set, years & depth of experience, certifications and specific office location. This may differ in other locations due to cost of labor considerations. The total compensation package for this position may also include annual performance bonus, stock, benefits and/or other applicable incentive compensation plans. For additional information, visit: https://careers.linkedin.com/benefits. Equal Opportunity Statement LinkedIn is committed to diversity in its workforce and is proud to be an equal opportunity employer. LinkedIn considers qualified applicants without regard to race, color, religion, creed, gender, national origin, age, disability, veteran status, marital status, pregnancy, sex, gender expression or identity, sexual orientation, citizenship, or any other legally protected class. LinkedIn is an Affirmative Action and Equal Opportunity Employer as described in our equal opportunity statement here: https://microsoft.sharepoint.com/:b:/t/LinkedInGCI/EeE8sk7CTIdFmEp9ONzFOTEBM62TPrWLMHs4J1C_QxVTbg?e=5hfhpE. Please reference https://www.eeoc.gov/sites/default/files/2023-06/22-088_EEOC_KnowYourRights6.12ScreenRdr.pdf and https://www.dol.gov/ofccp/regs/compliance/posters/pdf/OFCCP_EEO_Supplement_Final_JRF_QA_508c.pdf for more information. LinkedIn is committed to offering an inclusive and accessible experience for all job seekers, including individuals with disabilities. Our goal is to foster an inclusive and accessible workplace where everyone has the opportunity to be successful. If you need a reasonable accommodation to search for a job opening, apply for a position, or participate in the interview process, connect with us at accommodations@linkedin.com and describe the specific accommodation requested for a disability-related limitation. Reasonable accommodations are modifications or adjustments to the application or hiring process that would enable you to fully participate in that process. Examples of reasonable accommodations include but are not limited to: -Documents in alternate formats or read aloud to you -Having interviews in an accessible location -Being accompanied by a service dog -Having a sign language interpreter present for the interview A request for an accommodation will be responded to within three business days. However, non-disability related requests, such as following up on an application, will not receive a response. LinkedIn will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by LinkedIn, or (c) consistent with LinkedIn's legal duty to furnish information. Pay Transparency Policy Statement As a federal contractor, LinkedIn follows the Pay Transparency and non-discrimination provisions described at this link: https://lnkd.in/paytransparency. Global Data Privacy Notice for Job Candidates This document provides transparency around the way in which LinkedIn handles personal data of employees and job applicants: https://lnkd.in/GlobalDataPrivacyNotice",
        "url": "https://www.linkedin.com/jobs/view/3868947359",
        "summary": "LinkedIn is seeking a Software Engineer to build the next generation of infrastructure and platforms. The role focuses on distributed systems, large-scale systems, databases, and backend APIs. Responsibilities include owning technical strategy, designing and optimizing performance of systems, improving observability, collaborating with the open-source community, and mentoring other engineers.",
        "industries": [
            "Technology",
            "Software Development",
            "Internet"
        ],
        "soft_skills": [
            "Communication",
            "Leadership",
            "Teamwork",
            "Mentorship",
            "Problem Solving",
            "Analytical Skills",
            "Strategic Thinking",
            "Collaboration"
        ],
        "hard_skills": [
            "Java",
            "C++",
            "Python",
            "Go",
            "Rust",
            "C#",
            "Scala",
            "Distributed Systems",
            "Large-Scale Systems",
            "Databases",
            "Backend APIs",
            "Hadoop",
            "Spark",
            "Kubernetes",
            "Feather",
            "GraphQL",
            "GRPC",
            "Apache Kafka",
            "Pinot",
            "Samza",
            "Venice",
            "Open-Source Project Management",
            "Open-Source Project Governance"
        ],
        "tech_stack": [
            "Apache Kafka",
            "Pinot",
            "Azkaban",
            "Samza",
            "Venice",
            "Datahub",
            "Feather",
            "Kubernetes",
            "GRPC",
            "GraphQL",
            "Apache Iceberg",
            "Hadoop",
            "Spark"
        ],
        "programming_languages": [
            "Java",
            "C++",
            "Python",
            "Go",
            "Rust",
            "C#",
            "Scala"
        ],
        "experience": 4,
        "education": {
            "min_degree": "BA/BS",
            "fields": [
                "Computer Science",
                "Related Technical Discipline"
            ]
        },
        "salary": {
            "max": 255000,
            "min": 156000
        },
        "benefits": [
            "Health and Wellness Programs",
            "Time Away",
            "Annual Performance Bonus",
            "Stock",
            "Other Applicable Incentive Compensation Plans"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco Bay Area",
        "job_id": 3964562072,
        "company": "Radley James",
        "title": "Graduate Software Engineer",
        "created_on": 1720635624.6740794,
        "description": "A world leading proprietary trading firm is looking for a junior or graduate developer to join a core engineering team building out high-throughout systems for automated trading. You'll be working alongside senior engineers within a meritocratic environment with significant learning opportunities and the chance to make an impact on the business from day one. Requirements: Bachelors in Computer Science from a top university (3.5+ GPA). Recent graduate or 1-2 years experience in a professional setting. Experience in C++ essential. Passionate about technology and coding from a young age. Demonstrable examples of working on challenging computer science projects. An interest in finance and high frequency trading.",
        "url": "https://www.linkedin.com/jobs/view/3964562072",
        "summary": "A junior or graduate developer is needed to join a core engineering team at a leading proprietary trading firm. This role focuses on building high-throughput systems for automated trading. The ideal candidate will have a strong background in C++ and a passion for technology, with demonstrable experience in challenging computer science projects. Prior experience in finance or high frequency trading is beneficial.",
        "industries": [
            "Finance",
            "Trading",
            "Technology"
        ],
        "soft_skills": [
            "Passionate about technology",
            "Coding",
            "Problem Solving",
            "Learning"
        ],
        "hard_skills": [
            "C++"
        ],
        "tech_stack": [
            "High-throughput systems",
            "Automated trading"
        ],
        "programming_languages": [
            "C++"
        ],
        "experience": 1,
        "education": {
            "min_degree": "Bachelors",
            "fields": [
                "Computer Science"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Sunnyvale, CA",
        "job_id": 3910736667,
        "company": "Walmart",
        "title": "Principal, Data Engineer",
        "created_on": 1720635626.4239612,
        "description": "Position Summary... What you'll do... About Team The mission of Walmart Display Ads is to connect brand owners and shoppers with relevant display ads. We bring brand awareness and rich product information to shoppers and help brand owners grow their business with advanced targeting and optimization techniques. This is a fast-growing business never lack ing opportunities ! We are looking for an experienced tech lead to spearhead advertising data analytics related to deman d, supply, and overall marketplace healt h. You will be responsible for extracting meaningful insights about campaign performance, marketplace efficiency, and gaps in systems and products , surfacing marketplace health metrics via dashboards , and working with cross-functiona l partners to move the needle in display advertising. What You Will Do Building data pipelines and dashboards to monitor display ads serving funnel and marketplace health. Leading analytics for display ads marketplace and presenting findings and recommendations to engineering teams and cross functional partners. Collaborating with engineering and data science leads to improve data quality, build ing data warehouse, and delivering predictive models for market place insights and anomaly detection. Bringing data driven culture to the engineering team to assure product and system quality. What You'll Bring Bachelor's degree in data science , computer science, statistics, operation research, or related fields 6 years' experience of building data pipelines, extracting signals from noisy data, and establishing metrics for monitoring. Proficiency in data analysis tools (e.g., as Python, R, SQL) and data visualization tools (e.g., Tableau, Superset, Looker ) . Analytical thinking and detail-oriented mindset. Familiar with S QL and N on- SQL database s. String communication skills. Ability to convey complex ideas and findings to non-technical stakeholders . Knowledge of A/B testing. Preferred Qualifications Advanced degree in data science, computer science, statistics, operation research, or related fields 6 + years' experience in programmatic advertising, on-line content distribution, or e-commerce. Knowledge of optimization, auction, and ML applications. People management experience is a big plus. About Walmart Global Tech Imagine working in an environment where one line of code can make life easier for hundreds of millions of people. That is what we do at Walmart Global Tech. We are a team of software engineers, data scientists, cybersecurity expert's and service professionals within the world's leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered. We train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity and those looking for the opportunity to define their career. Here, you can kickstart a distinguished career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail. Flexible, Hybrid Work We use a hybrid way of working that is primarily in the office coupled with virtual when not onsite. Our campuses serve as a hub to enhance collaboration, bring us together for purpose and deliver on business needs. This approach helps us make quicker decisions, remove location barriers across our global team and be more flexible in our personal lives. Benefits Benefits: Beyond our great compensation package, you can receive incentive awards for your performance. Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO , multiple health plans, and much more. Equal Opportunity Employer Walmart, Inc. is an Equal Opportunity Employer - By Choice. We believe we are best equipped to help our associates, customers, and the communities we serve live better when we really know them. That means understanding, respecting and valuing diversity- unique styles, experiences, identities, ideas, and opinions - while being inclusive of all people. The above information has been designed to indicate the general nature and level of work performed in the role. It is not designed to contain or be interpreted as a comprehensive inventory of all responsibilities and qualifications required of employees assigned to this job. The full Job Description can be made available as part of the hiring process. At Walmart, we offer competitive pay as well as performance-based bonus awards and other great benefits for a happier mind, body, and wallet. Health benefits include medical, vision and dental coverage. Financial benefits include 401(k), stock purchase and company-paid life insurance. Paid time off benefits include PTO (including sick leave), parental leave, family care leave, bereavement, jury duty, and voting. Other benefits include short-term and long-term disability, company discounts, Military Leave Pay, adoption and surrogacy expense reimbursement, and more. ‎ ‎ ‎ You will also receive PTO and/or PPTO that can be used for vacation, sick leave, holidays, or other purposes. The amount you receive depends on your job classification and length of employment. It will meet or exceed the requirements of paid sick leave laws, where applicable. ‎ For information about PTO, see https://one.walmart.com/notices . ‎ ‎ Live Better U is a Walmart-paid education benefit program for full-time and part-time associates in Walmart and Sam's Club facilities. Programs range from high school completion to bachelor's degrees, including English Language Learning and short-form certificates. Tuition, books, and fees are completely paid for by Walmart. ‎ Eligibility requirements apply to some benefits and may depend on your job classification and length of employment. Benefits are subject to change and may be subject to a specific plan or program terms. ‎ For Information About Benefits And Eligibility, See One.Walmart . ‎ Bellevue, Washington US-11075:The annual salary range for this position is $132,000.00-$264,000.00 ‎ SUNNYVALE, California US-04396:The annual salary range for this position is $143,000.00-$286,000.00 ‎ ‎ ‎ ‎ ‎ ‎ ‎ ‎ ‎ ‎ Additional Compensation Includes Annual Or Quarterly Performance Bonuses. ‎ Additional Compensation For Certain Positions May Also Include ‎ ‎ Stock ‎ ‎ Minimum Qualifications... Outlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications. Option 1: Bachelor's degree in Computer Science and 5 years' experience in software engineering or related field. Option 2: 7 years' experience in software engineering or related field. Option 3: Master's degree in Computer Science and 3 years' experience in software engineering or related field. 4 years' experience in data engineering, database engineering, business intelligence, or business analytics. Preferred Qualifications... Outlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications. Data engineering, database engineering, business intelligence, or business analytics, ETL tools and working with large data sets in the cloud, Master's degree in Computer Science or related field and 5 years' experience in software engineering or related field, We value candidates with a background in creating inclusive digital experiences, demonstrating knowledge in implementing Web Content Accessibility Guidelines (WCAG) 2.2 AA standards, assistive technologies, and integrating digital accessibility seamlessly. The ideal candidate would have knowledge of accessibility best practices and join us as we continue to create accessible products and services following Walmart's accessibility standards and guidelines for supporting an inclusive culture. Primary Location... 840 W CALIFORNIA AVE, SUNNYVALE, CA 94086-4828, United States of America",
        "url": "https://www.linkedin.com/jobs/view/3910736667",
        "summary": "Walmart is looking for an experienced tech lead to spearhead advertising data analytics related to demand, supply, and overall marketplace health. Responsibilities include building data pipelines and dashboards to monitor display ads serving funnel and marketplace health, leading analytics for display ads marketplace and presenting findings and recommendations to engineering teams and cross functional partners, collaborating with engineering and data science leads to improve data quality, building data warehouse, and delivering predictive models for market place insights and anomaly detection, and bringing data driven culture to the engineering team to assure product and system quality.",
        "industries": [
            "Retail",
            "E-commerce",
            "Advertising",
            "Data Analytics",
            "Marketing"
        ],
        "soft_skills": [
            "Analytical Thinking",
            "Detail-Oriented",
            "Communication Skills",
            "Problem-Solving",
            "Collaboration",
            "Leadership"
        ],
        "hard_skills": [
            "Python",
            "R",
            "SQL",
            "Tableau",
            "Superset",
            "Looker",
            "A/B Testing",
            "Data Pipelines",
            "Data Warehousing",
            "Predictive Modeling",
            "Anomaly Detection"
        ],
        "tech_stack": [
            "Python",
            "R",
            "SQL",
            "Tableau",
            "Superset",
            "Looker"
        ],
        "programming_languages": [
            "Python",
            "R",
            "SQL"
        ],
        "experience": 6,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Data Science",
                "Computer Science",
                "Statistics",
                "Operation Research"
            ]
        },
        "salary": {
            "max": 286000,
            "min": 143000
        },
        "benefits": [
            "401(k) match",
            "stock purchase plan",
            "paid maternity and parental leave",
            "PTO",
            "multiple health plans",
            "performance-based bonus awards",
            "medical, vision and dental coverage",
            "company-paid life insurance",
            "short-term and long-term disability",
            "company discounts",
            "Military Leave Pay",
            "adoption and surrogacy expense reimbursement",
            "Live Better U education benefit program"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Mountain View, CA",
        "job_id": 3937516157,
        "company": "LinkedIn",
        "title": "Senior Software Engineer, Tools",
        "created_on": 1720635628.0792842,
        "description": "Company Description LinkedIn is the world’s largest professional network, built to create economic opportunity for every member of the global workforce. Our products help people make powerful connections, discover exciting opportunities, build necessary skills, and gain valuable insights every day. We’re also committed to providing transformational opportunities for our own employees by investing in their growth. We aspire to create a culture that’s built on trust, care, inclusion, and fun – where everyone can succeed. Join us to transform the way the world works. Job Description This role can be based in Mountain View, CA or Bellevue, WA. At LinkedIn, we trust each other to do our best work where it works best for us and our teams. This role offers a hybrid work option, meaning you can both work from home and commute to a LinkedIn office, depending on what’s best for you and when it is important for your team to be together. This role is part of the Productivity and Happiness (PH) organization where you will get an opportunity to influence, transform, and create an amazing experience for developers at LinkedIn. We are a data driven organization and you will lead strategic investments to make step function improvements in the lives of LinkedIn developers. This spans multiple areas including but not limited to: Providing tools and infrastructure that creates delightful development experience; Provide and support application frameworks that empower LinkedIn Engineers to build member facing products; Provide actionable insights that leverages data and metrics; Provide tools and data that help LinkedIn teams listen to their customers; and most importantly deliver reliable and scalable infrastructure. You will have the opportunity to engage the industry-wide developer community and contribute to open-source software as well. Responsibilities - You will build and ship software at scale that delivers impact. - You will improve all aspects of developer experience with a data driven mindset. - You will design and build tools and frameworks to automate development, testing, management, monitoring, data gathering and analysis of our 24x7 services and products. - You will scale the infrastructure and tools required to keep our 6000+ developers in step when they are all sharing the same code, building and testing our software stacks, and releasing and deploying their services continuously without compromising site reliability. - You will provide technical leadership, driving and performing best engineering practices to initiate, plan, and execute critical, large-scale, cross-functional, and company-wide projects. Basic Qualifications - BA/BS Degree in Computer Science or related technical discipline, or related practical experience. - 2+ years experience in software design, development, and algorithm related solutions. - 2+ years programming experience in object-oriented programming languages such as Python, Java, Javascript, C/C++, C#, Objective-C. Preferred Qualifications - BS and 5+ years of relevant work experience, MS and 4+ years of relevant work experience, or PhD and 2+ years of relevant work experience - Experience designing and building infrastructure and web services at large scale. - Experience in working in a Unix environment (Linux preferred). - Experience in large-scale distributed systems and client-server architectures. - Experience in driving automated cloud management systems at scale. - Experience in containerization, cluster schedulers, infrastructure configuration and orchestration. - Experience in standard build tools (e.g. gradle, bazel) and version control systems (e.g. git). Knowledge of Internet protocols and network programming. - Experience in working with SQL/NoSQL databases (e.g. MySQL, Dynamo, Cassandra, MongoDB) - Knowledge of messaging & big data systems/solutions (e.g. Spark, Presto, Alation, Hadoop, Kafka Suggested Skills - Object-oriented programming - Web services - Infrastructure You will Benefit from our Culture We strongly believe in the well-being of our employees and their families. That is why we offer generous health and wellness programs and time away for employees of all levels. LinkedIn is committed to fair and equitable compensation practices. The pay range for this role is $117,000 - $192,000. Actual compensation packages are based on several factors that are unique to each candidate, including but not limited to skill set, depth of experience, certifications, and specific work location. This may be different in other locations due to differences in the cost of labor. The total compensation package for this position may also include annual performance bonus, stock, benefits and/or other applicable incentive compensation plans. For more information, visit https://careers.linkedin.com/benefits. Equal Opportunity Statement LinkedIn is committed to diversity in its workforce and is proud to be an equal opportunity employer. LinkedIn considers qualified applicants without regard to race, color, religion, creed, gender, national origin, age, disability, veteran status, marital status, pregnancy, sex, gender expression or identity, sexual orientation, citizenship, or any other legally protected class. LinkedIn is an Affirmative Action and Equal Opportunity Employer as described in our equal opportunity statement here: https://microsoft.sharepoint.com/:b:/t/LinkedInGCI/EeE8sk7CTIdFmEp9ONzFOTEBM62TPrWLMHs4J1C_QxVTbg?e=5hfhpE. Please reference https://www.eeoc.gov/sites/default/files/2023-06/22-088_EEOC_KnowYourRights6.12ScreenRdr.pdf and https://www.dol.gov/ofccp/regs/compliance/posters/pdf/OFCCP_EEO_Supplement_Final_JRF_QA_508c.pdf for more information. LinkedIn is committed to offering an inclusive and accessible experience for all job seekers, including individuals with disabilities. Our goal is to foster an inclusive and accessible workplace where everyone has the opportunity to be successful. If you need a reasonable accommodation to search for a job opening, apply for a position, or participate in the interview process, connect with us at accommodations@linkedin.com and describe the specific accommodation requested for a disability-related limitation. Reasonable accommodations are modifications or adjustments to the application or hiring process that would enable you to fully participate in that process. Examples of reasonable accommodations include but are not limited to: -Documents in alternate formats or read aloud to you -Having interviews in an accessible location -Being accompanied by a service dog -Having a sign language interpreter present for the interview A request for an accommodation will be responded to within three business days. However, non-disability related requests, such as following up on an application, will not receive a response. LinkedIn will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by LinkedIn, or (c) consistent with LinkedIn's legal duty to furnish information. Pay Transparency Policy Statement As a federal contractor, LinkedIn follows the Pay Transparency and non-discrimination provisions described at this link: https://lnkd.in/paytransparency. Global Data Privacy Notice for Job Candidates This document provides transparency around the way in which LinkedIn handles personal data of employees and job applicants: https://lnkd.in/GlobalDataPrivacyNotice",
        "url": "https://www.linkedin.com/jobs/view/3937516157",
        "summary": "LinkedIn is seeking a Software Engineer to join their Productivity and Happiness organization. This role will focus on building and shipping software at scale that delivers impact, improving developer experience with a data-driven mindset, and designing and building tools and frameworks to automate development, testing, management, monitoring, data gathering and analysis. The ideal candidate will have experience designing and building infrastructure and web services at large scale, working in a Unix environment, large-scale distributed systems, and client-server architectures. Experience in driving automated cloud management systems at scale, containerization, cluster schedulers, infrastructure configuration and orchestration, standard build tools, and version control systems is also desired. Knowledge of Internet protocols, network programming, SQL/NoSQL databases, and messaging & big data systems/solutions is beneficial.",
        "industries": [
            "Technology",
            "Software Development",
            "Internet",
            "Social Media",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Data-driven",
            "Problem-solving",
            "Communication",
            "Collaboration",
            "Leadership",
            "Teamwork",
            "Technical leadership"
        ],
        "hard_skills": [
            "Python",
            "Java",
            "Javascript",
            "C/C++",
            "C#",
            "Objective-C",
            "Infrastructure",
            "Web services",
            "Unix",
            "Linux",
            "Distributed systems",
            "Client-server architectures",
            "Cloud management",
            "Containerization",
            "Cluster schedulers",
            "Infrastructure configuration",
            "Orchestration",
            "Gradle",
            "Bazel",
            "Git",
            "Internet protocols",
            "Network programming",
            "SQL",
            "NoSQL",
            "MySQL",
            "Dynamo",
            "Cassandra",
            "MongoDB",
            "Spark",
            "Presto",
            "Alation",
            "Hadoop",
            "Kafka",
            "Object-oriented programming"
        ],
        "tech_stack": [
            "Python",
            "Java",
            "Javascript",
            "C/C++",
            "C#",
            "Objective-C",
            "Unix",
            "Linux",
            "Distributed systems",
            "Client-server architectures",
            "Cloud management",
            "Containerization",
            "Cluster schedulers",
            "Infrastructure configuration",
            "Orchestration",
            "Gradle",
            "Bazel",
            "Git",
            "MySQL",
            "Dynamo",
            "Cassandra",
            "MongoDB",
            "Spark",
            "Presto",
            "Alation",
            "Hadoop",
            "Kafka"
        ],
        "programming_languages": [
            "Python",
            "Java",
            "Javascript",
            "C/C++",
            "C#",
            "Objective-C"
        ],
        "experience": 2,
        "education": {
            "min_degree": "BA/BS",
            "fields": [
                "Computer Science",
                "Technical"
            ]
        },
        "salary": {
            "max": 192000,
            "min": 117000
        },
        "benefits": [
            "Health and wellness programs",
            "Time away",
            "Annual performance bonus",
            "Stock",
            "Other applicable incentive compensation plans"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Fremont, CA",
        "job_id": 3818365094,
        "company": "Info Way Solutions",
        "title": "Big data engineer�",
        "created_on": 1720635629.7391813,
        "description": "Hi Hope you are doing good, This is Sangeetha from Info Way Solutions; LLC We have job opening for the below requirement , the detailed Job description is given below: Kindly check the JD and share your views Role : Big data engineer Location : Austin, TX Long-term Contract Need Strong Experience in Hadoop & GCP Requirement A person with strong Bigdata background with hands on and exposure to Hadoop system. Should have good experience in migrating the Data (Hadoop) from On-premise to Cloud (GCP). Data Proc(Hadoop/Spark Services), GKE (Google Kubernetes Services) and Hive(Analysis of tables in Data lake) experience. Thanks & Regards, Sangeetha Email: sangeetha@Infowaygroup.com Direct: (925)241-4886 Work: (925)-592-6160 Ext 104 https://www.linkedin.com/in/sangeetha-kannan-291636206/ Info Way Solutions LLC | 46520 Fremont Blvd, Suite 614 | Fremont, CA - 94538",
        "url": "https://www.linkedin.com/jobs/view/3818365094",
        "summary": "Big Data Engineer with strong experience in Hadoop and GCP, specializing in migrating data from on-premise Hadoop systems to Google Cloud Platform (GCP). Experience with Data Proc (Hadoop/Spark Services), GKE (Google Kubernetes Services), and Hive for data lake analysis is required.",
        "industries": [
            "Information Technology",
            "Cloud Computing",
            "Data Engineering"
        ],
        "soft_skills": [
            "Communication",
            "Problem Solving",
            "Teamwork"
        ],
        "hard_skills": [
            "Hadoop",
            "GCP",
            "Data Migration",
            "Data Proc",
            "Spark",
            "GKE",
            "Kubernetes",
            "Hive",
            "Data Lake Analysis"
        ],
        "tech_stack": [
            "Hadoop",
            "GCP",
            "Data Proc",
            "Spark",
            "GKE",
            "Kubernetes",
            "Hive"
        ],
        "programming_languages": [],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Santa Clara, CA",
        "job_id": 3899693909,
        "company": "Shockwave Medical",
        "title": "Senior Data Engineer",
        "created_on": 1720635631.4397304,
        "description": "Shockwave Medical, Inc. is a pioneer in the development and commercialization of Intravascular Lithotripsy (IVL) to treat complex calcified cardiovascular disease. Shockwave Medical aims to establish a new standard of care for medical device treatment of atherosclerotic cardiovascular disease through its differentiated and proprietary local delivery of sonic pressure waves for the treatment of calcified plaque. Position Overview We are searching for a talented and experienced Modern Data Stack Data Engineer to join our growing team. You will be responsible for designing, building, and maintaining data pipelines using technologies like Fivetran, Azure Data Factory and Python into our Snowflake data warehouse and transforming the data using tools like dbt and Coalesce. You will also be responsible for ensuring data quality, security, and performance of the data pipelines. Essential Job Functions Design, develop, and maintain data pipelines using Fivetran, Python, dbt/coalesce and Airflow from varied sources. Primary sources include Oracle Fusion Cloud, Salesforce, among many others. Write efficient SQL queries to transform and clean data for optimal consumption in Snowflake. Implement data quality checks and monitoring processes to ensure data accuracy and consistency. Collaborate with data analysts and business stakeholders to understand data requirements and design data models. Document data pipelines and processes for maintainability and knowledge sharing. Monitor and optimize data pipeline performance for efficiency and scalability. Troubleshoot and resolve data pipeline issues to ensure data availability. Create dashboards and advanced visualizations in Power BI Implement data observability tools like Monte Carlo Stay up to date on the latest trends and technologies in the modern data stack (Snowflake, Fivetran, dbt, etc.). Requirements 5+ years of experience as a Data Engineer or similar role. Proven experience with designing and building data pipelines. Must have skills – Snowflake, dbt, and Python Advanced knowledge of Snowflake, Fivetran, Airflow and dbt Monte Carlo or other data observability tools experience a plus Strong knowledge of SQL and experience with data transformation techniques. Excellent problem-solving and analytical skills. Proficiency in extracting and loading data from the following two source applications - Oracle Fusion Cloud (Finance, Supply Chain, Manufacturing), Salesforce a huge plus Strong communication and collaboration skills. Ability to work independently and as part of a team. Excellent project management and team leadership skills. Strong interpersonal and communication skills to collaborate effectively with business partners cross-functional teams. Strong strategic mindset with the ability to align analytics initiatives with overall business goals and proactively identify opportunities for leveraging data to drive business value. Market Range: $121,000 - $151,000 Exact compensation may vary based on skills, experience, and location. Benefits Shockwave Medical offers a competitive total compensation package as well as the following benefits and perks: Core Benefits : Medical, Dental, Vision, Pre-tax and Roth 401k options with a fully vested match, Short-Term and Long-Term Disability, and Life Insurance, Employer contribution toward Health Savings Account (HSA), Competitive PTO balance Perks : ESPP, Calm App, Pet Insurance, Student Loan Refinancing, Spot Bonus awards EEO Employer",
        "url": "https://www.linkedin.com/jobs/view/3899693909",
        "summary": "Shockwave Medical is seeking a Data Engineer to design, build, and maintain data pipelines using technologies like Fivetran, Azure Data Factory, and Python. The role requires expertise in Snowflake, dbt, and Python, and experience with Oracle Fusion Cloud and Salesforce is a plus. ",
        "industries": [
            "Healthcare",
            "Medical Devices",
            "Technology",
            "Data Engineering",
            "Software"
        ],
        "soft_skills": [
            "Problem-solving",
            "Analytical",
            "Communication",
            "Collaboration",
            "Teamwork",
            "Project Management",
            "Leadership",
            "Strategic Mindset"
        ],
        "hard_skills": [
            "Snowflake",
            "Fivetran",
            "Azure Data Factory",
            "Python",
            "dbt",
            "SQL",
            "Airflow",
            "Power BI",
            "Monte Carlo",
            "Oracle Fusion Cloud",
            "Salesforce"
        ],
        "tech_stack": [
            "Snowflake",
            "Fivetran",
            "Azure Data Factory",
            "Python",
            "dbt",
            "Airflow",
            "Power BI",
            "Monte Carlo",
            "Oracle Fusion Cloud",
            "Salesforce"
        ],
        "programming_languages": [
            "Python",
            "SQL"
        ],
        "experience": 5,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 151000,
            "min": 121000
        },
        "benefits": [
            "Medical",
            "Dental",
            "Vision",
            "401k",
            "Disability Insurance",
            "Life Insurance",
            "HSA",
            "ESPP",
            "Calm App",
            "Pet Insurance",
            "Student Loan Refinancing",
            "Spot Bonus",
            "PTO"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "California, United States",
        "job_id": 3891930607,
        "company": "Enexus Global Inc.",
        "title": "GCP Data Engineer",
        "created_on": 1720635635.3695319,
        "description": "GCP Big Data Engineer Location: Phoenix, AZ W2 / C2C / 1099 6-10 years of big data technologies or data platform architecture experience with deep technology expertise in the following: Hive, HDFS, Spark, Kafka, Java or Scala and etc. Experience in service architecture and development experience with high performance and scalability Experience in Spark, SQL performance tuning and optimization Architecture design and development of large scale data platforms and data application Expertise in design and management of complex data structures and data processes like ETL/ELT Expertise in managing and operating distributed big data systems, including but not limited to Hadoop eco system Deep understanding of issues in multiple areas such as data acquisition and processing, data management, distributed processing, and high availability is required Knowledge/Familiarity with one of the cloudnology GCP.",
        "url": "https://www.linkedin.com/jobs/view/3891930607",
        "summary": "This is a job for a Big Data Engineer with 6-10 years of experience in big data technologies and data platform architecture. The ideal candidate will have deep expertise in Hive, HDFS, Spark, Kafka, Java or Scala, and experience in service architecture, high-performance development, Spark/SQL tuning, and large-scale data platform development. Knowledge of GCP is also preferred.",
        "industries": [
            "Technology",
            "Data Science",
            "Software Development",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Problem Solving",
            "Analytical Skills",
            "Communication Skills",
            "Teamwork",
            "Time Management"
        ],
        "hard_skills": [
            "Hive",
            "HDFS",
            "Spark",
            "Kafka",
            "Java",
            "Scala",
            "Service Architecture",
            "Scalability",
            "Performance Tuning",
            "Optimization",
            "Data Platform Architecture",
            "ETL",
            "ELT",
            "Distributed Systems",
            "Hadoop",
            "Data Acquisition",
            "Data Processing",
            "Data Management",
            "GCP"
        ],
        "tech_stack": [
            "Hive",
            "HDFS",
            "Spark",
            "Kafka",
            "Java",
            "Scala",
            "Hadoop",
            "GCP"
        ],
        "programming_languages": [
            "Java",
            "Scala"
        ],
        "experience": 6,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Data Science",
                "Engineering"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Jose, CA",
        "job_id": 3818339621,
        "company": "Intelliswift Software",
        "title": "Data Engineer 4",
        "created_on": 1720635637.030998,
        "description": "Title - Data Engineer Location - San Jose, CA Duration - 8 Months Pay rate - $91.55 per hour on w2 Write complex SQL queries that help Adobe identify and measure the non-genuine software base. Apply business logic to determine categorize the non-genuine base into opportunities Use Big Data best practices to help scale the piracy conversion program Create dashboards in tableau Apply statistical methodologies and data mining skill sets on large volume of data Analyze data and provide insights to senior executives SQL Expertise. Hadoop expertise. Can work in hive, pig etc Create dashboards in tableau and excel Modelling in Excel. Power Pivot expertise a must Understanding of statistics concepts Ability to interpret data and present Excellent written and verbal communication skills",
        "url": "https://www.linkedin.com/jobs/view/3818339621",
        "summary": "Data Engineer needed for 8-month contract in San Jose, CA to analyze data and help Adobe identify and measure non-genuine software. Responsibilities include writing complex SQL queries, applying business logic to categorize non-genuine software, using Big Data best practices, creating dashboards in Tableau, applying statistical methodologies, and presenting data insights to executives.",
        "industries": [
            "Software",
            "Data Analytics",
            "Big Data"
        ],
        "soft_skills": [
            "Communication",
            "Presentation",
            "Problem Solving",
            "Analytical",
            "Data Interpretation",
            "Business Logic"
        ],
        "hard_skills": [
            "SQL",
            "Hadoop",
            "Hive",
            "Pig",
            "Tableau",
            "Excel",
            "Power Pivot",
            "Statistics",
            "Data Mining"
        ],
        "tech_stack": [
            "SQL",
            "Hadoop",
            "Hive",
            "Pig",
            "Tableau",
            "Excel",
            "Power Pivot"
        ],
        "programming_languages": [
            "SQL"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Fremont, CA",
        "job_id": 3818359186,
        "company": "Info Way Solutions",
        "title": "Data engineers (with strong Python and AWS experience)",
        "created_on": 1720635641.2700121,
        "description": "Hi Professionals, Hope you are doing good Job Description This is Jayaraman from Info Way Solutions, LLC We have job opening for Data engineers (with strong Python and AWS experience) and the detailed Job description is given below: Kindly check the JD and share your views Data engineers (with strong Python and AWS experience) Locations: DC/VA/MD Need Strong Hands on experience Role Description Data Pipeline Development: Design, implement, and manage robust data pipelines using Python, PySpark, SQL to efficiently extract, transform, and load data from diverse sources(Batch & Streaming) AWS Expertise Demonstrate expertise in core AWS services such as AWS DMS, AWS Glue, AWS Step Functions, Amazon S3, Amazon Redshift, Amazon RDS, Amazon EMR, AWS IAM, AWS LAMBDA etc., and apply them to build scalable and reliable data solutions. Data Modeling Develop and maintain efficient data models to support the analytical and reporting needs. Database Management Administer databases using AWS services like Amazon RDS or Amazon Redshift, focusing on schema design, performance optimization, and monitoring. Data Warehousing Utilize Amazon Redshift or Amazon Snowflake to create high-performing analytical databases that empower data-driven decision-making. ETL Best Practices Implement industry best practices for ETL processes, including data validation, error handling, and data quality checks. Performance Optimization Optimize query performance through continuous tuning of databases and leveraging AWS's scalability capabilities. Monitoring And Logging Establish robust monitoring and logging mechanisms using AWS CloudWatch, Amazon CloudTrail, or comparable tools to ensure pipeline reliability. Security And Compliance Ensure adherence to security best practices and relevant compliance standards, tailoring solutions to meet GDPR, HIPAA, or other regulatory requirements. Automation Drive automation of deployment and scaling of data pipelines using infrastructure as code (IaC) tools like AWS CloudFormation and Terraform. Collaboration Collaborate closely with cross-functional teams, including data scientists, analysts, and other stakeholders, to understand their data needs and provide effective solutions. Continuous Learning Stay updated on the latest developments in AWS services and data engineering methodologies, applying new insights to enhance our data infrastructure. Soft Skills Exhibit strong communication skills to facilitate effective teamwork and interaction with diverse groups. Thanks & Regards, Jayaraman Email: jayaraman@infowaygroup.com Direct: (925)-241-5719 Work: (925)-592-6160 Ext 105 Info Way Solutions LLC | 46520 Fremont Blvd, Suite 614 | Fremont, CA - 94538",
        "url": "https://www.linkedin.com/jobs/view/3818359186",
        "summary": "Data Engineer position at Info Way Solutions, LLC requiring strong Python and AWS expertise to design, implement, and manage data pipelines. Responsibilities include ETL processes, data modeling, database management, data warehousing, performance optimization, and security/compliance.",
        "industries": [
            "Data Engineering",
            "Cloud Computing",
            "Software Development",
            "Information Technology"
        ],
        "soft_skills": [
            "Communication",
            "Teamwork",
            "Collaboration",
            "Problem Solving",
            "Analytical Thinking"
        ],
        "hard_skills": [
            "Python",
            "PySpark",
            "SQL",
            "AWS DMS",
            "AWS Glue",
            "AWS Step Functions",
            "Amazon S3",
            "Amazon Redshift",
            "Amazon RDS",
            "Amazon EMR",
            "AWS IAM",
            "AWS LAMBDA",
            "Data Modeling",
            "Database Administration",
            "Data Warehousing",
            "ETL",
            "Performance Optimization",
            "Monitoring",
            "Logging",
            "Security",
            "Compliance",
            "Automation",
            "AWS CloudFormation",
            "Terraform"
        ],
        "tech_stack": [
            "AWS",
            "Python",
            "PySpark",
            "SQL",
            "AWS DMS",
            "AWS Glue",
            "AWS Step Functions",
            "Amazon S3",
            "Amazon Redshift",
            "Amazon RDS",
            "Amazon EMR",
            "AWS IAM",
            "AWS LAMBDA",
            "AWS CloudFormation",
            "Terraform",
            "Amazon Snowflake"
        ],
        "programming_languages": [
            "Python",
            "PySpark",
            "SQL"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Jose, CA",
        "job_id": 3888462836,
        "company": "Zortech Solutions",
        "title": "Snowflake Data Engineer-US",
        "created_on": 1720635643.1473708,
        "description": "Role: Snowflake Data Engineer Location San Jose, CA Later it will be onsite- Initial remote (Please take consent letter) Duration: 6+ Months Job Description Strong Snowflake, Unix Shell Scripting, Python, Experience in any NoSQL, Strong in Programming, Experience in Large volume Data Processing and ETLs and Java",
        "url": "https://www.linkedin.com/jobs/view/3888462836",
        "summary": "Snowflake Data Engineer with strong experience in Snowflake, Unix Shell Scripting, Python, NoSQL databases, programming, large volume data processing, ETLs, and Java. Remote initially, with onsite work in San Jose, CA after.",
        "industries": [
            "Data Engineering",
            "Software Development",
            "Technology",
            "Big Data",
            "Analytics"
        ],
        "soft_skills": [
            "Problem Solving",
            "Communication",
            "Teamwork",
            "Analytical Skills"
        ],
        "hard_skills": [
            "Snowflake",
            "Unix Shell Scripting",
            "Python",
            "NoSQL",
            "Programming",
            "ETL",
            "Java"
        ],
        "tech_stack": [
            "Snowflake",
            "Unix",
            "Python",
            "NoSQL"
        ],
        "programming_languages": [
            "Python",
            "Java"
        ],
        "experience": 6,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Carlsbad, CA",
        "job_id": 3963334783,
        "company": "myGwork - LGBTQ+ Business Community",
        "title": "Data Engineer",
        "created_on": 1720635644.8858547,
        "description": "This inclusive employer is a member of myGwork – the largest global platform for the LGBTQ+ business community. Work Schedule Other Environmental Conditions Office Job Description As part of the Thermo Fisher Scientific team, you'll discover meaningful work that makes a positive impact on a global scale! Join our colleagues in bringing our Mission to life every single day to enable our customers to make the world healthier, cleaner and safer. We provide our global teams with the resources needed to achieve individual career goals while helping to take science a step beyond by developing solutions for some of the world's toughest challenges, like protecting the environment, making sure our food is safe or helping find cures for cancer. How Will You Make An Impact? The Data Engineer will be part of the NA Marketing Science team within the Research and Safety NA division. They will work with Data Science and Data Architecture teams to build Databricks-based data pipelines and bring data onto our enterprise-level data platform for Data Science, Analytics and Digital Marketing needs. The data platform is primarily based on Oracle Exadata database and Databricks-based Delta technologies for critical application and business enablement A Day In The Life Work cross-functionality to understand data opportunities and build pipelines and dictionaries for potential data sources Evaluate integration capabilities and opportunities to leverage and automate 3rd party data Design, develop, test, deploy, support, enhance data integration solutions to connect and integrate enterprise systems Follow agile development methodologies to deliver solutions and product features by following DevOps practices. Assist with implementing standard operating procedures, facilitate requirements gathering and review sessions with functional owners and end-user representatives, and leverage technical knowledge and expertise to drive improvements. Support standard processes for defining, designing and documenting reference architecture and leading the implementation of BI and analytical solutions. Minimum Requirements 4-year degree with a major in computer science engineering (or equivalent) from an accredited university plus at least one year of relevant experience or a minimum of 8-10 years professional IT experience Strong prior technical, development background in either data Services or Engineering Proven experience in Databricks/Spark-based Data Engineering Pipeline development and Data/Delta Lake, Oracle, or AWS Redshift type relational databases. Preferred Requirements 3+ years working experience in Python-based data integration and pipeline development. Data lake and Delta Lake experience with AWS Glue and Athena. Proven experience with AWS Cloud on data integration with various ecosystems. Strong experience in python development especially in AWS Cloud environment with knowledge of common python libraries Strong experience with source control systems such as Git and Jenkins Demonstrated experience resolving complex data integration problems. Knowledge, Skills, Abilities Ability to work multi-functionally and present solutions to variety of technical and non-technical audiences Highly determined, execution-focused, with ability to balance multiple priorities in an effective manner Adept at debugging and solving problems Ability to design, develop test, deploy, maintain, and improve data integration pipeline Strong analytical experience with database in query optimization, debugging, user defined functions, views, indexes etc. Excellent verbal and written communication skills Compensation And Benefits The salary range estimated for this position based in Pennsylvania is $75,800.00-$113,675.00. This Position May Also Be Eligible To Receive a Variable Annual Bonus Based On Company, Team, And/or Individual Performance Results In Accordance With Company Policy. We Offer a Comprehensive Total Rewards Package That Our U.S. Colleagues And Their Families Can Count On, Which Includes A choice of national medical and dental plans, and a national vision plan, including health incentive programs Employee assistance and family support programs, including commuter benefits and tuition reimbursement At least 120 hours paid time off (PTO), 10 paid holidays annually, paid parental leave (3 weeks for bonding and 8 weeks for caregiver leave), accident and life insurance, and short- and long-term disability in accordance with company policy Retirement and savings programs, such as our competitive 401(k) U.S. retirement savings plan Employees' Stock Purchase Plan (ESPP) offers eligible colleagues the opportunity to purchase company stock at a discount For more information on our benefits, please visit: https://jobs.thermofisher.com/global/en/total-rewards",
        "url": "https://www.linkedin.com/jobs/view/3963334783",
        "summary": "Data Engineer needed for NA Marketing Science team to build Databricks-based data pipelines and bring data onto the enterprise-level data platform for Data Science, Analytics and Digital Marketing. They will work cross-functionally to understand data opportunities and build pipelines and dictionaries for potential data sources.  The role involves evaluating integration capabilities and opportunities to leverage and automate 3rd party data. ",
        "industries": [
            "Technology",
            "Science",
            "Research",
            "Marketing",
            "Data",
            "Analytics"
        ],
        "soft_skills": [
            "Communication",
            "Problem Solving",
            "Collaboration",
            "Analytical Thinking",
            "Presentation Skills",
            "Time Management",
            "Organization",
            "Detail-Oriented"
        ],
        "hard_skills": [
            "Databricks",
            "Spark",
            "Python",
            "Data Engineering",
            "Data Pipelines",
            "Delta Lake",
            "Oracle",
            "AWS Redshift",
            "AWS Glue",
            "Athena",
            "AWS Cloud",
            "Git",
            "Jenkins"
        ],
        "tech_stack": [
            "Databricks",
            "Spark",
            "Python",
            "Delta Lake",
            "Oracle",
            "AWS Redshift",
            "AWS Glue",
            "Athena",
            "AWS Cloud",
            "Git",
            "Jenkins"
        ],
        "programming_languages": [
            "Python"
        ],
        "experience": 1,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Computer Science",
                "Engineering"
            ]
        },
        "salary": {
            "max": 113675,
            "min": 75800
        },
        "benefits": [
            "Medical Insurance",
            "Dental Insurance",
            "Vision Insurance",
            "Health Incentive Programs",
            "Employee Assistance Programs",
            "Family Support Programs",
            "Commuter Benefits",
            "Tuition Reimbursement",
            "Paid Time Off",
            "Paid Holidays",
            "Paid Parental Leave",
            "Accident Insurance",
            "Life Insurance",
            "Short-Term Disability",
            "Long-Term Disability",
            "Retirement Savings Plan",
            "401k",
            "Employee Stock Purchase Plan"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Mateo, CA",
        "job_id": 3837053632,
        "company": "Veryfi",
        "title": "Data Visualization Engineer",
        "created_on": 1720635646.6002984,
        "description": "We are looking for the builder of our next-generation libraries, APIs and data pipelines, capable of creating data visualization and statistical programming tools with our data to enable first class instrumentation. Responsibilities Develop UI components to enhance our in-house visualization tools in Data analytics Monitoring Alerts Anomaly Detection Transform data sets, quantitative and qualitative analysis from AI/ML predictions into compelling visuals. Provide visual, technical and editorial guidance. Critical thinking: Able to look at numbers, trends, and data and come to new conclusions based on the findings Work closely with the Data/AI team to help create compelling visual data. Qualifications We would love to see the kind of work you have been able to create thus far in your career Extensive experience building Web Applications using best practices for UI frameworks such as Angular/TypeScript, Node, Webpack, React Proficiency w/ DBs not only SQL (MySQL, Postgres), but NoSQL, GraphDB and corresponding data analysis Expertise with creating and editing visual content including networks, charts, and graphs.",
        "url": "https://www.linkedin.com/jobs/view/3837053632",
        "summary": "We are seeking a skilled developer to build data visualization and statistical programming tools, enhancing our in-house visualization tools for data analytics, monitoring, alerts, and anomaly detection. You'll transform data sets into compelling visuals, provide visual and technical guidance, and work closely with the Data/AI team to create compelling visual data.",
        "industries": [
            "Data Science",
            "Software Development",
            "Analytics"
        ],
        "soft_skills": [
            "Critical Thinking",
            "Problem Solving",
            "Communication",
            "Collaboration"
        ],
        "hard_skills": [
            "UI Development",
            "Data Visualization",
            "Statistical Programming",
            "Data Analysis",
            "Quantitative Analysis",
            "Qualitative Analysis",
            "AI/ML",
            "SQL",
            "MySQL",
            "Postgres",
            "NoSQL",
            "GraphDB",
            "Data Modeling",
            "Network Visualization",
            "Charting",
            "Graphing"
        ],
        "tech_stack": [
            "Angular",
            "TypeScript",
            "Node",
            "Webpack",
            "React",
            "SQL",
            "MySQL",
            "Postgres",
            "NoSQL",
            "GraphDB"
        ],
        "programming_languages": [
            "TypeScript",
            "SQL"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco Bay Area",
        "job_id": 3956322965,
        "company": "Cypress HCM",
        "title": "Python Developer",
        "created_on": 1720635648.2497654,
        "description": "We have an exciting opportunity for a Python Data Engineer - Content Operations with the top leading multimedia and creative software company in the world. We are seeking an enthusiastic and inquisitive Data Engineer who is passionate about data and proficient in programming with Python. This role involves working on diverse projects that include processing, analyzing, and interpreting large datasets within the context of global financial markets. Responsibilities: Extract, transform, join, and load data from one or multiple systems to other systems, repositories, or tools supporting internal projects associated with the operational management of customer-ready help content. Build, format, and publish aggregated data sets containing customer-facing help content and associated attributes. Design and develop data pipelines to ingest, transform, and load unstructured and structured data from various sources. Document all programming tasks and data sets for future reference and troubleshooting. Leverage Large Language Models (LLMs) to automate specific content-related tasks. Enhance productivity for content authoring processes and teams by integrating tooling, including AI-based tooling. Skills and Experience: Bachelor's degree or higher in Science, Computer Science, Data Science, or a related field with substantial coursework in Computer Science. 1 to 3 years of relevant experience Python coding: Write clean, efficient, and maintainable code as standalone scripts in Python to implement data solutions and algorithms tailored to project requirements. Experience and knowledge should include: Proficiency with Git, artifactory, and modern developer tooling for creating and managing code in a large enterprise. Experience with a framework for AI applications is also a plus. Data typing and data structures Calling RESTful and GraphQL web service APIs Querying or populating both No-SQL and SQL databases Utilizing cloud storage and platforms such as AWS or Azure Demonstrated ability to perform exploratory data analysis to uncover insights and patterns within help content-related datasets and utilize statistical methods. Experience with prompt engineering for LLMs. Some experience with standard use of basic features of cloud platforms like Azure or AWS Proven ability to be self-driven and work relatively independently Experience as a user of Photoshop or other Creative Cloud products is a plus Specific experience working with digital content management systems or tools would be ideal. Compensation: Up to $56.34 per hour. 33317729",
        "url": "https://www.linkedin.com/jobs/view/3956322965",
        "summary": "Python Data Engineer with experience in processing, analyzing, and interpreting large datasets for content operations. Responsibilities include data extraction, transformation, and loading, building data pipelines, and leveraging LLMs for content automation.",
        "industries": [
            "Software",
            "Technology",
            "Multimedia",
            "Data Science",
            "Financial Markets"
        ],
        "soft_skills": [
            "Inquisitive",
            "Passionate about data",
            "Self-driven",
            "Independent"
        ],
        "hard_skills": [
            "Python",
            "Git",
            "Artifactory",
            "Data typing",
            "Data structures",
            "RESTful APIs",
            "GraphQL APIs",
            "SQL databases",
            "No-SQL databases",
            "Cloud storage",
            "AWS",
            "Azure",
            "Exploratory data analysis",
            "Statistical methods",
            "Prompt engineering",
            "Photoshop",
            "Creative Cloud products"
        ],
        "tech_stack": [
            "Python",
            "Git",
            "Artifactory",
            "RESTful APIs",
            "GraphQL APIs",
            "SQL databases",
            "No-SQL databases",
            "AWS",
            "Azure",
            "LLMs"
        ],
        "programming_languages": [
            "Python"
        ],
        "experience": 1,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Science",
                "Computer Science",
                "Data Science"
            ]
        },
        "salary": {
            "max": 5634,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Mountain View, CA",
        "job_id": 3833891568,
        "company": "Georgia IT, Inc.",
        "title": "Data Engineer- Mountain View, CA",
        "created_on": 1720635649.9322405,
        "description": "Position: Data Engineer Location: Mountain View, CA Duration: Contract Rate: DOE Required Skills 10+ years of overall experience in data management space and at least 5 years of working in large data sets in a data lake environment : Highly proficient in SQL, Solid understanding of Spark including performance tuning. Solid understanding of the AWS Platform Experience in Python Nontechnical Skills Ability to work with Business and technical stake holders independently with minimal guidance - Must have Strong written and verbal communication and ability to articulate data with the business stake holders - Must have",
        "url": "https://www.linkedin.com/jobs/view/3833891568",
        "summary": "Data Engineer with 10+ years of experience in data management, specializing in large data sets and data lakes. Strong proficiency in SQL, Spark, AWS, and Python. Excellent communication and collaboration skills.",
        "industries": [
            "Data Management",
            "Technology",
            "Software Development",
            "Cloud Computing",
            "Data Analytics"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Analytical Thinking",
            "Independent Work"
        ],
        "hard_skills": [
            "SQL",
            "Spark",
            "AWS",
            "Python",
            "Data Lake"
        ],
        "tech_stack": [
            "AWS",
            "Spark"
        ],
        "programming_languages": [
            "SQL",
            "Python"
        ],
        "experience": 10,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3946976847,
        "company": "Fintool.com [YC]",
        "title": "Founding Data Engineer (Spark/Python/Elastic)",
        "created_on": 1720635653.9136717,
        "description": "Apply here About Fintool is a AI Equity Research Copilot for institutional investors. It’s a LLM on top of financial documents, starting with SEC filings. Fintool is engineered to discover financial insights beyond the reach of timely human analysis or search software. It helps from summarizing long 10-K to finding new investment opportunities. Fintool is backed by Y Combinator as well as entrepreneurs such as the co-founders of Datadog, Vercel, HuggingFace or domain experts from OpenAI to Deepmind. Team Nicolas Bustamante: spent 7 years building one of the largest AI-driven legal search engines (Bloomberg for lawyers). Nicolas hired nearly 200 people, secured millions of dollars in debt and equity funding, and the profitable business was successfully acquired by Summit Partners, a $43B billion growth equity fund, for $x00M+ Edouard Godrey: worked for nine years at Apple, leading teams of data scientists and engineers. He worked on Apple Search (Spotlight) and Apple Pay, maintaining big data pipelines and deploying cutting-edge AI models. He received the 2019 Apple Pay Innovation Award for outstanding contributions and fresh insights. Our philosophy Small team: small in-person teams outperform large and well-funded companies. When people visit our office, they should be surprised by how few people we are. Ship code: we avoid meetings, PM jargon to release early, release often, and listen to customers. In-person: we believe high-performing teams do their best work, build long-term relationships, and have the most fun in person. Company Values Clone and improve the best: we're not about reinventing the wheel but about enhancing proven success. We are shameless cloners who stand on the shoulders of giants. We draw inspiration and then create differentiation because distinctiveness drives dominance. Release early, release often, and listen to your customers: speed matters in business, so we push better-than-perfect updates for customers asap. Mastery comes from repeated experiments and learning from mistakes rather than putting in a set number of hours. It’s 10,000 iterations, not 10,000 hours. Warren Buffett: We model our personal and professional ethos on the principles he exemplifies. Upholding integrity, valuing honesty, practicing frugality, championing lifelong learning, embracing humility, extending generosity, applying rationality, and demonstrating patience. Every day, we strive to mirror these Buffett-inspired virtues. Job Desc We are building real-time data pipelines for millions of unstructured financial documents to feed our financial LLM. It’s cutting-edge data engineering at the AI frontier. Tech stack: Spark/Databricks, Python, Elastic, Postgres and LLM. Knowing React, Next.js, and TypeScript is a plus. Experience: 5+ years of deploying production code at a company with a large infrastructure. Location: San Francisco Contract: Full-time Apply here",
        "url": "https://www.linkedin.com/jobs/view/3946976847",
        "summary": "Fintool, a Y Combinator-backed AI Equity Research Copilot, seeks a Data Engineer to build real-time data pipelines for millions of unstructured financial documents. The role involves cutting-edge data engineering at the AI frontier using technologies like Spark/Databricks, Python, Elastic, Postgres, and LLMs. Ideal candidate has 5+ years of production code deployment experience and knowledge of React, Next.js, and TypeScript.",
        "industries": [
            "Fintech",
            "Artificial Intelligence",
            "Data Engineering",
            "Equity Research"
        ],
        "soft_skills": [
            "Problem Solving",
            "Communication",
            "Collaboration",
            "Teamwork",
            "Leadership",
            "Data Analysis",
            "Critical Thinking"
        ],
        "hard_skills": [
            "Spark",
            "Databricks",
            "Python",
            "Elastic",
            "Postgres",
            "LLMs",
            "React",
            "Next.js",
            "TypeScript",
            "Data Pipelines",
            "Unstructured Data",
            "Financial Data"
        ],
        "tech_stack": [
            "Spark",
            "Databricks",
            "Python",
            "Elastic",
            "Postgres",
            "LLM",
            "React",
            "Next.js",
            "TypeScript"
        ],
        "programming_languages": [
            "Python",
            "TypeScript"
        ],
        "experience": 5,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Jose, CA",
        "job_id": 3967448415,
        "company": "VARITE INC",
        "title": "Senior Data Developer",
        "created_on": 1720635655.5311544,
        "description": "Pay Range: $90-95/hr Duties You will translate the business requirements into technical specifications using logical and critical thinking skills. You will profile data, join them, cleanse them, and build data products that meet the business requirements. You will troubleshoot and resolve issues related to data quality, data incompleteness, and data pipeline. You will have the ability to identify the root cause of poor code performance and resolve them through code optimization, modifying data structures, and configuration changes. You will support multiple ongoing projects in a fast-paced environment across a global company You should be comfortable with the design and implementation of any stage of an end-to-end data pipeline, including data ingestion, processing, transformation, and storage. Skills: A willingness to learn Client's business processes, systems, and the technologies used. Strong communication and collaboration skills essential. The ability to work with Product Owners and Stakeholders to reach consensus during design and development is crucial. You will need to clearly explain data nuances, data quality issues, and limitations to non-technical colleagues. The ability to read complex SQL or Python queries, understand them, and modify them for efficiency and readability. A minimum of 8 years of experience in data engineering or related roles, with a solid background in data warehousing, data modeling, data access, and data storage techniques. Proficiency in SQL or Python with at least 8 years of experience. Experience with cloud platforms such as Databricks, Azure and proficiency in cloud services such as Azure Data Factory. Education Education: Bachelors or Masters",
        "url": "https://www.linkedin.com/jobs/view/3967448415",
        "summary": "This role involves translating business requirements into technical specifications, profiling and cleaning data, building data products, and troubleshooting data issues. You will design and implement all stages of an end-to-end data pipeline. Strong communication and collaboration skills are essential, along with the ability to work with stakeholders and explain data concepts to non-technical colleagues. You'll also need proficiency in SQL or Python, cloud platforms like Databricks and Azure, and experience in data warehousing and modeling.",
        "industries": [
            "Data Engineering",
            "Software Engineering",
            "Technology",
            "Data Science",
            "Analytics"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Critical Thinking",
            "Analytical Skills",
            "Decision Making",
            "Time Management",
            "Teamwork",
            "Leadership",
            "Interpersonal Skills",
            "Presentation Skills",
            "Negotiation"
        ],
        "hard_skills": [
            "SQL",
            "Python",
            "Data Warehousing",
            "Data Modeling",
            "Data Access",
            "Data Storage",
            "Data Pipeline",
            "Data Ingestion",
            "Data Processing",
            "Data Transformation",
            "Data Quality",
            "Data Incompleteness",
            "Code Optimization",
            "Azure Data Factory",
            "Azure",
            "Databricks"
        ],
        "tech_stack": [
            "Azure",
            "Databricks",
            "Azure Data Factory",
            "SQL",
            "Python"
        ],
        "programming_languages": [
            "SQL",
            "Python"
        ],
        "experience": 8,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Computer Science",
                "Data Science",
                "Engineering",
                "Mathematics",
                "Statistics"
            ]
        },
        "salary": {
            "max": 95,
            "min": 90
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Sunnyvale, CA",
        "job_id": 3948425564,
        "company": "Capgemini Engineering",
        "title": "Senior System Engineer (Unix, Python, SQL)",
        "created_on": 1720635659.5394845,
        "description": "Position Description: We are looking for a talented and experienced Infrastructure Engineer / Systems Engineer to join our IT Infrastructure team. The ideal candidate will have a strong background in SQL, Unix shell scripting, and Python development. As a mid-level engineer, you will be responsible for maintaining and enhancing our infrastructure, ensuring the reliability and performance of our systems, and developing automation scripts to streamline operations. The person will work in the CAD Infra team supporting different design groups (Architecture, Design, Verification, Physical Implementation) providing Quality metrics for the overall management chain to anticipate risks and help for the design making during the various design phases. The dashboards are consumed by project managers, engineering managers and overall leadership team. The person will leverage our Meta tooling to store and visualize the data. Key Responsibilities: Database Management: Utilize advanced SQL skills to manage and optimize databases. Perform database administration tasks, including backups, restores, and performance tuning. Develop complex queries, stored procedures, and scripts for data analysis and reporting. System Administration: Manage and maintain Unix/Linux servers, ensuring high availability and performance. Develop and maintain advanced Unix shell scripts for system automation and monitoring. Perform regular system updates, patches, and security configurations. Automation and Scripting: Develop Python scripts to automate repetitive tasks and improve system efficiency. Integrate Python scripts with various systems and tools to enhance functionality. Troubleshoot and debug Python code to resolve issues quickly. Requirements: Technical Skills: Advanced proficiency in SQL for database management and optimization. Advanced knowledge of Unix shell scripting and command-line tools. Strong Python development skills, with experience in automation and system integration. Experience: Proven experience in infrastructure or systems engineering roles. Familiarity with system monitoring tools and practices. Understanding of network protocols and security best practices. Nice to Have: Experience with cloud platforms such as AWS, Azure, or Google Cloud. Knowledge of configuration management tools like Ansible, Puppet, or Chef. Experience with containerization technologies such as Docker or Kubernetes. Familiarity with version control systems, particularly Git. Life at Capgemini Capgemini supports all aspects of your well-being throughout the changing stages of your life and career. For eligible employees, we offer: Flexible work Healthcare including dental, vision, mental health, and well-being programs Financial well-being programs such as 401(k) and Employee Share Ownership Plan Paid time off and paid holidays Paid parental leave Family building benefits like adoption assistance, surrogacy, and cryopreservation Social well-being benefits like subsidized back-up child/elder care and tutoring Mentoring, coaching and learning programs Employee Resource Groups Disaster Relief Disclaimer Capgemini discloses salary range information in compliance with state and local pay transparency obligations. The disclosed range represents the lowest to highest salary we, in good faith, believe we would pay for this role at the time of this posting, although we may ultimately pay more or less than the disclosed range, and the range may be modified in the future. The disclosed range takes into account the wide range of factors that are considered in making compensation decisions including, but not limited to, geographic location, relevant education, qualifications, certifications, experience, skills, seniority, performance, sales or revenue-based metrics, and business or organizational needs. At Capgemini, it is not typical for an individual to be hired at or near the top of the range for their role. The base salary range for the tagged location is $104K - $170K / year. This role may be eligible for other compensation including variable compensation, bonus, or commission. Full time regular employees are eligible for paid time off, medical/dental/vision insurance, 401(k), and any other benefits to eligible employees. Note: No amount of pay is considered to be wages or compensation until such amount is earned, vested, and determinable. The amount and availability of any bonus, commission, or any other form of compensation that are allocable to a particular employee remains in the Company's sole discretion unless and until paid and may be modified at the Company’s sole discretion, consistent with the law. Capgemini is an Equal Opportunity Employer encouraging diversity in the workplace. All qualified applicants will receive consideration for employment without regard to race, national origin, gender identity/expression, age, religion, disability, sexual orientation, genetics, veteran status, marital status or any other characteristic protected by law. This is a general description of the Duties, Responsibilities and Qualifications required for this position. Physical, mental, sensory or environmental demands may be referenced in an attempt to communicate the manner in which this position traditionally is performed. Whenever necessary to provide individuals with disabilities an equal employment opportunity, Capgemini will consider reasonable accommodations that might involve varying job requirements and/or changing the way this job is performed, provided that such accommodations do not pose an undue hardship. Capgemini is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to your recruiting contact. Click the following link for more information on your rights as an Applicant http://www.capgemini.com/resources/equal-employment-opportunity-is-the-law Please be aware that Capgemini may capture your image (video or screenshot) during the interview process and that image may be used for verification, including during the hiring and onboarding process. Applicants for employment in the US must have valid work authorization that does not now and/or will not in the future require sponsorship of a visa for employment authorization in the US by Capgemini.",
        "url": "https://www.linkedin.com/jobs/view/3948425564",
        "summary": "This role involves managing and optimizing databases, managing Unix/Linux servers, developing automation scripts in Python, and providing data visualizations for design teams. The ideal candidate will have strong SQL, Unix shell scripting, and Python skills, as well as experience in infrastructure or systems engineering roles. ",
        "industries": [
            "Information Technology",
            "Software Development",
            "Engineering",
            "Computer Hardware",
            "Database Management"
        ],
        "soft_skills": [
            "Communication",
            "Problem Solving",
            "Teamwork",
            "Organization",
            "Attention to Detail"
        ],
        "hard_skills": [
            "SQL",
            "Unix Shell Scripting",
            "Python",
            "Database Administration",
            "System Administration",
            "Automation",
            "System Monitoring",
            "Network Protocols",
            "Security Best Practices",
            "Data Visualization"
        ],
        "tech_stack": [
            "SQL",
            "Unix",
            "Python",
            "AWS",
            "Azure",
            "Google Cloud",
            "Ansible",
            "Puppet",
            "Chef",
            "Docker",
            "Kubernetes",
            "Git",
            "Meta tooling"
        ],
        "programming_languages": [
            "SQL",
            "Python"
        ],
        "experience": 3,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Information Technology",
                "Engineering"
            ]
        },
        "salary": {
            "max": 170000,
            "min": 104000
        },
        "benefits": [
            "Flexible work",
            "Healthcare",
            "Dental",
            "Vision",
            "Mental health",
            "Well-being programs",
            "401(k)",
            "Employee Share Ownership Plan",
            "Paid time off",
            "Paid holidays",
            "Paid parental leave",
            "Adoption assistance",
            "Surrogacy",
            "Cryopreservation",
            "Subsidized back-up child/elder care",
            "Tutoring",
            "Mentoring",
            "Coaching",
            "Learning programs",
            "Employee Resource Groups",
            "Disaster Relief"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Diego, CA",
        "job_id": 3970981905,
        "company": "Talent Groups",
        "title": "Big Data Developer",
        "created_on": 1720635661.123756,
        "description": "AWS Big Data Lead Engineer San Diego, CA or Acton, Boston, MA ( Onsite only ) Full-Time Job Description: We are seeking a talented Big Data Engineer/Developer to join our dynamic team. In this role, you will be responsible for designing, implementing, and maintaining scalable data pipelines and systems using Apache Spark and Apache Hadoop ecosystem tools. You will work closely with our data science and engineering teams to integrate cutting-edge algorithms and models into production systems. Experience with Java/Scala programming languages and Kubernetes orchestration is essential for this role. Responsibilities: Design and develop robust and scalable data processing pipelines using Apache Spark. Implement data ingestion, transformation, and storage solutions leveraging Apache Hadoop ecosystem components. Collaborate with data scientists to integrate machine learning algorithms and models into production systems. Optimize and tune Spark jobs for performance and efficiency. Design and implement data architectures that support large-scale data processing. Deploy and manage containerized applications using Kubernetes. Troubleshoot and resolve issues in development, test, and production environments.",
        "url": "https://www.linkedin.com/jobs/view/3970981905",
        "summary": "We are looking for a Big Data Engineer/Developer to design, implement, and maintain scalable data pipelines and systems using Apache Spark and Apache Hadoop ecosystem tools. You will work closely with our data science and engineering teams to integrate cutting-edge algorithms and models into production systems. Experience with Java/Scala programming languages and Kubernetes orchestration is essential.",
        "industries": [
            "Data Engineering",
            "Software Development",
            "Information Technology"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Troubleshooting"
        ],
        "hard_skills": [
            "Apache Spark",
            "Apache Hadoop",
            "Java",
            "Scala",
            "Kubernetes"
        ],
        "tech_stack": [
            "Apache Spark",
            "Apache Hadoop",
            "Kubernetes"
        ],
        "programming_languages": [
            "Java",
            "Scala"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Ontario, CA",
        "job_id": 3848984345,
        "company": "ClifyX",
        "title": "Software Engineer - Python, Java",
        "created_on": 1720635662.8545651,
        "description": "Software Engineer - Python, Java, SQL, UNIX/Linux, FastAPI, VueJS, Boo Yet to share Toronto, ON",
        "url": "https://www.linkedin.com/jobs/view/3848984345",
        "summary": "Software Engineer position in Toronto, ON, utilizing Python, Java, SQL, UNIX/Linux, FastAPI, VueJS, and Boo.",
        "industries": [
            "Software Development",
            "Technology"
        ],
        "soft_skills": [],
        "hard_skills": [
            "Python",
            "Java",
            "SQL",
            "UNIX",
            "Linux",
            "FastAPI",
            "VueJS",
            "Boo"
        ],
        "tech_stack": [
            "Python",
            "Java",
            "SQL",
            "UNIX",
            "Linux",
            "FastAPI",
            "VueJS",
            "Boo"
        ],
        "programming_languages": [
            "Python",
            "Java",
            "SQL",
            "Boo"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3956576946,
        "company": "Intelliswift Software",
        "title": "Python Data Engineer - Content Operations",
        "created_on": 1720635664.5135045,
        "description": "$56.34 per hour on W2 Location - San Francisco, CA Hybrid Duration - 2 Months We are seeking an enthusiastic and inquisitive Data Engineer who is passionate about data and proficient in programming with Python. This role involves working on diverse projects that include processing, analyzing, and interpreting large datasets within the context of global financial markets. Responsibilities Extract, transform, join, and load data from one or multiple systems to other systems, repositories, or tools supporting internal projects associated with the operational management of customer-ready help content. Build, format, and publish aggregated data sets containing customer-facing help content and associated attributes. Design and develop data pipelines to ingest, transform, and load unstructured and structured data from various sources. Document all programming tasks and data sets for future reference and troubleshooting. Leverage Large Language Models (LLMs) to automate specific content-related tasks. Enhance productivity for content authoring processes and teams by integrating tooling, including AI-based tooling. 1 to 3 years of relevant experience Python coding: Write clean, efficient, and maintainable code as standalone scripts in Python to implement data solutions and algorithms tailored to project requirements. Experience and knowledge should include: Data typing and data structures Calling RESTful and GraphQL web service APIs Querying or populating both No-SQL and SQL databases Utilizing cloud storage and platforms such as AWS or Azure Proficiency with Git, artifactory, and modern developer tooling for creating and managing code in a large enterprise. Demonstrated ability to perform exploratory data analysis to uncover insights and patterns within help content-related datasets and utilize statistical methods. Experience with prompt engineering for LLMs. Some experience with standard use of basic features of cloud platforms like Azure or AWS Proven ability to be self-driven and work relatively independently Experience as a user of Photoshop or other Creative Cloud products is a plus Specific experience working with digital content management systems or tools would be ideal Experience with a framework for AI applications is also a plus",
        "url": "https://www.linkedin.com/jobs/view/3956576946",
        "summary": "We are seeking a Data Engineer with 1-3 years of experience to work on data processing, analysis, and interpretation of large datasets related to global financial markets. Responsibilities include extracting, transforming, and loading data, building data sets, designing data pipelines, documenting tasks, leveraging LLMs for automation, and integrating tooling for content authoring. Strong Python skills are required, including data typing, data structures, RESTful and GraphQL APIs, querying databases, cloud storage, Git, and developer tooling. Experience with exploratory data analysis, prompt engineering, and cloud platforms is also essential. Bonus points for experience with Photoshop, content management systems, and AI frameworks.",
        "industries": [
            "Finance",
            "Data Science",
            "Technology",
            "Content Management"
        ],
        "soft_skills": [
            "Enthusiastic",
            "Inquisitive",
            "Passionate",
            "Self-driven",
            "Independent"
        ],
        "hard_skills": [
            "Python",
            "Data Typing",
            "Data Structures",
            "RESTful APIs",
            "GraphQL APIs",
            "No-SQL Databases",
            "SQL Databases",
            "Cloud Storage",
            "AWS",
            "Azure",
            "Git",
            "Artifactory",
            "Exploratory Data Analysis",
            "Statistical Methods",
            "Prompt Engineering",
            "Photoshop",
            "Content Management Systems",
            "AI Frameworks"
        ],
        "tech_stack": [
            "Python",
            "RESTful APIs",
            "GraphQL APIs",
            "No-SQL Databases",
            "SQL Databases",
            "AWS",
            "Azure",
            "Git",
            "Artifactory",
            "LLMs",
            "AI Frameworks",
            "Photoshop"
        ],
        "programming_languages": [
            "Python"
        ],
        "experience": 1,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 5634,
            "min": 5634
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Oakland, CA",
        "job_id": 3912306053,
        "company": "Replica",
        "title": "Software Engineer (Data Production +Access)",
        "created_on": 1720635666.216597,
        "description": "Software Engineer (Data Production + Access) Oakland, New York, Kansas City, or Remote _______________________________________________________________________________ Replica is a privacy-centric urban data platform that delivers critical insights about the built environment. With better data, human-context, and an intuitive design, Replica helps public and private sectors make informed, effective, and responsive decisions. Our platform models travel behavior over time to show how people across the country live, move, and work. This data is used by planners, scientists, analysts, and policymakers who are working to make our cities more sustainable, equitable, and resilient. We contextualize hard choices so that our clients understand the trade-offs surrounding their decisions. Whether for a city planner increasing public transit to underserved neighborhoods or for a grocery chain evaluating where to open a new location, Replica enables cities and businesses to make more informed, people-centered decisions. We spun out of Alphabet in 2019 when we secured series A funding from venture firms such as Innovation Endeavors, Firebrand Ventures, and Revolution’s Rise of the Rest Seed Fund. Our series B round was led by Founders Fund in 2021. Today, we are a team of 39 employees both working remotely and from our offices in Oakland, New York, and Kansas City. Our data is used to make decisions affecting the physical places where we work, study, and live. These places are complex and deeply human. This responsibility necessitates providing transparency, pursuing equity, and preserving privacy. We are committed to bringing together a diverse workforce and creating an environment of inclusion. We value our differences and we encourage all to apply. We are committed to equal opportunity regardless of race, color, ancestry, religion, gender, gender identity, parental or pregnancy status, national origin, sexual orientation, age, citizenship, marital status, disability, Veteran status, or any other status protected by the laws or regulations in the locations where we operate. Responsibilities This is a full-stack software engineering role in both front-end and back-end web application development. Build product features to access, visualize, and interact with complex urban data; enable customers to understand changes, evaluate solutions, and make predictions for residents Design, build, and maintain RESTful APIs which use spatial context to efficiently query and aggregate nationwide geospatial and temporal data Collaborate across teams to deeply understand users’ needs, develop and evolve requirements based on user research, set delivery timelines, and implement and deploy urban data applications at nationwide scale Work with Engineering managers and leaders to define the team's technical vision and roadmap Drive engineering best practices to enable a robust production environment and maintain an effective development velocity Achieve uptime requirements and maintain API stability through remote logging and observability tools Participate in on-call rotation; investigate and resolve production problems as they arise Understand the urban data science context to ensure that correct, consistent, and meaningful data is delivered to customers The Team The Data Production and Access team is made up of urbanist-minded engineers and designers who are passionate about evolving our product to address critical economic, equity, and sustainability challenges. The team designs, implements and maintains the building blocks and reusable components that power Replica’s data production and simulation pipelines, and the web application features and other tools to make Replica’s data accessible, discoverable, and meaningful for users. Team members are empowered to operate autonomously within a culture of iteration, mutual support, and learning. Most of the team is based in the Bay Area and New York, but we work with cross-functional teams in Kansas City, and serve customers across North America. Minimum Qualifications Education: Bachelor’s degree in Computer Science or equivalent practical experience Experience: 3+ years of experience as a full-stack engineer Technical: Experience in web application development using a modern JavaScript client framework Bonus Technical: Experience with our tech stack which includes: React, Typescript, Node, Mapbox, Python, Pandas, Postgres, BigQuery, Kubernetes, and Google Cloud Platform Data background: Experience with big data frameworks and data visualization Urban context: Academic or work experience in Urban Science or related disciplines What We Value We work in the service of others We understand that talent + diversity + curiosity + relentlessness wins We believe walking > talking We operate with thoughtful urgency We communicate openly and directly We build products people use Benefits Our people! We work as a team and are excited to contribute to city planning Competitive salary based on experience and potential for impact Equity at an early stage startup Health benefits including medical, dental, vision, FSA & HSA options. 401k account + employer contribution Offices in Oakland, New York, & Kansas City Flexible PTO Compensation And Benefits Replica is committed to fair and equitable compensation practices The salary range for this position is $98,100 to 149,600. Actual compensation packages are based on several factors that are unique to each candidate, including but not limited to relevant qualifications, depth of experience, skill set, certifications, training, education, and specific work location. The compensation packages may be adjusted based on the candidate’s work location, due to differences in the cost of living for the given location. Our position titles may also span multiple career levels The total compensation package for this position additionally includes equity stock options, employee benefit package, 401k with 3% safe harbor employer contribution, unlimited PTO, and may also include other applicable incentive compensation and/or bonuses. Employees, and their families, may elect coverage to Replica’s Benefit Plan including Medical (PPO base, PPO buy-up, HMO, HSA), Dental (PPO base, PPO buy-up), and Vision (PPO base, PPO buy-up) plans. Employees may also choose to elect an FSA or HSA account Replica provides Basic Life Insurance and AD&D, however, additional Voluntary Life Insurance and AD&D can be purchased for the employee, spouse, and/or dependent child(ren) Replica observes 13 paid holidays annually and provides employees with an unlimited PTO policy Additionally, all employees receive access to our Employee Assistance Program For more information, visit https://replicahq.com/careers/ Replica is an Equal Opportunity Employer Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions To perform this job successfully, an individual must be able to perform each essential function satisfactorily The requirements listed are representative of the knowledge, skill and/or ability required Ability to operate at both a strategic/conceptual level and at a detailed, operational level metrics driven; highly disciplined Must have strong interpersonal skills, maturity and good judgment and be capable of communicating with a diverse range of individuals A hands-on, action-oriented approach that fits well with the entrepreneurial, fast-paced culture Engaging leadership style that builds and sustains credibility with colleagues, clients and other stakeholders Broad functional experience in areas of strategic planning and marketing, sales and market development and planning If you don't think you meet all of the criteria above, but still are interested in the job, please apply. Nobody checks every box, and we're looking for someone excited to join the team. Powered by JazzHR eR5ZEIie3l",
        "url": "https://www.linkedin.com/jobs/view/3912306053",
        "summary": "Replica, a privacy-centric urban data platform, seeks a full-stack Software Engineer to build features for accessing, visualizing, and interacting with complex urban data. The role involves designing, building, and maintaining RESTful APIs for querying and aggregating nationwide geospatial and temporal data, collaborating with cross-functional teams to understand user needs, and ensuring the delivery of correct, consistent, and meaningful data to customers.",
        "industries": [
            "Urban Planning",
            "Data Science",
            "Software Development",
            "Technology",
            "Geospatial Analysis",
            "City Planning"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Decision Making",
            "Time Management",
            "Leadership",
            "Interpersonal Skills",
            "Critical Thinking"
        ],
        "hard_skills": [
            "JavaScript",
            "React",
            "Typescript",
            "Node",
            "Mapbox",
            "Python",
            "Pandas",
            "Postgres",
            "BigQuery",
            "Kubernetes",
            "Google Cloud Platform",
            "RESTful APIs",
            "Spatial Context",
            "Data Visualization",
            "Agile Development"
        ],
        "tech_stack": [
            "React",
            "Typescript",
            "Node",
            "Mapbox",
            "Python",
            "Pandas",
            "Postgres",
            "BigQuery",
            "Kubernetes",
            "Google Cloud Platform"
        ],
        "programming_languages": [
            "JavaScript",
            "Typescript",
            "Python"
        ],
        "experience": 3,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Computer Science"
            ]
        },
        "salary": {
            "max": 149600,
            "min": 98100
        },
        "benefits": [
            "Competitive Salary",
            "Equity",
            "Health Benefits",
            "401k",
            "Flexible PTO",
            "Unlimited PTO",
            "Employee Assistance Program"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Los Angeles, CA",
        "job_id": 3847574568,
        "company": "CyberCoders",
        "title": "Founding Data Engineer - Python, SQL, Scala",
        "created_on": 1720635667.9943318,
        "description": "Location: 100% Remote Salary: Up to $210K for Base + Bonus + Stock Options (High Upside) We are a Technology Company that focuses on human interaction and providing access to events, peoples, and recommendations globally. As our company grows, we're in need of a Founding Data Engineer to build out our core data infrastructure, and looking to take a leadership role. Position Overview: The Founding Data Engineer will be responsible for developing, maintaining, and optimizing data pipelines and architectures. This role requires a strong technical background in Python, SQL, Scala, and other related technologies. The Data Engineer will be the go-to person for data engineering and work with stakeholders to define and implement data infrastructure solutions. Key Responsibilities Design, develop, and maintain data architectures and pipelines using a variety of technologies such as Python and SQL. Scala and MongoDB are a plus Lead data engineering projects from start to finish, and collaborate with stakeholders to ensure successful delivery. Monitor and optimize existing data pipelines and architectures. Develop and document best practices for data engineering. Identify and troubleshoot performance issues in data pipelines and architectures. Strong experience with data pipeline and workflow management tools like Apache Airflow, Luigi, or Prefect. In-depth knowledge of real-time data processing frameworks such as Kinesis, Kafka, Flink, or Spark Streaming. Experience with Segment is a plus. Experience with data modeling tools and ETL frameworks, with a strong emphasis on performance optimization. Qualifications 5+ years of experience in a Data Engineer role. Proven experience with Python, SQL and other related technologies. Experience with AWS Strong knowledge of data modeling and ETL processes. Experience leading data engineering projects. Benefits We offer comprehensive medical and dental coverage, $50 a day food delivery budget Equity Learning opportunities Unlimited vacation, 12 weeks paid parental leave, and we pay all employees $1,000 a year to go somewhere in the world Email Your Resume In Word To Looking forward to receiving your resume through our website and going over the position with you. Clicking apply is the best way to apply, but you may also: rajeev.peterson@cybercoders.com Please do NOT change the email subject line in any way. You must keep the JobID: linkedin : RP6-1790602 -- in the email subject line for your application to be considered.*** Rajeev Peterson - Associate Manager Applicants must be authorized to work in the U.S. CyberCoders is proud to be an Equal Opportunity Employer All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, sexual orientation, gender identity or expression, national origin, ancestry, citizenship, genetic information, registered domestic partner status, marital status, status as a crime victim, disability, protected veteran status, or any other characteristic protected by law. CyberCoders will consider qualified applicants with criminal histories in a manner consistent with the requirements of applicable law. CyberCoders is committed to working with and providing reasonable accommodation to individuals with physical and mental disabilities. If you need special assistance or an accommodation while seeking employment, please contact a member of our Human Resources team to make arrangements. CyberCoders will consider for Employment in the City of Los Angeles qualified Applicants with Criminal Histories in a manner consistent with the requirements of the Los Angeles Fair Chance Initiative for Hiring (Ban the Box) Ordinance.",
        "url": "https://www.linkedin.com/jobs/view/3847574568",
        "summary": "We are seeking a Founding Data Engineer to develop and maintain data pipelines and architectures using Python, SQL, Scala, and other related technologies. This role will lead data engineering projects and work with stakeholders to ensure successful delivery.",
        "industries": [
            "Technology",
            "Data Engineering"
        ],
        "soft_skills": [
            "Leadership",
            "Communication",
            "Collaboration",
            "Problem-solving",
            "Technical"
        ],
        "hard_skills": [
            "Python",
            "SQL",
            "Scala",
            "MongoDB",
            "Apache Airflow",
            "Luigi",
            "Prefect",
            "Kinesis",
            "Kafka",
            "Flink",
            "Spark Streaming",
            "Segment",
            "ETL",
            "AWS"
        ],
        "tech_stack": [
            "Python",
            "SQL",
            "Scala",
            "MongoDB",
            "Apache Airflow",
            "Luigi",
            "Prefect",
            "Kinesis",
            "Kafka",
            "Flink",
            "Spark Streaming",
            "Segment",
            "AWS"
        ],
        "programming_languages": [
            "Python",
            "SQL",
            "Scala"
        ],
        "experience": 5,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 210000,
            "min": 0
        },
        "benefits": [
            "Medical and dental coverage",
            "Food delivery budget",
            "Equity",
            "Learning opportunities",
            "Unlimited vacation",
            "Paid parental leave",
            "Travel stipend"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Fremont, CA",
        "job_id": 3792289336,
        "company": "Pony.ai",
        "title": "Software Engineer, Data and Evaluation",
        "created_on": 1720635669.6951647,
        "description": "Description Founded in 2016 in Silicon Valley, Pony.ai has quickly become a global leader in autonomous mobility and is a pioneer in extending autonomous mobility technologies and services at a rapidly expanding footprint of sites around the world. Operating Robotaxi, Robotruck and Personally Owned Vehicles (POV) business units, Pony.ai is an industry leader in the commercialization of autonomous driving and is committed to developing the safest autonomous driving capabilities on a global scale. Pony.ai’s leading position has been recognized, with CNBC ranking Pony.ai #10 on its CNBC Disruptor list of the 50 most innovative and disruptive tech companies of 2022. In June 2023, Pony.ai was recognized on the XPRIZE and Bessemer Venture Partners inaugural “XB100” 2023 list of the world’s top 100 private deep tech companies, ranking #12 globally. As of August 2023, Pony.ai has accumulated nearly 15 million miles of autonomous driving globally. Responsibility As a member of the Data and Evaluation team, you will design and implement a diverse set of backend services and tools. The systems you build will have a large impact on ADAS, from fleet data collection & processing, to Machine Learning workflows, to evaluation and validation of the ADAS software stack. Design and implement tools and pipeline to handle data from autonomous vehicles including data labeling, batch processing, simulation, system and module evaluation Design and implement smart labeling pipeline using deep learning to generate evaluation and training ground truth Setup and maintain monitoring for system metrics, latency and alerts. Work closely with different autonomous driving components and dive deep into each component and design corresponding evaluation metrics and tools Requirements Strong programming skills in C/C++, Python, and software design BS/MS or Ph.D. in Computer Science or a related field Experience in large data set processing and familiarity with real time systems Solid experience in a fast-paced and structured engineering environment Full stack experience including both front end and back end is preferred Statistics analysis experience is preferred Experience with Linux, networking, storage and virtualization automation with tools like Kubernetes, Terraform, Ansible, Puppet or similar In-depth knowledge of container orchestrators and cluster management software Experience with data stores and indexers like PostgreSQL, ElasticSearch, Redis Compensation and Benefits Base Salary Range: $120,000 - $180,000 Annually Compensation may vary outside of this range depending on many factors, including the candidate’s qualifications, skills, competencies, experience, and location. Base pay is one part of the Total Compensation and this role may be eligible for bonuses/incentives and restricted stock units. Also, we provide the following benefits to the eligible employees: Health Care Plan (Medical, Dental & Vision) Retirement Plan (Traditional and Roth 401k) Life Insurance (Basic, Voluntary & AD&D) Paid Time Off (Vacation & Public Holidays) Family Leave (Maternity, Paternity) Short Term & Long Term Disability Free Food & Snacks",
        "url": "https://www.linkedin.com/jobs/view/3792289336",
        "summary": "Pony.ai, a leading autonomous mobility company, is seeking a Data and Evaluation Engineer to design and implement backend services and tools for ADAS (Advanced Driver-Assistance Systems). Responsibilities include data labeling, batch processing, simulation, system/module evaluation, and developing smart labeling pipelines using deep learning. The role requires strong programming skills in C/C++, Python, experience with large datasets, real-time systems, and full-stack development. Experience with Linux, Kubernetes, Terraform, data stores (PostgreSQL, ElasticSearch, Redis), and container orchestration is preferred.",
        "industries": [
            "Autonomous Vehicles",
            "Artificial Intelligence",
            "Software Development",
            "Technology"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Analytical Thinking",
            "Time Management",
            "Attention to Detail"
        ],
        "hard_skills": [
            "C/C++",
            "Python",
            "Software Design",
            "Data Processing",
            "Real-time Systems",
            "Machine Learning",
            "Deep Learning",
            "Data Labeling",
            "Batch Processing",
            "Simulation",
            "System Evaluation",
            "Metrics Design",
            "Monitoring",
            "Linux",
            "Kubernetes",
            "Terraform",
            "Ansible",
            "Puppet",
            "PostgreSQL",
            "ElasticSearch",
            "Redis",
            "Container Orchestration",
            "Statistics"
        ],
        "tech_stack": [
            "C/C++",
            "Python",
            "Kubernetes",
            "Terraform",
            "Ansible",
            "Puppet",
            "PostgreSQL",
            "ElasticSearch",
            "Redis"
        ],
        "programming_languages": [
            "C/C++",
            "Python"
        ],
        "experience": 0,
        "education": {
            "min_degree": "BS",
            "fields": [
                "Computer Science",
                "Related Fields"
            ]
        },
        "salary": {
            "max": 180000,
            "min": 120000
        },
        "benefits": [
            "Health Insurance",
            "Dental Insurance",
            "Vision Insurance",
            "Retirement Plan",
            "Life Insurance",
            "Paid Time Off",
            "Family Leave",
            "Disability Insurance",
            "Free Food"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3889286308,
        "company": "University of California, San Francisco",
        "title": "Data Engineer",
        "created_on": 1720635671.4760067,
        "description": "Neuro-Memory and Aging Full Time 77338BR Job Summary This is an onsite position @ Mission Bay; Local SF The Brain Aging Network for Cognitive Health (BRANCH) study aims to understand the complex biological, genetic, and lifestyle factors that underlie differential aging trajectories. Our program enrolls hundreds of older adults, who participate in a variety of procedures including blood draws, neuropsychological testing, and neurological exams. The overarching goals are to gain a better understanding of what healthy aging is and identify the earliest changes associated with degenerative brain disease. The Data Engineer role is a core resource in the BRANCH team and will be critical in bringing thoughtful Data Engineering practices in the space of healthy aging. This position is a rare opportunity to make a meaningful contribution to neurology clinical research via a Data Engineering position. Data Engineering In this role, you will work to programmatically build new and expand existing systems for collecting, validating, managing, and preparing high-quality data/sets for use by research scientists and their collaborators. This work will include acquiring access to multi-modal internal and external data sources (applications, relational databases, object stores, large-scale -omic data sets, video libraries, imaging pipelines), developing pipelines to transform all or a subset of the data sources into useful data sets for relevant analyses by research scientists, and feature engineering. In addition, building, testing, and maintaining database pipeline architectures are a necessity along with creating new and expanding existing data validation methods. Data Analysis Under the guidance of the research team, you will build bespoke data analysis tools to assist in the communication and access of data sets to internal and external researchers and collaborators. This role will also collaborate with data managers and data scientists at the center to gather and prepare data for analysis. Data Management A portion of this role will require basic data management activities such as creating best practices for streamlined data cleaning, data harmonization, manual data wrangling, collecting, organizing, and accessing data for use in creating bespoke data sets. A key responsibility of this role is being responsible for ensuring the integrity of the data. Security Compliance with the institution’s data governance and security policies is a must. Reporting The Data Engineer will report to Drs. Kramer and Casaletto will have a mentoring relationship with Drs. Paolillo, Rose George, and Joe Hesse. The final salary and offer components are subject to additional approvals based on UC policy. To see the salary range for this position (we recommend that you make a note of the job code and use that to look up): TCS Non-Academic Titles Search (https://tcs.ucop.edu/non-academic-titles) Please note: An offer will take into consideration the experience of the final candidate AND the current salary level of individuals working at UCSF in a similar role. For roles covered by a bargaining unit agreement, there will be specific rules about where a new hire would be placed on the range. To learn more about the benefits of working at UCSF, including total compensation, please visit: https://ucnet.universityofcalifornia.edu/compensation-and-benefits/index.html Department Description Throughout the Department of Neurology at UCSF, our mission is to deliver superb patient care, to apply state-of-the-art translational research methods to discover the causes of and treatments for human nervous system disorders, and to educate each generation of medical students, neurology residents, and postdoctoral fellows. By fostering cross-disciplinary interactions among scientists around the world, we accelerate the pace of discovery and champion the University's global health initiatives. Required Qualifications Bachelor's degree in a related area and/or equivalent experience/training 1-2 years relevant experience Understanding of data management operations and database administration, including data modeling, data definition, data conversion, and management of content or unstructured information. Knowledge of relevant rules and regulations. Demonstrated ability to work with others from diverse backgrounds. Demonstrated effective communication and interpersonal skills. Demonstrated ability to communicate technical information to technical and non-technical personnel at various levels in the organization. Self-motivated and works independently and as part of a team. Demonstrated problem-solving skills. Able to learn effectively and meet deadlines. Strong organizational skills. Strong analytical and design skills, including the ability to abstract information requirements from real-world processes to understand information flows in computer systems. Ability to represent relevant information in abstract models. Critical thinking skills and attention to detail. Knowledge of best practices around securing protected health information Experience working in a research/academic setting Experience deploying web services to integrate multi-modal data systems Demonstrated ability working with large data sets Excellent proficiency in MS Excel Familiarity with Relational Database Systems Excellent proficiency with query/query languages such as SQL and XQuery Intermediate to excellent proficiency in R Intermediate to excellent proficiency in Python Intermediate proficiency in MATLAB Preferred Qualifications Demonstrated service orientation skills. Knowledge of HIPAA Basic understanding of genetics, proteomics, and *omics desired About UCSF The University of California, San Francisco (UCSF) is a leading university dedicated to promoting health worldwide through advanced biomedical research, graduate-level education in the life sciences and health professions, and excellence in patient care. It is the only campus in the 10-campus UC system dedicated exclusively to the health sciences. We bring together the world’s leading experts in nearly every area of health. We are home to five Nobel laureates who have advanced the understanding of cancer, neurodegenerative diseases, aging and stem cells. Pride Values UCSF is a diverse community made of people with many skills and talents. We seek candidates whose work experience or community service has prepared them to contribute to our commitment to professionalism, respect, integrity, diversity and excellence – also known as our PRIDE values. In addition to our PRIDE values, UCSF is committed to equity – both in how we deliver care as well as our workforce. We are committed to building a broadly diverse community, nurturing a culture that is welcoming and supportive, and engaging diverse ideas for the provision of culturally competent education, discovery, and patient care. Additional information about UCSF is available at diversity.ucsf.edu Join us to find a rewarding career contributing to improving healthcare worldwide. Equal Employment Opportunity The University of California San Francisco is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran or disabled status, or genetic information. Organization Campus Job Code and Payroll Title 007198 DATA SYS ANL 2 Job Category Engineering, Professional (Non-Clinical) Bargaining Unit 99 - Policy-Covered (No Bargaining Unit) Employee Class Career Percentage 100% Location Mission Bay (SF) Shift Days Shift Length 8 Hours Additional Shift Details M-F 9am-5pm",
        "url": "https://www.linkedin.com/jobs/view/3889286308",
        "summary": "The Data Engineer role supports the BRANCH study, a research program focused on understanding aging and degenerative brain disease. This position will be responsible for data management, engineering, and analysis, working with multi-modal data sources, building pipelines for data transformation, and developing bespoke data analysis tools.",
        "industries": [
            "Healthcare",
            "Research",
            "Biotechnology",
            "Neurology"
        ],
        "soft_skills": [
            "Communication",
            "Interpersonal skills",
            "Problem-solving",
            "Organizational skills",
            "Critical thinking",
            "Attention to detail",
            "Teamwork",
            "Self-motivation",
            "Independence"
        ],
        "hard_skills": [
            "Data management",
            "Database administration",
            "Data modeling",
            "Data definition",
            "Data conversion",
            "Data harmonization",
            "Data cleaning",
            "Data validation",
            "Data security",
            "Data governance",
            "SQL",
            "XQuery",
            "R",
            "Python",
            "MATLAB",
            "Excel",
            "Web services",
            "HIPAA",
            "Genetics",
            "Proteomics",
            "Omics"
        ],
        "tech_stack": [
            "Relational Database Systems",
            "SQL",
            "XQuery",
            "R",
            "Python",
            "MATLAB",
            "Excel",
            "Web services"
        ],
        "programming_languages": [
            "SQL",
            "XQuery",
            "R",
            "Python",
            "MATLAB"
        ],
        "experience": 2,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Related area"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Total compensation",
            "Health insurance",
            "Retirement plan",
            "Paid time off",
            "Disability insurance",
            "Life insurance"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3964812144,
        "company": "Tatari",
        "title": "Senior Data Engineer",
        "created_on": 1720635673.18195,
        "description": "Tatari is on a mission to revolutionize TV advertising. We work with some of your favorite disruptor brands—like Calm, Fiverr, and RocketMoney—to grow their business through linear and streaming TV. We combine a sophisticated media buying platform with proprietary analytics to turn TV advertising into an automated, digital-like experience. Named one of the Hottest Ad Tech Companies by Business Insider, and Best Places to Work by Inc. Magazine, our team includes founders and leaders from Google, Microsoft, Stripe, Shazam and Facebook. We are growing rapidly as we accelerate our mission to automate the complex landscape of managing and measuring television advertising. We have a long-term goal to make marketing on TV available to businesses of any size.. The Measurement Calculation team is responsible for providing accurate and reliable brand-focused metrics, measurement methodologies, and systems to our clients and Client Service team (CS) so they can create marketing strategies via informed audience targeting. This includes designing, building, and maintaining robust data pipelines that serve as the cornerstone of Tatari's business operations. As dedicated professionals, we collectively work on developing and supporting the intricate calculations and algorithms necessary for both linear and streaming TV platforms. Our focus lies in creating scalable and efficient solutions that enable Tatari to leverage data-driven insights effectively and drive success in the dynamic TV advertising landscape. Responsibilities: Building, managing and optimizing data infrastructure, designing and developing data pipelines, and ensuring the reliability and scalability of data systems. Data Infrastructure Design: Designing and implementing scalable, efficient, and reliable data infrastructure, including data storage, processing, and retrieval systems. Data Pipeline Development: Developing and maintaining robust and efficient data pipelines to ingest, transform, and deliver data from various sources to data storage and analytical systems. Data Modeling and Architecture: Designing and implementing data models and database schemas that support efficient data storage, retrieval, and analysis. ETL (Extract, Transform, Load) Processes: Building and maintaining ETL processes to extract data from different sources, transform it into a suitable format, and load it into data storage systems. Performance Optimization: Identifying and resolving performance bottlenecks in data pipelines and database systems. Tuning and optimizing queries, indexes, and data storage configurations to improve overall system performance. Collaboration and Leadership: Collaborating with cross-functional teams, including data scientists, analysts, and software engineers, to understand their data requirements and provide them with the necessary infrastructure and tools. Mentoring and providing technical guidance to junior data engineers. Monitoring and Troubleshooting: Implementing monitoring systems and practices to ensure the availability and reliability of data systems. Proactively identifying and resolving issues and investigating data-related incidents or anomalies. Technology Evaluation and Implementation: Keeping up with the latest trends and technologies in the data engineering field. Evaluating and recommending new tools, frameworks, and technologies to improve data engineering processes and efficiency. Qualifications: 6+ years of experience working in data architecture, data modeling, and building data pipelines & distributed systems at scale. Recent accomplishments working with relational and NoSQL data stores, methods, and approaches (STAR, Dimensional Modeling). 2+ years of experience with a modern data stack (Kafka, Spark, Airflow, lakehouse architectures, real-time databases, dbt, etc.) and cloud data warehouses such as RedShift, Snowflake. Cloud Computing Platforms: Familiarity with cloud computing platforms like Amazon Web Services (AWS) and proficiency in leveraging cloud-based services for data storage, processing, and analytics, such as Amazon S3, EC2, and Lambda. Strong Technical Background: Proficiency in programming languages commonly used in data engineering, such as Python, Java, Scala, or SQL. Experience with data processing frameworks and tools like Apache Spark (including Databricks), and Hadoop and knowledge of database technologies like SQL databases (e.g., MySQL, PostgreSQL). Problem-solving and analytical thinking: Ability to identify and troubleshoot data-related issues, optimize systems, and propose innovative solutions. Communication and Collaboration: Excellent communication skills to effectively collaborate with cross-functional teams, stakeholders, and business users and ability to explain technical concepts to non-technical audiences and translate business requirements into technical solutions. Leadership and Mentoring: Experience in providing technical guidance, mentoring junior data engineers, and leading data engineering initiatives and ability to drive projects, prioritize tasks, and manage timelines. Benefits : Competitive salary ($170-210K/annually) Equity compensation 100% health insurance premium coverage for you and your dependents Unlimited PTO and sick days Snacks, drinks, and catered lunches at the office Team building events $1000 annual continued education benefit $500 WFH reimbursement $125 pre-tax monthly stipend to spend on whatever you want Annual mental health awareness app reimbursement FSA and commuter benefits Monthly Company Wellness Day Off Hybrid RTO (currently 2 days in office). This is an in-office position At Tatari, we believe in the importance of cultivating teams with diverse backgrounds and offering equal opportunities to all. We strive to create a welcoming, inclusive environment where every team member feels valued and diversity is celebrated.",
        "url": "https://www.linkedin.com/jobs/view/3964812144",
        "summary": "Tatari is seeking a Data Engineer to build, manage, and optimize data infrastructure for their TV advertising platform. This role involves designing data pipelines, data models, and ETL processes, optimizing performance, collaborating with cross-functional teams, and staying up-to-date with data engineering trends.",
        "industries": [
            "Advertising",
            "Marketing",
            "Technology",
            "Media",
            "Data Science",
            "Data Engineering",
            "Analytics",
            "Television"
        ],
        "soft_skills": [
            "Problem-solving",
            "Analytical thinking",
            "Communication",
            "Collaboration",
            "Leadership",
            "Mentoring",
            "Technical guidance",
            "Project management",
            "Prioritization",
            "Time management"
        ],
        "hard_skills": [
            "Data architecture",
            "Data modeling",
            "Data pipelines",
            "Distributed systems",
            "Relational databases",
            "NoSQL databases",
            "STAR schema",
            "Dimensional modeling",
            "Kafka",
            "Spark",
            "Airflow",
            "Lakehouse architectures",
            "Real-time databases",
            "dbt",
            "Redshift",
            "Snowflake",
            "AWS",
            "Amazon S3",
            "EC2",
            "Lambda",
            "Python",
            "Java",
            "Scala",
            "SQL",
            "Apache Spark",
            "Databricks",
            "Hadoop",
            "MySQL",
            "PostgreSQL"
        ],
        "tech_stack": [
            "Kafka",
            "Spark",
            "Airflow",
            "dbt",
            "Redshift",
            "Snowflake",
            "AWS",
            "Amazon S3",
            "EC2",
            "Lambda",
            "Apache Spark",
            "Databricks",
            "Hadoop",
            "MySQL",
            "PostgreSQL"
        ],
        "programming_languages": [
            "Python",
            "Java",
            "Scala",
            "SQL"
        ],
        "experience": 6,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 210000,
            "min": 170000
        },
        "benefits": [
            "Competitive salary",
            "Equity compensation",
            "100% health insurance premium coverage",
            "Unlimited PTO and sick days",
            "Snacks, drinks, and catered lunches",
            "Team building events",
            "Annual continued education benefit",
            "WFH reimbursement",
            "Monthly stipend",
            "Annual mental health awareness app reimbursement",
            "FSA and commuter benefits",
            "Monthly Company Wellness Day Off",
            "Hybrid RTO"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Palo Alto, CA",
        "job_id": 3960419042,
        "company": "Tesla",
        "title": "Software Engineer, Data & Analytics, Self-Driving",
        "created_on": 1720635676.881393,
        "description": "What To Expect The Self-Driving Data and Analytics team is responsible for mining fleet data from millions of Tesla cars to improve our neural networks and creating real-time analytics to measure Self-Driving performance. This is a very high-impact role that helps the team set overall goals and priorities, but also contributes directly to those goals by sourcing high volume and high-quality data. What You'll Do Write fleet queries and Python scripts to source new datasets Create metrics to analyze performance of Self-Driving and specific driving events Visualize data and metrics using tools like Grafana (custom JavaScript is a plus) Create Spark jobs to support data pipelines with appropriate alerting to monitor infrastructure health Manage inflow rate and volume of fleet data Iterate with AI engineering partners on effectiveness of new datasets Build automation and tools to make these workflows maximally efficient What You'll Bring Bachelor's Degree in Computer Science, Engineering, Physics, Math or proof of exeptional skills in related field Experience with Python (Pandas, NumPy) and SQL (PostgreSQL) Experience with Spark, Caspian, JavaScript, Grafana, and/or OpenSearch is a plus Ability to thrive in a rapidly changing environment with high ambiguity; self-starter mentality Benefits Compensation and Benefits Along with competitive pay, as a full-time Tesla employee, you are eligible for the following benefits at day 1 of hire: Aetna PPO and HSA plans > 2 medical plan options with $0 payroll deduction Family-building, fertility, adoption and surrogacy benefits Dental (including orthodontic coverage) and vision plans, both have options with a $0 paycheck contribution Company Paid (Health Savings Account) HSA Contribution when enrolled in the High Deductible Aetna medical plan with HSA Healthcare and Dependent Care Flexible Spending Accounts (FSA) LGBTQ+ care concierge services 401(k) with employer match, Employee Stock Purchase Plans, and other financial benefits Company paid Basic Life, AD&D, short-term and long-term disability insurance Employee Assistance Program Sick and Vacation time (Flex time for salary positions), and Paid Holidays Back-up childcare and parenting support resources Voluntary benefits to include: critical illness, hospital indemnity, accident insurance, theft & legal services, and pet insurance Weight Loss and Tobacco Cessation Programs Tesla Babies program Commuter benefits Employee discounts and perks program Expected Compensation $104,000 - $348,000/annual salary + cash and stock awards + benefits Pay offered may vary depending on multiple individualized factors, including market location, job-related knowledge, skills, and experience. The total compensation package for this position may also include other elements dependent on the position offered. Details of participation in these benefit plans will be provided if an employee receives an offer of employment. , Tesla",
        "url": "https://www.linkedin.com/jobs/view/3960419042",
        "summary": "Tesla is seeking a Data and Analytics Engineer to work on their Self-Driving team. This role involves using fleet data from millions of Tesla cars to improve neural networks and create real-time analytics to measure Self-Driving performance. Responsibilities include writing queries and scripts to source new datasets, creating metrics to analyze performance, visualizing data, creating Spark jobs for data pipelines, and managing data inflow rate and volume. The ideal candidate will have experience with Python, SQL, Spark, Grafana, and JavaScript, and be able to thrive in a fast-paced environment.",
        "industries": [
            "Automotive",
            "Technology",
            "Artificial Intelligence",
            "Data Science",
            "Analytics"
        ],
        "soft_skills": [
            "Problem-solving",
            "Analytical",
            "Communication",
            "Teamwork",
            "Self-starter",
            "Adaptability",
            "Detail-oriented",
            "Organizational"
        ],
        "hard_skills": [
            "Python",
            "SQL",
            "Spark",
            "Caspian",
            "JavaScript",
            "Grafana",
            "OpenSearch",
            "Pandas",
            "NumPy",
            "PostgreSQL"
        ],
        "tech_stack": [
            "Python",
            "SQL",
            "Spark",
            "Caspian",
            "JavaScript",
            "Grafana",
            "OpenSearch"
        ],
        "programming_languages": [
            "Python",
            "SQL",
            "JavaScript"
        ],
        "experience": 0,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Engineering",
                "Physics",
                "Math"
            ]
        },
        "salary": {
            "max": 348000,
            "min": 104000
        },
        "benefits": [
            "Medical",
            "Dental",
            "Vision",
            "HSA",
            "FSA",
            "401(k)",
            "Employee Stock Purchase Plans",
            "Life Insurance",
            "Disability Insurance",
            "Employee Assistance Program",
            "Sick and Vacation Time",
            "Paid Holidays",
            "Back-up Childcare",
            "Weight Loss and Tobacco Cessation Programs",
            "Tesla Babies program",
            "Commuter benefits",
            "Employee discounts and perks program"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Palo Alto, CA",
        "job_id": 3787336320,
        "company": "Lever Middleware Test Company 2",
        "title": "Software Engineer",
        "created_on": 1720635678.592008,
        "description": "You will be a founding member of the engineering team. You will be responsible for working with the team to build and deploy our first product and will be critical to the success of the team and the company. You will be applying the latest advances in deep learning to solve important and difficult vision and NLP problems. You will have a tremendous impact on the team and company’s culture. Qualifications BS in Computer Science/Engineering or related field Completed computer vision or natural language processing projects from start to finish 1 - 5 years of experience as a software engineer in industry Knowledge of modern software engineering best practices in algorithms, data structures, and clean code methodologies Experience collaborating with other software engineers on large and complex codebases Enthusiasm for staying up-to-date on latest CV and NLP research Lever builds modern recruiting software for teams to source, interview, and hire top talent. Our team strives to set a new bar for enterprise software with modern, well-designed, real-time apps. We participated in Y Combinator in summer 2012, and since then have raised $73 million. As the applicant tracking system of choice for Netflix, Eventbrite, ClearSlide, change.org, and thousands more leading companies, Lever means you hire the best by hiring together. Lever is an equal opportunity employer. We are committed to providing reasonable accommodations and will work with you to meet your needs. If you are a person with a disability and require assistance during the application process, please don’t hesitate to reach out! We celebrate our inclusive work environment and welcome members of all backgrounds and perspectives. Learn more about our team culture and commitment to diversity and inclusion.",
        "url": "https://www.linkedin.com/jobs/view/3787336320",
        "summary": "Lever, a Y Combinator-backed company, seeks a founding engineer to build and deploy their first product using deep learning for vision and NLP.  This role involves modern software engineering best practices and collaboration within a large codebase.",
        "industries": [
            "Software",
            "Technology",
            "Artificial Intelligence",
            "Machine Learning",
            "Recruiting",
            "Human Resources"
        ],
        "soft_skills": [
            "Collaboration",
            "Communication",
            "Teamwork",
            "Problem Solving",
            "Enthusiasm"
        ],
        "hard_skills": [
            "Deep Learning",
            "Computer Vision",
            "Natural Language Processing",
            "Software Engineering",
            "Algorithms",
            "Data Structures",
            "Clean Code Methodologies"
        ],
        "tech_stack": [
            "Deep Learning",
            "Computer Vision",
            "Natural Language Processing"
        ],
        "programming_languages": [],
        "experience": 1,
        "education": {
            "min_degree": "BS",
            "fields": [
                "Computer Science",
                "Engineering"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Palo Alto, CA",
        "job_id": 3941587803,
        "company": "Machinify, Inc.",
        "title": "Sr/Staff Data Engineer",
        "created_on": 1720635680.598308,
        "description": "Machinify is a revolutionary healthcare software company with a mission to ensure that patients get the right medical treatment, at the right time, at the right price. Our cloud-based Machinify AI platform leverages the latest advances in machine learning, large language models, data analytics, and cloud processing to solve previously intractable problems, transforming healthcare administration and payment operations. We are seeking a Sr./Staff Data Engineer to build and own critical data pipelines. What you’ll do: Independently understand all aspects of a business problem including those unrelated to their area of expertise, weigh pros and cons of different approaches and suggest ones likely to succeed Work with a cross-functional organization including engineering, delivering, subject-matter experts, product managers, as well as platform engineers to deliver a scalable framework. Map the customer data into Machinify canonical form. Identify and ingest non canonical fields and generalize the process to a minimal level of customization. Proactively design and adapt the canonical form to suit changing query patterns and needs. Ultimately own data availability and quality for the Data Science organization. What You Bring: Deep experience as a hands-on Data Engineer building production data pipelines Experience managing the delivery of complex data Experience in ETL orchestration and workflow management tools with a strong preference for Apache Airflow Experience in Spark or other distributed computing frameworks SQL and Python Advanced SQL performance tuning Kubernetes and building Docker images AWS & GCP Experience working with APIs to collect or ingest data Manage SLA for all pipelines in allocated areas of ownership Streaming technologies like kafka , spark streaming etc ELK stack , Grafana etc The base salary for this position will vary based on an array of factors unique to each candidate such as qualifications, years and depth of experience, skill set, certifications, etc. The base salary range for this role is $200k-250k. We are hiring for different seniorities, and our Recruiting team will let you know if you qualify for a different role/range. Salary is one component of the total compensation package, which includes meaningful equity, excellent healthcare, flexible time off, and other benefits and perks. Equal Employment Opportunity at Machinify Machinify is committed to hiring talented and qualified individuals with diverse backgrounds for all of its positions. Machinify believes that the gathering and celebration of unique backgrounds, qualities, and cultures enriches the workplace.",
        "url": "https://www.linkedin.com/jobs/view/3941587803",
        "summary": "Machinify is seeking a Sr./Staff Data Engineer to design and build data pipelines for their cloud-based AI platform. This role involves working with cross-functional teams to ensure data quality and availability, and includes responsibilities such as data mapping, ETL orchestration, and managing SLAs for pipelines. The ideal candidate will have experience with Spark, Airflow, SQL, Python, Kubernetes, Docker, AWS/GCP, and streaming technologies like Kafka.",
        "industries": [
            "Healthcare",
            "Software",
            "Technology",
            "Artificial Intelligence",
            "Machine Learning"
        ],
        "soft_skills": [
            "Problem-solving",
            "Communication",
            "Collaboration",
            "Proactive",
            "Analytical",
            "Ownership",
            "Project Management",
            "Time Management"
        ],
        "hard_skills": [
            "Data Engineering",
            "Data Pipelines",
            "ETL",
            "Apache Airflow",
            "Spark",
            "Distributed Computing",
            "SQL",
            "Python",
            "SQL Performance Tuning",
            "Kubernetes",
            "Docker",
            "AWS",
            "GCP",
            "APIs",
            "Kafka",
            "Spark Streaming",
            "ELK Stack",
            "Grafana"
        ],
        "tech_stack": [
            "Apache Airflow",
            "Spark",
            "Kafka",
            "ELK Stack",
            "Grafana",
            "Kubernetes",
            "Docker",
            "AWS",
            "GCP"
        ],
        "programming_languages": [
            "Python",
            "SQL"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Data Science",
                "Engineering"
            ]
        },
        "salary": {
            "max": 250000,
            "min": 200000
        },
        "benefits": [
            "Equity",
            "Healthcare",
            "Flexible Time Off"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Sunnyvale, CA",
        "job_id": 3963464125,
        "company": "Amazon",
        "title": "Jr. Software Development Engineer - Sunnyvale",
        "created_on": 1720635682.3129451,
        "description": "Description Amazon is looking for a highly-motivated Jr. Software Development Engineer (SDE)! Jr. SDEs write real software and collaborate with experienced software engineers who provide guidance and opportunities for ownership on projects that matter to our customers. As a year-round intern, Jr. SDEs become fully integrated into their teams and regularly contribute to impactful deliverables. Your design and code will contribute to solving some of Amazon's most complex technical challenges. The Jr. SDE role is part of Amazon's Jr Developer Program - a year-round internship opportunity that offers a symbiotic relationship between work and school. Jrs. receive 1:1 mentoring throughout their time in the program, receiving guidance and insight from a full-time Amazonian on their team. Because of the internship's extended tenure, our Jrs. become immersed in an Amazon team and gain real-life technical experience. Flexible part-time schedules during the school year and full-time employment over the summer creates an environment where students can succeed in both their work and their education. Seattle has a growing population of Jrs, allowing for deep connections with fellow students engaged in similar roles. Upon successful completion of the Jr. Developer Program, the opportunity for full-time employment may be available at an Amazon corporate site. Role Highlights Part-time work during the school year (16 hours/week) Full-time work during the summer (40 hours/week) 1:1 mentoring with an experienced Software Engineer Effective performance management and integrated opportunities for growth Basic Qualifications Currently enrolled in an accredited college or university Bachelor's or Master's degree program. Majoring in Computer Science, Software Engineering, or related STEM field. Graduating between June 2026 and beyond Ability to work year-round until graduation (part-time during the school year and full-time during the summer). Living within commutable distance to Sunnyvale, CA and able to work in-person year-round. Programming experience with at least one modern language such as Java, Python, or C++ including object-oriented design. Preferred Qualifications Previous technical internship(s), if applicable. Ability to effectively articulate technical challenges and solutions. Adept at handling ambiguous or undefined problems as well as ability to think abstractly. Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us. Our compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $16.83/hr in our lowest geographic market up to $75.63/hr in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. For more information, please visit https://www.aboutamazon.com/workplace/employee-benefits. This position will remain posted until filled. Applicants should apply via our internal or external career site. Company - Amazon.com Services LLC Job ID: A2687204",
        "url": "https://www.linkedin.com/jobs/view/3963464125",
        "summary": "Amazon is looking for a Jr. Software Development Engineer (SDE) for a year-round internship opportunity that offers a symbiotic relationship between work and school. Jr. SDEs will write real software, collaborate with experienced engineers, and contribute to impactful deliverables. This role is part of Amazon's Jr Developer Program, offering 1:1 mentoring, effective performance management, and integrated opportunities for growth. Upon successful completion, there may be an opportunity for full-time employment at an Amazon corporate site.",
        "industries": [
            "Information Technology",
            "Software Development",
            "E-commerce"
        ],
        "soft_skills": [
            "Highly Motivated",
            "Collaboration",
            "Communication",
            "Problem Solving",
            "Abstract Thinking",
            "Ambiguity Tolerance"
        ],
        "hard_skills": [
            "Object-Oriented Design",
            "Java",
            "Python",
            "C++"
        ],
        "tech_stack": [
            "Java",
            "Python",
            "C++"
        ],
        "programming_languages": [
            "Java",
            "Python",
            "C++"
        ],
        "experience": 0,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Computer Science",
                "Software Engineering",
                "STEM"
            ]
        },
        "salary": {
            "max": 7563,
            "min": 1683
        },
        "benefits": [
            "Medical",
            "Financial",
            "Equity",
            "Sign-On Payments"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Los Angeles, CA",
        "job_id": 3949624553,
        "company": "Incedo Inc.",
        "title": "Lead Data Engineer",
        "created_on": 1720635683.8857467,
        "description": "Lead Data Engineer Job Location: Southern California (Hybrid) Job Purpose: Must have: Strong background in solutions across Data Engineering value chain with expertise on Azure-Databricks Strengthen the Data engineering team with Big Data solutions running on Azure Designing and planning of new ETLs and migration of existing ETL processes Ability to write quality technical documentation Experience in data warehouse design and best practices Establish DevOps processes for marshalling big data work products from development to production. Analyzing and recommending new product capabilities on cloud environment Strong organizational skills, with the ability to work autonomously as well as in a team based environment. Collaborate with business, technology and architecture teams across the Bank to design and develop next generation data platforms using traditional DB technologies, open source frameworks and cloud services. Exposure to various ETL and Business Intelligence tools Must be very strong in writing SQL queries Deep Data Warehouse and relational database experience Minimum Qualifications: Masters or bachelor’s Degree in computer science or equivalent 10+ years of experience in delivering technology solutions Five+ years of experience with Hadoop, including HDFS, Spark, Hive, HBase Five+ years of experience with NoSQL such as MongoDB, Elastic Search, Solr Three+ years of experience in leading design of Big Data solutions across Cloud platforms Two+ years of architect design experience with Azure Services Experience in data modeling, data vault architecture, & CDW architecture with Kimball model, data fabric or databricks Expertise with data lakes, data warehouse, data marts Experience with Data Governance, Master Data Management Data Warehouse experience with Apache Kylin, Apache Nifi, Apache Airflow, and Kylo Strong Delivery Background with ability to translate product objectives into an execution and delivery plan with milestones and resource needs Strong communication and presentation skills Experience with git and other source control systems Solid grounding in Agile methodologies Good to have: Knowledge of how to implement serverless architecture on Azure A strong delivery background across the delivery of high-value, business-facing technical projects in major organizations Strong client relationship management skills to identify and close suitable business development opportunities Experience of managing client delivery teams, ideally coming from a Data Engineering / Data Science environment. Hands-on working experience in analyzing source system data and data flows, working with structured and unstructured data Must be very strong in writing SQL queries Strengthen the Data engineering team with Big Data solutions Strong technical, analytical, and problem-solving skills Strong organizational skills, with the ability to work autonomously as well as in a team-based environment Pleasant Personality, Strong Communication & Interpersonal Skills Certifications (Preferred) Azure Developer AZ-204 DP-100, DP-203 and DP-300 Azure Solutions Architect AZ-305",
        "url": "https://www.linkedin.com/jobs/view/3949624553",
        "summary": "Lead Data Engineer role focused on Azure Databricks and Big Data solutions. Requires strong experience in ETL, data warehousing, data modeling, and cloud platforms.  Strong communication, organization, and technical skills needed for this leadership position. ",
        "industries": [
            "Banking",
            "Finance"
        ],
        "soft_skills": [
            "Communication",
            "Presentation",
            "Organizational",
            "Teamwork",
            "Problem-Solving",
            "Client Relationship Management",
            "Analytical"
        ],
        "hard_skills": [
            "ETL",
            "Data Engineering",
            "Azure Databricks",
            "Big Data",
            "Hadoop",
            "HDFS",
            "Spark",
            "Hive",
            "HBase",
            "NoSQL",
            "MongoDB",
            "Elastic Search",
            "Solr",
            "Cloud Platforms",
            "Azure Services",
            "Data Modeling",
            "Data Vault Architecture",
            "CDW Architecture",
            "Kimball Model",
            "Data Fabric",
            "Databricks",
            "Data Lakes",
            "Data Warehouse",
            "Data Marts",
            "Data Governance",
            "Master Data Management",
            "Apache Kylin",
            "Apache Nifi",
            "Apache Airflow",
            "Kylo",
            "SQL",
            "Git",
            "Agile Methodologies",
            "Serverless Architecture"
        ],
        "tech_stack": [
            "Azure",
            "Databricks",
            "Hadoop",
            "HDFS",
            "Spark",
            "Hive",
            "HBase",
            "MongoDB",
            "Elastic Search",
            "Solr",
            "Apache Kylin",
            "Apache Nifi",
            "Apache Airflow",
            "Kylo",
            "Git"
        ],
        "programming_languages": [
            "SQL"
        ],
        "experience": 10,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Computer Science"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3897983223,
        "company": "Unreal Staffing, Inc",
        "title": "Lead Big Data Engineer",
        "created_on": 1720635685.5747743,
        "description": "Company Overview: Welcome to the forefront of data-driven innovation! Our company is dedicated to leveraging the power of big data to drive transformative change and solve complex problems across industries. We're committed to building cutting-edge big data solutions that enable advanced analytics, machine learning, and business intelligence. Join us and lead our team in shaping the future of big data engineering. Position Overview: As the Lead Big Data Engineer, you'll be at the forefront of our big data engineering efforts, leading a team of talented engineers to design, build, and maintain our big data infrastructure and pipelines. You'll drive the development of scalable and efficient data solutions that support the needs of our data-driven organization. If you're a seasoned engineer with expertise in big data technologies and a track record of leading successful data projects, we want you on our team. Requirements Key Responsibilities: Technical Leadership: Lead a team of big data engineers, providing guidance, mentorship, and technical leadership in big data engineering best practices and technologies Big Data Infrastructure Design: Lead the design and architecture of our big data infrastructure, including data lakes, data warehouses, and streaming data platforms, ensuring scalability, reliability, and performance Data Pipeline Development: Lead the development of scalable and efficient data pipelines for ingesting, processing, and transforming large volumes of structured and unstructured data from diverse sources, ensuring reliability, scalability, and performance Data Modeling: Lead the design and implementation of data models and schemas to support analytical and operational requirements, ensuring data integrity, consistency, and performance in a distributed environment Data Integration: Lead efforts to integrate data from disparate sources and systems, ensuring data consistency, quality, and completeness throughout the data lifecycle Performance Optimization: Lead optimization efforts for data pipelines and queries, identifying and addressing bottlenecks and inefficiencies to improve system scalability and reliability Monitoring and Alerting: Lead the implementation of monitoring and alerting systems to track big data platform performance and health, detecting and mitigating issues proactively to minimize downtime and data loss Security and Compliance: Lead efforts to implement security controls and data governance policies to ensure data security, privacy, and compliance with regulatory requirements Documentation and Best Practices: Define and promote best practices for big data engineering, documentation, and usage, ensuring clear and comprehensive documentation to facilitate understanding and collaboration among team members Collaboration: Collaborate with cross-functional teams, including data scientists, software engineers, and business stakeholders, to understand requirements and deliver big data solutions that meet business needs Mentorship and Development: Mentor junior engineers, providing guidance, support, and opportunities for growth and development in their big data engineering careers Qualifications: Bachelor's degree or higher in Computer Science, Engineering, Mathematics, or related field 7+ years of experience in big data engineering, with a focus on designing, building, and maintaining big data infrastructure and pipelines Leadership experience, with a demonstrated ability to lead and mentor a team of engineers Proficiency in programming languages such as Java, Scala, or Python, and expertise with big data technologies such as Hadoop, Spark, Kafka, Hive, HBase, and more Strong understanding of distributed systems and parallel processing, with experience designing and optimizing data pipelines for performance and scalability Experience with cloud platforms such as AWS, Azure, or Google Cloud Platform, and familiarity with cloud-based big data services such as Amazon EMR, Azure HDInsight, or Google Dataproc Experience with SQL and NoSQL databases, data warehousing, and ETL/ELT processes Strong problem-solving skills and analytical thinking, with the ability to troubleshoot complex data issues and optimize system performance Excellent communication and collaboration skills, with the ability to work effectively in cross-functional teams and communicate technical concepts to non-technical stakeholders Benefits Competitive salary: The industry standard salary for Lead Big Data Engineers typically ranges from $200,000 to $300,000 per year, depending on experience and qualifications Comprehensive health, dental, and vision insurance plans Flexible work hours and remote work options Generous vacation and paid time off Professional development opportunities, including access to training programs, conferences, and workshops State-of-the-art technology environment with access to cutting-edge tools and resources Vibrant and inclusive company culture with opportunities for growth and advancement Exciting projects with real-world impact at the forefront of big data innovation Join Us: Ready to lead the charge in big data engineering? Apply now to join our team and drive the future of data-driven innovation!",
        "url": "https://www.linkedin.com/jobs/view/3897983223",
        "summary": "We are looking for a Lead Big Data Engineer to lead a team of engineers in designing, building, and maintaining our big data infrastructure and pipelines. You will be responsible for the development of scalable and efficient data solutions that support the needs of our data-driven organization.",
        "industries": [
            "Technology",
            "Data Science",
            "Analytics",
            "Software Development",
            "Engineering"
        ],
        "soft_skills": [
            "Leadership",
            "Mentorship",
            "Communication",
            "Collaboration",
            "Problem-solving",
            "Analytical Thinking"
        ],
        "hard_skills": [
            "Java",
            "Scala",
            "Python",
            "Hadoop",
            "Spark",
            "Kafka",
            "Hive",
            "HBase",
            "SQL",
            "NoSQL",
            "ETL",
            "ELT",
            "AWS",
            "Azure",
            "Google Cloud Platform",
            "Amazon EMR",
            "Azure HDInsight",
            "Google Dataproc"
        ],
        "tech_stack": [
            "Hadoop",
            "Spark",
            "Kafka",
            "Hive",
            "HBase",
            "AWS",
            "Azure",
            "Google Cloud Platform",
            "Amazon EMR",
            "Azure HDInsight",
            "Google Dataproc"
        ],
        "programming_languages": [
            "Java",
            "Scala",
            "Python"
        ],
        "experience": 7,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Computer Science",
                "Engineering",
                "Mathematics"
            ]
        },
        "salary": {
            "max": 300000,
            "min": 200000
        },
        "benefits": [
            "Competitive salary",
            "Health insurance",
            "Dental insurance",
            "Vision insurance",
            "Flexible work hours",
            "Remote work options",
            "Vacation",
            "Paid time off",
            "Professional development opportunities",
            "State-of-the-art technology environment",
            "Company culture",
            "Growth opportunities",
            "Exciting projects"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Sunnyvale, CA",
        "job_id": 3940986661,
        "company": "HireIO, Inc.",
        "title": "Senior Data Security Engineer",
        "created_on": 1720635687.1648748,
        "description": "Be responsible for security incident and event response and aid in threat and vulnerability research, including triage, remediation and documentation Be responsible for the assessment, verification, review, and audit of security/privacy controls and overall security posture across the organization Perform forensic testing, review test results, and collaborate with business stakeholders to establish sustainable resolution plans for vulnerabilities, gaps, and control deficiencies Develop situational awareness, stay informed on current technology and vulnerabilities, and utilize current information security disciplines and industry standards to ensure the confidentiality, integrity, and availability of information assets Improve the overall strategy, operations, and risk management of Data Security Design, develop, document and deploy enterprise security solutions to safeguard data and privacy Contribute to the development of security policies, security standards, and risk governance reporting processes in collaboration with cross-functional teams to continuously improve data governance Uphold data security by implementing DLP rules, monitoring of sensitive data access, and developing data cataloging tools for search and discovery, data lineage, and data quality improvements The base salary range for this position is $93,600- $218,400 annually The range displayed on each job posting reflects the minimum and maximum target for new hire base salary for the position across our US office locations. Individual pay is determined by a number of factors, including job-related skills, experience, and relevant education and training. This role may be eligible for additional discretionary bonuses and/or incentives, as well as a comprehensive benefits package. Requirements Experience with information technology at scale Knowledge of PCI DSS, GDPR, ISO27701, NYDFS 500, or other regulatory frameworks Experience working with SIEM systems, threat intelligence platforms, security automation and orchestration solutions, intrusion detection and prevention systems (IDS/IPS), file integrity monitoring (FIM), DLP and other network and system monitoring tools Familiarity with classes of vulnerabilities and appropriate remediation of industry-standard classification schemes (CVE, CVSS, CPE) Information security monitoring and response or related experience; In-depth and up-to-date understanding of the threat landscape and the techniques to defend against them - including tactics, techniques, and procedures Experience writing scripts (Python, Perl etc) and SQL, Konw how of Identity Lifecycle Management, Data Loss Prevention, Data Security, Security Assurance, or similar areas in a medium or large corporate environment Hands-on experience in the assessment, design, implementation, and configuration of data security products High ethical standards, personal integrity, and the ability to professionally handle confidential matters Fluent English and Mandarin FINTECH security experience is preferred",
        "url": "https://www.linkedin.com/jobs/view/3940986661",
        "summary": "This role involves managing security incidents, conducting threat and vulnerability research, assessing security posture, performing forensic testing, developing security solutions, and contributing to data governance initiatives. The individual will work to ensure the confidentiality, integrity, and availability of information assets.",
        "industries": [
            "Information Technology",
            "Cybersecurity",
            "FinTech",
            "Data Security",
            "Risk Management"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Analytical Thinking",
            "Decision Making",
            "Time Management",
            "Organizational Skills",
            "Attention to Detail",
            "Ethical Standards",
            "Personal Integrity"
        ],
        "hard_skills": [
            "Security Incident Response",
            "Threat Intelligence",
            "Vulnerability Research",
            "Security Assessment",
            "Forensic Testing",
            "Data Security",
            "Data Governance",
            "DLP",
            "SIEM",
            "Threat Intelligence Platforms",
            "Security Automation",
            "Orchestration",
            "IDS/IPS",
            "FIM",
            "Network Monitoring",
            "System Monitoring",
            "Vulnerability Remediation",
            "CVE",
            "CVSS",
            "CPE",
            "Scripting",
            "Python",
            "Perl",
            "SQL",
            "Identity Lifecycle Management",
            "Data Loss Prevention",
            "Security Assurance"
        ],
        "tech_stack": [
            "SIEM",
            "Threat Intelligence Platforms",
            "Security Automation and Orchestration Solutions",
            "Intrusion Detection and Prevention Systems (IDS/IPS)",
            "File Integrity Monitoring (FIM)",
            "DLP",
            "Network and System Monitoring Tools"
        ],
        "programming_languages": [
            "Python",
            "Perl",
            "SQL"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 218400,
            "min": 93600
        },
        "benefits": [
            "Discretionary Bonuses",
            "Incentives",
            "Comprehensive Benefits Package"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Los Angeles, CA",
        "job_id": 3948403369,
        "company": "FOX Tech",
        "title": "Senior Data Engineer",
        "created_on": 1720635689.3012843,
        "description": "Overview Of The Company Fox Corporation Under the FOX banner, we produce and distribute content through some of the world’s leading and most valued brands, including: FOX News Media, FOX Sports, FOX Entertainment, FOX Television Stations and Tubi Media Group. We empower a diverse range of creators to imagine and develop culturally significant content, while building an organization that thrives on creative ideas, operational expertise and strategic thinking. Job Description Under the FOX banner, we produce and distribute content through some of the world’s leading and most valued brands, including: FOX News Media, FOX Sports, FOX Entertainment, FOX Television Stations, and Tubi Media Group. We empower a diverse range of creators to imagine and develop culturally significant content while building an organization that thrives on creative ideas, operational expertise, and strategic thinking. About The Role We are seeking a Senior Data Engineer who is passionate about building robust, scalable, efficient, and high-quality Data Engineering solutions. As a key member of our data team at FOX, you will play a pivotal role in architecting and maintaining the data infrastructure that drives our content creation and distribution efforts. Your work will directly impact our ability to harness data-driven insights, enabling us to deliver exceptional content to our global audience and stay at the forefront of the media and entertainment industry. a Snapshot Of Your Responsibilities Design, build, and maintain efficient data pipelines for data ingestion, transformation, and storage Collaborate with data scientists, analysts, and other stakeholders to understand data requirements and deliver data solutions that meet business needs Optimize and troubleshoot data engineering processes to ensure data quality and reliability Work with various cloud-based technologies and services to manage and process large volumes of data Stay up-to-date with industry trends and best practices in data engineering and cloud technologies What You Will Need Proven ETL/Data Engineering experience, preferably on AWS ecosystem Experience working with cloud databases such as Redshift, Snowflake, or BigQuery Strong development experience with Python Proficiency in SQL Excellent problem-solving skills and the ability to work in a fast-paced, collaborative environment Strong communication skills and the ability to work effectively with cross-functional teams Bachelor's degree in Computer Science, Engineering, or a related field (or equivalent work experience) Experience in Data Engineering or a similar role Strong experience with AWS services, including S3, Lambda, API Gateway, and EMR Familiarity with data warehousing concepts and tools Experience with data modeling and schema design Knowledge of data integration and data transformation techniques NICE TO HAVE, BUT NOT A DEALBREAKER Experience working in an Agile development environment is a plus Learn more about Fox Tech at #foxtech We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, gender identity, disability, protected veteran status, or any other characteristic protected by law. We will consider for employment qualified applicants with criminal histories consistent with applicable law. At FOX, we foster a culture and environment where everyone feels welcome and can thrive. We are deeply committed to diversity, equity, and inclusion, including attracting, retaining, and promoting diverse talent across our company. We live in a diverse world, with different ideas and different perspectives that come together to spark new ideas and make great things happen. That means reflecting the diversity of the world around us is critical to our company’s success. We ensure that our viewers, communities and employees feel heard, represented, and celebrated both on screen and off. Pursuant to state and local pay disclosure requirements, the pay range for this role, with final offer amount dependent on education, skills, experience, and location is: $121,500.00-170,000.00 annually for California. This role is also eligible for various benefits, including medical/dental/vision, insurance, a 401(k) plan, paid time off, and other benefits in accordance with applicable plan documents. Benefits for Union represented employees will be in accordance with the applicable collective bargaining agreement.",
        "url": "https://www.linkedin.com/jobs/view/3948403369",
        "summary": "FOX Corporation is seeking a Senior Data Engineer to design, build, and maintain data pipelines, collaborate with data scientists and analysts, optimize data engineering processes, and work with cloud-based technologies. The ideal candidate will have proven ETL/Data Engineering experience on AWS, experience with cloud databases like Redshift, Snowflake, or BigQuery, strong Python and SQL development skills, excellent problem-solving abilities, and strong communication skills.",
        "industries": [
            "Media",
            "Entertainment",
            "Technology",
            "Data Engineering",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Problem-solving",
            "Communication",
            "Collaboration",
            "Teamwork",
            "Critical thinking",
            "Analytical skills",
            "Time management",
            "Organization",
            "Adaptability"
        ],
        "hard_skills": [
            "ETL",
            "Data Engineering",
            "AWS",
            "Redshift",
            "Snowflake",
            "BigQuery",
            "Python",
            "SQL",
            "Data Warehousing",
            "Data Modeling",
            "Schema Design",
            "Data Integration",
            "Data Transformation"
        ],
        "tech_stack": [
            "AWS",
            "Redshift",
            "Snowflake",
            "BigQuery",
            "Python",
            "SQL",
            "S3",
            "Lambda",
            "API Gateway",
            "EMR"
        ],
        "programming_languages": [
            "Python",
            "SQL"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Computer Science",
                "Engineering"
            ]
        },
        "salary": {
            "max": 170000,
            "min": 121500
        },
        "benefits": [
            "Medical",
            "Dental",
            "Vision",
            "Insurance",
            "401(k)",
            "Paid time off"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Santa Monica, CA",
        "job_id": 3958912526,
        "company": "The Walt Disney Company",
        "title": "Lead Data Engineer",
        "created_on": 1720635690.9366171,
        "description": "Disney Entertainment & ESPN Technology On any given day at Disney Entertainment & ESPN Technology, we’re reimagining ways to create magical viewing experiences for the world’s most beloved stories while also transforming Disney’s media business for the future. Whether that’s evolving our streaming and digital products in new and immersive ways, powering worldwide advertising and distribution to enhance flexibility and efficiency, or delivering Disney’s unmatched entertainment and sports content, every day is a moment to make a difference to partners and to hundreds of millions of people around the world. A few reasons why we think you’d love working for Disney Entertainment & ESPN Technology Building the future of Disney’s media business: DE&E Technologists are designing and building the infrastructure that will power Disney’s media, advertising, and distribution businesses for years to come. Reach & Scale: The products and platforms this group builds and operates delight millions of consumers every minute of every day – from Disney+ and Hulu, to ABC News and Entertainment, to ESPN and ESPN+, and much more. Innovation: We develop and execute groundbreaking products and techniques that shape industry norms and enhance how audiences experience sports, entertainment & news. The Product & Data Engineering team is responsible for end to end development for Disney’s world-class consumer-facing products, including streaming platforms Disney+, Hulu, and ESPN+, and digital products & experiences across ESPN, Marvel, Disney Studios, NatGeo, and ABC News. The team drives innovation at scale for millions of consumers around the world across Apple, Android, Smart TVs, game consoles, and the web, with our platforms powering core experiences like personalization, search, messaging and data. Job Summary: This role involves building and maintaining content metadata and performance pipelines, as well as collaborating closely with business, analytics, and engineering teams. You will be responsible for creating and maintaining user-friendly data structures to streamline reporting and monitor key performance indicators across various use-cases, including executive reporting, analytics, data science, and personalization. These efforts are crucial for effectively supporting organizational business objectives. The role requires expert knowledge of building scalable, fault-tolerant data processing pipelines to ensure the reliable delivery of data to our partners. This role is important in ensuring the quality and expediency of our reporting in regards to content metrics. Responsibilities and Duties of the Role: Lead the successful design and development of our Data Products and Data Warehouses around Content Performance and Content Engagement data. Design and develop scalable data warehousing solutions, building ETL pipelines in Big Data environments (cloud, on-prem, hybrid) Support data pipelines within AWS, Snowflake, Databricks, Spark and Airflow Help architect data solutions/frameworks and define data models for the underlying data warehouse and data marts Collaborate with Data Product Managers, Data Architects and Data Engineers to design, implement, and deliver successful data solutions Maintain detailed documentation of your work and changes to support data quality and data governance Ensure high operational efficiency and quality of your solutions to meet SLAs and support commitment to our customers (Data Science, Data Analytics teams) Be an active participant and advocate of agile/scrum practice to ensure health and process improvements for your team Serve as an advanced resource to other Data Engineers on the team, and mentor and coach more junior members of the team helping to improve their skills, knowledge, and productivity. Participate in on-call rotations Basic Qualifications 7+ years of data engineering experience modeling and developing large data pipelines Strong SQL skills and ability to create queries to extract data and build performant datasets Hands-on experience with distributed systems such as Spark, Hadoop (HDFS, Hive, Presto, PySpark) to query and process data Experience with at least one major MPP or cloud database technology (Snowflake, Redshift, Big Query, Databricks) Familiarity with Data Modeling techniques and Data Warehousing standard methodologies and practices You are a problem solver with strong attention to detail and excellent analytical and communication skills Preferred Qualifications In-depth experience with Cloud technologies like AWS (S3, ECS, EMR, EC2, Lambda, etc.) Solid experience with data integration toolsets (i.e Airflow) and writing and maintaining Data Pipelines Good Scripting skills, including Python and Bash Familiar with Scrum and Agile methodologies 7+ years of programming experience using Java/Scala, Python, or similar languages Required Education Bachelor’s Degree in Computer Science, Information Systems or related field or equivalent work experience The hiring range for this position in Los Angeles is $149,300 - $200,200 per year and in San Francisco is $163,426 - $219,230 per year. The base pay actually offered will take into account internal equity and also may vary depending on the candidate’s geographic region, job-related knowledge, skills, and experience among other factors. A bonus and/or long-term incentive units may be provided as part of the compensation package, in addition to the full range of medical, financial, and/or other benefits, dependent on the level and position offered.",
        "url": "https://www.linkedin.com/jobs/view/3958912526",
        "summary": "This role involves building and maintaining content metadata and performance pipelines, as well as collaborating closely with business, analytics, and engineering teams. The ideal candidate will have extensive experience with data engineering, particularly in building scalable and fault-tolerant data processing pipelines, and will be proficient in various technologies such as Spark, Hadoop, Snowflake, and Airflow.",
        "industries": [
            "Media & Entertainment",
            "Technology",
            "Streaming",
            "Data & Analytics"
        ],
        "soft_skills": [
            "Problem Solving",
            "Attention to Detail",
            "Analytical Skills",
            "Communication Skills",
            "Collaboration",
            "Teamwork",
            "Mentorship",
            "Leadership"
        ],
        "hard_skills": [
            "Data Engineering",
            "SQL",
            "Spark",
            "Hadoop",
            "HDFS",
            "Hive",
            "Presto",
            "PySpark",
            "Snowflake",
            "Redshift",
            "Big Query",
            "Databricks",
            "Data Modeling",
            "Data Warehousing",
            "AWS",
            "S3",
            "ECS",
            "EMR",
            "EC2",
            "Lambda",
            "Airflow",
            "Python",
            "Bash",
            "Java",
            "Scala"
        ],
        "tech_stack": [
            "Spark",
            "Hadoop",
            "HDFS",
            "Hive",
            "Presto",
            "PySpark",
            "Snowflake",
            "Redshift",
            "Big Query",
            "Databricks",
            "AWS",
            "S3",
            "ECS",
            "EMR",
            "EC2",
            "Lambda",
            "Airflow"
        ],
        "programming_languages": [
            "SQL",
            "Python",
            "Bash",
            "Java",
            "Scala"
        ],
        "experience": 7,
        "education": {
            "min_degree": "Bachelor’s Degree",
            "fields": [
                "Computer Science",
                "Information Systems"
            ]
        },
        "salary": {
            "max": 219230,
            "min": 149300
        },
        "benefits": [
            "Medical",
            "Financial",
            "Bonus",
            "Long-Term Incentive Units"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Los Gatos, CA",
        "job_id": 3965705135,
        "company": "Netflix",
        "title": "Software Engineer (L5) - Cloud Games Platform",
        "created_on": 1720635692.4693987,
        "description": "At Netflix, we want to entertain the world and are constantly innovating on how entertainment is imagined, created and delivered to a global audience. We currently stream content in more than 30 languages in 190 countries, topping over 220 million paid subscribers and are expanding into new forms of entertainment such as gaming. Bring your passion for games and the joy of enabling others as we build our games experience. We are rapidly expanding new gaming offerings, so we are seeking an engineer with a wide breadth of experience who is excited to help us lead the continued building and scaling of our efforts for our international audience. This role involves architecting, authoring, and maintaining highly-complex software stacks for our cloud gaming service. This requires an engineer comfortable working throughout the application pipeline, including hardware accelerated video/audio/data processing, media encoding/decoding, and network transport algorithms. This role requires engineers whose expertise covers optimizing both local system performance (writing high-performance, efficient code with a deep understanding of both hardware and software dynamics) and total system performance (including efficient and effective use of Internet connectivity). All engineers must be comfortable with producing high-quality solutions to business problems in the midst of high ambiguity to continue to push the boundary on what they can accomplish. We provide the freedom to execute, learn and pivot, and the responsibility to be self directed, collaborative and insightful. This gives you the freedom to do your best work and contribute directly to the success of the business. You Will Be Successful In This Role If You Are a quick learner and excited about learning new technologies. Advocate software craftsmanship and take pride in your work. Thrive in an environment with lots of context and minimal guidance. Enjoy collaborating with engineers across functional teams and have excellent communication skills. Enjoy taking full ownership of projects from conception to production. Are comfortable working in multiple different areas of a backend software stack. Qualifications Experience developing and optimizing high-performance, real-time applications such as games or simulations. Strong low-level coding skills in Golang and/or C++. Experience working in complex codebases. Experience developing for a variety of different hardware platforms. Comfortable learning and working with a wide variety of technologies. L5: Experience with modern graphics APIs (Vulkan preferred). L5: Experience working with game engine internals. Nice To Have Experience with low-level hardware optimization. Experience with game development, media processing, and/or networking. Experience developing game consoles. Experience working with embedded systems and/or microcontrollers. Experience with DevOps and/or containerization. Our compensation structure consists solely of an annual salary; we do not have bonuses. You choose each year how much of your compensation you want in salary versus stock options. To determine your personal top of market compensation, we rely on market indicators and consider your specific job family, background, skills, and experience to determine your compensation in the market range. The range for this role is $300,000 - $600,000. Netflix provides comprehensive benefits including Health Plans, Mental Health support, a 401(k) Retirement Plan with employer match, Stock Option Program, Disability Programs, Health Savings and Flexible Spending Accounts, Family-forming benefits, and Life and Serious Injury Benefits. We also offer paid leave of absence programs. Full-time hourly employees accrue 35 days annually for paid time off to be used for vacation, holidays, and sick paid time off. Full-time salaried employees are immediately entitled to flexible time off. See more detail about our Benefits here. Netflix is a unique culture and environment. Learn more here. We are an equal-opportunity employer and celebrate diversity, recognizing that diversity of thought and background builds stronger teams. We approach diversity and inclusion seriously and thoughtfully. We do not discriminate on the basis of race, religion, color, ancestry, national origin, caste, sex, sexual orientation, gender, gender identity or expression, age, disability, medical condition, pregnancy, genetic makeup, marital status, or military service.",
        "url": "https://www.linkedin.com/jobs/view/3965705135",
        "summary": "Netflix is seeking a highly experienced engineer to join their growing gaming team and help build and scale their cloud gaming service. The role involves architecting, developing, and maintaining complex software stacks for their cloud gaming platform. Ideal candidates will have experience in optimizing both local and total system performance, working with various hardware platforms, and comfortable working within a complex codebase with minimal guidance.",
        "industries": [
            "Entertainment",
            "Gaming",
            "Technology",
            "Media Streaming"
        ],
        "soft_skills": [
            "Quick learner",
            "Software craftsmanship",
            "Thriving in ambiguous environments",
            "Collaboration",
            "Excellent communication",
            "Full project ownership",
            "Adaptability",
            "Self-directed",
            "Insightful"
        ],
        "hard_skills": [
            "Golang",
            "C++",
            "Low-level coding",
            "Hardware acceleration",
            "Video/audio/data processing",
            "Media encoding/decoding",
            "Network transport algorithms",
            "System optimization",
            "Hardware and software dynamics",
            "Graphics APIs",
            "Game engine internals",
            "Hardware optimization",
            "Game development",
            "Media processing",
            "Networking",
            "Game console development",
            "Embedded systems",
            "Microcontrollers",
            "DevOps",
            "Containerization"
        ],
        "tech_stack": [
            "Golang",
            "C++",
            "Vulkan",
            "Game Engine Internals"
        ],
        "programming_languages": [
            "Golang",
            "C++"
        ],
        "experience": 5,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 600000,
            "min": 300000
        },
        "benefits": [
            "Health Plans",
            "Mental Health support",
            "401(k) Retirement Plan with employer match",
            "Stock Option Program",
            "Disability Programs",
            "Health Savings and Flexible Spending Accounts",
            "Family-forming benefits",
            "Life and Serious Injury Benefits",
            "Paid leave of absence programs",
            "Paid time off",
            "Flexible time off"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Ramon, CA",
        "job_id": 3950089754,
        "company": "Diverse Lynx",
        "title": "Data Engineer",
        "created_on": 1720635694.0309606,
        "description": "Job Description The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. Responsibilities for Data Engineer Create and maintain optimal data pipeline architecture, Assemble large, complex data sets that meet functional / non-functional business requirements. Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ?big data? technologies. Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics. Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader. Work with data and analytics experts to strive for greater functionality in our data systems. Qualifications for Data Engineer Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. Experience building and optimizing ?big data? data pipelines, architectures and data sets. Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. Strong analytic skills related to working with unstructured datasets. Build processes supporting data transformation, data structures, metadata, dependency and workload management. A successful history of manipulating, processing and extracting value from large disconnected datasets. Working knowledge of message queuing, stream processing, and highly scalable ?big data? data stores. Strong project management and organizational skills. Experience supporting and working with cross-functional teams in a dynamic environment. This position will be NEARSHORE which will align with San Ramon time zone. Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.",
        "url": "https://www.linkedin.com/jobs/view/3950089754",
        "summary": "The Data Engineer will be responsible for building and maintaining data pipelines, optimizing data delivery, and providing actionable insights into business performance metrics. They will work with various teams to ensure optimal data delivery architecture and support data-related technical issues. This role requires strong SQL skills, experience with big data technologies, and expertise in data analysis and data pipeline optimization.",
        "industries": [
            "Technology",
            "Data Analytics",
            "Software Development",
            "Business Intelligence"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Project Management",
            "Organizational Skills"
        ],
        "hard_skills": [
            "SQL",
            "Relational Databases",
            "Big Data",
            "Data Pipelines",
            "Data Architecture",
            "Data Analysis",
            "Root Cause Analysis",
            "Unstructured Data",
            "Data Transformation",
            "Data Structures",
            "Metadata",
            "Dependency Management",
            "Workload Management",
            "Message Queuing",
            "Stream Processing",
            "Scalable Data Stores"
        ],
        "tech_stack": [
            "SQL",
            "AWS",
            "Big Data"
        ],
        "programming_languages": [
            "SQL"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "California, United States",
        "job_id": 3862431923,
        "company": "Globant",
        "title": "Data Engineer, Sr Level 3 - Remote",
        "created_on": 1720635695.8337624,
        "description": "At Globant, we dream and build Digital Journeys that matter to millions of users. We do that by leveraging engineering, design and innovation with our own industry-leading practices, like our Agile PODs and specialized Studios. We want you to join us in creating these journeys for the biggest clients in financial services, banking, retail, travel, e-commerce and media, revolutionizing and growing their core businesses while helping them (and you!) stay ahead of the curve. WHAT ARE WE LOOKING FOR? If you are a Data Engineer, we want to meet you! YOU WILL GET A CHANCE TO: Mentoring and Gatekeeping Presales Engage with project stakeholders to gather requirements and understand project objectives. Communicate technical concepts effectively to both technical and non-technical team members. Collaborate with cross-functional teams including designers, product managers, and quality assurance to deliver exceptional products. Participate in team meetings, stand-ups, and brainstorming sessions to foster collaboration and innovation. WHAT WILL MAKE YOU SUCCEED: Programming Language (OOP, Functional) Python, Java, SQL Software Engineer Practices (SOLID, Code Reviews, Quality Code & Refactoring, Desing Patterns, Data Structures) Software Architecture Types, Architecture Design RDBMS (ACID, Integrity, Indexes, Scalability, Partitioning, Tuning, Normalization, OLTP, vs OLAP) Distributed FileSystems (Resiliency, Replication, Parquet, Data Serialization) Data Modeling (DW, Modeling and Tools like Snowflake, Redshift) NoSQL (Architecture, Sharding, BASE, CAP, Graph databases, Columnar DBs, Document Stores)Knowlegde at least in one of the most popular Cloud Providers (AWS; GCP, Azure)Containerization Infrastructure as Code (How IaC works, Tools like Terraform / CloudFormation) CI/CD (Deployment Strategies Canary / Blue Green, Tools like Jenkins/Harness/Gitlab)Soft Skills (Critial thinking, Problem Solving, Teamwork, leadership, career management amoung others) Globant reasonably expects the annual base compensation for this role to be between $200,000 to $215,000 CAD. Specific offer details are determined by carefully considering a variety of factors, including the candidate’s primary work location, skills, experience, education, market demands, and internal parity. Job Segment: Data Modeler, QA, Software Engineer, Database, Pre-Sales, Data, Quality, Engineering, Technology, Sales",
        "url": "https://www.linkedin.com/jobs/view/3862431923",
        "summary": "Globant seeks a Data Engineer to join their team and work on projects for clients in financial services, banking, retail, travel, e-commerce and media. The role involves collaborating with cross-functional teams, gathering requirements, communicating technical concepts, and participating in team meetings. Successful candidates will possess strong programming skills (Python, Java, SQL), experience with software engineering practices, knowledge of various data technologies (RDBMS, NoSQL, distributed file systems, data modeling), cloud experience, and familiarity with infrastructure as code and CI/CD. The expected annual base compensation is between $200,000 to $215,000 CAD.",
        "industries": [
            "Financial Services",
            "Banking",
            "Retail",
            "Travel",
            "E-commerce",
            "Media"
        ],
        "soft_skills": [
            "Critical Thinking",
            "Problem Solving",
            "Teamwork",
            "Leadership",
            "Career Management"
        ],
        "hard_skills": [
            "Python",
            "Java",
            "SQL",
            "OOP",
            "Functional Programming",
            "SOLID",
            "Code Reviews",
            "Quality Code",
            "Refactoring",
            "Design Patterns",
            "Data Structures",
            "Software Architecture",
            "RDBMS",
            "ACID",
            "Integrity",
            "Indexes",
            "Scalability",
            "Partitioning",
            "Tuning",
            "Normalization",
            "OLTP",
            "OLAP",
            "Distributed FileSystems",
            "Resiliency",
            "Replication",
            "Parquet",
            "Data Serialization",
            "Data Modeling",
            "DW",
            "Snowflake",
            "Redshift",
            "NoSQL",
            "Sharding",
            "BASE",
            "CAP",
            "Graph Databases",
            "Columnar DBs",
            "Document Stores",
            "AWS",
            "GCP",
            "Azure",
            "Containerization",
            "Infrastructure as Code",
            "Terraform",
            "CloudFormation",
            "CI/CD",
            "Deployment Strategies",
            "Canary",
            "Blue Green",
            "Jenkins",
            "Harness",
            "Gitlab"
        ],
        "tech_stack": [
            "Python",
            "Java",
            "SQL",
            "RDBMS",
            "NoSQL",
            "Snowflake",
            "Redshift",
            "AWS",
            "GCP",
            "Azure",
            "Terraform",
            "CloudFormation",
            "Jenkins",
            "Harness",
            "Gitlab"
        ],
        "programming_languages": [
            "Python",
            "Java",
            "SQL"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 215000,
            "min": 200000
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Jose, CA",
        "job_id": 3969282500,
        "company": "Google",
        "title": "Software Engineer III, Chrome OS",
        "created_on": 1720635697.5095978,
        "description": "Minimum qualifications: Bachelor’s degree or equivalent practical experience. 2 years of experience with software development in one or more programming languages, or 1 year of experience with an advanced degree in an industry setting. 2 years of experience with data structures or algorithms in either an academic or industry setting. Preferred qualifications: Master's degree or PhD in Computer Science or related technical fields. 2 years of experience with performance, large scale systems data analysis, visualization tools, and/or debugging. Experience developing accessible technologies. Proficiency in code and system health, diagnosis and resolution, and software test engineering. About the job Google's software engineers develop the next-generation technologies that change how billions of users connect, explore, and interact with information and one another. Our products need to handle information at massive scale, and extend well beyond web search. We're looking for engineers who bring fresh ideas from all areas, including information retrieval, distributed computing, large-scale system design, networking and data storage, security, artificial intelligence, natural language processing, UI design and mobile; the list goes on and is growing every day. As a software engineer, you will work on a specific project critical to Google’s needs with opportunities to switch teams and projects as you and our fast-paced business grow and evolve. We need our engineers to be versatile, display leadership qualities and be enthusiastic to take on new problems across the full-stack as we continue to push technology forward. With your technical expertise you will manage project priorities, deadlines, and deliverables. You will design, develop, test, deploy, maintain, and enhance software solutions.Chrome OS delivers quality computing at scale to provide universal and unfettered access to information, entertainment, and tools. Our mission is to empower anyone to create and access information freely through fast, secure, simple, and intelligent computing. The US base salary range for this full-time position is $136,000-$200,000 + bonus + equity + benefits. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process. Please note that the compensation details listed in US role postings reflect the base salary only, and do not include bonus, equity, or benefits. Learn more about benefits at Google . Responsibilities Write product or system development code. Participate in, or lead design reviews with peers and stakeholders to decide amongst available technologies. Review code developed by other developers and provide feedback to ensure best practices (e.g., style guidelines, checking code in, accuracy, testability, and efficiency). Contribute to existing documentation or educational content and adapt content based on product/program updates and user feedback. Triage product or system issues and debug/track/resolve by analyzing the sources of issues and the impact on hardware, network, or service operations and quality. Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing our Accommodations for Applicants form .",
        "url": "https://www.linkedin.com/jobs/view/3969282500",
        "summary": "Google is seeking a Software Engineer to develop next-generation technologies for their products. This role involves working on projects critical to Google's needs, managing project priorities, deadlines, and deliverables. The engineer will design, develop, test, deploy, maintain, and enhance software solutions. The position focuses on Chrome OS, aiming to provide universal and unfettered access to information and tools through fast, secure, simple, and intelligent computing.",
        "industries": [
            "Technology",
            "Software Development",
            "Internet",
            "Information Technology",
            "Computer Science"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Critical Thinking",
            "Leadership",
            "Adaptability",
            "Versatility",
            "Enthusiasm",
            "Time Management",
            "Organization",
            "Detail Oriented"
        ],
        "hard_skills": [
            "Software Development",
            "Programming Languages",
            "Data Structures",
            "Algorithms",
            "Performance Optimization",
            "Large Scale Systems",
            "Data Analysis",
            "Visualization Tools",
            "Debugging",
            "Accessibility",
            "Code Review",
            "Documentation",
            "Issue Triaging",
            "Software Test Engineering"
        ],
        "tech_stack": [
            "Chrome OS",
            "Information Retrieval",
            "Distributed Computing",
            "Large-Scale System Design",
            "Networking",
            "Data Storage",
            "Security",
            "Artificial Intelligence",
            "Natural Language Processing",
            "UI Design",
            "Mobile"
        ],
        "programming_languages": [
            "Programming Languages"
        ],
        "experience": 2,
        "education": {
            "min_degree": "Bachelor’s degree",
            "fields": [
                "Computer Science",
                "Related Technical Fields"
            ]
        },
        "salary": {
            "max": 200000,
            "min": 136000
        },
        "benefits": [
            "Bonus",
            "Equity",
            "Benefits"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3901622779,
        "company": "Unreal Staffing, Inc",
        "title": "Lead Real-Time Data Engineer",
        "created_on": 1720635699.279362,
        "description": "Company Overview: Welcome to the forefront of data-driven innovation! Our company is at the cutting edge of leveraging real-time data to drive transformative change and solve complex problems across industries. We're committed to building cutting-edge real-time data solutions that enable timely insights and actions. Join us and lead our efforts in shaping the future of real-time data engineering. Position Overview: As the Lead Real-Time Data Engineer, you'll lead our initiatives in designing, building, and optimizing real-time data infrastructure. You'll spearhead the development of scalable, reliable, and efficient systems for ingesting, processing, and analyzing real-time data streams. If you're a seasoned engineer with expertise in real-time data technologies and a proven track record of leadership in delivering successful projects, we invite you to lead our team in this exciting opportunity. Requirements Key Responsibilities: Technical Leadership: Provide strategic guidance, mentorship, and technical leadership to a team of real-time data engineers, fostering a culture of excellence, innovation, and collaboration Real-Time Data Architecture: Lead the design and implementation of scalable and reliable architecture for real-time data processing and analytics, encompassing data ingestion, processing, and serving layers Streaming Data Pipelines: Architect and develop robust real-time data streaming pipelines using technologies such as Apache Kafka, Apache Flink, or Apache Spark Streaming, ensuring low-latency and high-throughput processing Event-Driven Architecture: Design event-driven systems to enable real-time processing of data events and triggers, supporting use cases such as real-time monitoring, anomaly detection, and alerting Data Integration: Lead efforts to integrate real-time data streams from diverse sources and systems into the real-time data infrastructure, ensuring data consistency, integrity, and quality Real-Time Analytics: Develop real-time analytics systems and dashboards to enable timely insights and decision-making, leveraging technologies such as Apache Druid, Elasticsearch, or Grafana Data Governance: Implement data governance policies and procedures to ensure data quality, security, and compliance with regulatory requirements in real-time data environments Performance Optimization: Optimize real-time data pipelines and processing workflows for performance, scalability, and efficiency, leveraging distributed computing and streaming processing techniques Monitoring and Alerting: Implement robust monitoring and alerting solutions to track the performance and health of real-time data infrastructure and pipelines, proactively identifying and resolving issues Documentation and Best Practices: Define and promote best practices for real-time data engineering, ensuring clear and comprehensive documentation to facilitate understanding and collaboration among team members Collaboration: Collaborate closely with cross-functional teams, including data scientists, software engineers, and business analysts, to understand requirements and deliver real-time data solutions that meet business needs Mentorship and Development: Mentor and coach junior engineers, providing guidance, support, and opportunities for skill development and career growth, and foster a culture of continuous learning and improvement within the team Qualifications: Bachelor's degree or higher in Computer Science, Engineering, Mathematics, or related field 8+ years of experience in data engineering, with a focus on real-time data technologies Proven leadership experience, with a track record of successfully leading real-time data engineering teams and delivering complex projects Expertise in real-time data streaming technologies such as Apache Kafka, Apache Flink, or Apache Spark Streaming Strong programming skills in languages such as Python, Java, or Scala, with experience in distributed computing frameworks Experience with cloud platforms such as AWS, Azure, or Google Cloud Platform, and services like AWS Kinesis, Azure Stream Analytics, or Google Cloud Dataflow Strong understanding of event-driven architecture and stream processing concepts, with experience building event-driven systems Strong problem-solving skills and analytical thinking, with the ability to design and troubleshoot complex real-time data solutions Excellent communication and collaboration skills, with the ability to work effectively in cross-functional teams and communicate technical concepts to non-technical stakeholders Benefits Competitive salary: The industry standard salary for Lead Real-Time Data Engineers typically ranges from $200,000 to $300,000 per year, depending on experience and qualifications Comprehensive benefits package, including health insurance, retirement plans, and wellness programs Flexible work arrangements, including remote work options and flexible hours Generous vacation and paid time off Professional development opportunities, including access to training programs, conferences, and workshops State-of-the-art technology environment with access to cutting-edge tools and resources Vibrant and inclusive company culture with opportunities for growth and advancement Exciting projects with real-world impact at the forefront of data-driven innovation Join Us: Ready to lead the charge in real-time data engineering? Apply now to join our team and be part of the data revolution!",
        "url": "https://www.linkedin.com/jobs/view/3901622779",
        "summary": "This role involves leading a team of real-time data engineers in designing, building, and optimizing real-time data infrastructure. You will be responsible for developing scalable, reliable, and efficient systems for ingesting, processing, and analyzing real-time data streams.",
        "industries": [
            "Data Engineering",
            "Software Engineering",
            "Technology",
            "Data Science",
            "Analytics"
        ],
        "soft_skills": [
            "Leadership",
            "Mentorship",
            "Collaboration",
            "Communication",
            "Problem-Solving",
            "Analytical Thinking",
            "Technical Guidance",
            "Innovation"
        ],
        "hard_skills": [
            "Apache Kafka",
            "Apache Flink",
            "Apache Spark Streaming",
            "Python",
            "Java",
            "Scala",
            "AWS",
            "Azure",
            "Google Cloud Platform",
            "AWS Kinesis",
            "Azure Stream Analytics",
            "Google Cloud Dataflow",
            "Event-Driven Architecture",
            "Stream Processing",
            "Distributed Computing",
            "Data Governance",
            "Data Quality",
            "Security",
            "Compliance"
        ],
        "tech_stack": [
            "Apache Kafka",
            "Apache Flink",
            "Apache Spark Streaming",
            "AWS",
            "Azure",
            "Google Cloud Platform",
            "AWS Kinesis",
            "Azure Stream Analytics",
            "Google Cloud Dataflow",
            "Apache Druid",
            "Elasticsearch",
            "Grafana"
        ],
        "programming_languages": [
            "Python",
            "Java",
            "Scala"
        ],
        "experience": 8,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Computer Science",
                "Engineering",
                "Mathematics"
            ]
        },
        "salary": {
            "max": 300000,
            "min": 200000
        },
        "benefits": [
            "Competitive Salary",
            "Health Insurance",
            "Retirement Plans",
            "Wellness Programs",
            "Flexible Work Arrangements",
            "Remote Work",
            "Flexible Hours",
            "Vacation",
            "Paid Time Off",
            "Professional Development Opportunities",
            "Training Programs",
            "Conferences",
            "Workshops",
            "State-of-the-art Technology Environment",
            "Cutting-edge Tools",
            "Vibrant Company Culture",
            "Growth Opportunities",
            "Advancement Opportunities",
            "Real-world Impact"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Diego Metropolitan Area",
        "job_id": 3960270449,
        "company": "Evolution Staffing",
        "title": "Lead SQL Data Platform Engineer",
        "created_on": 1720635701.024103,
        "description": "Lead SQL Data Platform Engineer A Large Asset Management firm that is in need of a Data Platform Engineer Lead. This role leads development of data services and platform solutions needed to host business-critical applications. This is a hands-on, technical lead role in the I&O organization that will architect and develop data platforms to support overall platform and technology strategies. The primary focus of this position will be the architecture and engineering of high performant MS SQL Always On environments. What are we looking for? We want strong collaborators who can deliver a world-class client experience . We are looking for people who thrive in a fast-paced environment , are client-focused , team oriented , and are able to execute in a way that encourages creativity and continuous improvement . Core Competencies: · Possess a thorough understanding of modern work practices such as DevSecOps, Continuous Integration (CI), Continuous Delivery (CD), Agile project Delivery, Release Engineering · Proven experience using system management configuration tools (Ansible, Puppet, Terraform, CF/YAML/JSON) to deploy and manage infrastructure at scale · In depth understanding on AWS Well-Architected Framework / AWS Landing Zone · Extensive experience working in large technology ecosystems with emphasis on integrating multiple externally facing high scale, high volume, always-on mission-critical systems · Ability to convey technical solutions to the entire organization and experience working with senior leaders across the firm to leverage technology to drive positive business outcomes · Highly collaborative with experience in a hands-on technical leadership role. Requirements: · Bachelor’s/Advance degree and/or equivalent experience required · 5+ years of MS SQL Server and PostgresSQL, database architecture / design / implementation experience · 5+ years of advanced SQL programming skills, PL/SQL, pgSQL, T-SQL · Extensive experience designing and deploying AlwaysOn environments · Proven experience with data integration services like AWS Glue, data replication tools like FiveTran HVR, data modeling, ETL workflow management, database performance tuning, SQL tunning, and workload management. · Proven track record building end-to-end automation for core infrastructure services · Extensive engineering skills and experience developing scalable platforms (IaC / PaaS) Preferences: · AWS Certified Database · AWS Solutions Architect (Preferred) · Ansible / Puppet / Terraform Certification (Preferred) · Experience with open telemetry including Prometheus, Grafana, ELK, AWS Distro",
        "url": "https://www.linkedin.com/jobs/view/3960270449",
        "summary": "Lead SQL Data Platform Engineer responsible for architecting and developing high-performance MS SQL Always On environments.  The role involves leading development of data services and platform solutions for business-critical applications, working with senior leaders, and delivering a world-class client experience.",
        "industries": [
            "Asset Management",
            "Finance",
            "Technology",
            "Software Development",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Collaboration",
            "Client-focused",
            "Team-oriented",
            "Creativity",
            "Continuous Improvement",
            "Communication",
            "Leadership",
            "Problem-solving"
        ],
        "hard_skills": [
            "DevSecOps",
            "Continuous Integration (CI)",
            "Continuous Delivery (CD)",
            "Agile project Delivery",
            "Release Engineering",
            "Ansible",
            "Puppet",
            "Terraform",
            "CF/YAML/JSON",
            "AWS Well-Architected Framework",
            "AWS Landing Zone",
            "SQL",
            "PL/SQL",
            "pgSQL",
            "T-SQL",
            "MS SQL Server",
            "PostgreSQL",
            "AWS Glue",
            "FiveTran HVR",
            "Data Modeling",
            "ETL Workflow Management",
            "Database Performance Tuning",
            "SQL Tuning",
            "Workload Management",
            "Infrastructure Automation",
            "IaC",
            "PaaS",
            "Prometheus",
            "Grafana",
            "ELK",
            "AWS Distro"
        ],
        "tech_stack": [
            "MS SQL Always On",
            "AWS",
            "Ansible",
            "Puppet",
            "Terraform",
            "AWS Glue",
            "FiveTran HVR",
            "Prometheus",
            "Grafana",
            "ELK",
            "AWS Distro"
        ],
        "programming_languages": [
            "SQL",
            "PL/SQL",
            "pgSQL",
            "T-SQL"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Computer Science",
                "Engineering",
                "Information Technology",
                "Data Science",
                "Mathematics"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3966012108,
        "company": "Salesforce",
        "title": "Sr Data Engineer",
        "created_on": 1720635702.8368645,
        "description": "To get the best candidate experience, please consider applying for a maximum of 3 roles within 12 months to ensure you are not duplicating efforts. Job Category Software Engineering Job Details About Salesforce We’re Salesforce, the Customer Company, inspiring the future of business with AI+ Data +CRM. Leading with our core values, we help companies across every industry blaze new trails and connect with customers in a whole new way. And, we empower you to be a Trailblazer, too — driving your performance and career growth, charting new paths, and improving the state of the world. If you believe in business as the greatest platform for change and in companies doing well and doing good – you’ve come to the right place. Sr. Data Engineer Office hybrid - San Francisco or Seattle In school or graduated within the last 12 months? Please visit Futureforce for opportunities. The Chief Pipeline Office team goes deep on data to grow and enable a data-driven organization by building a world class data stack. In this role, you will own the end-to-end data delivery, from engaging with stakeholders and data modeling with analysts to building and running production pipelines. You will set the golden standard for what “run-the-business” data looks like, showing how teams around Salesforce can leverage data to drive their business. Successful candidates will be able to turn end-user requirements into project roadmaps and eventually running code, while providing strategic recommendations, and guiding the charge on execution and change management. What You’ll Be Doing… Designing flexible, easy-to-use data Kimball styled data models that describe the business to support self-service analytics. Building robust, scalable data processing and data ingestion pipelines into our data warehouse. Develop data quality automation and unit tests to ensure the accuracy of the data delivered to the Analysts, Data Scientists, and Business Customers. Collaborating cross-functionally with teams across Salesforce to ensure consistent data modeling standards and processes. Consulting on and driving the operational rigor and framework vital to unify and accelerate adoption of outstanding dashboards. Who You Are… Experienced. 5+ years of professional experience as a data engineer in Sales Operations, Finance, Marketing, or other related fields. Technically Savvy. Authority in Snowflake SQL queries. Authority in data modeling standard processes. Experience with Docker and AWS stack (Fargate, EC2, etc.) Specific work experience with DBT strongly preferred. Organized. Proven project execution skills with a strong desire to document standard methodologies and process improvements. Ability to see the bigger picture and prioritize accordingly. Analytical. Strong data analysis skills, including attention to detail and ability to manage various data points to achieve a common goal. Flexible & Nimble. Ability to excel in a fast growing/fast paced environment, delivering with care while managing deadlines. Multi-Tasker. Ability to understand broader business and financial issues, prioritize multiple projects and tasks, and communicate across various project stages and departments. Education. A related technical degree required. Accommodations If you require assistance due to a disability applying for open positions please submit a request via this Accommodations Request Form. Posting Statement At Salesforce we believe that the business of business is to improve the state of our world. Each of us has a responsibility to drive Equality in our communities and workplaces. We are committed to creating a workforce that reflects society through inclusive programs and initiatives such as equal pay, employee resource groups, inclusive benefits, and more. Learn more about Equality at www.equality.com and explore our company benefits at www.salesforcebenefits.com. Salesforce is an Equal Employment Opportunity and Affirmative Action Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender perception or identity, national origin, age, marital status, protected veteran status, or disability status. Salesforce does not accept unsolicited headhunter and agency resumes. Salesforce will not pay any third-party agency or company that does not have a signed agreement with Salesforce. ﻿Salesforce welcomes all. Pursuant to the San Francisco Fair Chance Ordinance and the Los Angeles Fair Chance Initiative for Hiring, Salesforce will consider for employment qualified applicants with arrest and conviction records. For Washington-based roles, the base salary hiring range for this position is $125,700 to $243,100. For California-based roles, the base salary hiring range for this position is $137,100 to $265,200. Compensation offered will be determined by factors such as location, level, job-related knowledge, skills, and experience. Certain roles may be eligible for incentive compensation, equity, benefits. More details about our company benefits can be found at the following link: https://www.salesforcebenefits.com.",
        "url": "https://www.linkedin.com/jobs/view/3966012108",
        "summary": "Salesforce is seeking a Sr. Data Engineer to join their Chief Pipeline Office team. This role involves building a data-driven organization by creating a world-class data stack. You will own the end-to-end data delivery, from stakeholder engagement and data modeling to building and running production pipelines. The ideal candidate will have 5+ years of experience in data engineering, strong technical skills in Snowflake SQL, data modeling, Docker, AWS, and DBT, and be a strong communicator with proven project execution skills.",
        "industries": [
            "Software",
            "Technology",
            "Data",
            "CRM",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Analytical Thinking",
            "Project Management",
            "Organizational Skills",
            "Attention to Detail",
            "Prioritization",
            "Adaptability",
            "Flexibility",
            "Leadership",
            "Teamwork",
            "Strategic Thinking",
            "Decision Making",
            "Time Management"
        ],
        "hard_skills": [
            "Snowflake SQL",
            "Data Modeling",
            "Docker",
            "AWS",
            "DBT",
            "Data Warehousing",
            "Data Pipelines",
            "Data Quality",
            "Data Ingestion",
            "Kimball Methodology",
            "Data Analysis",
            "Statistical Analysis"
        ],
        "tech_stack": [
            "Snowflake",
            "Docker",
            "AWS",
            "DBT",
            "Fargate",
            "EC2"
        ],
        "programming_languages": [
            "SQL"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Technical",
            "fields": [
                "Computer Science",
                "Data Science",
                "Engineering"
            ]
        },
        "salary": {
            "max": 265200,
            "min": 125700
        },
        "benefits": [
            "Health Insurance",
            "Dental Insurance",
            "Vision Insurance",
            "Life Insurance",
            "Disability Insurance",
            "Paid Time Off",
            "Paid Sick Leave",
            "Parental Leave",
            "Retirement Plan",
            "Employee Stock Purchase Plan",
            "Tuition Reimbursement",
            "Employee Assistance Program",
            "Wellness Program",
            "Employee Discounts"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Sunnyvale, CA",
        "job_id": 3888437736,
        "company": "Extend Information Systems Inc.",
        "title": "MySQL Database Engineer",
        "created_on": 1720635704.5945199,
        "description": "Hi Jobseekers , I hope you are doing well! We have an opportunity MySQL Database Engineer with one of our clients for Sunnyvale, CA. Please see the job details below and let me know if you would be interested in this role. If interested, please send me a copy of your resume, contact details, availability, and a good time to connect with you. Job Title: MySQL Database Engineer Location: Sunnyvale,CA/Austin,TX Duration: Full Time Experience level : 10+ Job Description Hands on knowledge of MySQL and its storage engines. Experience working with MySQL 5.7 and 8.0 Understanding of upgrade procedures. Experience in performing MySQL major upgrades Exposure to MySQL and Percona utilities, performance_schema. Deep knowledge in MySQL GTID based replication, group replication and Galera high availability solutions. Experience working in a fast paced patching cycle teams. Has to be knowledgeable in OS and database patching. Passion to automate any day-to-day activities and document the process/plan. Hands on knowledge in cloud platforms like AWS and GCP. Experience working with proxysql and orchestrator Working knowledge in Bash, GO and Python Experience with MySQL monitoring tools like prometheus, Grafana Advanced Performance tuning and troubleshooting on linux systems Python Programming - 2-3 years of exp Shell Scripting 3-4 years of exp . Thanks & Regards!! Anoop Tiwari Extend Information Systems Cell: - 571 - 386 - 2431 Email: Anoop@extendinfosys.com Address: 44258 Mercure Circle, UNIT 102 A, Sterling VA, USA 20166 Web: WWW.extendinfosys.com",
        "url": "https://www.linkedin.com/jobs/view/3888437736",
        "summary": "We are seeking a skilled MySQL Database Engineer with 10+ years of experience for a full-time opportunity in Sunnyvale, CA or Austin, TX. The role requires deep knowledge of MySQL, its storage engines, and high availability solutions.  Strong experience in MySQL upgrades, patching, and performance tuning is crucial. Experience with cloud platforms like AWS and GCP, and scripting languages like Bash, GO, and Python is a must. The ideal candidate will have a passion for automation and documentation.",
        "industries": [
            "Information Technology",
            "Software Development",
            "Database Administration",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Automation",
            "Documentation",
            "Passionate",
            "Problem-solving",
            "Troubleshooting",
            "Performance Tuning"
        ],
        "hard_skills": [
            "MySQL",
            "MySQL 5.7",
            "MySQL 8.0",
            "MySQL Utilities",
            "Percona",
            "Performance Schema",
            "GTID based replication",
            "Group Replication",
            "Galera",
            "AWS",
            "GCP",
            "Proxysql",
            "Orchestrator",
            "Bash",
            "GO",
            "Python",
            "Prometheus",
            "Grafana",
            "Linux"
        ],
        "tech_stack": [
            "MySQL",
            "AWS",
            "GCP",
            "Proxysql",
            "Orchestrator",
            "Prometheus",
            "Grafana"
        ],
        "programming_languages": [
            "Python",
            "Go",
            "Bash"
        ],
        "experience": 10,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "California, United States",
        "job_id": 3934994929,
        "company": "Hermitage Infotech",
        "title": "Data Engineer",
        "created_on": 1720635706.3167958,
        "description": "Hi Please go through the requirements given below and kindly send me your profile in word document along with your minimum expected yearly salary on our clients W2. We request your help to fill in the below positions. Number of positions are specified in brackets. HealthCare client- Sr Level, remote (PST) Data Engineer (3) - Experience in Data Vault, Azure Data Lake DBT & Snowflake and Collibra. Regards Varma Sr IT Recruiting Manager 732-338-7524 varma@hermitageinfotech.com www.hermitageinfotech.com",
        "url": "https://www.linkedin.com/jobs/view/3934994929",
        "summary": "Hermitage Infotech is seeking 3 Data Engineers with experience in Data Vault, Azure Data Lake, DBT, Snowflake, and Collibra for a remote healthcare client. The role is based in PST time zone.",
        "industries": [
            "Healthcare",
            "Information Technology"
        ],
        "soft_skills": [],
        "hard_skills": [
            "Data Vault",
            "Azure Data Lake",
            "DBT",
            "Snowflake",
            "Collibra"
        ],
        "tech_stack": [
            "Data Vault",
            "Azure Data Lake",
            "DBT",
            "Snowflake",
            "Collibra"
        ],
        "programming_languages": [],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Los Angeles, CA",
        "job_id": 3954174018,
        "company": "Motion Recruitment",
        "title": "Data Engineer",
        "created_on": 1720635708.0995378,
        "description": "We are a fast-moving Series D Gen AI SaaS start-up that is committed to making video animation accessible to music labels, artists, and creatives. Recently, we released our newest product version which can be accessed through their iOS app. On the app we feature several templates, artistic styles, and environments for you to transform to your liking by submitting a text-to-video prompt of what you want to be animated. Additionally, users can upload their own pictures and videos to use as environments for our AI generated text-to-video prompt. We already have an established user base of 1 Million + users and are currently in the process of further scaling up our Data Infrastructure We are hiring for a Senior Data Engineer with 5 yrs. of experience in data engineering and a proficiency in cloud technologies, particularly AWS (S3, EC2, RDS, Redshift) and cloud data warehouses like Snowflake, Databricks, and Redshift. In this role you will design, develop, and maintain scalable and efficient data pipelines to ingest, process, and transform large volumes of data from diverse sources, including databases, APIs, and third-party systems, ensuring optimal performance, scalability, and data quality. Additionally, you will build data warehouse solutions to support business intelligence, reporting, and analytics initiatives. Located in Woodland Hills, this position if full-time and on-site. If you want to help scale up the data architecture for an amazing & innovative use case for Artificial Intelligence, please APPLY NOW! Required Skills & Experience Bachelor’s in Computer Science, Engineering, or related field 5+ yrs. experience in data engineering AWS (S3, EC2, RDS, Redshift) Familiarity with Snowflake, Databricks, Redshift Python & SQL Experience with ETL, data warehousing concepts, and DBT Experience with API development and integration (REST, SOAP) Familiarity with CI/CD tools and practices (Jenkins, GitLab CI/CD) Benefits & Perks Equity Package Bonus Generous Paid-Time Off 401k Match Coverage for Medical, Dental, and Vision Applicants must be currently authorized to work in the US on a full-time basis now and in the future. Posted By: Connor Hart",
        "url": "https://www.linkedin.com/jobs/view/3954174018",
        "summary": "A fast-growing Gen AI SaaS startup focused on video animation for music labels and creatives seeks a Senior Data Engineer with 5+ years experience in data engineering and proficiency in cloud technologies, particularly AWS (S3, EC2, RDS, Redshift) and cloud data warehouses like Snowflake, Databricks, and Redshift.  The role involves designing, developing, and maintaining scalable data pipelines for processing large volumes of data from diverse sources, building data warehouse solutions, and supporting business intelligence, reporting, and analytics initiatives.",
        "industries": [
            "Software",
            "Artificial Intelligence",
            "SaaS",
            "Music"
        ],
        "soft_skills": [
            "Problem Solving",
            "Teamwork",
            "Communication",
            "Analytical Skills",
            "Detail-Oriented"
        ],
        "hard_skills": [
            "Data Engineering",
            "Cloud Technologies",
            "AWS",
            "S3",
            "EC2",
            "RDS",
            "Redshift",
            "Snowflake",
            "Databricks",
            "Python",
            "SQL",
            "ETL",
            "Data Warehousing",
            "DBT",
            "API Development",
            "REST",
            "SOAP",
            "CI/CD",
            "Jenkins",
            "GitLab CI/CD"
        ],
        "tech_stack": [
            "AWS",
            "S3",
            "EC2",
            "RDS",
            "Redshift",
            "Snowflake",
            "Databricks",
            "Python",
            "SQL",
            "DBT",
            "REST",
            "SOAP",
            "Jenkins",
            "GitLab CI/CD"
        ],
        "programming_languages": [
            "Python",
            "SQL"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Computer Science",
                "Engineering"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Equity Package",
            "Bonus",
            "Paid Time Off",
            "401k Match",
            "Medical",
            "Dental",
            "Vision"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3946902767,
        "company": "DeepScribe",
        "title": "Software Engineer",
        "created_on": 1720635709.7062013,
        "description": "Job Overview At DeepScribe, everything we do is focused on our mission - to bring back the joy of care to medicine. Our goal is to empower physicians with the tools they need to improve both efficiency and efficacy, and to improve patient outcomes by increasing the trust and understanding they have with their physician. As a software engineer at DeepScribe, you will both expand our current product offering to support new workflows and build out new product offerings on our platform. You will work across the stack, deploying web services in Node.js or Python, building and improving React apps, defining/refining infrastructure in Terraform, and optimizing our databases. Responsibilities Lead and expedite the delivery of products and features Build out the future generation of applications and architecture for the system that serves our growing customer base Write high-quality code and tests. We use React, Node.js, Python, PostgreSQL, and Terraform Build the products for our state-of-the-art models which are ingesting conversations and generating summaries for thousands of real-life patient-physician interactions daily across iOS and web platforms, and integrate into a variety of different EHR system vendors Become an integral part of DeepScribe, working on the core tech to delight the doctors who rely on our system and help their patients get the best care possible Utilize the largest conversational healthcare dataset in the world to rethink healthcare Qualifications You have experience building in high-growth settings Strong problem-solving skills and the ability to innovate under tight deadlines You move fast, work smart, and are thorough You have the ability to work across the stack. We use React, Node.js, Python, PostgreSQL, Swift for iOS, and Terraform. Facility with at least one of those technologies is required Additional Information Base Salary Range: $130,000 - $230,000 depending on level and experience Meaningful equity stake in the company Flexible PTO Work from home stipend Medical, Dental, Vision, 401K and other benefits are also offered About The Team At DeepScribe, we value trust, teamwork, and transparency, and we’re dedicated to promoting diversity and equity in the workforce through inclusive hiring practices. Candidates with backgrounds that are underrepresented in the technology industry are encouraged to apply. In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required eligibility verification form upon hire.",
        "url": "https://www.linkedin.com/jobs/view/3946902767",
        "summary": "DeepScribe, a healthcare technology company focused on empowering physicians with tools to improve efficiency, efficacy, and patient outcomes, is looking for a Software Engineer to build and expand their product offerings. This role involves developing web services (Node.js/Python), building React apps, defining/refining infrastructure (Terraform), optimizing databases, and integrating with various EHR systems. The ideal candidate has experience in high-growth environments, strong problem-solving skills, and proficiency in at least one of the technologies used (React, Node.js, Python, PostgreSQL, Swift, Terraform).",
        "industries": [
            "Healthcare",
            "Technology",
            "Software Development",
            "Artificial Intelligence"
        ],
        "soft_skills": [
            "Problem-solving",
            "Innovation",
            "Collaboration",
            "Teamwork",
            "Communication",
            "Time Management"
        ],
        "hard_skills": [
            "React",
            "Node.js",
            "Python",
            "PostgreSQL",
            "Terraform",
            "Swift",
            "EHR Systems",
            "API Integration"
        ],
        "tech_stack": [
            "React",
            "Node.js",
            "Python",
            "PostgreSQL",
            "Swift",
            "Terraform",
            "EHR Systems",
            "API Integration"
        ],
        "programming_languages": [
            "Javascript",
            "Python",
            "Swift"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 230000,
            "min": 130000
        },
        "benefits": [
            "Equity stake",
            "Flexible PTO",
            "Work from home stipend",
            "Medical",
            "Dental",
            "Vision",
            "401K"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Mountain View, CA",
        "job_id": 3943891128,
        "company": "ELITE MENTE LLC",
        "title": "Sr. Database Engineer( AWS exp must)",
        "created_on": 1720635711.5264401,
        "description": "Position: Sr. Database Engineer (SQL with DevOps) Location: Mountain View, CA (Hybrid) Job Description Bachelor’s degree in computer science, Engineering, or related field. Proven experience working with Amazon Aurora and/or PostgreSQL in a production environment. Strong SQL skills and experience with SQL tuning techniques. Proficiency in AWS services such as EC2, Route 53, VPC, IAM, and CloudFormation. Hands-on experience with scripting languages (e.g., Python, Bash) for automation. Familiarity with database security concepts and best practices. Excellent problem-solving skills and attention to detail.",
        "url": "https://www.linkedin.com/jobs/view/3943891128",
        "summary": "Senior Database Engineer with proven experience in Amazon Aurora or PostgreSQL, strong SQL skills, AWS services (EC2, Route 53, VPC, IAM, CloudFormation), scripting (Python, Bash), and database security.",
        "industries": [
            "Information Technology",
            "Software Development",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Problem-solving",
            "Attention to detail"
        ],
        "hard_skills": [
            "SQL",
            "SQL Tuning",
            "Amazon Aurora",
            "PostgreSQL",
            "AWS",
            "EC2",
            "Route 53",
            "VPC",
            "IAM",
            "CloudFormation",
            "Python",
            "Bash",
            "Database Security"
        ],
        "tech_stack": [
            "Amazon Aurora",
            "PostgreSQL",
            "AWS",
            "EC2",
            "Route 53",
            "VPC",
            "IAM",
            "CloudFormation",
            "Python",
            "Bash"
        ],
        "programming_languages": [
            "Python",
            "Bash"
        ],
        "experience": 0,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Computer Science",
                "Engineering"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3925992906,
        "company": "Intelliswift Software",
        "title": "Data Engineer - II",
        "created_on": 1720635713.086234,
        "description": "Title: Cloud Data Engineer Location: San Francisco - Hybrid (2+ days in office) Duration: 12+ Months Intelliswift Software Inc. conceptualizes, builds, and supports the world's most amazing technology products and solutions. Our team of rich experts from diverse backgrounds contributes to making Intelliswift one of the most reliable partners in IT and Talent solutions. We specialize in delivering world-class Digital Product Engineering, Data Management and Analytics, and Staffing Solutions services to Fortune companies, SMBs, ISVs, and fast-growing startup Qualifications 5 to 9+ years of experience in data engineering, data science, and software engineering. Bachelor’s degree in computer science, Information Systems, or another related field Must be a US Citizen or a Green Card holder. Responsibilities Designs, develops, modifies, tests, and automates the data warehouse and business intelligence applications solutions. This includes design, development, architecture recommendations, quality management, metadata and repository creation, trouble-shooting problems, and tuning warehouse applications. Develops transition and implementation plans. Recommends changes in development, maintenance, and standards. Advanced analytical ability and technical skill as well as the ability to provide innovative solutions to technical needs and business requirements. Ability to exercise independent judgment in making complex business decisions. Acute attention to detail with a high level of data integrity and accuracy Excellent oral and written communication, with interpersonal skills to work with people at all levels of the organization. Ability to translate highly technical information into non-technical terms. Excellent computer skills including Microsoft Office along with various other software applications as needed for the role. Broad knowledge of the programming tools, concepts, practices, and principles including design, implementation, and testing Position requires continuous visual concentration and manual dexterity to operate PC Requires prolonged sitting and minimal standing/walking. May require on-call status. Rare domestic travel including overnight stays may be necessary. Technical Skills Expert in developing and analyzing complex SQL on a variety of RDBMS (Microsoft SQL Server, Oracle) Expert knowledge of data modeling and understanding of different data structures and their benefits and limitations under particular use cases Experience with ETL tools (Informatica) Ability to create quality ERD’s (entity-relationship diagrams) Excellent writing skills for writing user and system documentation AWS Cloud Data Warehousing Technologies Experience using core AWS services to build and support data warehouse solutions leveraging AWS architecture best practices (S3, DMS, Glue, Lambda) Development/modeling experience with Amazon Redshift Experience using the AWS service APIs, AWS CLI, and SDKs to build applications. Proficiency in developing, deploying, and debugging cloud-based applications using AWS. Ability to use a CI/CD pipeline to deploy applications on AWS (GitLab, Terraform, DBMaestro) Ability to apply a basic understanding of cloud-native applications to write code. Proficiency writing code for serverless applications. Ability to write code using AWS security best practices (e.g., not using secret and access keys in the code, instead using IAM roles) Ability to author, maintain, and debug code modules on AWS. Experience with visualization tools (Tableau) Experience creating scripts with Python. Experience working on an Agile team. Understanding of application lifecycle management Understanding of the use of containers in the development process Responsibilities Contributes to the design, development, testing, implementation, and review of complex data warehouse and business intelligence solutions. Develops all or part of complex data warehouse applications, develops software from established requirements, builds reports and dashboards, plans and coordinates work with fellow programmers to meet delivery commitments, creates prototypes; offers insight on the feasibility of system designs. Contributes to the design of technology infrastructure and configurations, recommends process improvements. Reviews complex patches and new versions of data warehouse applications. Implements complex software packages and deploys code. Key participant in cross-functional team initiatives and process improvement project Intelliswift is committed to fair and equitable compensation practices. The range provided for this position is determined by various factors including, but not limited to, relevant work experience, skills, certifications, and location. Intelliswift offers medical insurance, dental insurance, vision insurance, supplemental life and AD&D insurance, and any other benefits per state law. Equal Employment Opportunity Statement Intelliswift celebrates a diverse and inclusive workforce. We offer equal employment opportunities to all applicants and employees. All qualified applicants will be considered regardless of race, color, sex, gender identity, gender expressions, religion, age, national origin or ancestry, citizenship, physical or mental disability, medical condition, family care status, marital status, domestic partner status, sexual orientation, genetic information, military or veteran status, or any other protected basis under the law. Americans With Disabilities Act (ADA) If you require a reasonable accommodation in completing this application, interviewing, completing any pre-employment testing, or otherwise participating in the employee selection process, please contact Intelliswift Human Resources Department Other Employment Statements Intelliswift participates in the E-Verify program.",
        "url": "https://www.linkedin.com/jobs/view/3925992906",
        "summary": "Intelliswift Software Inc. is seeking a Cloud Data Engineer to design, develop, and implement complex data warehouse and business intelligence solutions. The ideal candidate will have 5-9+ years of experience in data engineering, data science, and software engineering, expert knowledge of SQL, data modeling, ETL tools, and AWS cloud technologies, and experience with visualization tools and Python scripting.",
        "industries": [
            "Information Technology",
            "Data Engineering",
            "Business Intelligence",
            "Software Engineering",
            "Cloud Computing",
            "Analytics"
        ],
        "soft_skills": [
            "Analytical Skills",
            "Problem Solving",
            "Communication",
            "Interpersonal Skills",
            "Technical Writing",
            "Teamwork",
            "Time Management",
            "Organization",
            "Attention to Detail",
            "Adaptability",
            "Communication",
            "Presentation Skills"
        ],
        "hard_skills": [
            "SQL",
            "Data Modeling",
            "ETL",
            "AWS",
            "Redshift",
            "S3",
            "DMS",
            "Glue",
            "Lambda",
            "Terraform",
            "DBMaestro",
            "GitLab",
            "Tableau",
            "Python",
            "Agile"
        ],
        "tech_stack": [
            "AWS",
            "Redshift",
            "S3",
            "DMS",
            "Glue",
            "Lambda",
            "Terraform",
            "DBMaestro",
            "GitLab",
            "Tableau",
            "Python"
        ],
        "programming_languages": [
            "SQL",
            "Python"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Information Systems"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Medical Insurance",
            "Dental Insurance",
            "Vision Insurance",
            "Supplemental Life Insurance",
            "AD&D Insurance"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Los Angeles, CA",
        "job_id": 3960526899,
        "company": "Fuel Cycle",
        "title": "Software Engineer (Entry-level)",
        "created_on": 1720635715.16756,
        "description": "Fuel Cycle is looking to hire an entry-level Software Engineer who will work very closely with our product and engineering teams to produce high quality code. By driving engineering quality, troubleshooting application issues, and contributing to long-term strategy, you'll play a key role in the continuous enhancement of our SaaS platforms. The Software Engineer will report to our VP, Research & Development. This is a hybrid position with a minimum of 3 days in office requirement per week, based out of Fuel Cycle’s Los Angeles Headquarters. Fuel Cycle is regularly recognized as a great place to work. Our recent awards include 2024 Built In Best Places to Work, 2023 Inc Best Workplaces, as well as Comparably Best CEO, Best Company for Women, and Best Company for Diversity. Learn more about our Culture & Values here. Responsibilities: Follow overall architectural principles, frameworks, and standards. Help design solutions that are enterprise scalable and maintainable. Select and integrate open-source frameworks. Troubleshoot issues in existing applications. Work closely with Frontend Developers to expose API services via RESTful services and websockets. Provide input into long-range product requirements, best practices, and operational guidelines, with focus on continuous improvement of platform reliability and serviceability. Ensure that we are continuously raising our standard of engineering excellence. What Success Looks Like: SaaS software platforms are developed as scalable, high performing and maintainable solutions using best practices and current technologies. Scalability methodologies are well understood and implemented in a consistent and robust manner. Translate use cases into functional applications. Create design documents from software requirements. Design, build, and maintain efficient, reusable, reliable and scalable software. Identify performance bottlenecks and develop solutions to these problems. Identify and implement key application metrics and monitoring solutions. You'll Be Successful If You Have: Minimum 1 year of experience in software engineering, preferably within a start-up environment. Proficiency in Java 8 or higher and Object-Oriented Programming. Knowledge in Spring MVC, Spring Boot. Knowledge in SQL and NOSQL. Knowledge of data structure and algorithm. Knowledge in ORMs: Hibernate, etc. Knowledge Spring security, API security best practices, Authentication/Authorization. Experience developing RESTful web APIs and API Gateways. Familiarity with CI/CD pipelines and associated tools: Jenkins, etc. Understanding of fundamental design principles behind a scalable application how it fits into a larger, distributed system. Ability to work well independently, as well as collaboratively, to take initiative, and to maintain productivity under pressure in a fast-paced environment. Strong organizational skills that reflect ability to perform and prioritize multiple tasks with excellent attention to detail. Experience working with cross functional teams including business analysts, UI/UX designers, software developers, and QA analysts. Ability to write clean, readable, secure code and adhere to team’s coding guidelines. Bachelor’s degree in Computer Science or related engineering field. Preferred Skills & Qualifications: Experience in a SaaS organization. Familiarity with AWS Services: EC2, S3, SES, SNS, SQS, etc. Familiarity with Monitoring tools: CloudWatch, AppDynamics, NewRelic, etc. Familiarity with Security Scans, Penetration Tests, etc. Familiarity with Atlassian Services: Jira, Confluence, BitBucket. Familiarity with logging and analysis tools: Splunk, DataDog, etc. Benefits: Health/Dental/Vision Insurance 401k match Flexible Work Schedule 15 Vacation Days + 7 Sick Days annually 12 Holidays + 4 Synchronized Recharge Days Parental Leave Monthly Internet/Phone Stipend All Team Perks: many including: DoorDash, Wellhub (formerly GymPass), Headspace & more LA & NY HQ Perks: Free lunches, refreshments & snacks Pet Friendly HQs This position pays between $100,000-120,000 base salary + equity. Your final base salary will be determined based on location, work experience, skills, knowledge, education and/or certifications. If you are located within a 25-mile radius of our LA or NY HQs, you will be considered a hybrid associate and will be required to be in the office 3 days/week. About Fuel Cycle : Fuel Cycle accelerates decision intelligence for legendary brands by enabling organizations to capture, analyze, and act on insights required to launch new products, acquire customers, and sustain growth. By leveraging Fuel Cycle’s Research Engine, a SaaS software platform, brands forge connections with their key audiences and harness actionable insights that drive confident business decisions. Our technology enables decision-makers to maintain constant connections with their customers, prospects, and users to uncover real-world actionable intelligence and insights. By integrating human insight with critical business data, and through automated quantitative and qualitative research solutions, the Fuel Cycle Research Engine powers product innovation, brand durability and sustainable growth. At Fuel Cycle, we embrace the values of diversity, equity, and inclusion and are committed to fostering an inclusive company culture. We believe that everyone, regardless of their background or identity, should have equal access to opportunities for growth and advancement. Our selection processes and career pathways are designed to be fair, transparent, and free from bias. We value the unique perspectives and contributions of each team member, knowing that this diverse range of experiences strengthens our team. Fuel Cycle stands firmly against discrimination based on disability, sex, race, gender identity, sexual orientation, religion, national origin, age, veteran status, or any other protected status under the law. Fuel Cycle is an equal opportunity employer and fully comply with the Americans with Disabilities Act (ADA). We will provide reasonable accommodations for qualified applicants and employees with disabilities, as needed, to enable them to perform the essential functions of their job and participate in the application and interview process. If you require accommodations during any part of the application process, please contact us at hr@fuelcycle.com to discuss your needs.",
        "url": "https://www.linkedin.com/jobs/view/3960526899",
        "summary": "Fuel Cycle is seeking an entry-level Software Engineer to join their product and engineering teams. The ideal candidate will have a minimum of 1 year of experience in software engineering, proficiency in Java, and knowledge of Spring MVC, Spring Boot, SQL, NOSQL, data structures and algorithms, ORMs, Spring security, API security best practices, Authentication/Authorization, RESTful web APIs, API Gateways, CI/CD pipelines, and fundamental design principles behind scalable applications. This hybrid position requires a minimum of 3 days in the office per week at Fuel Cycle's Los Angeles headquarters. Responsibilities include following architectural principles, designing enterprise scalable and maintainable solutions, integrating open-source frameworks, troubleshooting application issues, collaborating with Frontend Developers, providing input into product requirements and best practices, and ensuring continuous improvement of platform reliability and serviceability.",
        "industries": [
            "Software",
            "SaaS",
            "Technology",
            "Research"
        ],
        "soft_skills": [
            "Troubleshooting",
            "Problem Solving",
            "Collaboration",
            "Communication",
            "Organization",
            "Time Management",
            "Prioritization",
            "Attention to Detail",
            "Teamwork",
            "Initiative",
            "Adaptability",
            "Work Ethic"
        ],
        "hard_skills": [
            "Java",
            "Object-Oriented Programming",
            "Spring MVC",
            "Spring Boot",
            "SQL",
            "NOSQL",
            "Data Structures",
            "Algorithms",
            "ORMs",
            "Hibernate",
            "Spring Security",
            "API Security",
            "Authentication",
            "Authorization",
            "RESTful Web APIs",
            "API Gateways",
            "CI/CD Pipelines",
            "Jenkins",
            "Scalable Applications",
            "Design Principles",
            "Code Writing",
            "Coding Guidelines",
            "AWS Services",
            "Monitoring Tools",
            "Security Scans",
            "Penetration Tests",
            "Atlassian Services",
            "Logging and Analysis Tools"
        ],
        "tech_stack": [
            "Java",
            "Spring MVC",
            "Spring Boot",
            "SQL",
            "NOSQL",
            "Hibernate",
            "Spring Security",
            "RESTful Web APIs",
            "API Gateways",
            "Jenkins",
            "AWS Services",
            "CloudWatch",
            "AppDynamics",
            "NewRelic",
            "Jira",
            "Confluence",
            "BitBucket",
            "Splunk",
            "DataDog"
        ],
        "programming_languages": [
            "Java"
        ],
        "experience": 1,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Engineering"
            ]
        },
        "salary": {
            "max": 120000,
            "min": 100000
        },
        "benefits": [
            "Health Insurance",
            "Dental Insurance",
            "Vision Insurance",
            "401k Match",
            "Flexible Work Schedule",
            "Vacation",
            "Sick Leave",
            "Holidays",
            "Parental Leave",
            "Internet Stipend",
            "Phone Stipend",
            "DoorDash",
            "Wellhub",
            "Headspace",
            "Free Lunches",
            "Refreshments",
            "Snacks",
            "Pet Friendly Office"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3908187128,
        "company": "Unreal Staffing, Inc",
        "title": "Software Engineer",
        "created_on": 1720635716.713183,
        "description": "About Us We are a leading modern cloud platform for battery data analytics in R&D and production. Our platform directly synchronizes data from lab hardware to the cloud and analyzes it to unlock hidden insights connecting design to performance. Our team comprises physicists, software engineers, and computer scientists with backgrounds from prestigious institutions like Cambridge, Carnegie Mellon, Rivian, AWS, etc. United by a shared passion for helping battery engineers innovate and scale their technologies more quickly and joyfully. Our Mission Batteries play a crucial role in tackling climate change, with significant investments and revenue growth expected in the coming decade. We aim to revolutionize battery engineering by providing a modern cloud platform that allows teams to analyze data from thousands of parallel experiments, unlocking previously hidden insights connecting battery design to performance. Requirements Who You Are You have 1+ years of software engineering experience, ideally in a startup environment, where you have taken full ownership of projects and driven products from 0-1 and 1-10 You are passionate about solving customer problems and translating their insights into tangible product improvements You thrive in a fast-paced environment, eager to take ownership and challenge yourself to grow You possess strong problem-solving skills and can effectively translate business logic into technical requirements The Role As a key member of our team, you will: Perform and optimize client onboarding and integration processes Gain a thorough understanding of our technology and underlying architectures to troubleshoot and resolve customer-reported problems Develop tools to expedite diagnosis and enhance the prompt identification and resolution of future issues Serve as a subject matter expert for internal stakeholders within engineering, sales, and customer support, dedicated to overcoming technical deployment challenges and enhancing overall performance Prioritize, track, and communicate client needs to the engineering team effectively Coordinate with cross-functional teams to streamline operational processes and improve efficiency Additionally, you will play a crucial role in shaping an exceptional engineering culture and fostering practices that set us up for long-term success. Our Current Tech Stack Django with heavy tasks on Lambda GraphQL Server Apollo Client React, Apollo Client, Redux in Typescript on Next.JS Docker compose for dev, AWS and Terraform for staging/prod Event-based processing and streaming ETLs Single Table and Relational Database Models Benefits Competitive salary and equity options Opportunities for professional growth and advancement Flexible work environment with remote work options Collaborative and inclusive team culture Health and wellness benefits Company-sponsored events and team-building activities",
        "url": "https://www.linkedin.com/jobs/view/3908187128",
        "summary": "We are a leading modern cloud platform for battery data analytics in R&D and production. Our platform directly synchronizes data from lab hardware to the cloud and analyzes it to unlock hidden insights connecting design to performance. We are seeking a Software Engineer to join our team and take full ownership of projects, driving products from 0-1 and 1-10. This role involves performing and optimizing client onboarding and integration processes, troubleshooting and resolving customer-reported problems, developing tools to expedite diagnosis and enhance the prompt identification and resolution of future issues, serving as a subject matter expert for internal stakeholders within engineering, sales, and customer support, prioritizing, tracking, and communicating client needs to the engineering team effectively, and coordinating with cross-functional teams to streamline operational processes and improve efficiency.",
        "industries": [
            "Cloud Computing",
            "Software",
            "Data Analytics",
            "Battery Technology",
            "Research and Development"
        ],
        "soft_skills": [
            "Problem-Solving",
            "Communication",
            "Teamwork",
            "Ownership",
            "Passion for Customer Success",
            "Adaptability",
            "Collaboration",
            "Time Management",
            "Organization",
            "Analytical Thinking"
        ],
        "hard_skills": [
            "Software Engineering",
            "Client Onboarding",
            "Integration Processes",
            "Troubleshooting",
            "Problem Resolution",
            "Tool Development",
            "Diagnosis",
            "Subject Matter Expertise",
            "Technical Deployment",
            "Performance Enhancement",
            "Prioritization",
            "Tracking",
            "Communication",
            "Coordination",
            "Process Improvement",
            "Project Management"
        ],
        "tech_stack": [
            "Django",
            "Lambda",
            "GraphQL",
            "Apollo Client",
            "React",
            "Redux",
            "Typescript",
            "Next.JS",
            "Docker Compose",
            "AWS",
            "Terraform",
            "Event-based Processing",
            "ETL",
            "Single Table",
            "Relational Database Models"
        ],
        "programming_languages": [
            "Python",
            "JavaScript",
            "TypeScript"
        ],
        "experience": 1,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Competitive Salary",
            "Equity Options",
            "Professional Growth",
            "Advancement Opportunities",
            "Flexible Work Environment",
            "Remote Work Options",
            "Collaborative Culture",
            "Inclusive Team Culture",
            "Health and Wellness Benefits",
            "Company-Sponsored Events",
            "Team-Building Activities"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Los Angeles, CA",
        "job_id": 3912061882,
        "company": "Unreal Staffing, Inc",
        "title": "Software Engineer",
        "created_on": 1720635724.383563,
        "description": "About You — The Role We're searching for a versatile Software Engineer who embodies curiosity and relishes in overcoming daunting challenges. We highly value practical experience and an unwavering determination to surpass personal limits Our ideal candidate effortlessly transitions across different domains and is driven by a fervent desire to tackle intricate problems head-on. Whether you've sharpened your skills through varied internships, mastered multiple tech stacks, or poured dedication into a long-term personal project, we want to hear your narrative Requirements What You'll Do Craft human-readable, high-performance, and sustainable code Collaborate closely with internal teams and clients to glean insights and translate them into exceptional products Assess and prioritize tasks and projects based on their strategic significance and potential impact Requirements Bachelor's degree in Computer Science or a related technical field Exceptional proficiency in an industry-standard programming language (preferably JS and Python) Strong communication skills, both written and verbal Drive to acquire new skills and tackle challenging problems Tools You'll Use Django + Python3 React + NodeJS PostgresSQL AWS + Terraform Git Benefits In addition to providing a competitive salary and outstanding equity, we aim to equip every team member with the necessary tools to perform at their best. Here's how: We Empower Your Ownership: Equity compensation for all employees, ensuring you benefit from your contributions to patient care and the healthcare revolution We Support Your Well-being: Comprehensive Medical, Dental, Vision coverage Unlimited PTO and sick days Flexible working hours to promote work-life balance We Foster Your Growth: Fast-paced work environment focused on professional development Opportunities to transition roles within the organization, enabling you to learn new skills and contribute in various areas We Cultivate Community: Monthly team events, dinners, & happy hours Special team outings and events like yacht parties, Universal Studios trips, Magic Castle visits, and more",
        "url": "https://www.linkedin.com/jobs/view/3912061882",
        "summary": "Software Engineer position seeking a versatile individual with strong programming skills (JS, Python), experience with Django, React, NodeJS, PostgresSQL, AWS, Terraform, and Git.  The role involves crafting high-performance code, collaborating with teams and clients, and prioritizing tasks based on strategic impact. Benefits include competitive salary, equity, comprehensive healthcare, unlimited PTO, flexible hours, professional development opportunities, and team events.",
        "industries": [
            "Software Development",
            "Healthcare Technology",
            "Information Technology"
        ],
        "soft_skills": [
            "Curiosity",
            "Determination",
            "Problem Solving",
            "Collaboration",
            "Communication",
            "Adaptability",
            "Drive",
            "Strategic Thinking"
        ],
        "hard_skills": [
            "Python",
            "JavaScript",
            "Django",
            "React",
            "Node.js",
            "PostgresSQL",
            "AWS",
            "Terraform",
            "Git"
        ],
        "tech_stack": [
            "Django",
            "Python3",
            "React",
            "NodeJS",
            "PostgresSQL",
            "AWS",
            "Terraform",
            "Git"
        ],
        "programming_languages": [
            "JavaScript",
            "Python"
        ],
        "experience": 0,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Computer Science",
                "Related Technical Field"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Equity Compensation",
            "Medical, Dental, Vision Coverage",
            "Unlimited PTO",
            "Sick Days",
            "Flexible Working Hours",
            "Professional Development Opportunities",
            "Role Transition Opportunities",
            "Team Events",
            "Monthly Team Dinners",
            "Happy Hours",
            "Special Team Outings",
            "Yacht Parties",
            "Universal Studios Trips",
            "Magic Castle Visits"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3910662951,
        "company": "Pachama",
        "title": "Staff Data Engineer",
        "created_on": 1720635726.3004515,
        "description": "Who we are. Pachama is a mission-driven company looking to restore nature to help address climate change. Pachama brings the latest technology in remote sensing and AI to the world of forest carbon in order to enable forest conservation and restoration to scale. Pachama’s core technology harnesses satellite imaging with artificial intelligence to measure carbon captured in forests. Through the Pachama marketplace, responsible companies and individuals can connect with carbon credits from projects that are protecting and restoring forests worldwide. We are backed by mission-aligned investors including Breakthrough Energy Ventures, Amazon Climate Fund, Chris Sacca, Saltwater Ventures, and Paul Graham. Recent press: Pachama is #1 most innovative AI company Jeff Bezos' Last Shareholder Update Pachama to monitor and manage Mercado Libre forest projects We are looking for a Staff Data Engineer to lead the development of cutting-edge data systems backing our products for our mission to restore and protect the planet's forests. As a leader on the DMRV (Digital measurement, reporting, and verification) team, you will build, scale and deploy systems for ingesting, storing and computing the data powering our AI and Remote Sensing insights and are responsible for delivering those data insights to our customers to enable them to identify and originate the highest quality nature-based projects. A typical day includes collaborating across engineering and science teams to understand new dataset ingest pathways for model or algorithm features, writing code to support efficient compute and scalable transformation and algorithms to unlock insights over project data, designing systems for easy data access and experimentation pathways, pair coding with other engineers to raise the standards and bar on our technical work, and roadmapping core improvements to our data, compute or measurement stack. We're looking for engineers who find joy in the craft of building but live for seeing the end-to-end impact and want to rally engineers around them. Engineers who push forward initiatives by asking great questions, cutting through ambiguity, and organizing to win. Engineers who are relentlessly detail-oriented, methodical in their approach to understanding trade-offs, and place the highest emphasis on building and building quickly. Location: This role is remote. However, given the cross-functional communication responsibilities, it is preferred that you be within 3 hours of Pacific time. What You Will Help Us With: Impact: Empower our interdisciplinary team and customers to derive insights needed to originate high quality nature based projects from our multi-TB datasets by building the ingest pipelines, access and compute supporting our geospatial and remote sensing data powering our products Technical leadershipand innovation: for cross-functional projects as our data and compute pipelines are core platform assets used across teams. Connect product value across teams with the core design and technologies available to develop strategies and vision for the data systems we need to build and how we build them. You will work with teams to implement this vision Advocating for and mentoring on best practices: applied to our data pipelines and compute. Mentoring teammates to raise the bar across the engineering teams to enable step-level increases in efficiency Hands on contributions: coding the systems and tools that enable all engineering and science to produce high-quality insight for forest carbon projects and optimizing methods to run efficiently on large amounts of geospatial and remote sensing data Experience & Skills We’re Looking For: Experience leading larger cross-team engineering efforts Experience with data engineering including ingest, storage, orchestration and compute at scale with an ability to apply these skills to new domains like forest science and remote sensing Strong software engineering practices and a background in Python programming, debugging/profiling, and version control and system design Distributed Compute - familiarity with distributed compute technologies and knowledge of distributed systems concepts (like CPU/GPU interactions/transfers, latency/throughput bottlenecks, pipelining/multiprocessing) Our tech stack includes Dask and Flyte deployed through Kubernetes and GCP Comfort with fast pace execution and rapid iteration startup environment. Excited by product impact Passion for environmental sustainability and a desire to make a meaningful impact on the planet Preferred (but not Required) Qualifications: Owned and operated distributed compute system - you aren’t just familiar with distributed workflows but have been responsible for deploying, scaling, overseeing and maintaining the infrastructure needed to run them Built Data pipelines and infra ML and Scientific applications- Have worked with ML and/or Science teams previously Geospatial - familiarity or willingness to get your hands dirty with raster and vector data, and nuances of geospatial data and common geospatial cloud-native data formats (geopackage, flatgeobuf, cloud-optimized geotiff). Our tech stack includes Zarr, Rasterio, Geopandas, and Xarray Even if you don’t meet all these requirements, we encourage you to apply if this job description excites you. We are looking for ambitious people to help make an impact on climate change. That purpose requires us to bring together a diverse set of people with different backgrounds, perspectives, and skills to create solutions that work for all. The salary range for this position is $117,180 USD to $186,000 USD. Salary ranges are determined by role, level, and location. The salary for each posting reflects the compensation for new hire salaries for the position across all geographic locations. Individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. The position is also eligible for equity.",
        "url": "https://www.linkedin.com/jobs/view/3910662951",
        "summary": "Pachama is looking for a Staff Data Engineer to build and scale data systems for their AI and remote sensing platform, which uses satellite imaging and AI to measure carbon captured in forests. The role involves collaborating with engineering and science teams, writing code for efficient compute and scalable transformations, designing systems for data access and experimentation, mentoring teammates on best practices, and contributing hands-on to building data pipelines and tools.",
        "industries": [
            "Climate Change",
            "Environmental Sustainability",
            "Remote Sensing",
            "Artificial Intelligence",
            "Forestry",
            "Data Science"
        ],
        "soft_skills": [
            "Collaboration",
            "Leadership",
            "Technical Innovation",
            "Mentoring",
            "Communication",
            "Detail-Oriented",
            "Methodical",
            "Problem-Solving",
            "Ambitious",
            "Passionate",
            "Teamwork"
        ],
        "hard_skills": [
            "Data Engineering",
            "Ingest",
            "Storage",
            "Orchestration",
            "Compute",
            "Python",
            "Debugging",
            "Profiling",
            "Version Control",
            "System Design",
            "Distributed Compute",
            "Dask",
            "Flyte",
            "Kubernetes",
            "GCP",
            "Machine Learning",
            "Geospatial",
            "Raster",
            "Vector Data",
            "Cloud-Native Data Formats",
            "Zarr",
            "Rasterio",
            "Geopandas",
            "Xarray"
        ],
        "tech_stack": [
            "Dask",
            "Flyte",
            "Kubernetes",
            "GCP",
            "Zarr",
            "Rasterio",
            "Geopandas",
            "Xarray"
        ],
        "programming_languages": [
            "Python"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Computer Science",
                "Data Science",
                "Engineering",
                "Forest Science",
                "Remote Sensing"
            ]
        },
        "salary": {
            "max": 186000,
            "min": 117180
        },
        "benefits": [
            "Equity"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Santa Clara, CA",
        "job_id": 3888425125,
        "company": "Noralogic Inc",
        "title": "Urgent Requirement For Data Engineer ( Immediate Interview )",
        "created_on": 1720635727.8280768,
        "description": "Job Title: Data Engineer Job Location: Santa Clara, CA (Onsite from Day 1) NOTE: Candidate must be ready for coding test for this role. Client data engineer with at least 4-7 years' experience, ideally within a Data Engineer role. Demonstrated experience working with large and complex data sets as well as experience analyzing volumes of data. Excellent experience working Python, Pandas, Flask/Fast/Django API, Middleware, Scheduler, SQL, Databases. Prior experience with Python frameworks such as Django or Flask and a strong knowledge of SQL (queries, joins, etc.). Good to have some experience in AWS/Azure. Capability of developing highly-scalable RESTful APIs. Excellent team player and can work well in an individual capacity as well. Detail-oriented and possess strong analytical skills. Pays strong attention to detail and delivers work that is of a high standard. Highly goal-driven and work well in fast-paced environments. Location - Santa Clara Regards, Ajay Dutta Technical RecruiterNoralogic Inc. 109 E 17th St, Cheyenne WY 82001 +1. (307) 274-3105| (Ajay@noralogic.com) |www.noralogic.com USA: WY, MD, NJwww.noralogic.com Mexico: Guadalajara, Monterrey India: Noida UP **WBE and MBE company** ** ISO 9001:2015** **WY Top 50 Minority owned growing company**",
        "url": "https://www.linkedin.com/jobs/view/3888425125",
        "summary": "Data Engineer position at Noralogic in Santa Clara, CA, requiring 4-7 years of experience. Candidate must be proficient in Python, Pandas, Flask/FastAPI/Django, SQL, Databases, and have experience with AWS/Azure. Strong communication, analytical, and problem-solving skills are essential.",
        "industries": [
            "Data Engineering",
            "Software Development",
            "IT",
            "Technology"
        ],
        "soft_skills": [
            "Communication",
            "Problem-solving",
            "Teamwork",
            "Analytical",
            "Detail-oriented",
            "Goal-driven",
            "Adaptable",
            "Fast-paced environment"
        ],
        "hard_skills": [
            "Python",
            "Pandas",
            "Flask",
            "FastAPI",
            "Django",
            "SQL",
            "Databases",
            "AWS",
            "Azure",
            "RESTful APIs",
            "Middleware",
            "Scheduler"
        ],
        "tech_stack": [
            "Python",
            "Pandas",
            "Flask",
            "FastAPI",
            "Django",
            "SQL",
            "Databases",
            "AWS",
            "Azure"
        ],
        "programming_languages": [
            "Python",
            "SQL"
        ],
        "experience": 4,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Santa Monica, CA",
        "job_id": 3935860409,
        "company": "The Walt Disney Company",
        "title": "Lead Data Engineer",
        "created_on": 1720635729.9808815,
        "description": "On any given day at Disney Entertainment & ESPN Technology, we’re reimagining ways to create magical viewing experiences for the world’s most beloved stories while also transforming Disney’s media business for the future. Whether that’s evolving our streaming and digital products in new and immersive ways, powering worldwide advertising and distribution to maximize flexibility and efficiency, or delivering Disney’s unmatched entertainment and sports content, every day is a moment to make a difference to partners and to hundreds of millions of people around the world. Building the future of Disney’s media business: DE&E Technologists are designing and building the infrastructure that will power Disney’s media, advertising, and distribution businesses for years to come. Reach & Scale: The products and platforms this group builds and operates delight millions of consumers every minute of every day – from Disney+ and Hulu, to ABC News and Entertainment, to ESPN and ESPN+, and much more. Innovation: We develop and execute groundbreaking products and techniques that shape industry norms and enhance how audiences experience sports, entertainment & news. Business Operations is the glue of DE&E Technology, with a relentless focus on connecting all the pieces of the business. All the components that make up the organization – engineering, product, design, operations, support and more – come together and move the business forward with oversight, effective prioritization, and guidance. This team synthesizes information from across the business to ensure leaders are armed with all the necessary data before making strategic and financial decisions. From forecasting, funding, tracking, and reporting on projects, to ways of working, and resource management, Business Operations has visibility and input into all aspects of the team. As a Lead Data Engineer at Disney, you will provide leadership and expertise in data engineering, fostering collaboration and best practices within the team. You will oversee and mentor subordinate engineers to ensure successful completion of work on all data projects. With 7+ years of experience, you'll play a key role in maximizing the value of our data by designing and delivering secure solutions, using various programming languages and cloud services. Responsibilities: Directly supervise engineering resources on or supporting the Data Team and provide formal technical leadership. Design, write, test, and deploy code for data solutions. Operate effectively in a dynamic business environment, finding innovative solutions. Contribute to the entire data solution lifecycle: ideation, design, development, testing, and sustainment. Conduct research and prototyping to test ideas that solve business needs. Participate in the technical evaluation of business requirements. Lead data solution design to meet business and user requirements. Mentor and coach junior team members to enhance their skills and productivity. Review code for correctness and its impact on software and data architecture. Leverage Continuous Integration/Continuous Deployment (CI/CD) practices for efficient code deployment and quality checks. Perform database administration tasks, including managing credentials, roles, permissions, and monitoring usage. Sustain, enhance, and optimize data solutions. Participate in sustainment, on-call rotations, identifying and fixing defects, performing upgrades, and seeking iterative improvements. Minimum Qualifications: 7+ years of Data Engineering experience. Technical supervision and leadership experience. Proficiency in dimensional modeling, star schema design, normalization, role-based security, and account provisioning. Proven track record delivering complex data solutions on AWS. Extensive experience in database design, development, testing, upgrading, and migration. Proficiency in GraphQL , Neo4j, and languages such as Python, and Nodejs. Collaborative code repository experience in a CI/CD deployment environment. Bachelor's degree or higher in Computer Science, Information Science, or a relevant field OR equivalent work experience. Preferred Qualifications: Relevant certifications in Python, Data Warehousing, DBA, AWS, or CICD. Database Administrator (DBA) experience. Experience with other cloud providers such as Azure or Google Cloud. Agile/Scrum team experience. Experience or strong desire to work in the generative AI space. Experience with model serving in Data Bricks. #DisneyTech The hiring range for this position in Santa Monica, Glendale, and Seattle is $149,300 - $200,200 per year. The base pay actually offered will take into account internal equity and also may vary depending on the candidate’s geographic region, job-related knowledge, skills, and experience among other factors. A bonus and/or long-term incentive units may be provided as part of the compensation package, in addition to the full range of medical, financial, and/or other benefits, dependent on the level and position offered.",
        "url": "https://www.linkedin.com/jobs/view/3935860409",
        "summary": "Lead Data Engineer at Disney Entertainment & ESPN Technology to design, build and maintain data solutions using various programming languages and cloud services. This role requires 7+ years of experience and involves leading a team of engineers, providing technical guidance, and implementing CI/CD practices.",
        "industries": [
            "Media & Entertainment",
            "Technology",
            "Streaming",
            "Advertising",
            "Sports"
        ],
        "soft_skills": [
            "Leadership",
            "Collaboration",
            "Problem Solving",
            "Mentoring",
            "Communication",
            "Research",
            "Innovation"
        ],
        "hard_skills": [
            "Data Engineering",
            "Dimensional Modeling",
            "Star Schema Design",
            "Normalization",
            "Role-Based Security",
            "Account Provisioning",
            "AWS",
            "Database Design",
            "Development",
            "Testing",
            "Upgrading",
            "Migration",
            "GraphQL",
            "Neo4j",
            "Python",
            "Nodejs",
            "CI/CD",
            "Database Administration",
            "Agile/Scrum"
        ],
        "tech_stack": [
            "AWS",
            "GraphQL",
            "Neo4j",
            "Python",
            "Nodejs",
            "CI/CD"
        ],
        "programming_languages": [
            "Python",
            "Nodejs"
        ],
        "experience": 7,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Computer Science",
                "Information Science"
            ]
        },
        "salary": {
            "max": 200200,
            "min": 149300
        },
        "benefits": [
            "Medical",
            "Financial",
            "Bonus",
            "Long-term Incentive Units"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3964607834,
        "company": "Acceler8 Talent",
        "title": "Data Infrastructure Engineer",
        "created_on": 1720635731.690948,
        "description": "Data Infrastructure Engineer Are you an experienced Data Infrastructure Engineer ready to tackle challenges in AI and data processing at an unprecedented scale? Our organization is looking for a skilled professional to join our dynamic team. Here, technology and innovation intersect to create smarter solutions that redefine human-computer interactions. About the Company: We are a prominent entity in the tech industry, celebrated for our innovative approach to Artificial Intelligence. Our work is about enhancing the partnership between humans and technology, striving to exceed the current capabilities and achievements in the field. Supported by leading investors and collaborators such as March Capital, Thrive Capital, AMD, and NVIDIA, our venture is well-capitalized and poised for significant breakthroughs. About the Role: As a Data Infrastructure Engineer, you will be pivotal in designing and building a robust infrastructure that can handle and process petabytes of data efficiently. This role demands a proactive approach to developing scalable systems that are crucial for training cutting-edge AI models. Your work will involve close collaboration with our data research and data crawling teams to ensure the infrastructure not only meets but exceeds the required standards of reliability and performance. What We Can Offer You: A chance to work on petabyte-scale data systems, pushing the boundaries of data processing. Opportunities to innovate and implement new data preparation methods alongside top-tier professionals. Exposure to cutting-edge technology in a supportive and challenging environment. Competitive compensation and benefits, including relocation assistance for qualified candidates. Key Responsibilities: Design and implement scalable data processing infrastructure suitable for extensive AI training. Orchestrate large-scale workloads across massive computing clusters. Maintain and enhance distributed computing environments to support evolving data needs. Collaborate with various teams to integrate new data preparation technologies and methods. Address and resolve infrastructure issues efficiently, ensuring high availability and performance. The Data Infrastructure Engineer role is designed for individuals who thrive in fast-paced environments and are eager to drive technological advancements. If you are passionate about developing systems that impact the future of AI, we encourage you to apply and join our mission to revolutionize digital interactions. Relevant Keywords: data orchestration, high-throughput systems, Kubernetes, cloud services, distributed computing, AI training, petabyte-scale data handling, machine learning model training.",
        "url": "https://www.linkedin.com/jobs/view/3964607834",
        "summary": "Data Infrastructure Engineer to design and build a robust infrastructure for petabyte-scale data processing and AI training. This role involves designing scalable systems, orchestrating large-scale workloads, maintaining distributed computing environments, collaborating with various teams, and addressing infrastructure issues. ",
        "industries": [
            "Technology",
            "Artificial Intelligence",
            "Machine Learning",
            "Data Science",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Proactive",
            "Collaborative",
            "Problem-solving",
            "Passionate",
            "Innovative",
            "Eager to learn",
            "Teamwork",
            "Communication",
            "Adaptable"
        ],
        "hard_skills": [
            "Data orchestration",
            "High-throughput systems",
            "Kubernetes",
            "Cloud services",
            "Distributed computing",
            "AI training",
            "Petabyte-scale data handling",
            "Machine learning model training"
        ],
        "tech_stack": [
            "Kubernetes",
            "Cloud services",
            "Distributed computing"
        ],
        "programming_languages": [],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Relocation assistance",
            "Competitive compensation and benefits"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3964607679,
        "company": "Acceler8 Talent",
        "title": "MTS: Back End Data Engineer",
        "created_on": 1720635733.2294004,
        "description": "About the Company: Join a pioneering Google Brain spinout at the forefront of AI and ML technology, where human-computer collaboration is not just a concept but a reality. Our team is dedicated to revolutionizing user experiences by innovating at every level, from user interfaces down to the most efficient models. Our founders have already published one of the most revolutionary papers of the century, and that is only the start of their success... We've raised $65M in funding from top-tier investors (including NVIDIA & AMD) to advance our mission to enhance human efficiency through human feedback and LLMs, enabling users and organizations to enhance their impact on society. About the Role, MTS, Back End Data Engineer: You will collaborate with researchers and engineers to design and implement core infrastructure and systems powered by foundation models. You will also deploy and scale AI-powered products from the ground up while defining standards and protocols around monitoring, evaluation, security considerations, etc. What We Offer You: A role where your contributions have a direct impact on groundbreaking AI advancements. A collaborative, innovative environment that fosters growth and learning. Competitive compensation and benefits, including relocation assistance for those moving to San Francisco. Access to cutting-edge technology and resources. Relevant Keywords: Large Language Models, Machine Learning, Python, Deep Learning, Software Development, Kubernetes, Scalable Systems, AI, Data Quality, Data Acquisition, Web Crawling, Data Parsing, Data Ingestion, Large-scale Data Management, Spark, DataFlow",
        "url": "https://www.linkedin.com/jobs/view/3964607679",
        "summary": "Join a cutting-edge AI startup spun out of Google Brain, focused on enhancing human efficiency through human feedback and LLMs. As a Back End Data Engineer, you'll collaborate with researchers and engineers to design, implement, and scale AI-powered products from the ground up, defining standards and protocols for monitoring, evaluation, and security.",
        "industries": [
            "Artificial Intelligence",
            "Machine Learning",
            "Software Development"
        ],
        "soft_skills": [
            "Collaboration",
            "Innovation",
            "Problem-Solving",
            "Growth Mindset"
        ],
        "hard_skills": [
            "Python",
            "Kubernetes",
            "Data Quality",
            "Data Acquisition",
            "Web Crawling",
            "Data Parsing",
            "Data Ingestion",
            "Large-scale Data Management",
            "Spark",
            "DataFlow"
        ],
        "tech_stack": [
            "Foundation Models",
            "LLMs",
            "Kubernetes",
            "Spark",
            "DataFlow"
        ],
        "programming_languages": [
            "Python"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Competitive Compensation",
            "Relocation Assistance",
            "Access to Cutting-Edge Technology"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Mountain View, CA",
        "job_id": 3816170697,
        "company": "Google",
        "title": "Software Engineer III, Machine Learning, Pixel",
        "created_on": 1720635737.4082403,
        "description": "Minimum qualifications: Bachelor’s degree or equivalent practical experience. 2 years of experience with software development in one or more programming languages, or 1 year of experience with an advanced degree in an industry setting. 2 years of experience with data structures or algorithms in either an academic or industry setting. 2 years of experience with machine learning algorithms and tools (e.g., TensorFlow), artificial intelligence, deep learning or natural language processing. Preferred qualifications: Master's degree or PhD in Computer Science or related technical field. 2 years of experience with performance, large scale systems data analysis, visualization tools, and/or debugging. Experience developing accessible technologies. Proficiency in code and system health, diagnosis and resolution, and software test engineering. About The Job Google's software engineers develop the next-generation technologies that change how billions of users connect, explore, and interact with information and one another. Our products need to handle information at massive scale, and extend well beyond web search. We're looking for engineers who bring fresh ideas from all areas, including information retrieval, distributed computing, large-scale system design, networking and data storage, security, artificial intelligence, natural language processing, UI design and mobile; the list goes on and is growing every day. As a software engineer, you will work on a specific project critical to Google’s needs with opportunities to switch teams and projects as you and our fast-paced business grow and evolve. We need our engineers to be versatile, display leadership qualities and be enthusiastic to take on new problems across the full-stack as we continue to push technology forward. With your technical expertise you will manage project priorities, deadlines, and deliverables. You will design, develop, test, deploy, maintain, and enhance software solutions. Google is an engineering company at heart. We hire people with a broad set of technical skills who are ready to take on some of technology's greatest challenges and make an impact on users around the world. At Google, engineers not only revolutionize search, they routinely work on scalability and storage solutions, large-scale applications and entirely new platforms for developers around the world. From Google Ads to Chrome, Android to YouTube, social to local, Google engineers are changing the world one technological achievement after another. The US base salary range for this full-time position is $136,000-$200,000 + bonus + equity + benefits. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process. Please note that the compensation details listed in US role postings reflect the base salary only, and do not include bonus, equity, or benefits. Learn more about benefits at Google . Responsibilities Write product or system development code. Participate in, or lead design reviews with peers and stakeholders to decide amongst available technologies. Review code developed by other developers and provide feedback to ensure best practices (e.g., style guidelines, checking code in, accuracy, testability, and efficiency). Contribute to existing documentation or educational content and adapt content based on product/program updates and user feedback. Triage product or system issues and debug/track/resolve by analyzing the sources of issues and the impact on hardware, network, or service operations and quality. Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing our Accommodations for Applicants form .",
        "url": "https://www.linkedin.com/jobs/view/3816170697",
        "summary": "Google is seeking a Software Engineer to develop next-generation technologies for their products. This role involves designing, developing, testing, deploying, maintaining, and enhancing software solutions, collaborating with teams, and ensuring high-quality code.  The engineer will be involved in a variety of projects across the full-stack, including information retrieval, distributed computing, large-scale system design, networking and data storage, security, artificial intelligence, natural language processing, UI design, and mobile.",
        "industries": [
            "Technology",
            "Software Development",
            "Internet",
            "Artificial Intelligence",
            "Machine Learning",
            "Data Science"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Leadership",
            "Problem Solving",
            "Analytical Skills",
            "Time Management",
            "Project Management",
            "Adaptability",
            "Versatility",
            "Detail Oriented"
        ],
        "hard_skills": [
            "Software Development",
            "Programming Languages",
            "Data Structures",
            "Algorithms",
            "Machine Learning",
            "TensorFlow",
            "Artificial Intelligence",
            "Deep Learning",
            "Natural Language Processing",
            "Performance Optimization",
            "Large Scale Systems",
            "Data Analysis",
            "Visualization Tools",
            "Debugging",
            "Accessibility",
            "Code Health",
            "System Diagnosis",
            "Software Test Engineering"
        ],
        "tech_stack": [
            "TensorFlow",
            "Google Cloud Platform (GCP)",
            "Kubernetes",
            "BigQuery",
            "Cloud Functions",
            "Cloud Storage",
            "Cloud Run",
            "Cloud SQL",
            "Dataflow",
            "Dataproc",
            "Data Studio",
            "Google Maps",
            "Google Search",
            "Android",
            "iOS",
            "Chrome",
            "YouTube",
            "Google Ads",
            "Google Assistant",
            "Firebase",
            "Angular",
            "React",
            "Vue.js",
            "Node.js",
            "Python",
            "Java",
            "C++",
            "Go",
            "Rust"
        ],
        "programming_languages": [
            "Python",
            "Java",
            "C++",
            "Go",
            "Rust"
        ],
        "experience": 2,
        "education": {
            "min_degree": "Bachelor’s degree",
            "fields": [
                "Computer Science",
                "Related Technical Field"
            ]
        },
        "salary": {
            "max": 200000,
            "min": 136000
        },
        "benefits": [
            "Bonus",
            "Equity",
            "Health Insurance",
            "Retirement Plan",
            "Paid Time Off",
            "Disability Insurance",
            "Life Insurance",
            "Employee Assistance Program"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Mountain View, CA",
        "job_id": 3940420431,
        "company": "Samsung Electronics America",
        "title": "Lead Data Engineer - Samsung Ads",
        "created_on": 1720635738.9517255,
        "description": "Position Summary Role and Responsibilities Samsung Ads is a leading innovator in advertising technology, dedicated to providing cutting-edge solutions that optimize ad performance and deliver exceptional results for our clients. We are seeking a highly skilled and experienced front-end staff engineer to join our dynamic team and help shape the future of the ad tech industry. We seek a talented Lead Data Engineer to play a pivotal role in enhancing our platform’s performance advertising capabilities. As an integral part of our engineering team, you will collaborate with cross-functional teams to design, implement, and optimize attribution models, ensuring accurate measurement of marketing campaign effectiveness. Responsibilities Develop and implement advanced attribution models to analyze and attribute the impact of various marketing channels on user conversion. Collaborate with data scientists, product managers, and other engineers to refine and improve attribution methodologies. Design and maintain scalable and optimized data pipelines for efficient collection, processing, and storage of attribution-related data. Work closely with stakeholders to understand and translate business requirements into technical solutions. Conduct A/B testing and performance analysis to validate and iterate on attribution models. Stay updated on industry trends and emerging technologies related to attribution modeling and ad tech. Qualifications Skills and Qualifications Bachelor’s or Master’s in Computer Science, Data Science, or a related field. Requires at least 8 years of related experience and a Bachelor's degree; or 6 years and a Master's degree; or a PhD with 3 years Proven experience in attribution modeling within the ad tech industry. Strong programming skills in Python, Java, or Scala. Experience in working with Kubernetes and stream data processing frameworks (Flink, Apache Ignite) Proficient in working with big data technologies and databases (e.g., Hadoop, Spark, SQL, and MapReduce). Hands-on experience with orchestration tools like Airflow or similar Solid understanding of statistical concepts and experience with relevant tools. Excellent problem-solving and communication skills. Preferred Qualifications Experience with machine learning techniques for attribution modeling. Familiarity with real-time data processing and streaming technologies. Knowledge of end-to-end digital advertising ecosystems and industry standards. Knowledge of Snowflake and related technologies California Only Compensation for this role is expected to be between $200,000 and $230,000. Actual pay will be determined considering factors such as relevant skills and experience, and comparison to other employees in the role. #Hybrid Please visit Samsung membership to see Privacy Policy, which defaults according to your location. You can change Country/Language at the bottom of the page. If you are European Economic Resident, please click here. At Samsung, we believe that innovation and growth are driven by an inclusive culture and a diverse workforce. We aim to create a global team where everyone belongs and has equal opportunities, inspiring our talent to be their true selves. Together, we are building a better tomorrow for our customers, partners, and communities. Samsung Electronics America, Inc. and its subsidiaries are committed to employing a diverse workforce, and provide Equal Employment Opportunity for all individuals regardless of race, color, religion, gender, age, national origin, marital status, sexual orientation, gender identity, status as a protected veteran, genetic information, status as a qualified individual with a disability, or any other characteristic protected by law. Reasonable Accommodations for Qualified Individuals with Disabilities During the Application Process Samsung Electronics America is committed to providing reasonable accommodations for qualified individuals with disabilities in our job application process. If you have a disability and require a reasonable accommodation in order to participate in the application process, please contact our Reasonable Accommodation Team (855-557-3247) or SEA_Accommodations_Ext@sea.samsung.com for assistance. This number is for accommodation requests only and is not intended for general employment inquiries.",
        "url": "https://www.linkedin.com/jobs/view/3940420431",
        "summary": "Samsung Ads seeks a Lead Data Engineer to develop and implement attribution models, analyze marketing campaign effectiveness, and optimize performance advertising. The role involves collaborating with data scientists, product managers, and engineers, designing data pipelines, and staying up-to-date on industry trends.",
        "industries": [
            "Advertising Technology",
            "Marketing Technology",
            "Data Science"
        ],
        "soft_skills": [
            "Collaboration",
            "Problem-Solving",
            "Communication"
        ],
        "hard_skills": [
            "Attribution Modeling",
            "Python",
            "Java",
            "Scala",
            "Kubernetes",
            "Flink",
            "Apache Ignite",
            "Hadoop",
            "Spark",
            "SQL",
            "MapReduce",
            "Airflow",
            "Statistical Concepts",
            "Machine Learning",
            "Real-time Data Processing",
            "Streaming Technologies",
            "Digital Advertising",
            "Snowflake"
        ],
        "tech_stack": [
            "Kubernetes",
            "Flink",
            "Apache Ignite",
            "Hadoop",
            "Spark",
            "SQL",
            "MapReduce",
            "Airflow",
            "Snowflake"
        ],
        "programming_languages": [
            "Python",
            "Java",
            "Scala"
        ],
        "experience": 8,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Computer Science",
                "Data Science"
            ]
        },
        "salary": {
            "max": 230000,
            "min": 200000
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco Bay Area",
        "job_id": 3964639975,
        "company": "Acceler8 Talent",
        "title": "Software Engineer - Pretraining Data",
        "created_on": 1720635740.8195422,
        "description": "Software Engineer - Pretraining Data Introduction: We are on a mission to build safe AGI that accelerates humanity’s progress on critical global challenges. Our strategy leverages frontier-scale pre-training, domain-specific RL, ultra-long context, and test-time compute. If you're a Software Engineer passionate about pretraining data and creating efficient, robust data pipelines, this role is for you. About the Company: Our organization is dedicated to automating research and code generation to improve models and solve alignment issues more effectively than humans alone. We focus on high-quality data processing and innovative solutions, contributing to significant advancements in AI and AGI safety. About the Role: As a Software Engineer specializing in pretraining data, you will develop and optimize web scraping techniques to handle massive, multimodal datasets. Your expertise will be crucial in building and maintaining data pipelines that support our advanced AI models. What We Can Offer You: Significant equity component 401(k) plan with 6% matching Comprehensive health, dental, and vision insurance for you and your dependents Unlimited paid time off Flexibility to work in-person in San Francisco or remotely Visa sponsorship and relocation stipend available Key Responsibilities: Design and implement multimodal web crawlers for large-scale data collection Develop and maintain large-scale data processing pipelines using tools like Ray, Apache Spark, and Google BigQuery Implement deduplication techniques across multiple data modalities Apply heuristic and model-based techniques for parsing and filtering data Identify and integrate new data sources into pre/post-training datasets Join us to shape the future of AGI by contributing to our innovative approach to data processing and AI model improvement. Your skills as a Software Engineer in pretraining data will drive our mission forward. Relevant Keywords Software Engineer , pretraining data , multimodal datasets , web scraping , data pipelines , distributed computing , data quality , AI models , AGI safety , data processing tools .",
        "url": "https://www.linkedin.com/jobs/view/3964639975",
        "summary": "We are seeking a Software Engineer specializing in pretraining data to develop and optimize web scraping techniques for massive, multimodal datasets. You will build and maintain data pipelines using tools like Ray, Apache Spark, and Google BigQuery, implement deduplication techniques, and apply heuristic and model-based techniques for parsing and filtering data.",
        "industries": [
            "Artificial Intelligence",
            "Machine Learning",
            "Data Science",
            "Software Development"
        ],
        "soft_skills": [
            "Problem-solving",
            "Analytical skills",
            "Communication",
            "Teamwork",
            "Adaptability",
            "Attention to detail"
        ],
        "hard_skills": [
            "Web scraping",
            "Data processing",
            "Data pipeline development",
            "Distributed computing",
            "Data quality assessment",
            "Multimodal data handling",
            "Heuristic and model-based techniques",
            "Data filtering",
            "Data parsing",
            "Ray",
            "Apache Spark",
            "Google BigQuery"
        ],
        "tech_stack": [
            "Ray",
            "Apache Spark",
            "Google BigQuery"
        ],
        "programming_languages": [],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Equity",
            "401(k) matching",
            "Health insurance",
            "Dental insurance",
            "Vision insurance",
            "Unlimited paid time off",
            "Remote work option",
            "Visa sponsorship",
            "Relocation stipend"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3897982219,
        "company": "Unreal Staffing, Inc",
        "title": "Lead Data Warehouse Engineer",
        "created_on": 1720635742.363119,
        "description": "Company Overview: Welcome to the forefront of data-driven innovation! Our company is dedicated to harnessing the power of data to drive transformative change and solve complex problems across industries. We're committed to building scalable and efficient data warehousing solutions that enable advanced analytics, reporting, and business intelligence. Join us and lead our team in shaping the future of data warehouse engineering. Position Overview: As the Lead Data Warehouse Engineer, you'll be responsible for leading our data warehouse engineering efforts and driving the design, development, and maintenance of our data warehousing solutions. You'll lead a team of talented engineers and collaborate closely with cross-functional teams to deliver end-to-end data solutions that support the needs of our data-driven organization. If you're a seasoned engineer with expertise in data warehousing technologies and a track record of leading successful data projects, we want you on our team. Requirements Key Responsibilities: Technical Leadership: Lead a team of data warehouse engineers, providing guidance, mentorship, and technical leadership in data warehouse engineering best practices and technologies Data Warehouse Design: Lead the design and architecture of our data warehousing solutions, ensuring scalability, reliability, and performance to meet the needs of our growing organization Data Modeling: Lead the design and implementation of data models and schemas to support analytical and operational requirements, ensuring data integrity, consistency, and performance in a data warehouse environment ETL/ELT Processes: Lead the development of ETL/ELT processes to extract, transform, and load data from various sources into the data warehouse, ensuring data quality and integrity throughout the process Performance Optimization: Lead efforts to optimize data warehouse performance, identifying and addressing bottlenecks and inefficiencies to improve system scalability and reliability Data Governance: Implement data governance policies and best practices to ensure data quality, security, and compliance with regulatory requirements Monitoring and Alerting: Implement monitoring and alerting systems to track data warehouse performance and health, detecting and mitigating issues proactively to minimize downtime and data loss Documentation and Best Practices: Define and promote best practices for data warehouse engineering, documentation, and usage, ensuring clear and comprehensive documentation to facilitate understanding and collaboration among team members Collaboration: Collaborate with cross-functional teams, including data scientists, software engineers, and business stakeholders, to understand requirements and deliver data solutions that meet business needs Mentorship and Development: Mentor junior engineers, providing guidance, support, and opportunities for growth and development in their data warehouse engineering careers Qualifications: Bachelor's degree or higher in Computer Science, Engineering, Mathematics, or related field 7+ years of experience in data warehouse engineering, with a focus on designing, building, and maintaining data warehousing solutions Leadership experience, with a demonstrated ability to lead and mentor a team of engineers Proficiency in SQL and experience with data warehousing technologies such as Snowflake, Amazon Redshift, Google BigQuery, or similar Strong understanding of data modeling concepts and techniques, with experience designing and implementing data models and schemas for data warehouses Experience with ETL/ELT processes, data integration, and data governance in a data warehouse environment Strong problem-solving skills and analytical thinking, with the ability to troubleshoot complex data issues and optimize system performance Excellent communication and collaboration skills, with the ability to work effectively in cross-functional teams and communicate technical concepts to non-technical stakeholders Benefits Competitive salary: The industry standard salary for Lead Data Warehouse Engineers typically ranges from $200,000 to $300,000 per year, depending on experience and qualifications Comprehensive health, dental, and vision insurance plans Flexible work hours and remote work options Generous vacation and paid time off Professional development opportunities, including access to training programs, conferences, and workshops State-of-the-art technology environment with access to cutting-edge tools and resources Vibrant and inclusive company culture with opportunities for growth and advancement Exciting projects with real-world impact at the forefront of data-driven innovation Join Us: Ready to lead the charge in data warehouse engineering? Apply now to join our team and drive the future of data-driven innovation!",
        "url": "https://www.linkedin.com/jobs/view/3897982219",
        "summary": "Lead Data Warehouse Engineer role at a company specializing in data-driven solutions. The position requires experience in data warehouse design, development, and maintenance, with a focus on technologies like Snowflake, Amazon Redshift, and Google BigQuery. Responsibilities include leading a team of engineers, designing data models, developing ETL/ELT processes, optimizing performance, and ensuring data governance. The company offers a competitive salary, comprehensive benefits, and opportunities for professional growth.",
        "industries": [
            "Data Engineering",
            "Data Warehousing",
            "Data Analytics",
            "Business Intelligence",
            "Technology"
        ],
        "soft_skills": [
            "Leadership",
            "Mentorship",
            "Communication",
            "Collaboration",
            "Problem-solving",
            "Analytical thinking"
        ],
        "hard_skills": [
            "SQL",
            "Snowflake",
            "Amazon Redshift",
            "Google BigQuery",
            "Data Modeling",
            "ETL/ELT",
            "Data Integration",
            "Data Governance",
            "Performance Optimization"
        ],
        "tech_stack": [
            "Snowflake",
            "Amazon Redshift",
            "Google BigQuery",
            "SQL"
        ],
        "programming_languages": [
            "SQL"
        ],
        "experience": 7,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Computer Science",
                "Engineering",
                "Mathematics"
            ]
        },
        "salary": {
            "max": 300000,
            "min": 200000
        },
        "benefits": [
            "Competitive salary",
            "Health insurance",
            "Dental insurance",
            "Vision insurance",
            "Flexible work hours",
            "Remote work options",
            "Vacation",
            "Paid time off",
            "Professional development",
            "State-of-the-art technology",
            "Company culture",
            "Growth opportunities"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Ramon, CA",
        "job_id": 3672816224,
        "company": "Sunrise Systems, Inc.",
        "title": "Data Engineer",
        "created_on": 1720635743.9926429,
        "description": "Duration: 06 Months Location: Remote (During Testing & Engineering need to be Onsite) Description ACTUAL ROLE: IoT (Internet of Things) Engineer LOCAL CANDIDATES ONLY: Onsite required during engineering and testing, but can be remote when not testing. San Ramon is the current lab, but the team will be moving a few miles to Dublin in May. Top Things IoT (Internet of Things) experience Hands on engineering experience software / data experience. Client laptop will be provided and all necessary computer equipment. The Data / IoT engineer will be on a team developing a system to help monitor our extensive service territory of environmental risks. This individual will research, create, test and document IoT commercially available solutions which will integrate devices, sensors, and software seamlessly with other systems. Must possess is an in-depth understanding of computer programming and network security. They must also have knowledge of sensors, machine learning and AI and must have experience in working with front-end and back-end systems as well as custom-built firmware and hardware. Qualifications Minimum BA/BS in Business, Engineering, Computer Science/Information Systems or equivalent 1-3 years of mixed experience in IoT – architecture, development, system administration and operations Ability to work independently, and hands on Ability to be creative. Firm understanding of IoT architecture and principles. Proficient knowledge of sensors In-depth understanding of computer programming and network security Knowledge of machine learning and AI Familiarity with Big Data and machine learning algorithms Knowledge of device and data security Experience working with front-end and back-end systems. Experience with custom-build firmware and hardware. Well-versed with multiple programming languages such as Embedded-C, Embedded C++, JavaScript and Python Work and collaborate well with teams Strong communication skills Strong analytical and problem-solving skills Desired 3-5 years of experience within any area of Electric Operations in the electric utility industry 3-5 years of experience with IoT development and installations. Thanks, Murali Potugari 732-272-0284 | murali.p@sunrisesys.com",
        "url": "https://www.linkedin.com/jobs/view/3672816224",
        "summary": "IoT Engineer needed to develop a system to monitor environmental risks for a utility company. Responsibilities include researching, creating, testing and documenting IoT solutions, integrating devices, sensors, and software, and collaborating with teams. Must have strong programming skills, network security knowledge, experience with sensors, machine learning and AI, front-end and back-end systems, custom firmware and hardware.",
        "industries": [
            "Utility",
            "Environmental",
            "Technology",
            "Engineering"
        ],
        "soft_skills": [
            "Independent",
            "Creative",
            "Communication",
            "Analytical",
            "Problem-Solving",
            "Collaboration"
        ],
        "hard_skills": [
            "IoT Architecture",
            "Sensors",
            "Computer Programming",
            "Network Security",
            "Machine Learning",
            "AI",
            "Big Data",
            "Data Security",
            "Front-end",
            "Back-end",
            "Firmware",
            "Hardware"
        ],
        "tech_stack": [
            "IoT",
            "Sensors",
            "Machine Learning",
            "AI",
            "Big Data",
            "Embedded-C",
            "Embedded C++",
            "JavaScript",
            "Python"
        ],
        "programming_languages": [
            "Embedded-C",
            "Embedded C++",
            "JavaScript",
            "Python"
        ],
        "experience": 1,
        "education": {
            "min_degree": "BA/BS",
            "fields": [
                "Business",
                "Engineering",
                "Computer Science",
                "Information Systems"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Irvine, CA",
        "job_id": 3926387167,
        "company": "Open Systems Inc.",
        "title": "Software Engineer 1",
        "created_on": 1720635748.050104,
        "description": "Job Title: Software Engineer 1 Duration: 3 to 6 Months Contract Location: Irvine CA 92612 Detailed Job Description: Job Summary: Assist in the development, testing, release, and integration efforts of software and hardware associated activities. Major Responsibilities: Hardware and software integration of new and/or existing systems, functions and LRUs designed by the Development Engineering and the Software Development groups. Integration test of LRU and system functionality. Ensure all testing is properly documented and submitted. Responsible for the integration and maintenance of a test rack configuration with regards to the systems installation. Software and system testing of all In-flight Entertainment (IFE) functionality for existing systems. Participated in the design and development of all IFE system functionality. Participate in all efforts in the testing and regression testing of software phase releases. Assist and support all engineering efforts company wide as required. Assist in developing test plans and procedures to validate system requirements. Troubleshoot problems reported for all Panasonic Avionics IFE products. Focal point for software development, integration and problem investigation for all assigned equipment and systems. Assist in troubleshooting and analyzing root cause of any system anomalies/discrepancies in hardware and/or software. Additional Job Details: Knowledge/Skill Requirements: A technical background in development and test. Familiar with Unix and Linux OS. Basic understanding of systems administrator capabilities and command language for Linux is preferred. Ability to plan engineering activities to perform tasks. Learn to use professional concepts and apply company policies and procedures to resolve routine issues. Generally, applies existing practices and procedures in analyzing situations or data. Possess solid writing and communication skills. Ability to effectively communicate in English, in person and on the phone. Proficiency with Microsoft Office products. Education/Experience Requirements: Bachelor of Science Degree in Computer Sciences, Computer Engineering, Electrical Engineering or Software Engineering, or equivalent experience. 0-3 years’ experience in software/system engineering development and/or software/hardware testing. IFE and/or Avionics industry experience is preferred. Other Requirements: Ability to lift to 25 pounds. Most time spent at test rack (75%) which includes standing, lifting, bending, twisting and stooping. Education/Experience Requirements: Bachelor of Science Degree in Computer Sciences, Computer Engineering, Electrical Engineering or Software Engineering, or equivalent experience. 0-3 years’ experience in software/system engineering development and/or software/hardware testing. IFE and/or Avionics industry experience is preferred. Other Requirements: Ability to lift up to 25 pounds. Most of the time spent at test rack (75%) which includes standing, lifting, bending, twisting and stooping. Who We Are: Open Systems Inc. (OSI) was founded in 1994 to provide information technology solutions and staffing services to large and mid-size companies across the U.S. Our corporate office is located at 6495 Shiloh Road, Ste 310 Alpharetta, GA 30005. We provide a full range of staffing services including contract, contract-to-hire, and direct hire solutions. Our technical recruiting experts are experienced in technical screening, candidate sourcing, and behavioral interviewing techniques. They focus on providing candidates who match your technical requirements and fit seamlessly into your company culture. Contact Open Systems, Inc. anytime by website, phone or email. We look forward to hearing from you!!",
        "url": "https://www.linkedin.com/jobs/view/3926387167",
        "summary": "Software Engineer 1 with a focus on hardware and software integration, testing, and problem investigation for In-Flight Entertainment (IFE) systems. This contract role involves integrating new and existing systems, conducting integration tests, documenting test results, maintaining test rack configurations, and participating in software release testing and regression testing.",
        "industries": [
            "Aerospace",
            "Aviation",
            "In-Flight Entertainment",
            "Software Development",
            "Systems Integration",
            "Hardware Engineering",
            "Testing"
        ],
        "soft_skills": [
            "Communication",
            "Problem Solving",
            "Teamwork",
            "Documentation",
            "Analytical",
            "Planning",
            "Troubleshooting"
        ],
        "hard_skills": [
            "Linux",
            "Unix",
            "Systems Administration",
            "Microsoft Office",
            "Test Planning",
            "Test Procedures",
            "Regression Testing",
            "Software Development",
            "Hardware Integration"
        ],
        "tech_stack": [
            "Linux",
            "Unix"
        ],
        "programming_languages": [],
        "experience": 3,
        "education": {
            "min_degree": "Bachelor of Science",
            "fields": [
                "Computer Sciences",
                "Computer Engineering",
                "Electrical Engineering",
                "Software Engineering"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Beverly Hills, CA",
        "job_id": 3816855212,
        "company": "Alo Yoga",
        "title": "Staff Data Engineer",
        "created_on": 1720635752.1126454,
        "description": "WHY JOIN ALO? Mindful movement. It’s at the core of why we do what we do at Alo—it’s our calling. Because mindful movement in the studio leads to better living. It changes who yogis are off the mat, making their lives and their communities better. That’s the real meaning of studio-to-street: taking the consciousness from practice on the mat and putting it into practice in life. OVERVIEW We are looking for a Staff Data Engineer to join our rapidly growing engineering team. They will join the data group, tasked with architecture, deployment, and automated maintenance of our Big Data platform. The role will collaborate with a cross-functional team to provide data solutions with engineering, marketing, merchandizing, and executive teams. RESPONSIBILITIES Contribute to technology selection and strategy for big data solutions Build, test and operate stable, scalable data pipelines that cleanse, structure and integrate disparate data sets into a readable and accessible format for end-user facing reports, data sciences and ad-hoc analyses Create data pipelines to handle data ingestion in a way that is both rapidly available and fault tolerant Support AI/Machine Learning Initiatives to drive value for consumers and internal stakeholders Review and contribute to technical documentation Knowledge sharing and coaching Support with the onboarding of new data team members QUALIFICATIONS Bachelor’s degree in Computer Science, Engineering, or a related technical field (Or equivalent knowledge/experience) A minimum of 12 years of Object Oriented Programming (OOP) and/or functional programming experience/education Familiarity with Cloud Big Data Solutions (EMR, AWS Redshift, Kinesis, AWS Data Streams, Big Query, Azure Data Pipelines, Azure Datawarehouse, etc) Familiarity with Apache Suite for Big Data Architecture (Spark, Akka, Cassandra, Kafka) and DBT based Data Modeling/Transformation. Expert in SQL Scripting, AWS Glue, PySpark Knowledge of Python programming language a plus Currently based in the greater LA area or open to relocation and consent to working onsite full-time The base salary range for this position is $160,000-$220,000 per year which represents the current range for the base salary for this exempt position. Please note that actual salaries will vary based on factors including but not limited to location, experience, and performance. As such, on occasion and when applicable, there is the possibility that the final, agreed-upon base salary may be outside of the upper end of the range. Please also note the range listed is just one component of the company’s total rewards package for exempt employees. Other rewards may include performance bonuses, long term incentives, a PTO policy, and many other progressive benefits. Please review our company California Job Applicant Privacy Policy HERE.",
        "url": "https://www.linkedin.com/jobs/view/3816855212",
        "summary": "Alo Yoga is seeking a Staff Data Engineer to join their growing engineering team. This individual will contribute to big data solutions, build data pipelines, and support AI/ML initiatives. The role requires strong programming skills, experience with cloud-based big data solutions, and expertise in SQL, AWS Glue, and PySpark.",
        "industries": [
            "Fitness",
            "Apparel",
            "Retail",
            "Technology",
            "E-commerce"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Time Management",
            "Teamwork",
            "Coaching"
        ],
        "hard_skills": [
            "Object Oriented Programming",
            "Functional Programming",
            "Cloud Big Data Solutions",
            "Apache Suite",
            "Spark",
            "Akka",
            "Cassandra",
            "Kafka",
            "DBT",
            "SQL Scripting",
            "AWS Glue",
            "PySpark",
            "Python"
        ],
        "tech_stack": [
            "AWS EMR",
            "AWS Redshift",
            "Kinesis",
            "AWS Data Streams",
            "Big Query",
            "Azure Data Pipelines",
            "Azure Datawarehouse",
            "Spark",
            "Akka",
            "Cassandra",
            "Kafka",
            "DBT",
            "AWS Glue",
            "PySpark",
            "Python"
        ],
        "programming_languages": [
            "Python",
            "SQL",
            "PySpark"
        ],
        "experience": 12,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Computer Science",
                "Engineering",
                "Related Technical Field"
            ]
        },
        "salary": {
            "max": 220000,
            "min": 160000
        },
        "benefits": [
            "Performance bonuses",
            "Long term incentives",
            "PTO",
            "Progressive benefits"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Mateo, CA",
        "job_id": 3765974062,
        "company": "Verkada",
        "title": "Software Engineer - Computer Vision",
        "created_on": 1720635753.889257,
        "description": "Who We Are Verkada is the largest cloud-based B2B physical security platform company in the world. Only Verkada offers six product lines — video security cameras, access control, environmental sensors, alarms, workplace and intercoms — integrated with a single cloud-based software platform. Designed with simplicity and scalability in mind, Verkada gives organizations the real-time insight to know what could impact the safety and comfort of people throughout their physical environment, while empowering them to take immediate action to minimize security risks, workplace frustrations and costly inefficiencies. Founded in 2016 with more than $460M in funding raised to date, Verkada has expanded rapidly with 16 offices across three continents, 1,900+ full-time employees and 25,000+ customers across 70+ countries. Recent Projects For This Role Include Implementing and deploying a binary classifier using TensorFlow for detecting the binary states across hundreds of cameras Detecting unusual object addition/removal in a scene Detecting and counting object and people frequencies Education And Experience Bachelor's Degree in Computer Science, preferably with research experience 2+ years of industry software engineering experience 1+ years of work or research experience with current neural net frameworks Mastery of at least one practical programming language Experience working in a agile team software development environment Requirements Python - writing clean, modular, pythonic code Traditional computer vision algorithms Training deep learning networks using TensorFlow, Keras, Caffe, or similar Data structures and architecture Perks & Benefits Generous company paid medical, dental & vision insurance coverage Unlimited paid time off & 11 companywide paid holidays Wellness allowance Commuter benefits Healthy lunches and dinners provided daily Generous paid parental leave policy & fertility benefits Pay Disclosure At Verkada, we want to attract and retain the best employees, and compensate them in a way that appropriately and fairly values their individual contribution to the company. With that in mind, we carefully consider a number of factors to determine the appropriate starting pay for an employee, including their primary work location and an assessment of a candidate’s skills and experience, as well as market demands and internal parity. This estimate can vary based on the factors described above, so the actual starting annual base salary may be above or below this range. This estimate is also just one component of Verkada’s total rewards package. A Verkada employee may be eligible for additional forms of compensation, depending on their role, including sales incentives, discretionary bonuses, and/or equity in the company in the form of Restricted Stock Units (RSUs). Estimated Annual Pay Range $130,000—$280,000 USD Verkada Is An Equal Opportunity Employer As an equal opportunity employer, Verkada is committed to providing employment opportunities to all individuals. All applicants for positions at Verkada will be treated without regard to race, color, ethnicity, religion, sex, gender, gender identity and expression, sexual orientation, national origin, disability, age, marital status, veteran status, pregnancy, or any other basis prohibited by applicable law. Your application will be handled in accordance with our Candidate Privacy Policy.",
        "url": "https://www.linkedin.com/jobs/view/3765974062",
        "summary": "Verkada, a leading cloud-based B2B physical security platform company, is looking for a software engineer with experience in computer vision and deep learning to develop and deploy AI-powered security solutions. The role involves implementing binary classifiers, object detection, and counting algorithms using TensorFlow and other frameworks. Verkada offers a competitive salary range of $130,000 - $280,000, along with comprehensive benefits such as unlimited PTO, health insurance, and wellness allowance.",
        "industries": [
            "Physical Security",
            "Cloud Computing",
            "Software",
            "AI",
            "Computer Vision"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Time Management",
            "Teamwork"
        ],
        "hard_skills": [
            "Python",
            "TensorFlow",
            "Keras",
            "Caffe",
            "Computer Vision",
            "Deep Learning",
            "Binary Classification",
            "Object Detection",
            "Object Counting",
            "Data Structures",
            "Software Engineering",
            "Agile Development"
        ],
        "tech_stack": [
            "TensorFlow",
            "Keras",
            "Caffe",
            "Python"
        ],
        "programming_languages": [
            "Python"
        ],
        "experience": 2,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science"
            ]
        },
        "salary": {
            "max": 280000,
            "min": 130000
        },
        "benefits": [
            "Medical Insurance",
            "Dental Insurance",
            "Vision Insurance",
            "Unlimited PTO",
            "Paid Holidays",
            "Wellness Allowance",
            "Commuter Benefits",
            "Lunch & Dinner",
            "Paid Parental Leave",
            "Fertility Benefits"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3938345162,
        "company": "Plaid",
        "title": "Software Engineer - Credit",
        "created_on": 1720635755.6340384,
        "description": "We believe that the way people interact with their finances will drastically improve in the next few years. We’re dedicated to empowering this transformation by building the tools and experiences that thousands of developers use to create their own products. Plaid powers the tools millions of people rely on to live a healthier financial life. We work with thousands of companies like Venmo, SoFi, several of the Fortune 500, and many of the largest banks to make it easy for people to connect their financial accounts to the apps and services they want to use. Plaid’s network covers 12,000 financial institutions across the US, Canada, UK and Europe. Founded in 2013, the company is headquartered in San Francisco with offices in New York, Washington D.C., London and Amsterdam. [Current] We believe the next evolution of Credit products at Plaid is to drive deeper into insights and make our products easier to use. By investing in insights, we can help our customers better evaluate raw data to provide a more complete and equitable view of an end user’s financial health. The Assets Team is responsible for building on top of the Credit organization’s technical foundations to (1) ship insights driven products which address needs in the lending lifecycle and (2) reduce the time it takes for customers to get value from Plaid suite of Credit products. This team works closely with product and legal to think through product requirements and compliance constraints, translating those inputs into insights driven products. [Future] Our north star is to have the Plaid Credit suite become the default analytics suite for risk assessment in the credit ecosystem. To achieve this, there are a number of opportunities on our mid-term horizon to expand the ways clients leverage Plaid Credit products. These projects include execution on the next evolution of an insights driven asset report product, architecting monitoring solutions to capture more of the lender lifecycle, and building out partnerships to better distribute Plaid credit products. This team also plans to capture this value in a credit dashboard to make our Credit suite of products easier to integrate with and use - delivering more value, faster. You will be responsible for working full-stack to plan, design, and build the next generation of Credit products. You’ll work closely with cross-functional partners from Product, Compliance, and Design to deliver new solutions for our Clients. You’ll have opportunities to learn new technologies and languages, and to grow as a project owner as we explore ambiguous problem spaces. Responsibilities You will work closely with a highly iterative, cross-functional team to understand product requirements and legal constraints to build next-generation Credit products. You will deeply understand client needs and architect solutions tailored to their use cases. You will partner with other teams within the Credit group, working closely with other engineering teams as well as Design, Data Science, and more to iterate on and productionize new products. You will have the opportunity to architect efficient, scalable services paired with well-designed UI as we work closely with our clients. You will own and lead projects, step up as a trusted member of the team’s oncall rotation, advocate for technical and process changes, and follow best practices to increase overall system reliability. Qualifications Excellent coding and testing skills Experience working in large scale backend systems Experience as a project lead, or desire to quickly take on that responsibility [Very-nice-to-have] strong communication skills [Nice-to-have] Experience with Python, Golang, SQL, and/or AWS $143,640 - $197,640 a year Target base salary for this role is between $143,640 and $197,640 per year. Additional compensation in the form(s) of equity and/or commission are dependent on the position offered. Plaid provides a comprehensive benefit plan, including medical, dental, vision, and 401(k). Pay is based on factors such as (but not limited to) scope and responsibilities of the position, candidate's work experience and skillset, and location. Pay and benefits are subject to change at any time, consistent with the terms of any applicable compensation or benefit plans. Our mission at Plaid is to unlock financial freedom for everyone. To support that mission, we seek to build a diverse team of driven individuals who care deeply about making the financial ecosystem more equitable. We recognize that strong qualifications can come from both prior work experiences and lived experiences. We encourage you to apply to a role even if your experience doesn't fully match the job description. We are always looking for team members that will bring something unique to Plaid! Plaid is proud to be an equal opportunity employer and values diversity at our company. We do not discriminate based on race, color, national origin, ethnicity, religion or religious belief, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, military or veteran status, disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state, and local laws. Plaid is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance with your application or interviews due to a disability, please let us know at accommodations@plaid.com. Please review our Candidate Privacy Notice here .",
        "url": "https://www.linkedin.com/jobs/view/3938345162",
        "summary": "Plaid is looking for a full-stack engineer to build next-generation credit products. The role involves understanding client needs, architecting solutions, collaborating with cross-functional teams, and owning projects. The engineer will work on building insights-driven products that make it easier for clients to evaluate financial data and make more informed decisions. This role requires strong coding and testing skills, experience with large-scale backend systems, and a desire to take on project leadership responsibilities.",
        "industries": [
            "FinTech",
            "Software Development",
            "Financial Services",
            "Technology"
        ],
        "soft_skills": [
            "Communication",
            "Problem Solving",
            "Collaboration",
            "Project Management",
            "Leadership",
            "Teamwork",
            "Adaptability"
        ],
        "hard_skills": [
            "Python",
            "Golang",
            "SQL",
            "AWS",
            "Backend Systems",
            "Code Testing",
            "Project Management",
            "UI Design"
        ],
        "tech_stack": [
            "Python",
            "Golang",
            "SQL",
            "AWS"
        ],
        "programming_languages": [
            "Python",
            "Golang",
            "SQL"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 197640,
            "min": 143640
        },
        "benefits": [
            "Medical",
            "Dental",
            "Vision",
            "401(k)"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Mountain View, CA",
        "job_id": 3971506099,
        "company": "Microsoft",
        "title": "Software Engineer",
        "created_on": 1720635757.446589,
        "description": "The Azure Networking team is looking for a Software Engineer who is interested in building the world’s most reliable cloud. We are redefining the nature of software in the network by writing the software that runs on the switches and building massive distributed systems to control Microsoft’s Cloud Datacenters. If you are interested in working in a start-up like environment, passionate about cloud computing technology, and building the next billion-dollar business, then look no further than the Azure Networking Team. Azure provides developers with on-demand compute and storage to create, host, scale, and manage scalable web application on the Internet through Microsoft’s global data centers, and via on-premises private cloud deployments of the Azure Platform. Microsoft’s mission is to empower every person and every organization on the planet to achieve more. As employees we come together with a growth mindset, innovate to empower others and collaborate to realize our shared goals. Each day we build on our values of respect, integrity, and accountability to create a culture of inclusion where everyone can thrive at work and beyond. Responsibilities Design and develop features and solutions. Fix software bugs and refactor code as needed. Develop and automate software unit and system testing for all code to ensure quality. Provide support and resolve issues for the Azure production network. Embody our culture and values. Qualifications Required Qualifications: Bachelor's Degree in Computer Science, or related technical discipline with experience coding in languages including, but not limited to, C, C++, C#, or Python OR equivalent experience. Other Requirements Ability to meet Microsoft, customer and/or government security screening requirements are required for this role. These requirements include, but are not limited to the following specialized security screenings: Microsoft Cloud Background Check: This position will be required to pass the Microsoft Cloud Background Check upon hire/transfer and every two years thereafter. Preferred Qualifications Knowledge of IP routing related protocols Knowledge of Linux kernel networking Knowledge of hardware architecture or device drivers Knowledge of data modelling or network management protocols Software Engineering IC2 - The typical base pay range for this role across the U.S. is USD $76,400 - $151,800 per year. There is a different range applicable to specific work locations, within the San Francisco Bay area and New York City metropolitan area, and the base pay range for this role in those locations is USD $100,300 - $165,400 per year. Certain roles may be eligible for benefits and other compensation. Find additional benefits and pay information here: https://careers.microsoft.com/us/en/us-corporate-pay Microsoft will accept applications and processes offers for these roles on an ongoing basis. #azurecorejobs Microsoft is an equal opportunity employer. Consistent with applicable law, all qualified applicants will receive consideration for employment without regard to age, ancestry, citizenship, color, family or medical care leave, gender identity or expression, genetic information, immigration status, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran or military status, race, ethnicity, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable local laws, regulations and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application process, read more about requesting accommodations.",
        "url": "https://www.linkedin.com/jobs/view/3971506099",
        "summary": "Software Engineer role within Azure Networking team at Microsoft, focused on building reliable cloud infrastructure. Responsibilities include designing, developing, and testing software for network switches, building distributed systems, and providing production support. The role emphasizes a start-up like environment and passion for cloud computing. ",
        "industries": [
            "Technology",
            "Cloud Computing",
            "Software Development",
            "Networking",
            "Telecommunications",
            "Data Centers"
        ],
        "soft_skills": [
            "Problem-solving",
            "Communication",
            "Teamwork",
            "Collaboration",
            "Passion for Cloud Computing",
            "Growth Mindset",
            "Innovation",
            "Integrity",
            "Accountability"
        ],
        "hard_skills": [
            "C",
            "C++",
            "C#",
            "Python",
            "IP Routing Protocols",
            "Linux Kernel Networking",
            "Hardware Architecture",
            "Device Drivers",
            "Data Modeling",
            "Network Management Protocols"
        ],
        "tech_stack": [
            "Azure",
            "Cloud Computing",
            "Distributed Systems",
            "Network Switches",
            "Linux Kernel",
            "IP Routing Protocols",
            "Data Modeling",
            "Network Management Protocols"
        ],
        "programming_languages": [
            "C",
            "C++",
            "C#",
            "Python"
        ],
        "experience": 0,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Related Technical Discipline"
            ]
        },
        "salary": {
            "max": 165400,
            "min": 76400
        },
        "benefits": [
            "Microsoft Cloud Background Check"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Mateo, CA",
        "job_id": 3765968592,
        "company": "Verkada",
        "title": "Senior Software Engineer - Data Platform",
        "created_on": 1720635764.9416018,
        "description": "Who We Are Verkada is the largest cloud-based B2B physical security platform company in the world. Only Verkada offers six product lines — video security cameras, access control, environmental sensors, alarms, workplace and intercoms — integrated with a single cloud-based software platform. Designed with simplicity and scalability in mind, Verkada gives organizations the real-time insight to know what could impact the safety and comfort of people throughout their physical environment, while empowering them to take immediate action to minimize security risks, workplace frustrations and costly inefficiencies. Founded in 2016 with more than $460M in funding raised to date, Verkada has expanded rapidly with 16 offices across three continents, 1,900+ full-time employees and 25,000+ customers across 70+ countries. Overview As a member of our Infrastructure team, you will build data infrastructure, data pipeline and services to allow the teams to make data driven decisions, and ultimately put quality features into the hands of users. We emphasize tools over processes, automation over operation, metrics over anecdotes, iteration over perfection, and proactive over responsive. Responsibilities Design and implement reliable, scalable and efficient data infrastructure to allow the team to gain insights and make quality data driven decisions Work cross-functionally to build data products that allows other teams to gain insights and ultimately improve Verkada products and Verkada customer experience Design and implement solutions to ensure data quality, security, and compliance Lead whole projects from design to implementation Help lead and grow junior engineers Requirements 5+ years of data engineering related development experience Flexible and adaptive in a fast-paced startup environment Passion for product quality and building systems to ship high quality releases In-depth expertise in distributed systems and data processing Experience with data infrastructure and data analytics Nice To Have Experience with Kubernetes Experience in data visualization and dashboard design Perks & Benefits Generous company paid medical, dental & vision insurance coverage Unlimited paid time off & 11 companywide paid holidays Wellness allowance Commuter benefits Healthy lunches and dinners provided daily Generous paid parental leave policy & fertility benefits Pay Disclosure At Verkada, we want to attract and retain the best employees, and compensate them in a way that appropriately and fairly values their individual contribution to the company. With that in mind, we carefully consider a number of factors to determine the appropriate starting pay for an employee, including their primary work location and an assessment of a candidate’s skills and experience, as well as market demands and internal parity. This estimate can vary based on the factors described above, so the actual starting annual base salary may be above or below this range. This estimate is also just one component of Verkada’s total rewards package. A Verkada employee may be eligible for additional forms of compensation, depending on their role, including sales incentives, discretionary bonuses, and/or equity in the company in the form of Restricted Stock Units (RSUs). Estimated Annual Pay Range $130,000—$280,000 USD Verkada Is An Equal Opportunity Employer As an equal opportunity employer, Verkada is committed to providing employment opportunities to all individuals. All applicants for positions at Verkada will be treated without regard to race, color, ethnicity, religion, sex, gender, gender identity and expression, sexual orientation, national origin, disability, age, marital status, veteran status, pregnancy, or any other basis prohibited by applicable law. Your application will be handled in accordance with our Candidate Privacy Policy.",
        "url": "https://www.linkedin.com/jobs/view/3765968592",
        "summary": "Verkada, a leading cloud-based physical security platform company, seeks a Data Engineer to build and maintain data infrastructure, pipelines, and services. The ideal candidate will have 5+ years of experience in data engineering, expertise in distributed systems and data processing, and a passion for building high-quality systems. Responsibilities include designing and implementing data infrastructure, working cross-functionally to build data products, ensuring data quality, security, and compliance, leading projects, and mentoring junior engineers. Preferred skills include experience with Kubernetes and data visualization.",
        "industries": [
            "Security",
            "Software",
            "Technology",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Flexible",
            "Adaptive",
            "Passion for Product Quality",
            "Cross-Functional Collaboration",
            "Leadership",
            "Mentorship"
        ],
        "hard_skills": [
            "Data Engineering",
            "Distributed Systems",
            "Data Processing",
            "Data Infrastructure",
            "Data Analytics",
            "Kubernetes",
            "Data Visualization",
            "Dashboard Design"
        ],
        "tech_stack": [
            "Kubernetes"
        ],
        "programming_languages": [],
        "experience": 5,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 280000,
            "min": 130000
        },
        "benefits": [
            "Medical Insurance",
            "Dental Insurance",
            "Vision Insurance",
            "Unlimited Paid Time Off",
            "Company Paid Holidays",
            "Wellness Allowance",
            "Commuter Benefits",
            "Lunch and Dinner",
            "Parental Leave",
            "Fertility Benefits"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "California, United States",
        "job_id": 3732276824,
        "company": "InterEx Group",
        "title": "Data QA Engineer",
        "created_on": 1720635766.5217226,
        "description": "A key client of mine is seeking a highly skilled and experienced Data QA Engineer to join their team. In this role, you will be responsible for performing data quality analysis and validation, writing test plans, test cases, and test scripts, and validating solutions built on REST APIs, Snowflake, and data pipelines. The successful candidate will be able to demonstrate a deep conceptual understanding of data analytics architectural approaches and data models (Data Vault experience is a plus). This role is Hybrid in either one of three locations Miami, Florida, Dallas, Texas, or Irvine, California Key Qualifications: - At least 4 years of experience as a Data Quality Engineer - At least 4 years of experience performing data quality analysis and validation - At least 2 years of experience writing test plans, test cases, and test scripts - At least 1 year of experience validating solutions with Snowflake and Azure - At least 1 year of experience validating data pipelines - Strong adherence to core software testing principles, including code modularization, versioning, git, testing, Agile, etc. Nice-to-haves: - Experience performing validation on: Prefect/workflow tool Dbt Hasura & GraphQL",
        "url": "https://www.linkedin.com/jobs/view/3732276824",
        "summary": "Data QA Engineer responsible for data quality analysis, validation, test plan creation, and validation of solutions built on REST APIs, Snowflake, and data pipelines. Requires experience with data analytics architectural approaches, data models, and strong software testing principles.",
        "industries": [
            "Technology",
            "Data Analytics",
            "Software Development"
        ],
        "soft_skills": [
            "Analytical",
            "Problem-solving",
            "Detail-oriented",
            "Communication",
            "Teamwork",
            "Agile"
        ],
        "hard_skills": [
            "Data Quality Analysis",
            "Data Validation",
            "Test Plan Writing",
            "Test Case Writing",
            "Test Scripting",
            "REST API Validation",
            "Snowflake",
            "Data Pipelines",
            "Azure",
            "Data Vault",
            "Prefect",
            "Dbt",
            "Hasura",
            "GraphQL",
            "Git",
            "Agile"
        ],
        "tech_stack": [
            "Snowflake",
            "Azure",
            "REST APIs",
            "Prefect",
            "Dbt",
            "Hasura",
            "GraphQL",
            "Git"
        ],
        "programming_languages": [],
        "experience": 4,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Los Angeles, CA",
        "job_id": 3835253240,
        "company": "CyberCoders",
        "title": "Data Engineer",
        "created_on": 1720635769.7995722,
        "description": "Position Overview: We are looking for a Data Engineer to join our team. The successful candidate will be responsible for providing data-driven insights and analysis to support business operations. You will be responsible for developing and maintaining BI dashboards, developing data models, and providing data-driven insights and recommendations to stakeholders. Responsibilities Develop and maintain data models and data profiles to support BI dashboards. Develop, maintain, and update BI dashboards in Tableau, Power BI, and SAP Business Objects. Extract and analyze data from various data sources. Perform ad hoc analysis and data exploration. Conduct deep-dive analysis to identify trends and patterns in data. Create reports and data visualizations to help stakeholders make informed decisions. Collaborate with stakeholders to identify and document data requirements. Provide data-driven insights and recommendations to stakeholders. Qualifications Bachelor’s degree in Computer Science, Data Science, or a related field. Experience working with SQL. Knowledge of Tableau, Power BI, and SAP Business Objects. Experience with data modeling and data profiling. Proficiency in creating and maintaining dashboards. Strong knowledge of business intelligence, analytics, and reporting. Experience in ad hoc analysis and data visualization. Excellent problem-solving and analytical skills. Strong communication and collaboration skills. Experience with SAS and enterprise analysis is a plus. Benefits Healthcare, dental, and vision Well-being programs (mental health services, mindfulness, well-being events, and more) Generous 401k plan 10 paid holidays per year and 23 PTO days for your first year Family planning and fertility resources Professional development and career support Hybrid; 6 days in office per month Colorado employees will receive paid sick leave. For additional information about available benefits, please contact undefined undefined Colorado employees will receive paid sick leave. For additional information about available benefits, please contact Nelle Stang Email Your Resume In Word To Looking forward to receiving your resume through our website and going over the position with you. Clicking apply is the best way to apply, but you may also: nelle.stang@cybercoders.com Please do NOT change the email subject line in any way. You must keep the JobID: linkedin : NS8-1788168 -- in the email subject line for your application to be considered.*** Nelle Stang - Executive Recruiter Applicants must be authorized to work in the U.S. CyberCoders is proud to be an Equal Opportunity Employer All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, sexual orientation, gender identity or expression, national origin, ancestry, citizenship, genetic information, registered domestic partner status, marital status, status as a crime victim, disability, protected veteran status, or any other characteristic protected by law. CyberCoders will consider qualified applicants with criminal histories in a manner consistent with the requirements of applicable law. CyberCoders is committed to working with and providing reasonable accommodation to individuals with physical and mental disabilities. If you need special assistance or an accommodation while seeking employment, please contact a member of our Human Resources team to make arrangements. CyberCoders will consider for Employment in the City of Los Angeles qualified Applicants with Criminal Histories in a manner consistent with the requirements of the Los Angeles Fair Chance Initiative for Hiring (Ban the Box) Ordinance.",
        "url": "https://www.linkedin.com/jobs/view/3835253240",
        "summary": "Data Engineer needed to develop and maintain BI dashboards, data models, and provide data-driven insights to support business operations. Requires SQL, Tableau, Power BI, and SAP Business Objects experience.",
        "industries": [
            "Data Analysis",
            "Business Intelligence",
            "Technology"
        ],
        "soft_skills": [
            "Problem-solving",
            "Analytical",
            "Communication",
            "Collaboration"
        ],
        "hard_skills": [
            "SQL",
            "Tableau",
            "Power BI",
            "SAP Business Objects",
            "Data Modeling",
            "Data Profiling",
            "Data Visualization",
            "Ad Hoc Analysis",
            "SAS",
            "Enterprise Analysis"
        ],
        "tech_stack": [
            "SQL",
            "Tableau",
            "Power BI",
            "SAP Business Objects",
            "SAS"
        ],
        "programming_languages": [
            "SQL"
        ],
        "experience": 0,
        "education": {
            "min_degree": "Bachelor’s degree",
            "fields": [
                "Computer Science",
                "Data Science"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Healthcare",
            "Dental",
            "Vision",
            "Well-being Programs",
            "401k",
            "Paid Holidays",
            "PTO",
            "Family Planning",
            "Professional Development",
            "Hybrid Work"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Sunnyvale, CA",
        "job_id": 3842166041,
        "company": "Teknowiz",
        "title": "Software Engineer",
        "created_on": 1720635771.5457046,
        "description": "We have an urgent opportunity for the open position with one of our direct clients for Onsite Sunnyvale, CA. Please let us know if you are interested for this role. Software Engineer Server Side Need Junior profile 3-4 Years exp candidate - Moving fast Location: Day 1 Onsite Sunnyvale, CA (Need only be local to Sunnyvale, CA, Relocation won't work) Long term contract Work Authorization: OPT, CPT, H1B, H4EAD, TN, GC, GC EAD can work on C2C. Major Key points Prior Experience with Apple Highly preferred but not mandatory* Candidate must have genuine LinkedIn account* A server-side Engineer with Strong expertise in AWS, Kubernetes, DevOps Experience, Java, Python Seeking for someone with can do attitude, knows his stuff and willing to work with others. Seeking a fast learner who can act on issues quickly. Involve in requirement gathering and automation of end-to-end application functionality to ensure quality consistency, usability, reliability, and maintainability. Groom user stores with clients/product owners and business team, define the main functionality of the software and capture high level requirements and document user stories in JIRA tool. Engage in developing Thread Safe blocks for multithread access and distributed transaction management. As multiple tasks are handled over the networks, computer security concepts such as Encryption/Decryption needs to be implemented to maintain the security of the application. Heavily used Spring Inheritance, Auto-wiring, Core Container, Security, AOP, ORM modules as part of migration from EJB to spring, Spring Quartz for scheduling tasks to generate reports and emails to clients. Used Micro services architecture with Spring Boot based services interacting through a combination of REST and Apache Kafka message brokers. Used Amazon Web Services (AWS) like EC2, S3, cloud watch and Elastic Bean Stalk for code deployment. Upgraded Spring Rest Controllers and Services classes to support migration to Spring framework. Perform work in the context of a large, complex, enterprise applications interfacing through SOAP Web Service, RESTful webservices exchanging data in XML and JSON data formats. Creating technical documents with methods and procedures used in solution development and testing by understanding various source systems. Skill Sets Niche Skill Experience Preference Kubernetes No 2-5 years Is Required Amazon Web Services (AWS) No 2-5 years Is Required DevOps/SRE No At least 1 year Is Required Core Java No 5-10 years Is Required Advanced Java No 5-10 years Is Required microservices No At least 1 year Is Required Apache SOLR No At least 1 year Is Required Cassandra No At least 1 year Is Required Elastic Search No At least 1 year Is Required Apache Kafka No At least 1 year Is Required",
        "url": "https://www.linkedin.com/jobs/view/3842166041",
        "summary": "This is a long-term contract opportunity for a Software Engineer Server Side with 3-4 years of experience. The role requires expertise in AWS, Kubernetes, DevOps, Java, and Python. Strong experience with Apple is preferred but not mandatory. The candidate must have a genuine LinkedIn account and be located in Sunnyvale, CA.",
        "industries": [
            "Software Development",
            "Information Technology",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Fast Learner",
            "Can-do Attitude",
            "Teamwork",
            "Communication",
            "Requirement Gathering",
            "Problem-solving",
            "Automation",
            "Documentation",
            "Communication"
        ],
        "hard_skills": [
            "Java",
            "Python",
            "AWS",
            "Kubernetes",
            "DevOps",
            "Spring",
            "Spring Boot",
            "Microservices",
            "REST",
            "Apache Kafka",
            "Amazon EC2",
            "Amazon S3",
            "CloudWatch",
            "Elastic Beanstalk",
            "SOAP",
            "RESTful Web Services",
            "XML",
            "JSON",
            "Apache SOLR",
            "Cassandra",
            "ElasticSearch",
            "Spring Inheritance",
            "Auto-wiring",
            "Core Container",
            "Security",
            "AOP",
            "ORM",
            "Spring Quartz",
            "Thread Safe Blocks",
            "Distributed Transaction Management",
            "Encryption",
            "Decryption"
        ],
        "tech_stack": [
            "AWS",
            "Kubernetes",
            "DevOps",
            "Java",
            "Python",
            "Spring",
            "Spring Boot",
            "Microservices",
            "REST",
            "Apache Kafka",
            "Amazon EC2",
            "Amazon S3",
            "CloudWatch",
            "Elastic Beanstalk",
            "SOAP",
            "RESTful Web Services",
            "XML",
            "JSON",
            "Apache SOLR",
            "Cassandra",
            "ElasticSearch"
        ],
        "programming_languages": [
            "Java",
            "Python"
        ],
        "experience": 3,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Mateo, CA",
        "job_id": 3954865212,
        "company": "Firework",
        "title": "Frontend Software Engineer",
        "created_on": 1720635773.1790285,
        "description": "Firework is the world’s leading unified video commerce platform that empowers its global partners to personalize the customer experience and engagement at scale. Firework bridges the offline and online for a robust omnichannel immersive brand experience cultivating a deeper emotional human connection between our partners and their end consumers. We are customer-centric and inspired to win together offering total solutions with endless possibilities to help our customers increase purchases and conversions using the power of video. At the heart, we are a global and diverse team of “SuperSpark” creators, entrepreneurs, life-long learners, and data geeks driven by the future of authenticity to transform commerce. Firework has raised over $235M to date, with its latest Series B round led by SoftBank Vision Fund 2. Come reimagine the online customer experience with us. Summary Our engineering team is growing! We’re looking for a talented Frontend Engineer to join our global team who will bring technical expertise and leadership along with hands-on development of backend software development and SaaS solutions. What You'll Be Doing Work closely with designers and other developers to create modern web applications Responsible for web application development, enhancement and maintenance Keep up-to-date on the latest front end technology and trends Work as part of a team to develop high quality products We'll be excited if you have A passionate story-teller, a life-hacker, and a love of programming Willing to take on a challenge with agility and working with talented teammates in a squad team environment Degree in Computing, Computer Science, Information Technology or related disciplines 3+ years experience with front end web application development Familiar with modern web technologies, including but not limited to, Typescript, ReactJS, RESTFul API, Websocket, etc. Proven experience with AJAX, XML, JSON, and best practice design patterns Familiarity with UX principles and site performance optimization Strong grasp of computer science fundamentals, such as data structures and algorithms Working experience with automated testing and version control systems like Git Team-oriented mindset and excellent communication skills; ability to understand different perspectives and contribute to meaningful technical discussions Experience with commerce and CMS platform integration as a big plus As a hybrid office-centric company, we are looking for candidates in the Bay Area. Candidates outside the location are encouraged to apply though must be willing to relocate. The following represents the expected range of compensation for this role: The estimated pay range for this role is $130,000-$150,000. Other factors that impact compensation include stock options. The posted pay range represents the anticipated low and high end of the compensation for this position and is subject to change based on business need. To determine a successful candidate’s starting pay, we carefully consider a variety of factors, including primary work location, an evaluation of the candidate’s skills and experience, market demands, and internal parity. Candidates may receive more information from the talent partner. Don’t hold back We understand some candidates may see the above and not apply because they don’t meet all the qualifications. We encourage you to apply anyway; we often find talented candidates that fit many other opportunities we have and look for potential too, not just what you did in the past. As an equal employment opportunity employer, we are a diverse team that strives for an inclusive environment for all. We prohibit discrimination and harassment of any kind based on race, color, sex, religion, sexual orientation, national origin, age, disability, genetic information, pregnancy, or any other protected characteristic as outlined by federal, state, or local laws. By submitting your application, you acknowledge that you have read and understood Firework's Applicant Privacy Policy located at : https://firework.com/legal/applicantpolicy/.",
        "url": "https://www.linkedin.com/jobs/view/3954865212",
        "summary": "Firework, a leading video commerce platform, seeks a Frontend Engineer to build and maintain their web applications. The ideal candidate has 3+ years of experience with modern web technologies like ReactJS, Typescript, and RESTful APIs, and is comfortable with UX, optimization, and testing. This role offers competitive compensation and the opportunity to join a growing team in the Bay Area.",
        "industries": [
            "E-commerce",
            "Technology",
            "Software",
            "Video"
        ],
        "soft_skills": [
            "Communication",
            "Teamwork",
            "Problem-solving",
            "Agility",
            "Adaptability",
            "Leadership"
        ],
        "hard_skills": [
            "ReactJS",
            "Typescript",
            "RESTful API",
            "Websocket",
            "AJAX",
            "XML",
            "JSON",
            "UX",
            "Performance Optimization",
            "Data Structures",
            "Algorithms",
            "Git",
            "Automated Testing"
        ],
        "tech_stack": [
            "ReactJS",
            "Typescript",
            "RESTful API",
            "Websocket",
            "AJAX",
            "XML",
            "JSON",
            "Git"
        ],
        "programming_languages": [
            "Typescript",
            "JavaScript"
        ],
        "experience": 3,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Computing",
                "Computer Science",
                "Information Technology"
            ]
        },
        "salary": {
            "max": 150000,
            "min": 130000
        },
        "benefits": [
            "Stock Options"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Calabasas, CA",
        "job_id": 3970102777,
        "company": "Jobot",
        "title": "Senior Data Engineer",
        "created_on": 1720635775.008597,
        "description": "Want to learn more about this role and Jobot? Click our Jobot logo and follow our LinkedIn page! Job details Senior Data Engineer with profitable Financial Services firm This Jobot Job is hosted by Oliver Belkin Are you a fit? Easy Apply now by clicking the \"Easy Apply\" button and sending us your resume. Salary $165,000 - $225,000 per year A Bit About Us We are seeking a highly skilled and experienced Senior Data Engineer to join our dynamic team in the technology industry. The ideal candidate will have a strong background in AWS, SQL, Airflow, Python, PySpark, and Redshift, or equivalent technologies. As a Senior Data Engineer, you will be responsible for designing, building, and maintaining our data infrastructure to support our growing business needs. This is a full-time position that requires a minimum of 5 years of experience in the field. ***This role is onsite in Los Angeles. Only local applicants or those that are willing to relocate. Relocation assistance will be provided*** Why join us? We offer top compensation and benefits as we are a leader within our industry. You'll have the ability to work with a group of other like-minded individuals to build great software. Job Details Responsibilities Design, build, and maintain our data infrastructure using AWS, SQL, Airflow, Python, PySpark, and Redshift. Develop and implement data pipelines to extract, transform, and load data from various sources. Collaborate with cross-functional teams to understand business requirements and translate them into technical solutions. Optimize data storage and retrieval for performance and scalability. Monitor and troubleshoot data pipelines to ensure data accuracy and availability. Develop and maintain documentation of data infrastructure and processes. Stay up-to-date with the latest technologies and trends in data engineering. Qualifications Bachelor's or Master's degree in Computer Science, Information Technology, or related field. Minimum of 5 years of experience in data engineering or related field. Strong proficiency in AWS, SQL, Airflow, Python, PySpark, and Redshift, or equivalent technologies. Experience with data modeling, data warehousing, and ETL processes. Strong problem-solving skills and attention to detail. Excellent communication and collaboration skills. Ability to work independently and as part of a team. Experience with Agile development methodologies is a plus. If you are passionate about data engineering and have a proven track record of designing and implementing scalable data infrastructure, we encourage you to apply for this exciting opportunity. We offer a competitive salary, comprehensive benefits package, and a dynamic work environment where you can grow and thrive. Interested in hearing more? Easy Apply now by clicking the \"Easy Apply\" button. Want to learn more about this role and Jobot? Click our Jobot logo and follow our LinkedIn page!",
        "url": "https://www.linkedin.com/jobs/view/3970102777",
        "summary": "We are seeking a Senior Data Engineer with 5+ years of experience in data engineering or a related field.  This role will focus on designing, building, and maintaining data infrastructure using AWS, SQL, Airflow, Python, PySpark, and Redshift.  You will also develop and implement data pipelines to extract, transform, and load data from various sources.  This role is onsite in Los Angeles and requires local applicants or those willing to relocate.",
        "industries": [
            "Technology",
            "Financial Services"
        ],
        "soft_skills": [
            "problem-solving",
            "attention to detail",
            "communication",
            "collaboration",
            "work independently"
        ],
        "hard_skills": [
            "AWS",
            "SQL",
            "Airflow",
            "Python",
            "PySpark",
            "Redshift",
            "data modeling",
            "data warehousing",
            "ETL"
        ],
        "tech_stack": [
            "AWS",
            "SQL",
            "Airflow",
            "Python",
            "PySpark",
            "Redshift"
        ],
        "programming_languages": [
            "Python",
            "SQL",
            "PySpark"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Computer Science",
                "Information Technology"
            ]
        },
        "salary": {
            "max": 225000,
            "min": 165000
        },
        "benefits": [
            "top compensation",
            "benefits"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Cupertino, CA",
        "job_id": 3952393591,
        "company": "Webologix Global",
        "title": "Urgent requirement of \"Snowflake Data Engineer\" @Austin, TX & Cupertino, CA & Sunnyvale, CA (Fulltime only)",
        "created_on": 1720635776.5126534,
        "description": "Job Title: Snowflake Data Engineer Locations: Austin, TX/ Cupertino, CA / Sunnyvale, CA Type of hire: Fulltime Job Description 8+ years of relevant experience as a Data Engineer Snowflake (mandatory and preferably certified) - Strong implementation experience with a good understanding of Snowflake Architecture being able to design and implement solutions and having good experience in Snowflake performance optimization techniques. Advanced SQL (mandatory and preferably certified) – Good understanding of the concept of Slowly Changing Dimensions (SCD), be able to write complex queries using Self Joins, Cursors also recursive, Views/Materialized, strong in PL/SQL etc. Strong experience in SQL Performance tuning especially when dealing with large datasets in millions of data. Understanding of Data semantics and data semantic models Python (mandatory) – Good experience in Python Expert in Single store procedure. Experience with Cloud Computing (AWS or Google Cloud) for deployment purposes is nice to have. Thanks & Regards Vijay Singh Bisht Webologix Ltd/Inc (India, USA, UK, Malaysia) ✉ Email: vijay.singh@webologix.net | www.webologics.com Disclaimer The contents of this e-mail and any attachment(s) are confidential and intended for the named recipient(s) only. It shall not attach any liability on the originator or Webologix. Any views or opinions presented in this email are solely those of the author and may not necessarily reflect the opinions of Webologix. Any form of reproduction, dissemination, copying, disclosure, modification, distribution and/or publication of this message without the prior written consent of the author of this e-mail is strictly prohibited. If you have received this email in error please delete it and notify the sender immediately. Before opening any mail.",
        "url": "https://www.linkedin.com/jobs/view/3952393591",
        "summary": "Webologix is looking for a Snowflake Data Engineer with 8+ years of experience. The ideal candidate will have strong Snowflake implementation experience, advanced SQL skills, and proficiency in Python.  Experience with Cloud Computing (AWS or Google Cloud) is a plus. ",
        "industries": [
            "Data Engineering",
            "Cloud Computing",
            "Software Development"
        ],
        "soft_skills": [
            "Communication",
            "Problem Solving",
            "Teamwork",
            "Analytical Thinking",
            "Attention to Detail"
        ],
        "hard_skills": [
            "Snowflake",
            "SQL",
            "Python",
            "Slowly Changing Dimensions (SCD)",
            "Self Joins",
            "Cursors",
            "Recursive Queries",
            "Views",
            "Materialized Views",
            "PL/SQL",
            "SQL Performance Tuning",
            "Data Semantics",
            "Data Semantic Models",
            "Cloud Computing",
            "AWS",
            "Google Cloud"
        ],
        "tech_stack": [
            "Snowflake",
            "SQL",
            "Python",
            "AWS",
            "Google Cloud"
        ],
        "programming_languages": [
            "SQL",
            "Python"
        ],
        "experience": 8,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Concord, CA",
        "job_id": 3941633223,
        "company": "Software Technology Inc.",
        "title": "SENIOR DATABASE ENGINEER (CASSANDRA)",
        "created_on": 1720635778.1193483,
        "description": "Role : SENIOR DATABASE ENGINEER (CASSANDRA) Location : Concord CA Position : Hybrid Your Roles Leads the design planning, impact analysis, administration, implementation and maintenance of the organization's application Cassandra databases. Consults with and advises senior management and multiple clients on high impact data or database management issues, influencing strategic direction. Handles and leads various large-scale or highly complex data/database management activities including one or more of the following: designing highly complex logical and/or physical database data model; large volume data transformation and migration; capacity planning; developing database design policies, procedures and standards; and security requirements identification, analysis and development. Provides mentoring, guidance and general oversight to lesser experienced staff in a variety of database design, performance tuning and/or administration activities. Candidate Will Be Required To Lead or participate in Cassandra database management activities including designing highly complex logical and physical databases Installation and administration of Cassandra databases Documentation of guides for production database administrators Defining standards for installation, deployment, security, authentication and authorization, management policies and best practices Defining and implementing backup and restoration strategies Defining and implementing monitoring and alarming strategies Perform the planning, research, design, implementation, maintenance, and control of server class databases Consult with and advise management and multiple clients on high impact data or database management issues, influencing strategic direction Minimum Qualifications 4+ years of experience with implementing and administrating Cassandra database Experience with Data tax Cassandra Experience with Data tax Ops Centre backup/restoration and monitoring/alarm implementation Experience with Ansible automation tool or equivalent 4+ years of Shell or Python or Perl experience Preferred Skills Proven experience with Mongo dB or other SQL and NoSQL databases a plus Experience with Agile methodology Demonstrated experience with UNIX and Shell Scripting Demonstrated experience in designing for high volume OLTP applications Demonstrated experience in Change Management and SDLC Experience with Version Control System such as Git Experience with Issue and Tracking software such as Jira Thanks & Regards Mohan sai Technical recruiter Phone# +1-619-605-0324|mohan.sai@stiorg.com Software Technology, Inc www.stiorg.com #w2requirements #c2crequirement #c2c #usrecruitment #usrecruiters #commentforbetterreach #comment #requirement #w2requirements #W2 #c2chotlist #C2C #vendorlist #vendorempanelment #shortlisted #hotlist#jolttek #c2crequirements #contractual #primevendors #c2c #c2cvendors #directclient #javadeveloper #javafullstackdeveloper #salesforcedeveloper #sqldba #sapfico #oraclejobs #oracledba #servicenow #LeadServiceNow #uideveloper #bi #reportsdeveloper #bianalyst #bi #upwork #upworksuccess #upworkfreelancer #upwork #thankyou #talent  #freelancers #aws #javafullstackdeveloper #awscloudexperience #migration #migrationservices #spring #springboot #springsecurity #springframework #awsservices #awsglue #javabackenddeveloper #javabackend #remoteopportunity #microservices #urgentopening #urgentrequirement #urgenthiring #newcareeropportunities #w2jobs #w2 #h1bjobs #c2c #c2crequirements #c2cusajobs #angular #UIDeveloper #DevOps #DevOpsengineer #Angularfrontenddeveloper.#fullstackdeveloper #datanalyst #DA #salesforcedevelopers #offerroles #interviewroles #hotlist #job #python #developer #jobs#hotlists #c2chotlist #benchlist #c2c #c2crequirements #c2cjobs #c2cvendors #c2cusajobs#share #recruiters #hotlist #email #comment #recruiters #share  #technicalrecruiters #databaseengineer #database #cassandra #dataengineer #dataadministrater #data",
        "url": "https://www.linkedin.com/jobs/view/3941633223",
        "summary": "Seeking a Senior Database Engineer with expertise in Cassandra to lead design, implementation, and maintenance of databases. Responsibilities include complex database modeling, data migration, capacity planning, security, backup/restore strategies, and monitoring. This role requires strong experience with Cassandra, Datastax, Ansible, and scripting languages like Python or Shell. Experience with other databases (MongoDB, SQL) and Agile methodologies is a plus.",
        "industries": [
            "Technology",
            "Software Development",
            "Database Administration",
            "Data Engineering"
        ],
        "soft_skills": [
            "Leadership",
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Decision Making",
            "Strategic Thinking",
            "Mentoring",
            "Technical Guidance"
        ],
        "hard_skills": [
            "Cassandra",
            "Datastax",
            "Ansible",
            "Python",
            "Shell",
            "Perl",
            "MongoDB",
            "SQL",
            "UNIX",
            "Shell Scripting",
            "OLTP",
            "Change Management",
            "SDLC",
            "Git",
            "Jira"
        ],
        "tech_stack": [
            "Cassandra",
            "Datastax",
            "Ansible",
            "Python",
            "Shell",
            "Perl",
            "MongoDB",
            "Git",
            "Jira"
        ],
        "programming_languages": [
            "Python",
            "Shell",
            "Perl"
        ],
        "experience": 4,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Los Angeles, CA",
        "job_id": 3958440800,
        "company": "Connexity, Inc.",
        "title": "Software Engineer",
        "created_on": 1720635779.776046,
        "description": "Connexity is a performance-marketing technology company whose core purpose is to help online retailers find new customers and drive sales at a cost that meets ROI objectives. As the leading independent source of new customers and sales for online retailers, we manage the complexities of a wide range of customer acquisition channels on behalf of thousands of retailers, with 20+ years of proven success in the US, UK and Europe. Since 2021, Connexity has served as the eCommerce division of Taboola, through which we help thousands of advertisers reach their audiences with compelling product ads in a brand-safe environment. Our software engineers develop next-generation technologies that help retailers reach qualified consumers. Our products need to handle information at a vast scale, from processing billions of retailer offerings per day to handling millions of clicks per day. We're looking for engineers with new ideas, as well as complementary ones. As a software engineer, you will work on a specific product team, but will often interact with other teams to deliver cross cutting efforts. These 90% time cross-team collaborations often yield new products and features during 10% time, when engineers are free to explore new technologies and business ideas. We need our engineers to be curious, communicative, and interested in turning challenges into opportunities. Responsibilities Write code (primarily Java, using Spring) Analyze a problem thoroughly and assist in division of work (Spikes) Participate in and lead design reviews with peers and stakeholders to decide amongst available technologies / approaches (Spike Review / Kickoff) Review code developed by other developers and provide feedback Contribute to existing documentation and adapt materials based on product/program updates Investigate product or system issues and debug/track/resolve by analyzing the sources of issues and their impact on the system's operation and quality Requirements Minimum qualifications Bachelor's degree in computer science or equivalent practical experience 2 years of experience with software development in one or more programming languages (at least 1 year using Java in production environment) 2 years of experience with algorithms and data structures in a results oriented setting 1 year practical experience working with database engines, such as MySQL 1 year experience developing data processing pipelines using distributed data processing platforms, such as Dataflow, Spark, MapReduce, etc Experience with comprehensive testing (e.g. unit, integration, etc.) Experience supporting production systems owned by your product team Preferred Qualifications 2 years of experience with large scale systems data analysis / processing 1 year of experience working with modern, cloud-native approaches (e.g. Kubernetes, microservices, utilizing GCP/AWS managed services, etc.) Benefits Headquartered in Santa Monica, the company operates sites and business services in the US, UK, EMEA, and APAC. We offer top benefits including equity, career growth, flexible time off, casual work environment, competitive comp, and much more! Health Care Plans (Medical, Dental & Vision) Retirement Plan (401k Matching) Life Insurance (Basic, Voluntary & AD&D), LTD & STD Paid Leave Benefits (Maternity, Paternity & Medical) Learning & Development Program (educational tool) Flexible work schedules Free Food & Snacks Wellness Resources Equity We are committed to providing a culture at Connexity that supports the diversity, equity and inclusion of our most valuable asset, our people. We encourage individuality, and are driven to represent a workplace that celebrates our differences, and provides opportunities equally across gender, race, religion, sexual orientation, and all other demographics. Our actions across Education, Recruitment, Retention, and Volunteering reflect our core company values and remind us that we're all in this together to drive positive change in our industry. The pay range for this position is $90,000-$125,000. The pay offered may vary depending on several factors such as job-related knowledge, skills, and experience (Level I, II, III). Compensation packages include a variety of perks such as equity, competitive 401(k) match, rich benefits, etc. This position is a hybrid role, with 1-2 days based in the Santa Monica office. Other Benefits Modern Technologies Engineers at Connexity often work with the latest technologies and tools, which helps them stay updated with industry trends and enhance their technical skills. Diverse Projects Exposure to a variety of projects, including API development, scalable data pipelines, user interfaces, AI and machine learning. , provides engineers with broad experience and the ability to specialize in different areas of interest. Career Growth Opportunities Connexity offers clear career paths with opportunities for advancement. Engineers can progress to senior roles, management positions, or specialize in niche technical fields. Collaborative Culture The company fosters a collaborative work environment where cross-functional teamwork is encouraged, allowing engineers to work closely with product managers, data scientists, and other stakeholders. Flexible Work Arrangements Employees benefit from flexible work hours and remote work options, contributing to a better work-life balance. Innovative Atmosphere Work in a dynamic and innovative atmosphere where new ideas and creative solutions are valued and implemented.",
        "url": "https://www.linkedin.com/jobs/view/3958440800",
        "summary": "Connexity, a performance-marketing technology company focused on helping online retailers acquire new customers and drive sales, is seeking a Software Engineer to develop next-generation technologies. The role involves writing code in Java, using Spring, analyzing problems, participating in design reviews, reviewing code, and contributing to documentation. The ideal candidate will have experience with Java, algorithms and data structures, database engines, distributed data processing platforms, and testing. Preferred qualifications include experience with large-scale systems data analysis, modern cloud-native approaches, and Kubernetes. The company offers competitive compensation and benefits, including equity, a 401(k) match, health insurance, paid leave, and a flexible work environment. The role is based in Santa Monica, California and is a hybrid position with 1-2 days in the office.",
        "industries": [
            "Technology",
            "Software Development",
            "Marketing",
            "Ecommerce",
            "Retail"
        ],
        "soft_skills": [
            "Communicative",
            "Curious",
            "Analytical",
            "Problem Solving",
            "Collaboration",
            "Leadership",
            "Teamwork",
            "Detail Oriented"
        ],
        "hard_skills": [
            "Java",
            "Spring",
            "Algorithms",
            "Data Structures",
            "MySQL",
            "Dataflow",
            "Spark",
            "MapReduce",
            "Unit Testing",
            "Integration Testing",
            "Kubernetes",
            "Microservices",
            "GCP",
            "AWS"
        ],
        "tech_stack": [
            "Java",
            "Spring",
            "MySQL",
            "Dataflow",
            "Spark",
            "MapReduce",
            "Kubernetes",
            "Microservices",
            "GCP",
            "AWS"
        ],
        "programming_languages": [
            "Java"
        ],
        "experience": 2,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Computer Science"
            ]
        },
        "salary": {
            "max": 125000,
            "min": 90000
        },
        "benefits": [
            "Equity",
            "Career Growth",
            "Flexible Time Off",
            "Casual Work Environment",
            "Competitive Compensation",
            "Health Care Plans",
            "Retirement Plan",
            "Life Insurance",
            "Paid Leave Benefits",
            "Learning & Development Program",
            "Flexible work schedules",
            "Free Food & Snacks",
            "Wellness Resources",
            "Modern Technologies",
            "Diverse Projects",
            "Career Growth Opportunities",
            "Collaborative Culture",
            "Flexible Work Arrangements",
            "Innovative Atmosphere"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Sunnyvale, CA",
        "job_id": 3873133264,
        "company": "LinkedIn",
        "title": "Staff Software Engineer - Systems Infrastructure (Performance Engineering Team)",
        "created_on": 1720635781.489643,
        "description": "Company Description LinkedIn is the world’s largest professional network, built to create economic opportunity for every member of the global workforce. Our products help people make powerful connections, discover exciting opportunities, build necessary skills, and gain valuable insights every day. We’re also committed to providing transformational opportunities for our own employees by investing in their growth. We aspire to create a culture that’s built on trust, care, inclusion, and fun – where everyone can succeed. Join us to transform the way the world works. Job Description This role will be based in Sunnyvale, CA. At LinkedIn, we trust each other to do our best work where it works best for us and our teams. This role offers a hybrid work option, meaning you can both work from home and commute to a LinkedIn office, depending on what’s best for you and when it is important for your team to be together. We're seeking a Staff Engineer with a passion for Performance Engineering to join our dynamic team. In this role, you'll lead efforts to optimize software performance, ensuring scalability, reliability, and efficiency across our product suite. With your deep technical expertise, you'll work closely with cross-functional teams to diagnose complex issues and architect high-performance solutions. You'll tackle challenging performance bottlenecks, guiding our teams with your expertise in system design, performance analysis, and the fine-tuning of applications that handle high traffic, making a significant impact on the user experience of our global customer base. Moreover, you'll set new benchmarks for performance and efficiency in an industry that's always advancing. Responsibilities - You will own the technical strategy for broad or complex requirements with insightful and forward-looking approaches that go beyond the direct team and solve large open-ended problems. - You will design, implement, and optimize the performance of large-scale distributed systems with security and compliance in mind. - You will Improve the observability and understandability of various systems with a focus on improving developer productivity and system sustenance - You will effectively communicate with the team, partners and stakeholders. - You will mentor other engineers, define our challenging technical culture, and help to build a fast-growing team - You will work closely with the open-source community to participate and influence cutting edge open-source projects (e.g., Apache Iceberg) - You will deliver incremental impact by driving innovation while iteratively building and shipping software at scale - You will diagnose technical problems, debug in production environments, and automate routine tasks Basic Qualifications - BA/BS Degree in Computer Science or related technical discipline, or related practical experience. - 4+ years of industry experience in software design, development, and algorithm related solutions. - 4+ years experience programming in object-oriented languages such as Java, C++, Python, Go, Rust, C# and/or Functional languages such as Scala or other relevant coding languages - Hands on experience developing distributed systems, large-scale systems, databases and/or Backend APIs Preferred Qualifications - BS and 8+ years of relevant work experience, MS and 7+ years of relevant work experience, or PhD and 4+ years of relevant work experience - Experience in architecting, building, and running large-scale distributed systems - Experience with industry, opensource, and/or academic research in technologies such as Hadoop, Spark, Kubernetes, Feather, GraphQL, GRPC, Apache Kafka, Pinot, Samza or Venice - Experience with open-source project management and governance Suggested Skills - Distributed systems - Backend Systems Infrastructure - Java/Golang/Rust/Python You will Benefit from our Culture We strongly believe in the well-being of our employees and their families. That is why we offer generous health and wellness programs and time away for employees of all levels. LinkedIn is committed to fair and equitable compensation practices. The pay range for this role is $156,000 - $255,000. Actual compensation packages are based on a wide array of factors unique to each candidate, including but not limited to skill set, years & depth of experience, certifications and specific office location. This may differ in other locations due to cost of labor considerations. The total compensation package for this position may also include annual performance bonus, stock, benefits and/or other applicable incentive compensation plans. For additional information, visit: https://careers.linkedin.com/benefits. Equal Opportunity Statement LinkedIn is committed to diversity in its workforce and is proud to be an equal opportunity employer. LinkedIn considers qualified applicants without regard to race, color, religion, creed, gender, national origin, age, disability, veteran status, marital status, pregnancy, sex, gender expression or identity, sexual orientation, citizenship, or any other legally protected class. LinkedIn is an Affirmative Action and Equal Opportunity Employer as described in our equal opportunity statement here: https://microsoft.sharepoint.com/:b:/t/LinkedInGCI/EeE8sk7CTIdFmEp9ONzFOTEBM62TPrWLMHs4J1C_QxVTbg?e=5hfhpE. Please reference https://www.eeoc.gov/sites/default/files/2023-06/22-088_EEOC_KnowYourRights6.12ScreenRdr.pdf and https://www.dol.gov/ofccp/regs/compliance/posters/pdf/OFCCP_EEO_Supplement_Final_JRF_QA_508c.pdf for more information. LinkedIn is committed to offering an inclusive and accessible experience for all job seekers, including individuals with disabilities. Our goal is to foster an inclusive and accessible workplace where everyone has the opportunity to be successful. If you need a reasonable accommodation to search for a job opening, apply for a position, or participate in the interview process, connect with us at accommodations@linkedin.com and describe the specific accommodation requested for a disability-related limitation. Reasonable accommodations are modifications or adjustments to the application or hiring process that would enable you to fully participate in that process. Examples of reasonable accommodations include but are not limited to: -Documents in alternate formats or read aloud to you -Having interviews in an accessible location -Being accompanied by a service dog -Having a sign language interpreter present for the interview A request for an accommodation will be responded to within three business days. However, non-disability related requests, such as following up on an application, will not receive a response. LinkedIn will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by LinkedIn, or (c) consistent with LinkedIn's legal duty to furnish information. Pay Transparency Policy Statement As a federal contractor, LinkedIn follows the Pay Transparency and non-discrimination provisions described at this link: https://lnkd.in/paytransparency. Global Data Privacy Notice for Job Candidates This document provides transparency around the way in which LinkedIn handles personal data of employees and job applicants: https://lnkd.in/GlobalDataPrivacyNotice",
        "url": "https://www.linkedin.com/jobs/view/3873133264",
        "summary": "LinkedIn seeks a Staff Engineer with a passion for Performance Engineering to optimize software performance, ensure scalability, reliability, and efficiency across its product suite. This role involves leading efforts to diagnose complex performance issues, architecting high-performance solutions, and setting new benchmarks for performance and efficiency.",
        "industries": [
            "Technology",
            "Software Development",
            "Internet",
            "Social Media"
        ],
        "soft_skills": [
            "Communication",
            "Leadership",
            "Problem Solving",
            "Collaboration",
            "Mentorship",
            "Technical Strategy",
            "Project Management"
        ],
        "hard_skills": [
            "Java",
            "C++",
            "Python",
            "Go",
            "Rust",
            "C#",
            "Scala",
            "Distributed Systems",
            "Large-Scale Systems",
            "Databases",
            "Backend APIs",
            "Hadoop",
            "Spark",
            "Kubernetes",
            "Feather",
            "GraphQL",
            "GRPC",
            "Apache Kafka",
            "Pinot",
            "Samza",
            "Venice"
        ],
        "tech_stack": [
            "Hadoop",
            "Spark",
            "Kubernetes",
            "Feather",
            "GraphQL",
            "GRPC",
            "Apache Kafka",
            "Pinot",
            "Samza",
            "Venice",
            "Java",
            "C++",
            "Python",
            "Go",
            "Rust",
            "C#",
            "Scala"
        ],
        "programming_languages": [
            "Java",
            "C++",
            "Python",
            "Go",
            "Rust",
            "C#",
            "Scala"
        ],
        "experience": 4,
        "education": {
            "min_degree": "BA/BS",
            "fields": [
                "Computer Science",
                "Related Technical Discipline"
            ]
        },
        "salary": {
            "max": 255000,
            "min": 156000
        },
        "benefits": [
            "Health and Wellness Programs",
            "Time Away",
            "Annual Performance Bonus",
            "Stock",
            "Other Applicable Incentive Compensation Plans"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Sunnyvale, CA",
        "job_id": 3925331808,
        "company": "Amazon",
        "title": "Data Engineer II, Alexa Devices",
        "created_on": 1720635783.1447392,
        "description": "Description The Alexa Echo Device Team is looking for a talented, highly motivated Data Engineer to join our Business Intelligence team. Alexa is the groundbreaking cloud-based intelligent agent that powers Echo and other devices designed around your voice. We provide actionable business insights that inform future products and services that will power the next generation of Echo and Alexa devices. As a Senior Data Engineer, you will work in one of the world's largest and most complex data warehouse environments. You will work closely with Product Management, Software Development, Data Science, and other Data Engineering teams to develop scalable and innovative analytical solutions, process and store terabytes of low latency structured and unstructured data, and enable the Echo Device team to build successful, data driven strategies. You will be responsible for designing and implementing an analytical environment using third-party and in-house tools and using Python, Scala, or Java to automate the ETL, analytics, and data quality platform from the ground up. You will design and implement complex data models, model metadata, build reports and dashboards, and own data presentation and dashboarding tools for the end users of our data products and systems. You will work with leading edge technologies like Redshift, EMR, Hadoop/Hive/Pig, and more. You will write scalable, highly tuned SQL queries running over billions of rows of data and will develop learning and training programs to drive adoption of data driven decision making across the Echo and Alexa organization. You should have deep expertise in the design, creation, management, and business use of large datasets, across a variety of data platforms. You should have excellent business and interpersonal skills to be able to work with business owners to understand data requirements, and to implement efficient and scalable ETL solutions. You should be an authority at crafting, implementing, and operating stable, scalable, low cost solutions to replicate data from production systems into the BI data store. Key job responsibilities Work with the product and development teams within Alexa org to understand the product vision and requirements. Work with Product Managers, BI engineers and Software Engineers to design, implement and support high quality data products. Partner with other teams across Alexa to ingest relevant datasets into the Alexa Devices BI data-warehouse. Collaborate with other data engineers within the team to build a data platform that is self-service and possesses key capabilities like data discovery, data lineage, proactive data monitoring and security/compliance monitoring. Leverage and manage AWS services like Bedrock, Sagemaker, S3, Redshift, Athena, Kinesis, Lambda, Data Lake etc.. Conduct workshops and lunch-n-learn sessions to educate customers on product capabilities. About The Team BASIC QUALIFICATIONS 3+ years of data engineering experience Experience with data modeling, warehousing and building ETL pipelines Experience with SQL Preferred Qualifications Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions Experience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases) Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us. Our compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $118,900/year in our lowest geographic market up to $205,600/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. For more information, please visit https://www.aboutamazon.com/workplace/employee-benefits. This position will remain posted until filled. Applicants should apply via our internal or external career site. Company - Amazon.com Services LLC Job ID: A2642719",
        "url": "https://www.linkedin.com/jobs/view/3925331808",
        "summary": "Amazon is seeking a Senior Data Engineer to join their Business Intelligence team for Alexa Echo Devices. This role involves designing and implementing analytical solutions, processing large datasets, building ETL pipelines, and working with AWS services to enable data-driven decision making for the Echo and Alexa product development.",
        "industries": [
            "Technology",
            "Consumer Electronics",
            "Artificial Intelligence",
            "Cloud Computing",
            "Data & Analytics"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Analytical",
            "Data-Driven",
            "Presentation",
            "Time Management",
            "Teamwork",
            "Interpersonal",
            "Business Acumen"
        ],
        "hard_skills": [
            "Data Engineering",
            "Data Modeling",
            "Data Warehousing",
            "ETL",
            "SQL",
            "Redshift",
            "S3",
            "AWS Glue",
            "EMR",
            "Kinesis",
            "Firehose",
            "Lambda",
            "IAM",
            "Non-relational Databases",
            "Object Storage",
            "Document Stores",
            "Key-Value Stores",
            "Graph Databases",
            "Column-Family Databases",
            "Python",
            "Scala",
            "Java",
            "Bedrock",
            "Sagemaker",
            "Athena",
            "Data Lake",
            "Data Discovery",
            "Data Lineage",
            "Data Monitoring",
            "Security Monitoring"
        ],
        "tech_stack": [
            "AWS",
            "Redshift",
            "S3",
            "AWS Glue",
            "EMR",
            "Kinesis",
            "Firehose",
            "Lambda",
            "IAM",
            "Bedrock",
            "Sagemaker",
            "Athena",
            "Data Lake"
        ],
        "programming_languages": [
            "Python",
            "Scala",
            "Java",
            "SQL"
        ],
        "experience": 3,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 205600,
            "min": 118900
        },
        "benefits": [
            "Medical",
            "Financial",
            "Equity",
            "Sign-on Payments"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco Bay Area",
        "job_id": 3964672346,
        "company": "Acceler8 Talent",
        "title": "Data Infrastructure Engineer",
        "created_on": 1720635787.2979486,
        "description": "Join Us as a Data Infrastructure Engineer Our mission is to deepen the partnership between humans and computers, unlocking collaborative capabilities that far exceed what could be achieved today. We believe that building delightful end-user experiences requires innovating across the stack - from the UX all the way down to models that achieve the best user value per FLOP. We believe that a small, focused team of motivated individuals can create outsized breakthroughs. We are building a world-class multi-disciplinary team who are excited to solve hard real-world AI problems. We are well-capitalized and supported by March Capital and Thrive Capital, with participation from AMD, Franklin Venture Partners, Google, KB Investment, NVIDIA. About the Role: Data Infrastructure Engineer As a Data Infrastructure Engineer, you will design, implement, and optimize a scalable infrastructure to prepare the data that powers our AI training. This infrastructure must be reliable and capable of efficiently processing petabytes of data. You will collaborate closely with the data research team and data crawling team when designing this system. What You Will Be Working On: Building petabyte-scale, high-throughput data processing systems for preparing and curating datasets for AI training. Orchestrating workloads across large clusters; Architecting and maintaining distributed computing environments. Working directly with our data research team on implementing new methods of data preparation. Troubleshooting and resolving infrastructure-related issues in a timely manner. What We Are Looking For: Minimum of 6+ years of experience in data-intensive applications and software development. Proficient with Kubernetes & containerization and with building cloud services using providers like AWS, GCP etc. Ability to write, debug and optimize distributed systems and understanding of data orchestration and automation tools (or strong willingness to learn). Proficient in high performance programming languages like Go or Rust or C++. You have previous experience in creating and maintaining infrastructure for processing datasets for ML model training and/or serving. We encourage you to apply for this position even if you don’t check all of the above requirements but want to spend time pushing on these techniques. We are based in-person in SF. We offer relocation assistance to new employees.",
        "url": "https://www.linkedin.com/jobs/view/3964672346",
        "summary": "Data Infrastructure Engineer responsible for building petabyte-scale data processing systems to prepare and curate datasets for AI training. You will work on orchestrating workloads across large clusters, architecting and maintaining distributed computing environments, and collaborating with the data research team on implementing new methods of data preparation. The role requires strong proficiency in Kubernetes, containerization, cloud services (AWS, GCP), distributed systems, data orchestration/automation tools, and high-performance programming languages like Go, Rust, or C++. Prior experience with infrastructure for processing datasets for ML model training and/or serving is a plus.",
        "industries": [
            "Artificial Intelligence",
            "Machine Learning",
            "Data Science",
            "Software Development",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Problem Solving",
            "Collaboration",
            "Communication",
            "Time Management",
            "Troubleshooting",
            "Teamwork",
            "Willingness to Learn"
        ],
        "hard_skills": [
            "Kubernetes",
            "Containerization",
            "AWS",
            "GCP",
            "Distributed Systems",
            "Data Orchestration",
            "Data Automation",
            "Go",
            "Rust",
            "C++"
        ],
        "tech_stack": [
            "Kubernetes",
            "AWS",
            "GCP",
            "Go",
            "Rust",
            "C++"
        ],
        "programming_languages": [
            "Go",
            "Rust",
            "C++"
        ],
        "experience": 6,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Relocation Assistance"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Mountain View, CA",
        "job_id": 3960091206,
        "company": "Qventus, Inc",
        "title": "Sr. Data Engineer - GenAI Apps",
        "created_on": 1720635789.0538385,
        "description": "About Us: At Qventus, we’re transforming healthcare with real-time decision-making platforms that allow hospitals to focus on what matters most: patient care. Backed by leading venture capital, our innovative solutions harness the power of Machine Learning and Generative AI to empower nurses, doctors, and staff to anticipate and resolve issues before they arise. Join us in making healthcare smarter and more efficient for everyone. About the Role: As a Senior Data Engineer at Qventus, you will play a pivotal role in developing and driving new data solutions. You will design, develop, and manage data pipelines that ensure the scalability, reliability, and efficiency of our data platform. Collaborate with cross-functional teams to unlock new features and drive innovation. Your work will improve patient and doctor experiences by integrating complex data sources and creating robust data transformations. As a Data Engineer, you will: Lead the design, development, and management across investments to platform, model infrastructure, and data pipelines in support of data intensive AI solutions. Work closely with a close knit team of solution experts & innovators to design, iterate, and develop key pipelines to unlock new solution functionality, analytical insights, and machine learning features. Identify, monitor, and lead initiatives to ensure our data platform remains scalable, reliable, and efficient in light of evolving data requirements of our products and services - particularly in the rapidly changing Gen AI / LLM ecosystem Partner with cross-functional stakeholders & peers to translate data needs into technical solutions with rigorous technical scope to enable more junior engineers Key Responsibilities: Spearhead the discovery, evaluation, and integration of new datasets, collaborating (incl. pipeline development and data modeling/documentation), working closely with key data stakeholders to understand their impact and relevance to our core products and the healthcare domain. Translate product / analytical vision into highly functional data pipelines supporting high quality & highly trusted data products (incl. designing data structures, building and scheduling data transformation pipelines, improving transparency etc.). Engage in the rapid new AI product development from concept to deployment, ensuring robust platform supporting easy iteration and monitoring. Set the standard for data engineering practices within the company, guiding the architectural approaches, data pipeline designs, and the integration of cutting-edge technologies to foster a culture of innovation and continuous improvement. What We’re Looking For: Proven experience as a Data Engineer; Excellence in quality data pipeline design, development, and optimization to create reliable, modular, secure data foundations for the organization's data delivery system from applications to analytics & ML Expertise in Python, SQL, and familiarity with AWS tools, Databricks, and DBT. Strong understanding of database structure systems and data mining. Innate aptitude for interpreting complex datasets with demonstrated ability to discern underlying patterns, identify anomalies, and extract meaningful insights, demonstrating advanced data intuition and analytical skills. (Healthcare experience preferred) Proven ability to independently handle ambiguous project requirements and lead data initiatives from start to finish, while collaborating extensively with cross-functional, non-technical teams to inform and shape product development. It’s a Plus if You Have… Experience with Data Science and machine learning techniques, particularly operationalizing models in production environments and working with novel techniques (LLM, Gen AI) Experience navigating high compliance (HIPAA) data ecosystems Experience with data visualization tools and analytics technologies (Looker, Tableau, etc.) Prior exposure to Agile development environments.. Qventus is on a mission to take modern technologies and principles that have been proven in other industries — artificial intelligence, machine learning, behavioral science, and data science — and apply them to simplify healthcare operations. At Qventus, you will have the opportunity to work with an exceptional, mission-driven team, and the ability to directly impact the lives of patients. We’re inspired to work with healthcare leaders on our founding vision and unlock world-class medicine through world-class operations. The salary range for this role is $140,000 to $190,000. This salary represents market data's middle to the high end across different geographies. We consider several factors when determining compensation, including location, experience, and the role’s responsibilities. Salary/OTE is just one component of Qventus’ total rewards package. Our benefits and perks currently include, but are not limited to: Competitive medical, dental, and vision coverage with a 90% employer-paid premiums for employees option Generous HSA contribution, when elected and participating in an eligible plan, up to $1,500 annual company contribution Employer-provided (100% paid) Short Term and Long Term Disability insurance and Basic Life and AD&D insurance 100% paid Parental and Pregnancy Leave Monthly Wellness and Technology stipend up to $50 per month Ability to participate in the 401(k) plan Generous Stock Option awards We believe that diversity, equity, inclusion, and belonging are fundamental to improving healthcare and society, and that’s why we’re building a company that leads the way. We hold ourselves accountable to using fair hiring processes that mitigate the negative impacts of unconscious bias. We also work to ensure that people from underrepresented groups play meaningful roles on both sides of the interview table. We are an equal opportunity employer and give all qualified applicants consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances. Candidate information will be treated in accordance with our candidate privacy notice which can be found here: https://qventus.com/ccpa-privacy-notice/ Employment is contingent upon the satisfactory completion of our pre-employment background investigation and drug test.",
        "url": "https://www.linkedin.com/jobs/view/3960091206",
        "summary": "Qventus, a healthcare technology company focused on real-time decision-making platforms, is seeking a Senior Data Engineer to design, develop, and manage data pipelines for their AI solutions. The ideal candidate will have proven experience in data engineering, expertise in Python, SQL, and AWS tools, and strong understanding of database systems. The role involves leading data initiatives, collaborating with cross-functional teams, and ensuring data scalability, reliability, and efficiency.",
        "industries": [
            "Healthcare",
            "Technology",
            "Artificial Intelligence",
            "Machine Learning"
        ],
        "soft_skills": [
            "Leadership",
            "Collaboration",
            "Communication",
            "Problem-Solving",
            "Analytical Thinking",
            "Data Intuition",
            "Project Management",
            "Teamwork"
        ],
        "hard_skills": [
            "Python",
            "SQL",
            "AWS",
            "Databricks",
            "DBT",
            "Data Pipeline Design",
            "Data Modeling",
            "Data Mining",
            "Data Visualization",
            "Machine Learning",
            "Gen AI",
            "LLM",
            "HIPAA"
        ],
        "tech_stack": [
            "AWS",
            "Databricks",
            "DBT",
            "Python",
            "SQL",
            "Looker",
            "Tableau"
        ],
        "programming_languages": [
            "Python",
            "SQL"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Computer Science",
                "Data Science",
                "Engineering"
            ]
        },
        "salary": {
            "max": 190000,
            "min": 140000
        },
        "benefits": [
            "Medical",
            "Dental",
            "Vision",
            "HSA",
            "Disability Insurance",
            "Life Insurance",
            "Parental Leave",
            "Wellness Stipend",
            "401(k)",
            "Stock Options"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Mateo, CA",
        "job_id": 3954867007,
        "company": "Firework",
        "title": "Backend Software Engineer",
        "created_on": 1720635790.6684246,
        "description": "Firework is the world’s leading unified video commerce platform that empowers its global partners to personalize the customer experience and engagement at scale. Firework bridges the offline and online for a robust omnichannel immersive brand experience cultivating a deeper emotional human connection between our partners and their end consumers. We are customer-centric and inspired to win together offering total solutions with endless possibilities to help our customers increase purchases and conversions using the power of video. At the heart, we are a global and diverse team of “SuperSpark” creators, entrepreneurs, life-long learners, and data geeks driven by the future of authenticity to transform commerce. Firework has raised over $235M to date, with its latest Series B round led by SoftBank Vision Fund 2. Come reimagine the online customer experience with us. Summary Our engineering team is growing! We’re looking for a talented Backend Engineer to join our global team who will bring technical expertise and leadership along with hands-on development of backend software development and SaaS solutions. What You’ll Be Doing Work closely with product managers, designers, and the rest of the engineering team to design, spec, and build elegant and scalable solutions across multiple platforms Design and create new API's in the Elixir Phoenix framework to be used in our core products Writing optimized PostgresQL and Cassandra queries and creating high performance indices Working with in-memory data stores, such as Redis, to improve scalability and responsiveness Debug issues across the entire stack using logging and monitoring tools Review code by your peers on GitHub, offering help and insight from your experience Improve engineering standards, tooling, and processes Ensure coherence of all aspects of projects as an integrated system and uphold overall technical quality Strategize and develop ideas for new programs, products, or features by monitoring industry developments and trends Define all aspects of development from appropriate technology and workflow to coding standards We’ll be excited if you have Master’s degree in computer programming, computer science, or a related field 3+ years of experience designing high-performance RESTful web services serving billions of requests per day Experience writing GraphQL queries and mutations High-level experience with SQL infrastructure and data modeling Comfortable with change: ability to demonstrate comfort with ambiguity, adapt quickly and be effective in new situations in a highly dynamic setting Data-driven but also imaginative and intuitive in coming up with ideas and solutions Must possess a start-up mindset: hunger to learn quickly and the ability to balance multiple priorities in a fast-paced team environment Advanced production with Elixir and Python experience a big plus Experience working with Elixir and Scalability A BIG PLUS if you have * Expertise in video streaming software such as FFmpeg or GStreamer * Experience working with video/audio codecs to encode/decode * Working with live streaming encoders and WebRTC technology As a hybrid office-centric company, we are looking for candidates in the Bay Area. Candidates outside the location are encouraged to apply though must be willing to relocate. The following represents the expected range of compensation for this role: The estimated pay range for this role is $130,000-$150,000. Other factors that impact compensation include stock options. The posted pay range represents the anticipated low and high end of the compensation for this position and is subject to change based on business need. To determine a successful candidate’s starting pay, we carefully consider a variety of factors, including primary work location, an evaluation of the candidate’s skills and experience, market demands, and internal parity. Candidates may receive more information from the talent partner. Don’t hold back We understand some candidates may see the above and not apply because they don’t meet all the qualifications. We encourage you to apply anyway; we often find talented candidates that fit many other opportunities we have and look for potential too, not just what you did in the past. As an equal employment opportunity employer, we are a diverse team that strives for an inclusive environment for all. We prohibit discrimination and harassment of any kind based on race, color, sex, religion, sexual orientation, national origin, age, disability, genetic information, pregnancy, or any other protected characteristic as outlined by federal, state, or local laws. By submitting your application, you acknowledge that you have read and understood Firework's Applicant Privacy Policy located at : https://firework.com/legal/applicantpolicy/.",
        "url": "https://www.linkedin.com/jobs/view/3954867007",
        "summary": "Firework is looking for a talented Backend Engineer to join their growing engineering team. This role will involve designing and developing scalable backend solutions, creating APIs, working with databases, and debugging issues. The ideal candidate will have 3+ years of experience in designing high-performance RESTful web services, experience with GraphQL, SQL, and data modeling. Advanced experience with Elixir and Python is a big plus. The role is based in the Bay Area, but candidates outside of the location are encouraged to apply, though they must be willing to relocate. ",
        "industries": [
            "Software",
            "Technology",
            "E-commerce",
            "Video Streaming",
            "SaaS"
        ],
        "soft_skills": [
            "Leadership",
            "Communication",
            "Problem Solving",
            "Collaboration",
            "Adaptability",
            "Data-Driven",
            "Imagination",
            "Intuitiveness",
            "Startup Mindset",
            "Learning Agility",
            "Multitasking"
        ],
        "hard_skills": [
            "Backend Software Development",
            "SaaS Solutions",
            "API Design",
            "Elixir",
            "Phoenix Framework",
            "PostgreSQL",
            "Cassandra",
            "Redis",
            "GraphQL",
            "SQL",
            "Data Modeling",
            "Debugging",
            "Code Review",
            "GitHub",
            "Engineering Standards",
            "Tooling",
            "Process Improvement",
            "System Integration",
            "Technical Quality",
            "Industry Trends",
            "Development Strategy",
            "FFmpeg",
            "GStreamer",
            "Video/Audio Codecs",
            "Live Streaming Encoders",
            "WebRTC"
        ],
        "tech_stack": [
            "Elixir",
            "Phoenix Framework",
            "PostgreSQL",
            "Cassandra",
            "Redis",
            "GraphQL",
            "GitHub",
            "FFmpeg",
            "GStreamer",
            "WebRTC"
        ],
        "programming_languages": [
            "Elixir",
            "Python",
            "SQL",
            "GraphQL"
        ],
        "experience": 3,
        "education": {
            "min_degree": "Master's",
            "fields": [
                "Computer Programming",
                "Computer Science"
            ]
        },
        "salary": {
            "max": 150000,
            "min": 130000
        },
        "benefits": [
            "Stock Options"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Mountain View, CA",
        "job_id": 3918252187,
        "company": "LinkedIn",
        "title": "Sr. Staff Software Engineer, Data Infrastructure",
        "created_on": 1720635792.3281007,
        "description": "LinkedIn is the world’s largest professional network, built to help members of all backgrounds and experiences achieve more in their careers. Our vision is to create economic opportunity for every member of the global workforce. Every day our members use our products to make connections, discover opportunities, build skills and gain insights. We believe amazing things happen when we work together in an environment where everyone feels a true sense of belonging, and that what matters most in a candidate is having the skills needed to succeed. It inspires us to invest in our talent and support career growth. Join us to challenge yourself with work that matters. This role will be based in Mountain View, CA, San Francisco, CA or Bellevue, WA. At LinkedIn, we trust each other to do our best work where it works best for us and our teams. This role offers a hybrid work option, meaning you can both work from home and commute to a LinkedIn office, depending on what’s best for you and when it is important for your team to be together. As part of our world-class software engineering team, you will take the lead in building the next-generation infrastructure and platforms for LinkedIn, including but not limited to: service delivery platform, scalable data storage infrastructure, graph infrastructure, analytics platform, streams processing and data pipelines, cutting-edge search platform, best-in-class AI/ML infrastructure, Kubernetes compute infrastructure, media infrastructure, etc. You will work and learn among the best, putting to use your passion for distributed technologies and algorithms, API design and systems design, and your passion for writing code that performs at massive scale. LinkedIn has pioneered many well-known open-source infrastructure projects including Apache Kafka, Pinot, Azkaban, Samza, Venice, Datahub, Feather, etc. We also work with industry standard open source infrastructure technologies like Kubernetes, GRPC and GraphQL - come join our infrastructure teams and share the knowledge with a broader community while making a real impact within our company. As a Sr. Staff Software Engineer, you will be a key technical leader and role model within the organization. We are looking for a technical lead who designs and develops technology to serve business and technology objectives, aligns points of view across teams and makes trade offs to help achieve the goals of individual teams as well as LinkedIn’s broader goals. You will foster LinkedIn’s culture and values around transformation, collaboration and results. You will work closely with technical leadership and management within and outside our organization to contribute to building best-in-class core systems infrastructure for LinkedIn. Responsibilities: -Deliver impact by driving innovation while building and shipping software at scale -Provide architectural guidance and mentorship to up-level the engineering organization -Actively improve the level of craftsmanship at LinkedIn by developing best practices and defining best strategies -Design products/services/tools and code that can be used by others while upholding operational impact of all decisions -Functioning as the tech-lead for multiple key initiatives, identify problems and opportunities and lead teams to architect, design, implement and operationalize systems -Partner closely with teams within the org and customers to execute on the vision for long-term success of our core infrastructure teams -Working closely with the open-source community to participate and influence cutting edge open-source projects. -Keep a platform first approach while designing products/service Basic Qualifications: - BS/BA in Computer Science or related technical field or equivalent technical experience - 5+ years of industry experience in software design, development, and algorithm related solutions - 5+ years of experience programming in object-oriented languages such as C/C++, Java, Go, Rust, Python, Scala - 2+ years of experience as an architect, or technical leadership position - Hands-on experience developing large-scale, distributed systems, and databases Preferred Qualifications: - MS or PhD degree in Computer Science or related technical discipline - 10+ years of experience in software design, development, and algorithm related solutions with at least 5 years of experience in a technical leadership position - 10+ years of experience in an object-oriented programming language such as C/C++, Java, Go, Rust, Python, Scala - 5+ years of experience with large-scale distributed systems and client-server architectures - Experience in architecting and designing large-scale distributed systems related to data infrastructure, IaaS, ML/AI infrastructure, storage, graph, Kubernetes, and platforms. Suggested Skills: -Distributed Systems -Technical Leadership -Infrastructure as a Service (IaaS) -Systems Infrastructure LinkedIn is committed to fair and equitable compensation practices. The pay range for this role is $180,000 to $300,000. Actual compensation packages are based on several factors that are unique to each candidate, including but not limited to skill set, depth of experience, certifications, and specific work location. This may be different in other locations due to differences in the cost of labor. The total compensation package for this position may also include annual performance bonus, stock, benefits and/or other applicable incentive compensation plans. For more information, visit https://careers.linkedin.com/benefits. Equal Opportunity Statement LinkedIn is committed to diversity in its workforce and is proud to be an equal opportunity employer. LinkedIn considers qualified applicants without regard to race, color, religion, creed, gender, national origin, age, disability, veteran status, marital status, pregnancy, sex, gender expression or identity, sexual orientation, citizenship, or any other legally protected class. LinkedIn is an Affirmative Action and Equal Opportunity Employer as described in our equal opportunity statement here: https://microsoft.sharepoint.com/:b:/t/LinkedInGCI/EeE8sk7CTIdFmEp9ONzFOTEBM62TPrWLMHs4J1C_QxVTbg?e=5hfhpE. Please reference https://www.eeoc.gov/sites/default/files/2023-06/22-088_EEOC_KnowYourRights6.12ScreenRdr.pdf and https://www.dol.gov/ofccp/regs/compliance/posters/pdf/OFCCP_EEO_Supplement_Final_JRF_QA_508c.pdf for more information. LinkedIn is committed to offering an inclusive and accessible experience for all job seekers, including individuals with disabilities. Our goal is to foster an inclusive and accessible workplace where everyone has the opportunity to be successful. If you need a reasonable accommodation to search for a job opening, apply for a position, or participate in the interview process, connect with us at accommodations@linkedin.com and describe the specific accommodation requested for a disability-related limitation. Reasonable accommodations are modifications or adjustments to the application or hiring process that would enable you to fully participate in that process. Examples of reasonable accommodations include but are not limited to: -Documents in alternate formats or read aloud to you -Having interviews in an accessible location -Being accompanied by a service dog -Having a sign language interpreter present for the interview A request for an accommodation will be responded to within three business days. However, non-disability related requests, such as following up on an application, will not receive a response. LinkedIn will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by LinkedIn, or (c) consistent with LinkedIn's legal duty to furnish information. Pay Transparency Policy Statement As a federal contractor, LinkedIn follows the Pay Transparency and non-discrimination provisions described at this link: https://lnkd.in/paytransparency. Global Data Privacy Notice for Job Candidates This document provides transparency around the way in which LinkedIn handles personal data of employees and job applicants: https://lnkd.in/GlobalDataPrivacyNotice",
        "url": "https://www.linkedin.com/jobs/view/3918252187",
        "summary": "LinkedIn is seeking a Senior Staff Software Engineer to lead the development and implementation of next-generation infrastructure and platforms. This role will focus on building scalable, distributed systems, including data storage, graph infrastructure, analytics, and AI/ML. The ideal candidate will have a strong background in distributed systems, technical leadership, and experience with open-source projects.",
        "industries": [
            "Technology",
            "Software Development",
            "Computer Science",
            "Engineering",
            "Data Science",
            "Artificial Intelligence"
        ],
        "soft_skills": [
            "Technical Leadership",
            "Innovation",
            "Mentorship",
            "Collaboration",
            "Problem-Solving",
            "Communication",
            "Teamwork",
            "Strategic Thinking",
            "Decision-Making"
        ],
        "hard_skills": [
            "C/C++",
            "Java",
            "Go",
            "Rust",
            "Python",
            "Scala",
            "Distributed Systems",
            "Data Infrastructure",
            "IaaS",
            "ML/AI Infrastructure",
            "Storage",
            "Graph",
            "Kubernetes",
            "Platforms",
            "API Design",
            "Systems Design",
            "Algorithm Design",
            "Apache Kafka",
            "Pinot",
            "Azkaban",
            "Samza",
            "Venice",
            "Datahub",
            "Feather",
            "GRPC",
            "GraphQL"
        ],
        "tech_stack": [
            "Apache Kafka",
            "Pinot",
            "Azkaban",
            "Samza",
            "Venice",
            "Datahub",
            "Feather",
            "Kubernetes",
            "GRPC",
            "GraphQL"
        ],
        "programming_languages": [
            "C/C++",
            "Java",
            "Go",
            "Rust",
            "Python",
            "Scala"
        ],
        "experience": 5,
        "education": {
            "min_degree": "BS/BA",
            "fields": [
                "Computer Science",
                "Related Technical Field"
            ]
        },
        "salary": {
            "max": 300000,
            "min": 180000
        },
        "benefits": [
            "Annual Performance Bonus",
            "Stock",
            "Benefits",
            "Incentive Compensation Plans"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Emeryville, CA",
        "job_id": 3966768057,
        "company": "Amyris",
        "title": "Senior Data Engineer",
        "created_on": 1720635794.4609268,
        "description": "Amyris has developed an industry-leading platform for designing and building synthetic organisms. Our technology is being used today to make clean beauty products, bio-based renewable chemicals, and even vaccine ingredients. The Data Engineering team at Amyris supports a suite of custom applications using various state of the art data technologies that make this cutting-edge science possible. Being on the Data Engineering team is all about learning and sharing experiences using the technologies that will make the difference for the company. The Senior Data Engineer will be primarily responsible for working with the Data team to participate in achieving the vision of the Data framework. The ideal candidate will come in with knowledge in multiple cloud and database technologies to hit the ground running from a technology perspective. This role would be working on architecture design, engineering, security, resiliency for all the Data environments we have at Amyris. The Senior Data Engineer will help develop governance around API’s and API management keeping in mind the HA\\DR needs while also understanding the needs of the company and satisfying various stakeholders. The team uses a wide range of technologies and frameworks, always applying the best tool for the job at hand making this a fantastic opportunity for you to expand your existing skill set. Responsibilities Manage and Engineer existing Postgres and MSSQL environments Be the point of contact for the manufacturing department at Amyris and also be able to gather requirements to create, manage, new projects on multiple cloud providers including GCP Work independently on projects ranging from simple to complex and from executive level ad hoc analysis to integration of new data into a complex reporting system. Ensure proper protocols exist that will monitor, capture, audit and alert for deviations and support for quick and timely resolution. Document all data, business rule and metric changes for our team and for executives/management and power users. Mentor juniors in the team to help them with technical onboarding and then to also help them grow technically. Spend about 5-10% of the time researching new Data products to know the latest and greatest in the field. Work with various stakeholders on understanding their asks and provide them a full proof solution which is a well thought out solution. Develop and document strategy for POC’s and onboarding of new products into Amyris data framework. Help develop API strategy for Amyris working with stakeholders and Data team leadership. Document API architecture at Amyris, including the upstream and downstream systems to have a full view of the design and data flow. Work closely with internal stakeholders and hold periodic meetings to make sure that we understand the direction in which Amyris is going, from business mindset. Responsibilities are subject to grow once more work is undertaken, specially from BI, middleware and publishing work is undertaken by the team. Basic Qualifications Bachelor’s Degree or higher in Computer Science, Information Technology, or related field and 5+ years of work experience as a data engineer; or a Master’s degree with 3+ years of experience; or a PhD with 1+ years of experience. Proven experience as a data engineer with data technologies for on-prem and Cloud environments, specifically GCP. Experience managing and prioritizing multiple projects simultaneously. Experience with API creation and management skills are needed, at least on GCP. Experience with data analysis and problem-solving skills who can handle the project delivery with minimal managerial intervention. Experience building and maintaining strong relationships with business users, software developers, other cross-functional co-workers, and outside partners. Experience in SQL\\NO-SQL is required. Experience translating between technical and non-technical stakeholders. Experience with hybrid (cloud/on-premises) data topologies. Experience designing data lake and data warehouse architectures using modern cloud solutions. Hands-on experience implementing data transformation (ETL/ELT, etc.) best practices at scale, preferably using modern cloud solutions. Experience with SQL and database architecture. Experience with both open source and off the shelf technologies. Experience with communicating across various levels, both written and verbally. This includes listening skills and an open attitude toward constructive feedback. Experience working both with a global team and individually. Preferred Qualifications Professional related certifications a plus. Experience with AWS and Azure would be beneficial. Experience with other cloud providers are a plus. Experience in python and java are preferred. Amyris is committed to fair and equitable compensation practices. The annual pay range for this role is $140,500 to $207,000. Salary offers are made based on internal equity and market analysis, and will vary based on the candidate’s skills, depth of experience and specific work location. Amyris is committed to providing a diverse array of inclusive benefits and perks to support employees and their families’ wellbeing including access to robust healthcare, mental wellness benefits, family leave, discounted fitness memberships, flexible paid time off, 17 paid holidays, 10 paid sick days, and additional paid time off including 3 volunteer days, and bereavement.",
        "url": "https://www.linkedin.com/jobs/view/3966768057",
        "summary": "Amyris is looking for a Senior Data Engineer to join their team and help build out their data framework. The ideal candidate will have experience with various cloud and database technologies, including GCP, Postgres, and MSSQL. They will be responsible for managing and engineering existing environments, working on projects ranging from simple to complex, ensuring proper protocols exist, documenting changes, mentoring junior team members, researching new data products, and developing API strategy. The role requires strong communication and collaboration skills, as well as experience working in both global and individual settings.",
        "industries": [
            "Biotechnology",
            "Manufacturing",
            "Software",
            "Data Science",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Leadership",
            "Mentorship",
            "Time Management",
            "Organization",
            "Relationship Building",
            "Stakeholder Management"
        ],
        "hard_skills": [
            "Postgres",
            "MSSQL",
            "GCP",
            "API Management",
            "Data Analysis",
            "SQL",
            "NoSQL",
            "Data Lake",
            "Data Warehouse",
            "ETL",
            "ELT",
            "Data Transformation",
            "Database Architecture",
            "Python",
            "Java"
        ],
        "tech_stack": [
            "Postgres",
            "MSSQL",
            "GCP",
            "API Management",
            "SQL",
            "NoSQL",
            "Data Lake",
            "Data Warehouse",
            "ETL",
            "ELT",
            "Data Transformation",
            "Database Architecture",
            "Python",
            "Java"
        ],
        "programming_languages": [
            "Python",
            "Java"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor’s Degree",
            "fields": [
                "Computer Science",
                "Information Technology"
            ]
        },
        "salary": {
            "max": 207000,
            "min": 140500
        },
        "benefits": [
            "Healthcare",
            "Mental Wellness Benefits",
            "Family Leave",
            "Discounted Fitness Memberships",
            "Flexible Paid Time Off",
            "Paid Time Off",
            "Paid Holidays",
            "Paid Sick Days",
            "Volunteer Days",
            "Bereavement"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Pleasanton, CA",
        "job_id": 3959287084,
        "company": "Maxonic",
        "title": "GCP Data Engineer with Python. Local candidates only.",
        "created_on": 1720635796.0652845,
        "description": "Maxonic maintains a close and long-term relationship with our direct client. In support of their needs, we are looking for a GCP Data Engineer with Python Location: Pleasanton, CA. Hybrid. (Local candidates only) W2 or C2C. Duration 12 Months contract. $55/hr to $64/hr on w2 Responsibilities: Data Warehousing Understanding of data warehousing concepts Experience with tools like BigQuery for storing and querying large datasets. ETL (Extract, Transform, Load) Proficiency in designing and implementing ETL pipelines using tools like Dataflow Apache Beam or Cloud Composer Programming Languages Strong programming skills in languages such as Python, Java, or Scala for building data pipelines and performing data manipulation tasks. Database Management Knowledge of database systems, particularly Big Query, Google Cloud SQL, Cloud Spanner Data Modeling Ability to design and implement data models for efficient storage and retrieval of data. Google Cloud Services GCP services such as Pub/Sub for messaging, Cloud functions etc Data Catalog for metadata management Data Studio for data visualization Version Control and Code Deployment Familiarity with version control systems like Git for managing code GitHub Actions About Maxonic: Since 2002 Maxonic has been at the forefront of connecting candidate strengths to client challenges. Our award winning, dedicated team of recruiting professionals are specialized by technology, are great listeners, and will seek to find a position that meets the long-term career needs of our candidates. We take pride in the over 10,000 candidates that we have placed, and the repeat business that we earn from our satisfied clients. Interested in Applying? Please apply with your most current resume. Feel free to contact Suresh Krishnamurthy (suresh.k@maxonic.com / (408) 400-2323) for more details.",
        "url": "https://www.linkedin.com/jobs/view/3959287084",
        "summary": "A 12-month contract position for a GCP Data Engineer with Python experience in Pleasanton, CA. Responsibilities include data warehousing, ETL pipeline design and implementation, data modeling, and working with GCP services like BigQuery, Dataflow, Cloud Composer, and Pub/Sub.  Experience with version control systems like Git and GitHub Actions is also required.",
        "industries": [
            "Technology",
            "Data Engineering",
            "Cloud Computing",
            "Software Development"
        ],
        "soft_skills": [
            "Problem Solving",
            "Communication",
            "Collaboration",
            "Teamwork",
            "Analytical Skills"
        ],
        "hard_skills": [
            "Data Warehousing",
            "ETL",
            "Data Modeling",
            "Data Visualization",
            "Database Management",
            "Version Control"
        ],
        "tech_stack": [
            "GCP",
            "BigQuery",
            "Dataflow",
            "Apache Beam",
            "Cloud Composer",
            "Python",
            "Java",
            "Scala",
            "Google Cloud SQL",
            "Cloud Spanner",
            "Pub/Sub",
            "Cloud Functions",
            "Data Catalog",
            "Data Studio",
            "Git",
            "GitHub Actions"
        ],
        "programming_languages": [
            "Python",
            "Java",
            "Scala"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 64,
            "min": 55
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3916825101,
        "company": "Hertz",
        "title": "Software Engineer",
        "created_on": 1720635799.3368208,
        "description": "Job Description A Day in the Life Come join us in our effort to digitally transform Hertz! Recent innovations such as smartphones, electric vehicles, and ride-hailing apps have created new and exciting opportunities in transportation that Hertz is uniquely positioned to capitalize on. We’re looking for software engineers who will modernize Hertz’s tech stack and, in the process, ship delightful products to meet the ever-increasing demands of our customers. You will be building scalable systems and shipping features in a complex environment, where one must contend with challenges such as modernizing legacy applications and managing technical debt. What You'll Do Design, develop, and deploy applications that can handle high request volumes with high reliability and low latency Collaborate with product managers to build product requirements against business objectives and drive teams through the complete software development lifecycle Envision system features and functionality, create detailed design documentation, and decide on tradeoffs between technical and design approaches. Identify any technical issues that arise and follow up with root-cause analysis and resolution Identify key application metrics, build necessary dashboards for monitoring performance, and add necessary logging for real-time debugging Review code, support continuous improvement, and investigate alternatives Utilize CI/CD tools to support system integration and deployment Mentor other engineers to help build a high-performing engineering culture What We're Looking For Less than 2 years of experience in the technology industry, and a B.S. in Computer Science or equivalent Proficiency in one or more programming languages and common data structures / algorithms Ability to write production-ready code with moderate supervision Strong communication skills. You must be able to work with cross-functional partners to gather requirements and explain outcomes Strong product sense. You must be able to align your work with business objectives and make appropriate tradeoffs Learning mentality. You must be able to pick up new skills as needed and demonstrate a curiosity about new technologies Highly Preferred Engineering experience at high-tech firms (e.g. Amazon, Meta, DoorDash, Twilio) Experience architecting and building large-scale systems in an agile development environment Experience working alongside technical product managers to drive projects and flesh out product requirements We expect the starting salary to be around $100,000.00 with annual bonus and profit sharing eligibility. The actual salary will be determined based on years of relevant work experience What You’ll Get Up to 40% off the base rate of any standard Hertz rental Paid Time Off Medical, Dental & Vision plan options Retirement programs, including 401(k) employer matching Paid Parental Leave & Adoption Assistance Employee Assistance Program for employees & family Educational Reimbursement & Discounts Voluntary Insurance Programs - Pet, Legal/Identity Theft, Critical Illness Perks & Discounts –Theme Park Tickets, Gym Discounts & more",
        "url": "https://www.linkedin.com/jobs/view/3916825101",
        "summary": "Hertz is looking for Software Engineers to modernize their tech stack and build scalable systems, handling high request volumes with reliability and low latency. Responsibilities include collaborating with product managers, designing and deploying applications, conducting root-cause analysis, and mentoring other engineers. They are looking for candidates with less than 2 years of experience, proficiency in programming languages, strong communication skills, and a learning mentality.",
        "industries": [
            "Transportation",
            "Technology",
            "Software Development",
            "Automotive"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Analytical Thinking",
            "Product Sense",
            "Learning Mentality",
            "Teamwork"
        ],
        "hard_skills": [
            "Programming Languages",
            "Data Structures",
            "Algorithms",
            "CI/CD",
            "Code Review",
            "Root Cause Analysis",
            "System Monitoring",
            "Performance Optimization",
            "Agile Development"
        ],
        "tech_stack": [
            "CI/CD",
            "Agile Development"
        ],
        "programming_languages": [
            "N/A"
        ],
        "experience": 2,
        "education": {
            "min_degree": "B.S.",
            "fields": [
                "Computer Science"
            ]
        },
        "salary": {
            "max": 100000,
            "min": 100000
        },
        "benefits": [
            "Hertz Rental Discount",
            "Paid Time Off",
            "Medical, Dental & Vision",
            "Retirement Programs",
            "Paid Parental Leave",
            "Adoption Assistance",
            "Employee Assistance Program",
            "Educational Reimbursement",
            "Discounts",
            "Voluntary Insurance",
            "Perks & Discounts"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Redwood City, CA",
        "job_id": 3961206783,
        "company": "Bear Robotics",
        "title": "Software Engineer - Data Engineering",
        "created_on": 1720635801.392718,
        "description": "Job Title: Software Engineer - Data Engineering Department: Software Engineering Job Level: L4 FLSA: Exempt Job Summary Bear Robotics is a cutting-edge robotics company focused on developing innovative automation solutions for various industries. Our products, including robot devices, cloud services, and public APIs, are designed to help businesses operate more efficiently and effectively. Bear Robotics is on the hunt for a dynamic and skilled Software Engineer to spearhead our Data Analytics team. If you are passionate about data and skilled at enabling data-driven decisions, then you could be our ideal candidate! Key Duties/Responsibilities Oversee, manage, and monitor GCP BigQuery databases. Administer Looker and create compelling data visualizations to serve our internal teams, or guide them in crafting their own. Aggregate granular/high frequency data and decide when to discard non-essential information. Guarantee the precision of all data points. Uphold the database security. Compose, review, and offer assistance with Terraform code. Collaborate with application and infrastructure teams to design and implement optimized pipelines for data ingestion and processing. Operate with significant autonomy, discerning and managing priorities effectively. Performs other duties as assigned Supervisory Responsibilities Lead a small Data Analytics team, providing guidance, mentorship, and direction. Oversee the training and development of team members, ensuring they have the necessary tools and knowledge. Conduct regular performance reviews, providing constructive feedback and recognizing accomplishments. Collaborate with HR and upper management on hiring, promotions, and disciplinary actions as necessary. Required Skills/Abilities/Qualifications Proven experience with enterprise-level visualization tools, such as Looker, Tableau, Superset, in a professional setting. Preferred Skills/Abilities/Qualifications Hands-on programming experience, notably in Python, C++, or Go. Prior professional leadership roles or experience. Related personal projects or a compelling portfolio showcasing your passion and skill in the field. Education/Experience Bachelor’s degree in Computer Science or a related data-centric discipline. A minimum of three years’ professional experience with SQL databases, either using or administrating. Physical Requirements The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. Prolonged periods of sitting/standing at a desk and working on a computer. The employee routinely is required to sit; stand, walk; talk and hear; use hands to keyboard Specific vision abilities required by this job include close vision, color vision, peripheral vision, depth perception, and ability to adjust focus. Ability to lift 20 lbs. Benefits Summary We hire the best, not only will you be surrounded by exceptionally smart and motivated people, but we believe excellent compensation and benefits are an essential part of our company's success. Comprehensive Medical/Dental/Vision Insurance Plans Company-Paid Long-Term and Short-Term Disability Company-Paid Life/AD&D Insurance Health Savings Account/Flexible Spending Account Stock Options Employee Assistance Program (EAP) Wellness Reward Programs Mental Health Benefits Paid Vacation Time Paid Sick Time 401K Plan Employer Match (No Vesting Schedule) Paid Bearental (Parental) Leave (16 weeks) Paid New Bearental (Parental) Transitional Time Off (2 weeks) Annual Paid Holidays (11 days) Annual Paid Family Time-Off (Between Christmas Holiday and New Year) Employee Recognition Bonus Program Peer Recognition Bonus Program Employee Referral Reward Program Patent Reward Program Monthly Mobile Phone Reimbursement Monthly Internet Reimbursement Casual Dress Policy Financial Wellness Education Sessions Free Daily In-Office Lunch Unlimited Office Snacks and Drinks Wellness Room with Massage Chair Day-Friendly Office Flexible Work Schedule Office Parties and Family Events The pay range is $120K-$215K. Pay is dependent on the applicant’s relevant experience Bear Robotics, Inc. is proud to be an Equal Opportunity Employer. We do not discriminate on the basis of race, color, ancestry, national origin, religion or religious creed, mental or physical disability, medical condition, genetic information, sex (including pregnancy, childbirth, and related medical conditions), sexual orientation, gender identity, gender expression, age, marital status, military or veteran status, citizenship, or other characteristics protected by state or federal law or local ordinance.",
        "url": "https://www.linkedin.com/jobs/view/3961206783",
        "summary": "Bear Robotics is seeking a skilled Software Engineer to lead their Data Analytics team. Responsibilities include managing GCP BigQuery databases, administering Looker for data visualization, aggregating and cleaning data, ensuring database security, collaborating on Terraform code, and designing data pipelines. The role also involves leadership, including team mentorship and performance reviews. ",
        "industries": [
            "Robotics",
            "Automation",
            "Data Analytics",
            "Software Development",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Leadership",
            "Mentorship",
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Decision Making",
            "Time Management",
            "Prioritization",
            "Organization",
            "Detail-Oriented",
            "Analytical",
            "Data-Driven"
        ],
        "hard_skills": [
            "GCP BigQuery",
            "Looker",
            "Data Visualization",
            "SQL",
            "Terraform",
            "Python",
            "C++",
            "Go",
            "Data Pipelines",
            "Data Ingestion",
            "Data Processing"
        ],
        "tech_stack": [
            "GCP",
            "BigQuery",
            "Looker",
            "Terraform",
            "Python",
            "C++",
            "Go",
            "SQL"
        ],
        "programming_languages": [
            "Python",
            "C++",
            "Go",
            "SQL"
        ],
        "experience": 3,
        "education": {
            "min_degree": "Bachelor’s degree",
            "fields": [
                "Computer Science",
                "Data Science",
                "Related Data-Centric Fields"
            ]
        },
        "salary": {
            "max": 215000,
            "min": 120000
        },
        "benefits": [
            "Comprehensive Medical/Dental/Vision Insurance Plans",
            "Company-Paid Long-Term and Short-Term Disability",
            "Company-Paid Life/AD&D Insurance",
            "Health Savings Account/Flexible Spending Account",
            "Stock Options",
            "Employee Assistance Program (EAP)",
            "Wellness Reward Programs",
            "Mental Health Benefits",
            "Paid Vacation Time",
            "Paid Sick Time",
            "401K Plan Employer Match",
            "Paid Bearental (Parental) Leave",
            "Paid New Bearental (Parental) Transitional Time Off",
            "Annual Paid Holidays",
            "Annual Paid Family Time-Off",
            "Employee Recognition Bonus Program",
            "Peer Recognition Bonus Program",
            "Employee Referral Reward Program",
            "Patent Reward Program",
            "Monthly Mobile Phone Reimbursement",
            "Monthly Internet Reimbursement",
            "Casual Dress Policy",
            "Financial Wellness Education Sessions",
            "Free Daily In-Office Lunch",
            "Unlimited Office Snacks and Drinks",
            "Wellness Room with Massage Chair",
            "Day-Friendly Office",
            "Flexible Work Schedule",
            "Office Parties and Family Events"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Los Angeles, CA",
        "job_id": 3953778803,
        "company": "Netflix",
        "title": "Senior Analytics Engineer, (L5) Games",
        "created_on": 1720635805.529711,
        "description": "Now is an amazing time to join Netflix as we seek to entertain the world. We have over 260 million paid members in over 190 countries, and we won’t stop there. Games are our next big frontier and an incredible opportunity for us to deliver new experiences to delight and entertain our quickly growing membership. You will be jumping in at the early stages of this adventure and be in a position to help us redefine what a Netflix subscription means for our members around the world. Data Science and Engineering (‘DSE’) at Netflix is aimed at using data, analytics, and sciences to improve various aspects of our business. We are looking for a Senior Analytics Engineer to join our Games DSE team, leading end-to-end analytics & data needs for the Games space. This role will be partnering closely with our stakeholders from Game Studios, product and engineering to do advanced analytics and define key metrics, build and maintain key analytic tools and provide data driven insights to our stakeholder set. As an early member of the team, you will also help shape our overall Games Data Strategy at Netflix. What You Will Do Partner directly with our Game stakeholders (e.g., Netflix Games Studio, Games Product, Game Strategy, Planning & Analysis team) on data, metrics & analytics initiatives. Lead end-to-end development of reports/dashboards/tools used by your direct stakeholders and a diverse set of teams across the company. Proactively perform data exploration and analytical deep dives to discover insights or future opportunities. Be a bridge between the business and tech, scaling access to insights that can empower better decision-making. Drive the direction and execution of your work, which span from developing scrappy analytic tools to designing scalable analytic systems. Balance handling engineering efforts, analytical deep dives, and building analytic products. Who You Are 5+ years of experience in leveraging technical skills in manipulating large data sets with complex SQL and Python (or similar languages), big data technologies (e.g., Hadoop, Spark) and visualization tools (e.g., Tableau). Exceptional communication and collaboration skills coupled with strong business acumen. Comfortable with ambiguity; able to take ownership, and thrive with minimal oversight and process. You are experienced in managing stakeholder asks, expectations, and relationships across a variety of stakeholders. Experience in the Gaming industry preferred. Netflix culture resonates with you. Our compensation structure consists solely of an annual salary; we do not have bonuses. You choose each year how much of your compensation you want in salary versus stock options. To determine your personal top of market compensation, we rely on market indicators and consider your specific job family, background, skills, and experience to determine your compensation in the market range. The range for this role is $170,000 - $720,000. Netflix provides comprehensive benefits including Health Plans, Mental Health support, a 401(k) Retirement Plan with employer match, Stock Option Program, Disability Programs, Health Savings and Flexible Spending Accounts, Family-forming benefits, and Life and Serious Injury Benefits. We also offer paid leave of absence programs. Full-time hourly employees accrue 35 days annually for paid time off to be used for vacation, holidays, and sick paid time off. Full-time salaried employees are immediately entitled to flexible time off. See more detail about our Benefits here. Netflix is a unique culture and environment. Learn more here. We are an equal-opportunity employer and celebrate diversity, recognizing that diversity of thought and background builds stronger teams. We approach diversity and inclusion seriously and thoughtfully. We do not discriminate on the basis of race, religion, color, ancestry, national origin, caste, sex, sexual orientation, gender, gender identity or expression, age, disability, medical condition, pregnancy, genetic makeup, marital status, or military service.",
        "url": "https://www.linkedin.com/jobs/view/3953778803",
        "summary": "Netflix is seeking a Senior Analytics Engineer to join their Games DSE team. This role will be responsible for leading end-to-end analytics & data needs for the Games space, partnering closely with stakeholders from Game Studios, product and engineering. This individual will help shape the overall Games Data Strategy at Netflix.",
        "industries": [
            "Entertainment",
            "Media",
            "Gaming",
            "Technology"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Business Acumen",
            "Ambiguity Tolerance",
            "Ownership",
            "Stakeholder Management"
        ],
        "hard_skills": [
            "SQL",
            "Python",
            "Hadoop",
            "Spark",
            "Tableau"
        ],
        "tech_stack": [
            "SQL",
            "Python",
            "Hadoop",
            "Spark",
            "Tableau"
        ],
        "programming_languages": [
            "SQL",
            "Python"
        ],
        "experience": 5,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 720000,
            "min": 170000
        },
        "benefits": [
            "Health Plans",
            "Mental Health Support",
            "401(k) Retirement Plan with Employer Match",
            "Stock Option Program",
            "Disability Programs",
            "Health Savings and Flexible Spending Accounts",
            "Family-forming Benefits",
            "Life and Serious Injury Benefits",
            "Paid Leave of Absence Programs",
            "Paid Time Off"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3963206718,
        "company": "TBD",
        "title": "Senior Software Engineer, Lightning Intelligence",
        "created_on": 1720635809.1193843,
        "description": "Company Description Founded in July 2021, TBD is one of Block's four business units, alongside Square, Cash App, and Tidal. At TBD, we believe in a decentralized future that returns ownership and control over your finances, data, and identity. Guided by this vision, TBD is building an open source platform and developer infrastructure that enables everyone to access and participate in the global economy. TBD is inviting the world to join us in building and adopting open and decentralized technologies that solve real problems for real people — from next generation infrastructure that lowers the cost of global payments and enables frictionless commerce, to decentralized identity technologies that enable anyone to access and participate in the global economy. Our mission advances economic empowerment through the power of decentralized solutions, built open source and collaboratively. The offerings we build will provide developers, institutions, and individuals with the capabilities needed to build the next wave of innovative apps and services, and access this decentralized future. We’re leveraging bitcoin and other decentralized technologies to reinvent the global financial system in an open and permissionless way to further Block’s mission of economic empowerment, with a global focus. Job Description Our Lightning Intelligence team is part of our Bitcoin Platform team which sits within TBD's business. We pride ourselves on being the first publicly traded company to adopt Bitcoin's Lightning network here at Block. Over the years, Lightning usage has increased and we're doubling down on providing tools and services to augment sending and receiving Lightning payments. The Lightning Intelligence team is looking for engineers to help improve and expand Lightning usage at Block. One of our first projects is improving data about the lightning network and helping our partners make more informed decisions when they peer with other nodes and send payments. As a key member of our Lightning Intelligence team, you won’t just be tasked with software development; we seek a visionary committed to our values. Your role will be innovating and building a robust risk management service, and working closely with our product and compliance teams. You will play a key role in protecting our ecosystem, ensuring compliance with regulatory standards while also balancing the privacy-preserving ethos of the Lightning Network. In this role you will: Design and build the first systems to help Bitcoin Platform to assess and act quickly on risks to our Lightning services Integrate with third party data sources, to feed in additional attribution data to our risk engine Work closely with cross-functional stakeholders to gather and refine requirements End to end own the life cycle of the systems that you will build, from design to release and beyond Qualifications You have: 8+ years of experience in developing large-scale platforms and backend systems A strong motivation to contribute to a meaningful product that will fundamentally change the way people interact with financial institutions Experience working with Bitcoin/Lightning or the desire to learn Experience working across teams and disciplines Experience building backend systems at scale, involving modern eventing, database, and infrastructure tooling A pragmatic approach with a focus on delivery Ability to excel in macro and micro elements of software design Willingness to contribute to the growth of those around you Demonstrated technical initiative and leadership on previous projects Technologies we use and teach: Go, Kotlin, Java AWS, Terraform, Docker, Kubernetes Datastores of all kinds (such as MySQL, DynamoDB, and Snowflake) Monitoring and visualization tools (such as Datadog and Looker) Additional Information Block takes a market-based approach to pay, and pay may vary depending on your location. U.S. locations are categorized into one of four zones based on a cost of labor index for that geographic area. The successful candidate’s starting pay will be determined based on job-related skills, experience, qualifications, work location, and market conditions. These ranges may be modified in the future. Zone A: USD $198,000 - USD $297,000 Zone B: USD $188,100 - USD $282,100 Zone C: USD $178,200 - USD $267,400 Zone D: USD $168,300 - USD $252,500 To find a location’s zone designation, please refer to this resource. If a location of interest is not listed, please speak with a recruiter for additional information. Full-time employee benefits include the following: Healthcare coverage (Medical, Vision and Dental insurance) Health Savings Account and Flexible Spending Account Retirement Plans including company match Employee Stock Purchase Program Wellness programs, including access to mental health, 1:1 financial planners, and a monthly wellness allowance Paid parental and caregiving leave Paid time off (including 12 paid holidays) Paid sick leave (1 hour per 26 hours worked (max 80 hours per calendar year to the extent legally permissible) for non-exempt employees and covered by our Flexible Time Off policy for exempt employees) Learning and Development resources Paid Life insurance, AD&D, and disability benefits These benefits are further detailed in Block's policies. This role is also eligible to participate in Block's equity plan subject to the terms of the applicable plans and policies, and may be eligible for a sign-on bonus. Sales roles may be eligible to participate in a commission plan subject to the terms of the applicable plans and policies. Pay and benefits are subject to change at any time, consistent with the terms of any applicable compensation or benefit plans. US and Canada EEOC Statement We’re working to build a more inclusive economy where our customers have equal access to opportunity, and we strive to live by these same values in building our workplace. Block is a proud equal opportunity employer. We work hard to evaluate all employees and job applicants consistently, without regard to race, color, religion, gender, national origin, age, disability, pregnancy, gender expression or identity, sexual orientation, citizenship, or any other legally protected class. We believe in being fair, and are committed to an inclusive interview experience, including providing reasonable accommodations to disabled applicants throughout the recruitment process. We encourage applicants to share any needed accommodations with their recruiter, who will treat these requests as confidentially as possible. Want to learn more about what we’re doing to build a workplace that is fair and square? Check out our I+D page . Additionally, we consider qualified applicants with criminal histories for employment on our team, and always assess candidates on an individualized basis. Block, Inc. (NYSE: SQ) is a global technology company with a focus on financial services. Made up of Square, Cash App, Spiral, TIDAL, and TBD, we build tools to help more people access the economy. Square helps sellers run and grow their businesses with its integrated ecosystem of commerce solutions, business software, and banking services. With Cash App, anyone can easily send, spend, or invest their money in stocks or Bitcoin. Spiral (formerly Square Crypto) builds and funds free, open-source Bitcoin projects. Artists use TIDAL to help them succeed as entrepreneurs and connect more deeply with fans. TBD is building an open developer platform to make it easier to access Bitcoin and other blockchain technologies without having to go through an institution. While there is no specific deadline to apply for this role, on average, U.S. open roles are posted for 70 days before being filled by a successful candidate.",
        "url": "https://www.linkedin.com/jobs/view/3963206718",
        "summary": "TBD, a Block company, is seeking a Senior Software Engineer to join its Lightning Intelligence team, focused on building risk management services for Bitcoin's Lightning Network. This role involves designing and building systems to assess risks, integrating with third-party data sources, working with stakeholders, and owning the systems lifecycle. Ideal candidates have 8+ years of experience in large-scale platform development, experience with Bitcoin/Lightning, and skills in Go, Kotlin, Java, AWS, Terraform, Docker, Kubernetes, and various datastores.",
        "industries": [
            "FinTech",
            "Blockchain",
            "Software Development",
            "Financial Services",
            "Payments",
            "Technology"
        ],
        "soft_skills": [
            "Problem-solving",
            "Communication",
            "Collaboration",
            "Teamwork",
            "Leadership",
            "Initiative",
            "Pragmatism",
            "Visionary",
            "Attention to Detail",
            "Risk Management",
            "Compliance"
        ],
        "hard_skills": [
            "Go",
            "Kotlin",
            "Java",
            "AWS",
            "Terraform",
            "Docker",
            "Kubernetes",
            "MySQL",
            "DynamoDB",
            "Snowflake",
            "Datadog",
            "Looker",
            "Risk Management",
            "Data Integration",
            "Software Development",
            "Backend Systems",
            "Bitcoin",
            "Lightning Network"
        ],
        "tech_stack": [
            "Go",
            "Kotlin",
            "Java",
            "AWS",
            "Terraform",
            "Docker",
            "Kubernetes",
            "MySQL",
            "DynamoDB",
            "Snowflake",
            "Datadog",
            "Looker"
        ],
        "programming_languages": [
            "Go",
            "Kotlin",
            "Java"
        ],
        "experience": 8,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 297000,
            "min": 168300
        },
        "benefits": [
            "Healthcare coverage (Medical, Vision and Dental insurance)",
            "Health Savings Account and Flexible Spending Account",
            "Retirement Plans including company match",
            "Employee Stock Purchase Program",
            "Wellness programs, including access to mental health, 1:1 financial planners, and a monthly wellness allowance",
            "Paid parental and caregiving leave",
            "Paid time off (including 12 paid holidays)",
            "Paid sick leave",
            "Learning and Development resources",
            "Paid Life insurance, AD&D, and disability benefits",
            "Equity plan",
            "Sign-on bonus"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Santa Barbara, CA",
        "job_id": 3971341412,
        "company": "Pearly",
        "title": "Software Engineer (Data Platform)",
        "created_on": 1720635810.849675,
        "description": "About Pearly Pearly is a revenue cycle management automation platform for DSOs, dental groups, and independent practices. We're on a mission to help dental practices save time and get paid faster, while providing patients with a transparent, modern, and convenient way to understand and pay for dental care from pre-visit through post-visit. As a high-growth, B2B software company, we are looking for high-talent, high-integrity people to join us on our mission to reimagine dental billing, payments, and engagement. About The Role We're looking for a Santa Barbara based Software Engineer that will own the evolution and scaling of our best-in-class data platform. Pearly maintains synchronized dental records for over 25 million patients across the United States. These records form the foundation of our software capabilities, allowing for workflows, reporting, and attribution that are trusted by the largest dental organizations in the country. You will be responsible for building the next generation of our data systems, including deploying ETLs across our integration partners, employing novel warehousing techniques at scale, and developing clean APIs for exposing data across a variety of application and reporting requirements. What You'll Do Deliver data at scale: You will build mission-critical ETL processes that structure and warehouse high-volume medical and financial data, emphasizing performance, integrity, and accuracy . Integrate with a modern stack: Pearly operates a hyper-modern stack with a focus on clean, functional programming and thoughtful DX. Your primary tools will be Typescript and PostgreSQL, and your code will interact with broader platform components built on top of GCP Serverless, GraphQL, Prisma, and React. Collaborate effectively: We operate as a flat, mutually supportive, highly connected team where frequent input and collaboration is expected and best ideas always win. Architect for the future: We believe simplicity is the greatest virtue, and any system you develop will be built with a focus on extensibility, modularity, and readability. Qualifications Sense of Craft: You take pride in and are consistently honing your technical and creative abilities. You seek out opportunities to introduce simplicity and elegance, and enjoy composing solutions to business requirements in a way that is both innovative and effective. 2+ Years Deploying to Prod: You have an understanding of the requirements and stakes involved in deploying live systems. You exhibit exceptional attention to detail and reliability, and are prepared to operate in a security and accuracy-sensitive domain. Customer-Driven Mindset: While your focus will be data, our mission at Pearly is to build software that serves the customer. You understand how data ultimately powers business use cases, and how to leverage data to deliver novel capabilities. Willing to Roll Up Your Sleeves: Taking the initiative is second nature to you. You will operate with a high degree of autonomy, take projects to the finish line, and own the outcome. Based in Santa Barbara, CA: Our team is in-office first, based out of downtown Santa Barbara, California, with hybrid flexibility following onboarding. Benefits Competitive salary, equity, and healthcare benefits Meeting-light culture Work with an A+ smart and passionate team Flexible vacation/time-off policy Opportunity to make your mark at an early stage company with great product-market fit",
        "url": "https://www.linkedin.com/jobs/view/3971341412",
        "summary": "Pearly, a revenue cycle management automation platform for dental practices, seeks a Santa Barbara-based Software Engineer to build and scale their data platform. The role involves building ETL processes, integrating with a modern tech stack, collaborating with a team, and architecting for future extensibility.  Experience with deploying production systems and a customer-driven mindset are essential.  The company offers competitive salary, equity, health benefits, a flexible work environment, and the chance to contribute at an early-stage company.",
        "industries": [
            "Software",
            "Healthcare",
            "Dental"
        ],
        "soft_skills": [
            "Sense of Craft",
            "Attention to detail",
            "Reliability",
            "Customer-Driven",
            "Initiative",
            "Collaboration",
            "Communication"
        ],
        "hard_skills": [
            "ETL",
            "Data Warehousing",
            "PostgreSQL",
            "Typescript",
            "GCP Serverless",
            "GraphQL",
            "Prisma",
            "React"
        ],
        "tech_stack": [
            "Typescript",
            "PostgreSQL",
            "GCP Serverless",
            "GraphQL",
            "Prisma",
            "React"
        ],
        "programming_languages": [
            "Typescript"
        ],
        "experience": 2,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Competitive salary",
            "Equity",
            "Healthcare benefits",
            "Meeting-light culture",
            "Flexible vacation/time-off policy"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "California, United States",
        "job_id": 3955872178,
        "company": "LHH",
        "title": "Senior Data Engineer (Healthcare)",
        "created_on": 1720635812.638491,
        "description": "LHH Recruitment Solutions is looking for a Senior Data Engineer for a medical solutions company located in San Francisco, CA. Position is Remote (MUST BE on WEST COAST), Direct Hire/W2, and it is Full Time. Apologies, this client isn't able to offer VISA or any other kind of sponsorship/transfer. Responsibilities: Handle the collection and processing of large-scale raw data. Develop and enhance our analytics engine by creating new tools and solutions that facilitate intuitive data consumption for stakeholders. Construct and improve infrastructure for data extraction, transformation, loading, and cleaning from diverse sources using APIs, SQL, and AWS ‘big data’ technologies. Implement continuous integration and continuous deployment (CI/CD) pipelines for data operations, ensuring efficient and smooth deployment of data models and applications. Address both ad-hoc data requests and core pipeline development tasks. Work closely with a range of stakeholders and functional teams to leverage data insights for driving business results. Detect inefficiencies, optimize processes and data flows, and suggest improvements. Provide mentorship to junior engineers, promoting a collaborative and innovative team atmosphere. Offer technical insights and strategic recommendations regarding data processing infrastructure to the management team. Qualifications: At least 7 years of experience in engineering. Minimum of 7 years in Big Data engineering, covering data ingestion, normalization, and quality assurance. 5 years of implementing CI/CD pipelines Over 2 years of experience handling healthcare claims and clinical data sets. More than 2 years developing data analytics engines and building applications using extensive data sets, preferably with healthcare data. At least 2 years of hands-on experience with Snowflake development 2 years in Python and SQL 3+ years of experience designing and building infrastructure for extraction, transformation, loading, and cleaning of data from a wide variety of sources using APIs, SQL, and AWS ‘big data’ technologies A bachelor's or master's degree in computer science, IT management, or a related field. Expertise in SAS analytics, and ETL. DBT, Terraform (managing SQL and documentation), Airflow experience. Expected salary compensation: 160k -220k standard PTO 40k standard medical, dental, and vision other company perks!",
        "url": "https://www.linkedin.com/jobs/view/3955872178",
        "summary": "Senior Data Engineer needed for a medical solutions company in San Francisco, CA. The position is remote (must be on the West Coast) and offers a salary range of $160,000 - $220,000, standard PTO, 401k, and standard medical, dental, and vision benefits. Responsibilities include handling large-scale data collection and processing, developing analytics engines, building data infrastructure using AWS, implementing CI/CD pipelines, and collaborating with stakeholders to drive business results.",
        "industries": [
            "Healthcare",
            "Medical Solutions",
            "Technology"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Analytical Thinking",
            "Leadership",
            "Mentorship",
            "Strategic Thinking"
        ],
        "hard_skills": [
            "Data Collection",
            "Data Processing",
            "Data Analysis",
            "Data Engineering",
            "Big Data",
            "AWS",
            "SQL",
            "APIs",
            "CI/CD",
            "ETL",
            "Snowflake",
            "Python",
            "SAS",
            "DBT",
            "Terraform",
            "Airflow"
        ],
        "tech_stack": [
            "AWS",
            "Snowflake",
            "Python",
            "SQL",
            "SAS",
            "DBT",
            "Terraform",
            "Airflow"
        ],
        "programming_languages": [
            "Python",
            "SQL"
        ],
        "experience": 7,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Computer Science",
                "IT Management"
            ]
        },
        "salary": {
            "max": 220000,
            "min": 160000
        },
        "benefits": [
            "PTO",
            "401k",
            "Medical",
            "Dental",
            "Vision"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Los Angeles, CA",
        "job_id": 3964812139,
        "company": "Tatari",
        "title": "Senior Data Engineer",
        "created_on": 1720635814.2882054,
        "description": "Tatari is on a mission to revolutionize TV advertising. We work with some of your favorite disruptor brands—like Calm, Fiverr, and RocketMoney—to grow their business through linear and streaming TV. We combine a sophisticated media buying platform with proprietary analytics to turn TV advertising into an automated, digital-like experience. Named one of the Hottest Ad Tech Companies by Business Insider, and Best Places to Work by Inc. Magazine, our team includes founders and leaders from Google, Microsoft, Stripe, Shazam and Facebook. We are growing rapidly as we accelerate our mission to automate the complex landscape of managing and measuring television advertising. We have a long-term goal to make marketing on TV available to businesses of any size.. The Measurement Calculation team is responsible for providing accurate and reliable brand-focused metrics, measurement methodologies, and systems to our clients and Client Service team (CS) so they can create marketing strategies via informed audience targeting. This includes designing, building, and maintaining robust data pipelines that serve as the cornerstone of Tatari's business operations. As dedicated professionals, we collectively work on developing and supporting the intricate calculations and algorithms necessary for both linear and streaming TV platforms. Our focus lies in creating scalable and efficient solutions that enable Tatari to leverage data-driven insights effectively and drive success in the dynamic TV advertising landscape. Responsibilities: Building, managing and optimizing data infrastructure, designing and developing data pipelines, and ensuring the reliability and scalability of data systems. Data Infrastructure Design: Designing and implementing scalable, efficient, and reliable data infrastructure, including data storage, processing, and retrieval systems. Data Pipeline Development: Developing and maintaining robust and efficient data pipelines to ingest, transform, and deliver data from various sources to data storage and analytical systems. Data Modeling and Architecture: Designing and implementing data models and database schemas that support efficient data storage, retrieval, and analysis. ETL (Extract, Transform, Load) Processes: Building and maintaining ETL processes to extract data from different sources, transform it into a suitable format, and load it into data storage systems. Performance Optimization: Identifying and resolving performance bottlenecks in data pipelines and database systems. Tuning and optimizing queries, indexes, and data storage configurations to improve overall system performance. Collaboration and Leadership: Collaborating with cross-functional teams, including data scientists, analysts, and software engineers, to understand their data requirements and provide them with the necessary infrastructure and tools. Mentoring and providing technical guidance to junior data engineers. Monitoring and Troubleshooting: Implementing monitoring systems and practices to ensure the availability and reliability of data systems. Proactively identifying and resolving issues and investigating data-related incidents or anomalies. Technology Evaluation and Implementation: Keeping up with the latest trends and technologies in the data engineering field. Evaluating and recommending new tools, frameworks, and technologies to improve data engineering processes and efficiency. Qualifications: 6+ years of experience working in data architecture, data modeling, and building data pipelines & distributed systems at scale. Recent accomplishments working with relational and NoSQL data stores, methods, and approaches (STAR, Dimensional Modeling). 2+ years of experience with a modern data stack (Kafka, Spark, Airflow, lakehouse architectures, real-time databases, dbt, etc.) and cloud data warehouses such as RedShift, Snowflake. Cloud Computing Platforms: Familiarity with cloud computing platforms like Amazon Web Services (AWS) and proficiency in leveraging cloud-based services for data storage, processing, and analytics, such as Amazon S3, EC2, and Lambda. Strong Technical Background: Proficiency in programming languages commonly used in data engineering, such as Python, Java, Scala, or SQL. Experience with data processing frameworks and tools like Apache Spark (including Databricks), and Hadoop and knowledge of database technologies like SQL databases (e.g., MySQL, PostgreSQL). Problem-solving and analytical thinking: Ability to identify and troubleshoot data-related issues, optimize systems, and propose innovative solutions. Communication and Collaboration: Excellent communication skills to effectively collaborate with cross-functional teams, stakeholders, and business users and ability to explain technical concepts to non-technical audiences and translate business requirements into technical solutions. Leadership and Mentoring: Experience in providing technical guidance, mentoring junior data engineers, and leading data engineering initiatives and ability to drive projects, prioritize tasks, and manage timelines. Benefits: Competitive salary ($170-210K/annually) Equity compensation 100% health insurance premium coverage for you and your dependents Unlimited PTO and sick days Snacks, drinks, and catered lunches at the office Team building events $1000 annual continued education benefit $500 WFH reimbursement $125 pre-tax monthly stipend to spend on whatever you want Annual mental health awareness app reimbursement FSA and commuter benefits Monthly Company Wellness Day Off Hybrid RTO (currently 2 days in office). This is an in-office position At Tatari, we believe in the importance of cultivating teams with diverse backgrounds and offering equal opportunities to all. We strive to create a welcoming, inclusive environment where every team member feels valued and diversity is celebrated.",
        "url": "https://www.linkedin.com/jobs/view/3964812139",
        "summary": "Tatari is seeking a Data Engineer with 6+ years of experience to join their Measurement Calculation team. The ideal candidate will have a strong technical background, experience with cloud data warehouses and modern data stack technologies, and excellent communication and collaboration skills. They will be responsible for building, managing, and optimizing data infrastructure, designing and developing data pipelines, and ensuring the reliability and scalability of data systems.",
        "industries": [
            "Technology",
            "Advertising",
            "Media",
            "Marketing"
        ],
        "soft_skills": [
            "Problem-solving",
            "Analytical thinking",
            "Communication",
            "Collaboration",
            "Leadership",
            "Mentoring"
        ],
        "hard_skills": [
            "Data architecture",
            "Data modeling",
            "Data pipelines",
            "Distributed systems",
            "Relational databases",
            "NoSQL databases",
            "STAR Schema",
            "Dimensional Modeling",
            "Kafka",
            "Spark",
            "Airflow",
            "Lakehouse architectures",
            "Real-time databases",
            "dbt",
            "Redshift",
            "Snowflake",
            "AWS",
            "S3",
            "EC2",
            "Lambda",
            "Python",
            "Java",
            "Scala",
            "SQL",
            "Apache Spark",
            "Databricks",
            "Hadoop",
            "MySQL",
            "PostgreSQL"
        ],
        "tech_stack": [
            "Kafka",
            "Spark",
            "Airflow",
            "dbt",
            "Redshift",
            "Snowflake",
            "AWS",
            "S3",
            "EC2",
            "Lambda",
            "Apache Spark",
            "Databricks",
            "Hadoop",
            "MySQL",
            "PostgreSQL"
        ],
        "programming_languages": [
            "Python",
            "Java",
            "Scala",
            "SQL"
        ],
        "experience": 6,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 210000,
            "min": 170000
        },
        "benefits": [
            "Competitive salary",
            "Equity compensation",
            "100% health insurance premium coverage",
            "Unlimited PTO and sick days",
            "Snacks, drinks, and catered lunches at the office",
            "Team building events",
            "Annual continued education benefit",
            "WFH reimbursement",
            "Monthly stipend",
            "Annual mental health awareness app reimbursement",
            "FSA and commuter benefits",
            "Monthly Company Wellness Day Off",
            "Hybrid RTO"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Ontario, CA",
        "job_id": 3933016225,
        "company": "Capgemini",
        "title": "Software Engineer - Data Engineer",
        "created_on": 1720635815.988588,
        "description": "Job Description As a Senior Engineer, you will build distributed data processing solution and highly loaded database solutions for various cases including reporting, product analytics, marketing optimization and financial reporting. Supply as part of self-organized team of authority data engineers working in an exciting, innovative environment for our client, creating the foundation for decision-making at a company dealing with billions of events per day. Investigate, create, and implement the solutions for existing technical challenges. Provide guidance, instruction, direction, leadership to a development team with the purpose of achieving project goals. Responsibilities Obtains tasks from the project lead or Team Lead (TL), prepares functional and design specifications, approves them with all customers. Ensures that assigned area/areas are delivered within set deadlines and required quality objectives. Provides estimations, agrees task duration with the manager and gives to project plan of assigned area. Analyzes scope of alternative solutions and makes decision about area implementation based on their experience and technical expertise. Leads functional and architectural design of assigned areas. Makes sure design decisions on the project meet architectural and design requirements. Addresses area-level risks, provides and implements mitigation plan. Reports about area readiness/quality, and raises red flags in crisis situations which are beyond their AOR. Responsible for resolving crisis situations within their AOR. Initiates and conducts code reviews, creates code standards, conventions, and guidelines. Suggests technical and functional improvements to give to the product. Constantly improves their professional level. Collaborates with other teams. Requirements 5+ years of professional experience. University degree or equivalent experience in Computer Related Sciences or similar. Advanced proficiency in Complex SQL In-depth knowledge of Data Warehousing and Data Modeling techniques Proficiency in Python programming languages. Proficiency in Airflow for efficient workflow automation Experience with Spark/EC2/S3 will be effective. Expertise in Analytics and Reporting Familiarity with Snowflake/Databricks proven experience in ETL orchestration and workflow management tool Airflow. Authority in Database fundamentals, SQL and distributed computing. Experience in Spark, Snowflake & Databricks. Excellent communication skills and experience working with technical and non-technical teams. Able to clear hacker rank code test. Experience in AWS (EC2/S2/IAM) would be plus. confirmed experience with the Distributed data/similar ecosystem (Hive, Presto) and/or streaming technologies such as Kafka/Flink- would be plus. Experience working with Redshift, PostgreSQL and/or other DBMS platforms would be plus. Life at Capgemini Capgemini Supports All Aspects Of Your Well-being Throughout The Changing Stages Of Your Life And Career. For Eligible Employees, We Offer Collaborating with teams of creative, fun, and driven colleagues Flexible work options enabling time and location-based flexibility Company-provided home office equipment Virtual collaboration and productivity tools to enable hybrid teams Comprehensive benefits program (Health, Welfare, Retirement and Paid time off) Other perks and wellness benefits like discount programs, and gym/studio access. Paid Parental Leave and coaching, baby welcome gift, and family care/illness days Back-up childcare/elder care, childcare discounts, and subsidized virtual tutoring Tuition assistance and weekly hot skill development opportunities Experiential, high-impact learning series events Access to mental health resources and mindfulness programs Access to join Capgemini Employee Resource Groups around communities of interest About Capgemini Capgemini is a global business and technology transformation partner, helping organizations to accelerate their dual transition to a digital and sustainable world, while creating tangible impact for enterprises and society. It is a responsible and diverse group of 340,000 team members in more than 50 countries. With its strong over 55-year heritage, Capgemini is trusted by its clients to unlock the value of technology to address the entire breadth of their business needs. It delivers end-to-end services and solutions leveraging strengths from strategy and design to engineering, all fueled by its market leading capabilities in AI, cloud and data, combined with its deep industry expertise and partner ecosystem. The Group reported 2023 global revenues of €22.5 billion. Get The Future You Want | www.capgemini.com Disclaimer Capgemini is an Equal Opportunity Employer encouraging diversity in the workplace. All qualified applicants will receive consideration for employment without regard to race, national origin, gender identity/expression, age, religion, disability, sexual orientation, genetics, veteran status, marital status or any other characteristic protected by law. This is a general description of the Duties, Responsibilities and Qualifications required for this position. Physical, mental, sensory or environmental demands may be referenced in an attempt to communicate the manner in which this position traditionally is performed. Whenever necessary to provide individuals with disabilities an equal employment opportunity, Capgemini will consider reasonable accommodations that might involve varying job requirements and/or changing the way this job is performed, provided that such accommodations do not pose an undue hardship. Capgemini is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to your recruiting contact. Click the following link for more information on your rights as an Applicant http://www.capgemini.com/resources/equal-employment-opportunity-is-the-law Please be aware that Capgemini may capture your image (video or screenshot) during the interview process and that image may be used for verification, including during the hiring and onboarding process. Applicants for employment in Canada must have valid work authorization that does not now and/or will not in the future require sponsorship of a visa for employment authorization in Canada by Capgemini.",
        "url": "https://www.linkedin.com/jobs/view/3933016225",
        "summary": "Senior Engineer to build distributed data processing and database solutions for reporting, analytics, and financial reporting. Lead a team of data engineers, ensuring high quality and timely delivery. Work in a fast-paced environment handling billions of events per day.",
        "industries": [
            "Data Engineering",
            "Analytics",
            "Software Engineering",
            "Technology",
            "Financial Services",
            "Marketing"
        ],
        "soft_skills": [
            "Communication",
            "Leadership",
            "Problem Solving",
            "Collaboration",
            "Decision Making",
            "Time Management",
            "Organization"
        ],
        "hard_skills": [
            "SQL",
            "Data Warehousing",
            "Data Modeling",
            "Python",
            "Airflow",
            "Spark",
            "EC2",
            "S3",
            "Analytics",
            "Reporting",
            "Snowflake",
            "Databricks",
            "ETL",
            "Workflow Management",
            "Database Fundamentals",
            "Distributed Computing",
            "AWS",
            "Hive",
            "Presto",
            "Kafka",
            "Flink",
            "Redshift",
            "PostgreSQL",
            "DBMS"
        ],
        "tech_stack": [
            "SQL",
            "Python",
            "Airflow",
            "Spark",
            "EC2",
            "S3",
            "Snowflake",
            "Databricks",
            "AWS",
            "Hive",
            "Presto",
            "Kafka",
            "Flink",
            "Redshift",
            "PostgreSQL"
        ],
        "programming_languages": [
            "Python",
            "SQL"
        ],
        "experience": 5,
        "education": {
            "min_degree": "University Degree",
            "fields": [
                "Computer Related Sciences"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Flexible Work",
            "Home Office Equipment",
            "Virtual Collaboration Tools",
            "Health Insurance",
            "Welfare Benefits",
            "Retirement Plan",
            "Paid Time Off",
            "Discount Programs",
            "Gym Access",
            "Paid Parental Leave",
            "Coaching",
            "Baby Welcome Gift",
            "Family Care/Illness Days",
            "Back-up Childcare/Elder Care",
            "Childcare Discounts",
            "Subsidized Virtual Tutoring",
            "Tuition Assistance",
            "Skill Development Opportunities",
            "Learning Events",
            "Mental Health Resources",
            "Mindfulness Programs",
            "Employee Resource Groups"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Mountain View, CA",
        "job_id": 3921166312,
        "company": "Genesis10",
        "title": "Software Engineer",
        "created_on": 1720635817.584511,
        "description": "Our client, the world's leading search engine and technology company, is seeking a Software Engineer. This is a 6 month + contract and hybrid remote position located in Mountain View, CA. Summary: We're looking for software engineers interested in improving the coding capabilities of client Bard/Gemini. The primary focus will be on improving the quality of data. This will require manual review of coding datasets and cross-functional collaboration for solution design. Responsibilities: Data Quality Assurance: Regularly review code data, identify bugs & patterns and areas of improvement to improve data quality. Stakeholder Collaboration and Development: Ability to document & provide constructive feedback while developing and implementing frameworks, guidelines, and tools to enable high-quality data delivery that meets long-term project needs and scales effectively. Dataset Creation and Curation: Develop and refine datasets of coding use cases. Requirements: Bachelor's degree or equivalent practical experience. 4 years of experience in software development in multiple programming languages, and with data structures/algorithms. 2-4 years of experience testing, auditing and analyzing data. Proficiency in at least 5 out of these programming languages: Java. Python. C/C++. SQL. Javascript. HTML/CSS. Scala. Ruby. Only candidates available and ready to work directly as Genesis10 employees will be considered for this position. Desired Master's degree or PhD in Engineering, Computer Science, or a related technical field. 3 years of experience in a technical leadership role leading project teams and setting technical direction. Experience working on improving data quality for large language models. Ability to convince business stakeholders and communicate complex analysis insights to non-technical audiences. Background in AI/LLM industry Pay rate range: $72.20 - $92.64 hourly If you have the described qualifications and are interested in this exciting opportunity, please apply! Ranked a Top Staffing Firm in the U.S. by Staffing Industry Analysts for six consecutive years, Genesis10 puts thousands of consultants and employees to work across the United States every year in contract, contract-for-hire, and permanent placement roles. With more than 300 active clients, Genesis10 provides access to many of the Fortune 100 firms and a variety of mid-market organizations across the full spectrum of industry verticals. For contract roles, Genesis10 offers the benefits listed below. If this is a perm-placement opportunity, our recruiter can talk you through the unique benefits offered for that particular client. Benefits of Working with Genesis10 : Access to hundreds of clients, most who have been working with Genesis10 for 5-20+ years. The opportunity to have a career-home in Genesis10; many of our consultants have been working exclusively with Genesis10 for years. Access to an experienced, caring recruiting team (more than 7 years of experience, on average.) Behavioral Health Platform Medical, Dental, Vision Voluntary Hospital Indemnity (Critical Illness & Accident) Voluntary Term Life Insurance 401K Sick Pay (for applicable states/municipalities) Commuter Benefits (Dallas, NYC, SF) Remote opportunities available For multiple years running, Genesis10 has been recognized as a Top Staffing Firm in the U.S., as a Best Company for Work-Life Balance, as a Best Company for Career Growth, for Diversity, and for Leadership, amongst others. To learn more and to view all our available career opportunities, please visit us at our website. Genesis10 is an Equal Opportunity Employer. Candidates will receive consideration without regard to their race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.",
        "url": "https://www.linkedin.com/jobs/view/3921166312",
        "summary": "Software Engineer focused on improving the quality of coding data for client Bard/Gemini. Responsibilities include data quality assurance, stakeholder collaboration, and dataset creation and curation. The role involves manual review of coding datasets, identifying bugs and patterns, and working cross-functionally to implement solutions for data quality improvement.",
        "industries": [
            "Technology",
            "Software Development",
            "Artificial Intelligence",
            "Search Engines",
            "Data Science"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Analytical Skills",
            "Critical Thinking",
            "Attention to Detail",
            "Technical Leadership",
            "Project Management"
        ],
        "hard_skills": [
            "Software Development",
            "Data Structures",
            "Algorithms",
            "Data Quality Assurance",
            "Data Testing",
            "Data Auditing",
            "Data Analysis",
            "Java",
            "Python",
            "C/C++",
            "SQL",
            "Javascript",
            "HTML",
            "CSS",
            "Scala",
            "Ruby",
            "AI",
            "LLM",
            "Technical Writing"
        ],
        "tech_stack": [
            "Bard",
            "Gemini",
            "Large Language Models",
            "AI",
            "Data Quality Tools"
        ],
        "programming_languages": [
            "Java",
            "Python",
            "C/C++",
            "SQL",
            "Javascript",
            "HTML",
            "CSS",
            "Scala",
            "Ruby"
        ],
        "experience": 4,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Engineering",
                "Computer Science",
                "Technical Fields"
            ]
        },
        "salary": {
            "max": 9264,
            "min": 7220
        },
        "benefits": [
            "Medical",
            "Dental",
            "Vision",
            "Hospital Indemnity",
            "Term Life Insurance",
            "401K",
            "Sick Pay",
            "Commuter Benefits",
            "Remote opportunities"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Oakland, CA",
        "job_id": 3966712220,
        "company": "SPECTRAFORCE",
        "title": "Data Engineer II",
        "created_on": 1720635819.5096421,
        "description": "Job Title: Data Engineer Duration: 9+ Months Location: Remote Job Description In this role as a Data Engineer on the Banking Data Science team, your primary responsibility will be to build scalable and robust data solutions to comply with global regulatory requirements and build data automation mechanisms that are critical to operating Square Banking in licensed states and countries. You will be expected to partner with regulatory, compliance and operations teams across Square to prepare for banking license application or renewal, providing guidance to stakeholders on the effective use of technologies and performance optimization. You Will Build and maintain reliable data pipelines to meet regulatory and compliance data requirements for all the banking products, including but not limited to Global Loans, Credit Cards, Square Card, and Savings, etc. Deploy data quality checks to ensure data integrity and scalability, improving data validation and monitoring processes to proactively prevent and identify data issues. Constantly improve banking regulatory processes with advanced data streaming and reporting tools to enable quick and easy consumption of data for our legal partners. Manage communication with regulatory teams for request intakes, data clarification and report delivery. Skills 3+ years of data engineering experience or other quantitative field Advanced proficiency in SQL to produce summary data reports. Experience in designing and implementing ETL (Extract, Transform, Load) pipelines for data ingestion with ETL scheduling technologies such as Airflow and Prefect. Have experience building visualization dashboards (e.g. Looker), automating frequently used queries to generate data insights. Understanding of data quality management and data governance best practices. Excellent problem solving skills, being a fast learner by understanding business requirements and data architecture within a short period of time. Can work under pressure and prioritize multiple tasks concurrently in order to meet deadlines and clarity. Education Bachelor’s degree in Computer Science, Engineering, Math, Finance, Statistics or related discipline, or equivalent combination of education and experience Notice to California Applicants: SPECTRA FORCE ® is committed to complying with the California Privacy Rights Act (“CPRA”) effective January 1, 2023; and all data privacy laws in the jurisdictions in which it recruits and hires employees. A Notice to California Job Applicants Regarding the Collection of Personal Information can be located on our website. Applicants with disabilities may access this notice in an alternative format by contacting NAHR@spectraforce.com . About Us: Established in 2004, SPECTRA FORCE ® is one of the largest and fastest-growing diversity-owned staffing firms in the US. The growth of our company is a direct result of our global client service delivery model that is powered by our state-of-the-art A.I. proprietary talent acquisition platform, robust ISO 9001:2015/ISO 27001 certified processes, and strong and passionate client engaged teams. We have built our business by providing talent and project-based solutions, including Contingent, Permanent, and Statement of Work (SOW) services to over 140 clients in the US, Canada, Puerto Rico, Costa Rica, and India. Key industries that we service include Technology, Financial Services, Life Sciences, Healthcare, Telecom, Retail, Utilities and Transportation. SPECTRA FORCE is built on a concept of “human connection,” defined by our branding attitude of NEWJOBPHORIA®, which is the excitement of bringing joy and freedom to the work lifestyle so our people and clients can reach their highest potential. Learn more at: http://www.spectraforce.com Benefits: SPECTRA FORCE offers ACA compliant health benefits as well as dental, vision, accident, critical illness, voluntary life, and hospital indemnity insurances to eligible employees. Additional benefits offered to eligible employees include commuter benefits, 401K plan with matching, and a referral bonus program. SPECTRA FORCE provides unpaid leave as well as paid sick leave when required by law. Equal Opportunity Employer: SPECTRA FORCE is an equal opportunity employer and does not discriminate against any employee or applicant for employment because of race, religion, color, sex, national origin, age, sexual orientation, gender identity, genetic information, disability or veteran status, or any other category protected by applicable federal, state, or local laws. Please contact Human Resources at LOA@spectraforce.com if you require reasonable accommodation.",
        "url": "https://www.linkedin.com/jobs/view/3966712220",
        "summary": "As a Data Engineer on the Banking Data Science team, you will build scalable and robust data solutions to comply with global regulatory requirements, build data automation mechanisms, and partner with regulatory, compliance and operations teams to prepare for banking license applications. You will build and maintain data pipelines, deploy data quality checks, improve regulatory processes, and manage communication with regulatory teams.",
        "industries": [
            "Financial Services",
            "Technology",
            "Banking"
        ],
        "soft_skills": [
            "Problem Solving",
            "Communication",
            "Fast Learner",
            "Prioritization"
        ],
        "hard_skills": [
            "SQL",
            "ETL",
            "Airflow",
            "Prefect",
            "Looker",
            "Data Quality Management",
            "Data Governance"
        ],
        "tech_stack": [
            "SQL",
            "ETL",
            "Airflow",
            "Prefect",
            "Looker"
        ],
        "programming_languages": [
            "SQL"
        ],
        "experience": 3,
        "education": {
            "min_degree": "Bachelor’s degree",
            "fields": [
                "Computer Science",
                "Engineering",
                "Math",
                "Finance",
                "Statistics"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Health Insurance",
            "Dental Insurance",
            "Vision Insurance",
            "Accident Insurance",
            "Critical Illness Insurance",
            "Life Insurance",
            "Hospital Indemnity Insurance",
            "Commuter Benefits",
            "401K Plan with Matching",
            "Referral Bonus Program",
            "Unpaid Leave",
            "Paid Sick Leave"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3959582227,
        "company": "Gantri",
        "title": "Senior Data Engineer",
        "created_on": 1720635821.3002312,
        "description": "Company Gantri is the world’s first digital manufacturer for creative lighting. We help independent designers, studios, and influencers to develop original, sustainably made lighting designs and sell directly to consumers. We manufacture and fulfill all orders on-demand using 3D printing from innovative plant-based materials. Since launching in 2017, we’ve collaborated with the world’s best designers and studios, including Ammunition (Beats by Dre), Smart Design (OXO), and Karim Rashid. We’re the most awarded lighting brand in the US, winning Time’s Best Inventions, Fast Company’s Most Innovative in Design, and many other awards. Role Join Gantri as our first ever Data Engineer! Gantri is a highly collaborative, vertically integrated organization that operates across manufacturing, hardware, software, product design, and marketing. We believe that data is the key to helping us prioritize the right problems to solve and deliver even better products for our customers. As our founding Data Engineer, you’ll have the opportunity to shape our data strategy and architecture. You’ll get to design and implement scalable data pipelines, build robust data warehousing solutions, and enable advanced analytics capabilities. This role offers immense growth potential as you establish data engineering best practices across the company and mentor other team members. You’ll sit within our software organization and collaborate closely with all teams across the company. Responsibilities Design, build, and maintain scalable and efficient data pipelines to collect, process, and store data from various sources including our in-house manufacturing software and e-commerce platforms Develop and maintain data warehouse architecture to ensure reliability, accuracy, performance, and accessibility of data for analytics and reporting purposes Collaborate with software engineers to integrate data collection and instrumentation into existing and new systems, ensuring data quality and consistency Create data models and schema designs to support reporting, analytics, and visualization needs of different teams within the organization Implement data governance and security best practices to protect sensitive information and ensure compliance with regulations Choose, design and develop data visualization dashboards and tools to enable stakeholders to monitor key performance metrics and make data-driven decisions Conduct exploratory data analysis and provide insights to support strategic decision-making and identify opportunities for business improvement Stay current with industry trends, tools, and technologies related to data engineering, analytics, and visualization, and recommend and implement improvements to our data infrastructure and processes Requirements Bachelor's degree or higher in Computer Science, Engineering, or a related field Proven experience (5 years+) as a data engineer or similar role, with a strong track record of building and maintaining data infrastructure and pipelines Proficiency in SQL and experience working with relational databases (e.g., PostgreSQL, MySQL) and data warehousing solutions (e.g., Redshift, BigQuery) Experience with cloud platforms such as AWS, Azure, or Google Cloud, and familiarity with services like S3, EC2, EMR, or equivalent Strong programming skills in languages such as Python, Java, or Scala, and experience with data processing frameworks like Apache Spark or Apache Flink Experience with data visualization tools such as Tableau, Looker, or Power BI, and proficiency in data manipulation and visualization techniques Excellent communication and collaboration skills, with the ability to work effectively in cross-functional teams and translate business requirements into technical solutions Strong problem-solving skills and attention to detail, with a passion for continuous learning and improvement Bonus: Experience working in a direct-to-consumer or manufacturing industry, with knowledge of manufacturing processes and systems Bonus: Experience with machine learning, predictive analytics, or data science techniques Benefits Competitive salary and equity Medical, dental and vision insurance 401k Paid vacation days and paid holidays Work from X benefits Access to 3D printers for your personal projects Monthly team lunches And much more",
        "url": "https://www.linkedin.com/jobs/view/3959582227",
        "summary": "Gantri, a digitally manufactured lighting company, is seeking their first Data Engineer to design and implement scalable data pipelines, build data warehousing solutions, and enable advanced analytics. This role offers the opportunity to shape the company's data strategy and architecture, establishing best practices and mentoring team members.",
        "industries": [
            "Manufacturing",
            "E-commerce",
            "Technology",
            "Design",
            "Lighting"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem-solving",
            "Attention to Detail",
            "Continuous Learning",
            "Teamwork",
            "Leadership"
        ],
        "hard_skills": [
            "SQL",
            "PostgreSQL",
            "MySQL",
            "Redshift",
            "BigQuery",
            "AWS",
            "Azure",
            "Google Cloud",
            "S3",
            "EC2",
            "EMR",
            "Python",
            "Java",
            "Scala",
            "Apache Spark",
            "Apache Flink",
            "Tableau",
            "Looker",
            "Power BI",
            "Data Manipulation",
            "Data Visualization",
            "Machine Learning",
            "Predictive Analytics"
        ],
        "tech_stack": [
            "SQL",
            "PostgreSQL",
            "MySQL",
            "Redshift",
            "BigQuery",
            "AWS",
            "Azure",
            "Google Cloud",
            "S3",
            "EC2",
            "EMR",
            "Python",
            "Java",
            "Scala",
            "Apache Spark",
            "Apache Flink",
            "Tableau",
            "Looker",
            "Power BI"
        ],
        "programming_languages": [
            "Python",
            "Java",
            "Scala"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Computer Science",
                "Engineering"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Competitive Salary",
            "Equity",
            "Medical, Dental, and Vision Insurance",
            "401k",
            "Paid Vacation",
            "Paid Holidays",
            "Work From Home Benefits",
            "Access to 3D Printers",
            "Monthly Team Lunches"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco Bay Area",
        "job_id": 3956391882,
        "company": "Fractal",
        "title": "Software Development Engineer",
        "created_on": 1720635823.0109098,
        "description": "Fractal Analytics is a strategic AI partner to Fortune 500 companies with a vision to power every human decision in the enterprise. Fractal is building a world where individual choices, freedom, and diversity are the greatest assets. An ecosystem where human imagination is at the heart of every decision. Where no possibility is written off, only challenged to get better. We believe that a true Fractalite empowers imagination with intelligence. And that it will be such Fractalites that will continue to build the company for the next 100 years. **Please Note: This role is specifically located in the North Bay area of San Francisco. You'll need to be onsite Monday through Friday.** Role Overview We're looking for a Software Development Engineer to play a pivotal role in developing and deploying cutting-edge, AI-driven enterprise applications using our client’s proprietary AI Platform. This role involves crafting and enhancing applications and diving deep into problem-solving, performance optimization, and comprehensive documentation. You'll be expected to work closely with our client, offering technical expertise and innovative solutions to meet their needs. Responsibilities Design, develop, and deploy full-stack, AI-centric enterprise applications on the client AI Platform, ensuring their integration with advanced data solutions. Lead the architecture, development, and maintenance of comprehensive data integration systems, employing efficient ETL processes using Python, Pandas, and NumPy for large-scale data manipulation. Apply JavaScript or another object-oriented language (e.g., Python, C#) in client environments to seamlessly integrate various system functionalities. Proactively test, diagnose, and refine software applications alongside clients, aiming for high quality and optimal functionality. Identify and rectify performance issues within applications and integrated data systems, focusing on efficiency, optimization, and data integrity. Utilize shell scripting and cron job scheduling for automating routine data operations, ETL tasks, and ensuring data accuracy and harmonization. Actively participate in and lead the design and review processes for both internal and client software applications. Implement and manage version control using Git, adeptly handling repository operations including rebase, pull, push, and branch management. Create detailed application specifications and maintain precise documentation throughout the software lifecycle, guaranteeing the integrity and harmonization of all integrated data. Qualifications: A bachelor’s degree in computer science, a related field, or an equivalent combination of education and experience. Proficient in software development with JavaScript and Python. Experience with version control systems, preferably Git. Strong experience in shell scripting and cron job scheduling for automating data processes. Solid background in working with various JavaScript frameworks (e.g., React, Redux, Vue, Backbone, Angular). Experience in deploying software on leading cloud computing platforms (such as GCP, AWS, Azure). Understanding of both SQL and NoSQL database technologies. Strong skills in data structures, algorithm design, and implementation. Proficiency in handling and analyzing time-series data, including its cleansing and normalization. Familiarity with Agile software development methodologies. Pay: The wage range for this role takes into account the wide range of factors that are considered in making compensation decisions, including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. At Fractal, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. A reasonable estimate of the current range is: $100,000 - $180,000. Benefits: As a full-time employee of the company or as an hourly employee working more than 30 hours per week, you will be eligible to participate in the health, dental, vision, life insurance, and disability plans in accordance with the plan documents, which may be amended from time to time. You will be eligible for benefits on the first day of employment with the Company. In addition, you are eligible to participate in the Company 401(k) Plan after 30 days of employment, in accordance with the applicable plan terms. The Company provides for 11 paid holidays and 12 weeks of Parental Leave. We also follow a “free time” PTO policy, allowing you the flexibility to take the time needed for either sick time or vacation. Fractal provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.",
        "url": "https://www.linkedin.com/jobs/view/3956391882",
        "summary": "Fractal Analytics seeks a Software Development Engineer to build and deploy AI-driven enterprise applications using their proprietary platform. Responsibilities include full-stack development, data integration with Python/Pandas, JavaScript integration, performance optimization, and documentation. The role requires strong technical expertise, problem-solving skills, and collaboration with clients.",
        "industries": [
            "Artificial Intelligence",
            "Software Development",
            "Data Science",
            "Technology",
            "Consulting"
        ],
        "soft_skills": [
            "Problem-solving",
            "Collaboration",
            "Communication",
            "Technical Expertise",
            "Innovation"
        ],
        "hard_skills": [
            "Python",
            "JavaScript",
            "Pandas",
            "NumPy",
            "SQL",
            "NoSQL",
            "Git",
            "Shell Scripting",
            "Cron Jobs",
            "React",
            "Redux",
            "Vue",
            "Backbone",
            "Angular",
            "GCP",
            "AWS",
            "Azure",
            "Data Structures",
            "Algorithm Design",
            "Time-Series Data Analysis",
            "Agile"
        ],
        "tech_stack": [
            "Python",
            "Pandas",
            "NumPy",
            "JavaScript",
            "React",
            "Redux",
            "Vue",
            "Backbone",
            "Angular",
            "GCP",
            "AWS",
            "Azure",
            "SQL",
            "NoSQL",
            "Git",
            "Shell Scripting",
            "Cron Jobs"
        ],
        "programming_languages": [
            "Python",
            "JavaScript"
        ],
        "experience": 0,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Related Field"
            ]
        },
        "salary": {
            "max": 180000,
            "min": 100000
        },
        "benefits": [
            "Health Insurance",
            "Dental Insurance",
            "Vision Insurance",
            "Life Insurance",
            "Disability Insurance",
            "401(k) Plan",
            "Paid Holidays",
            "Parental Leave",
            "Paid Time Off"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Mountain View, CA",
        "job_id": 3929603229,
        "company": "Nuro",
        "title": "Software Engineer, Cloud Infrastructure",
        "created_on": 1720635826.2541175,
        "description": "Who We Are Nuro exists to better everyday life through robotics. The company’s custom electric autonomous vehicles are designed to bring the things you need—from produce to prescriptions—right to your home. Nuro’s autonomous, goods-focused solution can give you valuable time back and more freedom to do what you love. This convenient, eco-friendly alternative to driving has the potential to make streets safer and cities more livable. About The Work Collaborate and work cross-functionally amongst teams are Nuro to align on customer needs. Build high-quality and scalable solutions for infrastructure-as-code, application deployment, production system monitoring, health status tracking, and performance improvements. Build automation to improve developer experience, infrastructure reliability, and security. About You You are experienced with Terraform, Docker, and Kubernetes. You have software development skills, preferably in Python or Go You are experienced with developing in a public cloud environment. You have excellent communication skills. You are experienced with building, scaling, and maintaining production systems. You possess a bachelor's degree in Computer Science, Electrical Engineering, or a closely related field or 3+ years of equivalent experience. You have start-up DNA: You have demonstrated an ability to thrive in the ambiguity of a dynamic, start-up environment or have the DNA to do so. Bonus Points You have cloud/hybrid networking experience. You have software architecture experience. You have experience in deploying and managing large distributed applications (e.g. with Kubernetes and ArgoCD) At Nuro, your base pay is one part of your total compensation package. For this position, the reasonably expected base pay range is between $125,000 and $188,000 for the level at which this job has been scoped. Your base pay will depend on several factors, including your experience, qualifications, education, location, and skills. In the event that you are considered for a different level, a higher or lower pay range would apply. This position is also eligible for an annual performance bonus, equity, and a competitive benefits package. At Nuro, we celebrate differences and are committed to a diverse workplace that fosters inclusion and psychological safety for all employees. Nuro is proud to be an equal opportunity employer and expressly prohibits any form of workplace discrimination based on race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, genetic information, disability, veteran status, or any other legally protected characteristics.",
        "url": "https://www.linkedin.com/jobs/view/3929603229",
        "summary": "Nuro is seeking a skilled DevOps Engineer to build and maintain scalable infrastructure, automation, and production systems. This role involves collaborating with cross-functional teams, developing in a public cloud environment, and improving developer experience, infrastructure reliability, and security. The ideal candidate possesses strong expertise in Terraform, Docker, Kubernetes, Python or Go, and experience with building and scaling production systems. A Bachelor's degree in Computer Science, Electrical Engineering, or a related field, or equivalent experience, is required.",
        "industries": [
            "Robotics",
            "Autonomous Vehicles",
            "Technology",
            "Software Development",
            "Cloud Computing",
            "Infrastructure"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem-solving",
            "Adaptability",
            "Ambition",
            "Teamwork",
            "Self-motivation"
        ],
        "hard_skills": [
            "Terraform",
            "Docker",
            "Kubernetes",
            "Python",
            "Go",
            "Cloud Computing",
            "Infrastructure-as-Code",
            "Application Deployment",
            "Production System Monitoring",
            "Health Status Tracking",
            "Performance Improvement",
            "Automation",
            "Security",
            "Software Development"
        ],
        "tech_stack": [
            "Terraform",
            "Docker",
            "Kubernetes",
            "Python",
            "Go",
            "ArgoCD"
        ],
        "programming_languages": [
            "Python",
            "Go"
        ],
        "experience": 3,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Computer Science",
                "Electrical Engineering"
            ]
        },
        "salary": {
            "max": 188000,
            "min": 125000
        },
        "benefits": [
            "Performance Bonus",
            "Equity",
            "Competitive Benefits Package"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Pasadena, CA",
        "job_id": 3938881106,
        "company": "Misapplied Sciences, Inc.",
        "title": "Software Engineer - Cloud/Data",
        "created_on": 1720635827.88439,
        "description": "Misapplied Sciences is a VC-backed startup bringing an unprecedented innovation to the world. Parallel Reality displays are a new technology where many people, standing shoulder-to-shoulder looking at the same display at the same time, can each see something different . The technology can work for crowds of up to thousands of people, and no glasses are required. With Parallel Reality technology, public venues such as airports, stadiums, shopping malls, and resorts can be personalized for every individual simultaneously. We're seeking a Software Engineer to drive the development of the cloud and data platform that empowers creatives and engineers to tap the capabilities of a powerful new technology. As part of our team, you'll join us on the world stage. Your contributions will be experienced by millions at popular venues and events hosted by internationally renowned companies. Your colleagues will include former Disney Imagineers , Microsoft Researchers , and Stanford and Caltech alums. You'll help shape our products and experiences, with the opportunity to collaborate on highly unique software, hardware, and creative challenges. Our lab is in Old Town Pasadena - Southern California’s famous historic, cultural, and dining destination. This role will benefit from experience/interest in one or more of the following: Architecting high-performance, scalable, full-stack cloud applications Developing intuitive, delightful web interfaces and data visualization Establishing strong software engineering discipline and data security best practices AWS, GCP, Azure Python, React, REST APIs, SQL, Network Programming Please visit us at https://www.misappliedsciences.com for more information.",
        "url": "https://www.linkedin.com/jobs/view/3938881106",
        "summary": "Misapplied Sciences is a startup developing Parallel Reality displays, a new technology allowing for personalized experiences for large crowds without glasses.  They are seeking a Software Engineer to develop the cloud and data platform for this technology, requiring experience in full-stack cloud application architecture, web interfaces, data visualization, software engineering discipline, and data security best practices. The ideal candidate will have skills in AWS, GCP, Azure, Python, React, REST APIs, SQL, and Network Programming.",
        "industries": [
            "Technology",
            "Computer Software",
            "Artificial Intelligence",
            "Virtual Reality",
            "Augmented Reality"
        ],
        "soft_skills": [
            "Problem Solving",
            "Communication",
            "Collaboration",
            "Teamwork",
            "Creativity",
            "Innovation",
            "Data Security",
            "Engineering Discipline",
            "Time Management",
            "Organization"
        ],
        "hard_skills": [
            "Cloud Application Architecture",
            "Web Interface Development",
            "Data Visualization",
            "Software Engineering Discipline",
            "Data Security",
            "AWS",
            "GCP",
            "Azure",
            "Python",
            "React",
            "REST APIs",
            "SQL",
            "Network Programming"
        ],
        "tech_stack": [
            "AWS",
            "GCP",
            "Azure",
            "Python",
            "React",
            "REST APIs",
            "SQL",
            "Network Programming"
        ],
        "programming_languages": [
            "Python",
            "React"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Fremont, CA",
        "job_id": 3841222575,
        "company": "Info Way Solutions",
        "title": "Role: Data�Engineer� �",
        "created_on": 1720635829.8986926,
        "description": "Hi Professionals, Hope you are doing good Job Description This is Sangeetha from Info Way Solutions, LLC We have job opening for Data Scientist and the detailed Job description is given below: Kindly check the JD and share your view Role: Data Engineer Location: Montreal Canada ( Hybrid - 3days in a week) Responsibilities Defining and evaluating key metrics and understanding what moves them and why Ownership of conceptualizing, developing, and maintaining dashboards and visualizations Investigating evolving fraud trends to extract patterns, identify root causes and propose actionable solutions Communicating analyses and recommendations to cross functional stakeholders for decision making Empowering the team to answer data questions quickly and easily by building high-quality ground truth data sets Build machine learning models to detect high risk users and financial crime activities such as money laundering Required Skills 8+ years of professional industry experience in a quantitative analysis role Comfortable in SQL and some experience with a programming language (Python or R a plus) Ability to communicate clearly and effectively to cross functional partners of varying technical levels Ability to define relevant metrics that can guide and influence stakeholders to the appropriate and accurate insights Experience or willingness to learn tools to create data pipelines using Airflow Thanks & Regards, Sangeetha Email: sangeetha@Infowaygroup.com Direct: (925)241-4886 Work: (925)-592-6160 Ext 104 https://www.linkedin.com/in/sangeetha-kannan-291636206/ Info Way Solutions LLC | 46520 Fremont Blvd, Suite 614 | Fremont, CA - 94538",
        "url": "https://www.linkedin.com/jobs/view/3841222575",
        "summary": "Data Engineer role at Info Way Solutions in Montreal, Canada. Responsibilities include defining and evaluating key metrics, building dashboards and visualizations, investigating fraud trends, communicating analyses, empowering the team with data, and building machine learning models for fraud detection. Requires 8+ years of experience in quantitative analysis, SQL skills, programming (Python or R), communication skills, and experience with data pipelines using Airflow.",
        "industries": [
            "Data Science",
            "Analytics",
            "Financial Services",
            "Fraud Detection"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Data Analysis",
            "Decision Making"
        ],
        "hard_skills": [
            "SQL",
            "Python",
            "R",
            "Airflow",
            "Machine Learning",
            "Data Pipelines",
            "Data Visualization",
            "Dashboards",
            "Fraud Detection",
            "Money Laundering",
            "Metrics Definition"
        ],
        "tech_stack": [
            "SQL",
            "Python",
            "R",
            "Airflow"
        ],
        "programming_languages": [
            "SQL",
            "Python",
            "R"
        ],
        "experience": 8,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3958354491,
        "company": "Candid Health",
        "title": "Software Engineer",
        "created_on": 1720635833.9183314,
        "description": "What You’ll Be Doing You’ll conceptualize, design, build, and maintain complex services/platforms/features and develop ownership over large swaths of our product + infrastructure. You’ll interact closely with our current + prospective customers, developing intuition around their biggest pain points and thinking of creative ways to address them. You’ll play a critical role in shaping our engineering + broader company culture and help make this the best place we’ve ever worked. Who You Are You have Bachelors of Science or Bachelors of Art in Computer Science, Computer Engineering, Math or other similar degree. You have 3+ years of experience in a Software Engineering position. Experience with the technologies we currently use is a plus, but by no means required: Python, PostgreSQL, Docker, Kubernetes, React, Typescript, Google Cloud Platform, Auth0, Terraform. You enjoy building high-quality software, but you also anchor on outcomes and have good intuition around which corners are worth cutting and which aren’t. You enjoy owning features end-to-end and are comfortable learning new tools or moving across the stack to do so. You have a customer-first and learner’s mindset, and value teaching others. You’re a clear and concise communicator; you enjoy the challenge of explaining complicated ideas in simple terms, both in-person and in writing. Pay Transparency The estimated starting annual salary range for this position is $135,000 to $230,000 USD. The listed range is a guideline from Pave data, and the actual base salary may be modified based on factors including job-related skills, experience/qualifications, interview performance, market data, etc. Total compensation for this position may also include equity, sales incentives (for sales roles), and employee benefits. Given Candid Health’s funding and size, we heavily value the potential upside from equity in our compensation package. Further note that Candid Health has minimal hierarchy and titles, but has broad ranges of experience represented within roles.",
        "url": "https://www.linkedin.com/jobs/view/3958354491",
        "summary": "This role involves conceptualizing, designing, building, and maintaining complex services, platforms, and features.  You'll be responsible for a significant portion of the product and infrastructure, working directly with customers to understand their needs and find creative solutions. You'll also contribute to shaping the company culture and engineering team.",
        "industries": [
            "Software Development",
            "Technology",
            "Healthcare"
        ],
        "soft_skills": [
            "Communication",
            "Problem-Solving",
            "Customer Focus",
            "Learning",
            "Teamwork",
            "Ownership",
            "Creativity",
            "Collaboration",
            "Leadership",
            "Strategic Thinking"
        ],
        "hard_skills": [
            "Python",
            "PostgreSQL",
            "Docker",
            "Kubernetes",
            "React",
            "Typescript",
            "Google Cloud Platform",
            "Auth0",
            "Terraform"
        ],
        "tech_stack": [
            "Python",
            "PostgreSQL",
            "Docker",
            "Kubernetes",
            "React",
            "Typescript",
            "Google Cloud Platform",
            "Auth0",
            "Terraform"
        ],
        "programming_languages": [
            "Python",
            "Typescript"
        ],
        "experience": 3,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Computer Engineering",
                "Math"
            ]
        },
        "salary": {
            "max": 230000,
            "min": 135000
        },
        "benefits": [
            "Equity",
            "Sales Incentives",
            "Employee Benefits"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Foster City, CA",
        "job_id": 3843669417,
        "company": "Abode TechZone, LLC",
        "title": "AWS Data Engineer",
        "created_on": 1720635835.58631,
        "description": "AWS Data Engineer with Production Support with Pharma Domain Experience 12 Months Contract Raleigh, NC or Foster City, CA (hybrid) Must Have Pharma Domain Experience Looking for 12 plus years overall experience LinkedIn profile is mandatory NO Sub vending Consultant has to be local or Nearby location Requirements Software Engineer with 7+ years of experience in AWS Data Engineering. Must work in PST hours and provide support/be on bridge when there are production issues with high business impacts Must be technically independent to perform all activities across build-deployment. Expertise in end-to-end ETL Datawarehouse, Python, AWS managed services (EC2, EMR, Glue, Step function, SNS, Airflow, SQS, Lambda, Cloud Watch, Cloud Trail, S3, Redshift), different databases, Databricks, Starburst Must have good multi-tasking skills, time management. Expected to do final Technical Reviews for all the offshore code changes on daily basis, provide technical resolutions and necessary guidance to resolve blockers. Production Support Experience - Strong Triage and Troubleshooting experience, ability to work against timelines. Documentation of Root Cause, strong Analytical Skills for resolving production problems. Must own the SLAs and ensure zero customer escalations/follow ups. Experienced in executing projects in on-site/offshore model and successful implementation of end-to-end ETL Datawarehouse projects Comes with Rich Experience working with Customers directly. (stakeholder-facing roles). Must have ability to document the business requirements for the teams and clearly document and explain technical issues to business audience. Problem solving mindset and proactive with follow ups Must Own the Final Review of Deliverable and Post Deployment Checkouts have followed by customer communication. Nice To Have Terraform, DevOps, Tableau and BI tools on high level Incident and service request management (SPARC) - making sure tickets are acknowledged, updated, and met SLA by the team members",
        "url": "https://www.linkedin.com/jobs/view/3843669417",
        "summary": "Seeking an experienced AWS Data Engineer with 7+ years of experience, specializing in Pharma domain, to join a 12-month contract in Raleigh, NC or Foster City, CA (hybrid). Must be technically independent, proficient in ETL Datawarehouse, Python, and various AWS services including EC2, EMR, Glue, Step Functions, SNS, Airflow, SQS, Lambda, CloudWatch, Cloud Trail, S3, Redshift, Databricks, and Starburst. Strong production support experience, analytical skills, and communication skills are essential.",
        "industries": [
            "Pharmaceuticals",
            "Data Engineering",
            "Software Development"
        ],
        "soft_skills": [
            "Communication",
            "Problem Solving",
            "Time Management",
            "Multitasking",
            "Analytical Skills",
            "Documentation",
            "Stakeholder Management"
        ],
        "hard_skills": [
            "AWS Data Engineering",
            "ETL Datawarehouse",
            "Python",
            "EC2",
            "EMR",
            "Glue",
            "Step Functions",
            "SNS",
            "Airflow",
            "SQS",
            "Lambda",
            "CloudWatch",
            "Cloud Trail",
            "S3",
            "Redshift",
            "Databricks",
            "Starburst",
            "Triage",
            "Troubleshooting",
            "Root Cause Analysis",
            "Terraform",
            "DevOps",
            "Tableau",
            "BI Tools",
            "Incident Management",
            "Service Request Management"
        ],
        "tech_stack": [
            "AWS",
            "Python",
            "EC2",
            "EMR",
            "Glue",
            "Step Functions",
            "SNS",
            "Airflow",
            "SQS",
            "Lambda",
            "CloudWatch",
            "Cloud Trail",
            "S3",
            "Redshift",
            "Databricks",
            "Starburst",
            "Terraform",
            "Tableau"
        ],
        "programming_languages": [
            "Python"
        ],
        "experience": 12,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Newport Beach, CA",
        "job_id": 3934937901,
        "company": "Beacon Pointe",
        "title": "Senior Data Engineer, Information Technology – Data & Development",
        "created_on": 1720635837.2127807,
        "description": "Job Description The Senior Data Engineer will oversee the department’s data infrastructure, including developing a data model, integrating large amounts of data from different systems, building & enhancing a data lake-house & subsequent analytics environment, and writing scripts to facilitate data analysis. This role will work closely & collaboratively with members of the executive team and various departments across the organization to define requirements, mine & analyze data, and deploy high-quality data pipelines to support analytics needs & data ingestion from recently acquired firms. The Data team is responsible for turning data into information that leads to insights and actions to improve the business. Responsibilities Build & continuously enhance a data lake-house that ingests data from different sources to create a unified system for unstructured, semi-structured, and structured data. Combine & analyze data from the lake-house to create analytical reports & insights for executive management. Responsible for developing complex queries in SQL, SPL, stored procedures, or PowerBI from a very large data volume and multiple data sources. Able to use and apply the right analytical techniques to identify hidden patterns & trends that can be leveraged to improve the business. Lead data analysis to solve complex data issues and support data research requests. Perform data analytics, visualization, dashboard customization, and alerts in various cloud platforms such as Azure, Redshift, PowerBI, Tableau. Partner with strategic vendors to connect external data sources. Build logic that will connect & ingest data sources from newly acquired offices. Help improve data quality & efficiency for various platforms. Build process automation, algorithms, & prototypes. Audit & enhance data quality & reliability. Work as a team to create data integrations with other systems. Monitor and develop data integration tools to provide support for business process across internal platforms (Tamarac Reporting, eMoney, HubSpot, Box) Stay current with trends, techniques, technology, and other factors impacting the job. Qualifications Technical expertise with data models, data mining, and segmentation techniques. Extensive experience with processing large sets of unstructured, semi-structured, and structed data. Extensive experience building, maintaining, and enhancing data lakes and data warehouses. Extensive experience working in AWS (Amazon Web Services), Azure, Tableau, & Snowflake. Extensive experience in ETL design, implementation, and maintenance. Extensive experience in programing knowledge for Python & C/C++ and strong in wiring complex DB query SQL. Experience with data modeling and working with Big Data. Experience with data analysis and visualization, particularly PowerBI & Tableau. Strong analytical experience & skills that can extract actionable insights from raw data to help improve the business. Designing and implementing real time pipelines. Ability to effectively manage data, data storage, and data security. Great numerical and analytical skills. Excellent problem-solving skills and the ability to troubleshoot and resolve platform-related issues. Strong communication skills with the ability to collaborate effectively with cross-functional teams and stakeholders. Proactive and self-motivated with a passion for continuous learning and staying updated with Salesforce best practices and new features. Bachelor’s degree or above in computer science, Information Technology, or a related field is preferred. About The Beacon Pointe Family Of Companies Beacon Pointe Advisors is a multi-billion-dollar Registered Investment Advisor with headquarters in Southern California and affiliate offices nationwide. Beacon Pointe provides clear and objective investment advice, solely advocating for our diverse group of clients including institutions (i.e., endowments, foundations), high-net-worth individuals and families. Our advisors’ extensive expertise and strong commitment to our clients can be seen through numerous awards, including being recognized by Bloomberg, Forbes, Financial Advisor Magazine, CNBC, Barron’s and more. For more information, please visit www.beaconpointe.com.",
        "url": "https://www.linkedin.com/jobs/view/3934937901",
        "summary": "The Senior Data Engineer will be responsible for managing the company's data infrastructure, building and maintaining a data lake-house, creating analytical reports, and developing complex queries. The role requires extensive experience in data modeling, data mining, big data technologies, and cloud platforms such as AWS, Azure, Tableau, and Snowflake.",
        "industries": [
            "Financial Services",
            "Investment Management",
            "Data Analytics"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem-Solving",
            "Analytical",
            "Proactive",
            "Self-Motivated",
            "Continuous Learning"
        ],
        "hard_skills": [
            "Data Modeling",
            "Data Mining",
            "Segmentation Techniques",
            "Data Lake",
            "Data Warehouse",
            "AWS",
            "Azure",
            "Tableau",
            "Snowflake",
            "ETL",
            "Python",
            "C/C++",
            "SQL",
            "PowerBI",
            "Data Visualization",
            "Data Security",
            "Real Time Pipelines",
            "Tamarac Reporting",
            "eMoney",
            "HubSpot",
            "Box"
        ],
        "tech_stack": [
            "AWS",
            "Azure",
            "Tableau",
            "Snowflake",
            "Python",
            "C/C++",
            "SQL",
            "PowerBI"
        ],
        "programming_languages": [
            "Python",
            "C/C++",
            "SQL"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Computer Science",
                "Information Technology"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Sunnyvale, CA",
        "job_id": 3824369578,
        "company": "Google",
        "title": "Software Engineer III, Google Cloud Compute Infrastructure",
        "created_on": 1720635838.7568624,
        "description": "Note: By applying to this position you will have an opportunity to share your preferred working location from the following: Sunnyvale, CA, USA; Seattle, WA, USA; Kirkland, WA, USA . Minimum qualifications: Bachelor’s degree or equivalent practical experience. 2 years of experience with software development in one or more programming languages, or 1 year of experience with an advanced degree in an industry setting. 2 years of experience with data structures or algorithms in either an academic or industry setting. Preferred qualifications: Master's degree or PhD in Computer Science or related technical fields. 2 years of experience with performance, large scale systems data analysis, visualization tools, and/or debugging. Experience developing accessible technologies. Proficiency in code and system health, diagnosis and resolution, and software test engineering. About The Job Google's software engineers develop the next-generation technologies that change how billions of users connect, explore, and interact with information and one another. Our products need to handle information at massive scale, and extend well beyond web search. We're looking for engineers who bring fresh ideas from all areas, including information retrieval, distributed computing, large-scale system design, networking and data storage, security, artificial intelligence, natural language processing, UI design and mobile; the list goes on and is growing every day. As a software engineer, you will work on a specific project critical to Google’s needs with opportunities to switch teams and projects as you and our fast-paced business grow and evolve. We need our engineers to be versatile, display leadership qualities and be enthusiastic to take on new problems across the full-stack as we continue to push technology forward. Behind everything our users see online is the architecture built by the Technical Infrastructure team to keep it running. From developing and maintaining our data centers to building the next generation of Google platforms, we make Google's product portfolio possible. We're proud to be our engineers' engineers and love voiding warranties by taking things apart so we can rebuild them. We keep our networks up and running, ensuring our users have the best and fastest experience possible. With your technical expertise you will manage project priorities, deadlines, and deliverables. You will design, develop, test, deploy, maintain, and enhance software solutions. Google Cloud accelerates organizations’ ability to digitally transform their business with the best infrastructure, platform, industry solutions and expertise. We deliver enterprise-grade solutions that leverage Google’s cutting-edge technology – all on the cleanest cloud in the industry. Customers in more than 200 countries and territories turn to Google Cloud as their trusted partner to enable growth and solve their most critical business problems. The US base salary range for this full-time position is $136,000-$200,000 + bonus + equity + benefits. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process. Please note that the compensation details listed in US role postings reflect the base salary only, and do not include bonus, equity, or benefits. Learn more about benefits at Google . Responsibilities Write product or system development code. Participate in, or lead design reviews with peers and stakeholders to decide amongst available technologies. Review code developed by other developers and provide feedback to ensure best practices (e.g., style guidelines, checking code in, accuracy, testability, and efficiency). Contribute to existing documentation or educational content and adapt content based on product/program updates and user feedback. Triage product or system issues and debug/track/resolve by analyzing the sources of issues and the impact on hardware, network, or service operations and quality. Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing our Accommodations for Applicants form .",
        "url": "https://www.linkedin.com/jobs/view/3824369578",
        "summary": "Google is seeking a Software Engineer to join their Technical Infrastructure team, responsible for developing and maintaining the infrastructure that powers Google's products. The engineer will work on projects critical to Google's needs, contributing to code development, design reviews, code reviews, documentation, issue resolution, and more. They will have opportunities to switch teams and projects as they and the company grow.",
        "industries": [
            "Technology",
            "Software Development",
            "Cloud Computing",
            "Internet"
        ],
        "soft_skills": [
            "Leadership",
            "Versatility",
            "Problem Solving",
            "Communication",
            "Collaboration"
        ],
        "hard_skills": [
            "Software Development",
            "Programming Languages",
            "Data Structures",
            "Algorithms",
            "Performance Optimization",
            "Large Scale Systems",
            "Data Analysis",
            "Visualization Tools",
            "Debugging",
            "Accessibility",
            "Code and System Health",
            "Diagnosis and Resolution",
            "Software Test Engineering"
        ],
        "tech_stack": [
            "Google Cloud"
        ],
        "programming_languages": [],
        "experience": 2,
        "education": {
            "min_degree": "Bachelor’s degree",
            "fields": [
                "Computer Science",
                "Related Technical Fields"
            ]
        },
        "salary": {
            "max": 200000,
            "min": 136000
        },
        "benefits": [
            "Bonus",
            "Equity",
            "Benefits"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Fremont, CA",
        "job_id": 3890929282,
        "company": "Zortech Solutions",
        "title": "Data Center Support Engineer",
        "created_on": 1720635840.3768947,
        "description": "Responsibilities Responsible for inventory collection, provisioning, monitoring of pdu, ups and environmental sensor deployed in data centers. Maintain accurate pdu/ups inventory in our asset tracking and monitoring tools. Work with other data center engineers to monitor, analyze, and optimize power and cooling. Fine tune and maintain alert rules for pdu/ups/sensor monitoring. Conducting regular maintenance checks and upgrades software for pdu and ups to ensure compliance with company standards and best practices. Collaborating with other data center engineers, architects, infrastructure engineers, and vendors, ensuring effective communication and coordination across teams and projects. Requirements A bachelor's degree in computer science, engineering, or a related field, proof of exceptional skills in related fields with practical engineering experience, or equivalent Minimum of five years of experience in data center engineering, preferably in a large-scale environment. Strong understanding of data center power, cooling, fire suppression and cabling infrastructure as well as network In depth understanding of different hardware types (servers, switches, routers, storage), cabling and optics. Proficient in customizing data center monitoring tools with experience in using scripting for bulk updates and sql database. High level of analytical, problem-solving, and troubleshooting skills, with the ability to work under pressure and meet deadlines. Great communication and interpersonal skills, with the ability to work independently and as part of a team. A certification in network or data center design, such as CCNA, CDCP, CDCS, or CDCE would be a plus.",
        "url": "https://www.linkedin.com/jobs/view/3890929282",
        "summary": "This role involves managing and maintaining the power and cooling infrastructure of a data center. Responsibilities include inventory tracking, monitoring PDUs and UPSs, analyzing power usage, and collaborating with other engineers to ensure optimal performance. Strong knowledge of data center infrastructure, hardware, and monitoring tools is required.",
        "industries": [
            "Information Technology",
            "Data Center",
            "Telecommunications",
            "Cloud Computing",
            "Engineering"
        ],
        "soft_skills": [
            "Analytical",
            "Problem-solving",
            "Troubleshooting",
            "Communication",
            "Interpersonal",
            "Teamwork",
            "Independent Work",
            "Deadline Oriented"
        ],
        "hard_skills": [
            "Data Center Engineering",
            "Power Management",
            "Cooling Systems",
            "Fire Suppression",
            "Cabling Infrastructure",
            "Network Management",
            "Server Management",
            "Switch Management",
            "Router Management",
            "Storage Management",
            "Data Center Monitoring Tools",
            "Scripting",
            "SQL Database",
            "Asset Tracking",
            "Troubleshooting",
            "Maintenance",
            "Compliance"
        ],
        "tech_stack": [
            "PDU",
            "UPS",
            "Environmental Sensors",
            "Asset Tracking Tools",
            "Data Center Monitoring Tools",
            "Scripting",
            "SQL Database"
        ],
        "programming_languages": [],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Engineering",
                "Related Fields"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Monrovia, CA",
        "job_id": 3958005011,
        "company": "Terray Therapeutics",
        "title": "Data Engineer, R&D Informatics",
        "created_on": 1720635844.404598,
        "description": "Company Overview Terray is a biotechnology company with the technology, data, and mindset to radically change the way we discover and develop small molecule therapeutics. We explore molecules and targets broadly and deeply with a sophisticated integration of ultra-high throughput experimentation, generative AI, biology, medicinal chemistry, automation, and nanotechnology. Everything the company does is grounded in an iterative approach, producing massive amounts of precise, purpose-built data mapping interactions between small molecules and causes of disease that gets increasingly valuable with each cycle of design and experimentation. The company's platform uniquely blends experimentation and computation to improve the cost, speed, and success rate of small molecule drug discovery and development. Position Summary Terray Therapeutics is seeking a motivated, creative, and experienced Data Engineer with a focus on relational database management, data state management, and data vigilance. As an integral member of the Computational and Data Sciences Team, the candidate will be responsible for developing and maintaining RDS architecture powering the Laboratory Information Management (LIMS) capabilities of Terray's wet-lab and computational discovery platforms. The candidate will have experience with relational database management across a variety of systems including MySQL, PostgreSQL, and have a strong command of advanced SQL functions. Proficiency in Python scripting and automated ETL pipeline development integrating LIMS platforms, internal databases, and wet-lab automation will be a key component of this role. The position will report to the Research Informatics Manager. Core responsibilities of this position include: Design, configure, optimize, and maintain database schemas and collaborate to implement data migration solutions. Design and implement new dashboards and user interfaces to help our wet-lab and computational scientists' access and analyze their data. Design, develop, test, and maintain informatics infrastructure (data pipelines, storage, processing) that supports downstream data-driven applications and stakeholders. Work with both wet-lab and computational scientists to design and develop connections to our informatics systems API and database. Manage, improve, and customize our research informatics platforms Experience And Qualifications Part of Terray Therapeutics' success is nurtured by a hands-on work environment where everyone is accountable, everyone is vested in a vision of excellence, and everyone actively takes part in the success of the business. Terray Therapeutics supports a positive work environment comprised of engaged employees who feel appreciated, recognized, and free to be creative. A mindset for continuous learning, strong communication and problem-solving skills will be required for effective cross-functional collaboration with Automation, Computational Data Science and Scientific teams, including Platform Chemistry, Medicinal Chemistry, and pre-clinical Biology. Qualifications include: BS/MS/PhD in Computer Science or related Life Science discipline and 4+ years industry experience Highly proficient in SQL and interfacing with databases in code Significant experience in relational and non-relational database design Experience building out data platforms Proficiency in Python, Java, or Scala Familiarity with Snowflake, AWS cloud resources and API design Familiarity with containerization tools such as Docker and Kubernetes Knowledge of coding best practices, including standards, reviews, version control, and unit testing Excellent communication skills and demonstrated experience working with cross-functional teams Knowledge of general biotech wet-lab operations (direct or indirect) and LIMS platforms such as Benchling, Signals Notebook, and Hamilton Instinct Chromatography Data System experience and database administration is a plus. She/he/they will exhibit the ability to work well under pressure to provide results in a short timeframe. The company is looking for a highly responsive, goal-oriented individual who will bring significant energy and drive to solve complex technical problems and help us achieve our mission to advance human health. Compensation Details $132,000 - $198,000 (annually) depending on seniority; participation in the Company's option plan; 3% 401K contribution; full benefits.",
        "url": "https://www.linkedin.com/jobs/view/3958005011",
        "summary": "Terray Therapeutics seeks a Data Engineer with expertise in relational database management, data state management, and data vigilance. This role involves designing, configuring, and maintaining RDS architecture for LIMS capabilities, building data platforms, and developing data pipelines. The ideal candidate will have a strong background in SQL, Python scripting, and experience with LIMS platforms, databases, and wet-lab automation. Familiarity with cloud resources, containerization tools, and coding best practices is also required. The position offers a competitive salary, benefits, and equity participation.",
        "industries": [
            "Biotechnology",
            "Pharmaceuticals",
            "Healthcare"
        ],
        "soft_skills": [
            "Motivated",
            "Creative",
            "Experienced",
            "Strong communication",
            "Problem-solving",
            "Collaboration",
            "Continuous learning"
        ],
        "hard_skills": [
            "Relational database management",
            "Data state management",
            "Data vigilance",
            "MySQL",
            "PostgreSQL",
            "Advanced SQL",
            "Python scripting",
            "ETL pipeline development",
            "LIMS platforms",
            "Internal databases",
            "Wet-lab automation",
            "Database schema design",
            "Data migration solutions",
            "Dashboard design",
            "User interface development",
            "Informatics infrastructure design",
            "Data pipelines",
            "Data storage",
            "Data processing",
            "API development",
            "Database connectivity",
            "Research informatics platforms",
            "Cloud resources",
            "API design",
            "Containerization tools",
            "Docker",
            "Kubernetes",
            "Coding best practices",
            "Version control",
            "Unit testing"
        ],
        "tech_stack": [
            "MySQL",
            "PostgreSQL",
            "Python",
            "Java",
            "Scala",
            "Snowflake",
            "AWS",
            "Docker",
            "Kubernetes",
            "Benchling",
            "Signals Notebook",
            "Hamilton Instinct"
        ],
        "programming_languages": [
            "SQL",
            "Python",
            "Java",
            "Scala"
        ],
        "experience": 4,
        "education": {
            "min_degree": "BS",
            "fields": [
                "Computer Science",
                "Life Science"
            ]
        },
        "salary": {
            "max": 198000,
            "min": 132000
        },
        "benefits": [
            "Option plan",
            "401K contribution",
            "Full benefits"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3939835323,
        "company": "Greylock",
        "title": "Software Engineer",
        "created_on": 1720635845.9338446,
        "description": "Greylock has a history of creating category-defining companies, and we're privileged to invest in an emerging startup focused on AI agents for compliance. Financial diligence involves labor-intensive human processes, which are time-consuming and challenging to scale, hindering efficient risk identification and mitigation. Recent advancements in LLMs effectively address these challenges faced by financial institutions. We are seeking someone with at least 2 years of experience in full-stack development, preferably in a startup environment, though this is not required. These roles are particularly valuable for individuals who aspire to start their own company in the future. This opportunity will provide the highest amount of ownership and growth for someone early in their career. What to expect in the role: Within days: Deliver delightful user experiences using Next.JS and modern frontend technologies. Translate novel LLM capabilities into understandable, intuitive, and performant end-user experiences. Work directly with customers to solve their problems. We need someone who is highly empathic and excited to work side-by-side with financial crime teams to solve critical problems. Work directly with the CEO, CTO, and other founding members to help build our team. Within weeks: Build integration and product infrastructure for LLMs. Give Greenlite agents the infrastructure, data sources and tools they need to automate workflows for financial compliance teams. Lead customer calls and the development of new features end-to-end. Within months: Own the architecture and development of our API product. Scale our API and webhook services to reliably serve customers across geographies, tech stacks, and business types. Hire, onboard, and nurture future teammates. Stage: Seed Company Size: 6 Location: San Francisco/In Office Our investor's thoughts on the company: Greylock: AI Co-Pilot for Compliance Teams",
        "url": "https://www.linkedin.com/jobs/view/3939835323",
        "summary": "Greylock is seeking a full-stack developer with at least 2 years of experience to join an emerging startup focused on AI agents for compliance. The role involves building user experiences, integrating LLMs, and developing API infrastructure to automate workflows for financial compliance teams. This is a high-ownership role with potential for growth and the opportunity to work directly with customers and the founding team.",
        "industries": [
            "Financial Technology (FinTech)",
            "Artificial Intelligence (AI)",
            "Compliance",
            "Software Development"
        ],
        "soft_skills": [
            "Empathy",
            "Communication",
            "Problem Solving",
            "Ownership",
            "Growth Mindset"
        ],
        "hard_skills": [
            "Full-Stack Development",
            "Next.js",
            "Frontend Technologies",
            "LLM Integration",
            "API Development",
            "Data Integration",
            "Customer Interaction",
            "Architecture Design",
            "Team Management",
            "Hiring",
            "Onboarding"
        ],
        "tech_stack": [
            "Next.js",
            "LLMs",
            "API",
            "Webhooks"
        ],
        "programming_languages": [
            "JavaScript"
        ],
        "experience": 2,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Mountain View, CA",
        "job_id": 3813093932,
        "company": "1872 Consulting",
        "title": "Senior Data Engineer",
        "created_on": 1720635847.6682546,
        "description": "Sr. Data Engineer MOUNTAIN VIEW, CA / DENVER, CO / REMOTE FULL-TIME Position Summary: We are looking for a Sr. Data Engineer to join our growing Data Platform and Engineering teams. The ideal candidate has significant experience in building scalable data platforms that enable business intelligence, analytics, data science and data products. They must have strong, hands-on technical expertise in a variety of technologies and the proven ability to fashion robust scalable solutions. They must be at ease working in an agile environment with little supervision. The ability to work across teams with product managers, data scientists and business stakeholders to translate sometimes vague business requirements into working code will be critical to success in this role. This person should embody a passion for continuous improvement and data quality. Responsibilities: Design and implement data processing pipelines Integrate data from multiple data sources, develop cross-platform ETL processes Data Validation and Verification Analyze data, solve problems, and implement solutions for ensuring data quality and delivery Create systems for data acquisition and wrangling Develop new tools and processes for managing our data workflows and data infrastructure Collaborate with our Engineering and Data Science teams on building, maintaining, and monitoring the database infrastructure Collaborate with product managers, data scientists, business users and other engineers to define requirements and design solutions Client and analyze data from the web (census, open data, commercial vendors) Skills/Requirements: Expert in reporting, analytics, and databases Data ingestion, ETL and storage Interest in pulling data from many sources Experience in big data, data mining and statistical analysis Cloud computing, especially AWS technologies (S3, EC2, etc.) Comfortable choosing technologies that fit the application (e.g., MySQL versus PostgreSQL, Hadoop versus Cassandra) More than 5 years of experience in object-oriented development with Python Other languages like Scala, C++, Java, or similar are a plus Experience with spark Expertise with SQL Familiarity with Docker Machine Learning libraries and frameworks like scikit-learn, TensorFlow, Pytorch a plus Deploying algorithms at scale",
        "url": "https://www.linkedin.com/jobs/view/3813093932",
        "summary": "We are searching for a Senior Data Engineer to design and build scalable data platforms for business intelligence, analytics, data science and data products.  The ideal candidate will be experienced with building ETL processes, data quality management, and cloud computing (AWS).",
        "industries": [
            "Technology",
            "Data Science",
            "Analytics",
            "Business Intelligence"
        ],
        "soft_skills": [
            "Problem Solving",
            "Communication",
            "Collaboration",
            "Teamwork",
            "Passion for Improvement",
            "Data Quality"
        ],
        "hard_skills": [
            "Data Processing Pipelines",
            "ETL",
            "Data Validation",
            "Data Quality",
            "Data Acquisition",
            "Data Wrangling",
            "Database Infrastructure",
            "Data Analysis",
            "Web Data Acquisition",
            "Reporting",
            "Analytics",
            "Big Data",
            "Data Mining",
            "Statistical Analysis",
            "Cloud Computing",
            "AWS (S3, EC2)",
            "Object-Oriented Development",
            "Python",
            "Scala",
            "C++",
            "Java",
            "Spark",
            "SQL",
            "Docker",
            "Machine Learning",
            "Scikit-learn",
            "TensorFlow",
            "PyTorch"
        ],
        "tech_stack": [
            "AWS",
            "S3",
            "EC2",
            "MySQL",
            "PostgreSQL",
            "Hadoop",
            "Cassandra",
            "Python",
            "Scala",
            "C++",
            "Java",
            "Spark",
            "SQL",
            "Docker",
            "Scikit-learn",
            "TensorFlow",
            "PyTorch"
        ],
        "programming_languages": [
            "Python",
            "Scala",
            "C++",
            "Java"
        ],
        "experience": 5,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3953795669,
        "company": "Candid Health",
        "title": "Software Engineer, Full Stack",
        "created_on": 1720635849.296252,
        "description": "What we do We’re fixing one of the most broken and costly pieces of the US healthcare system: medical billing. Today, healthcare providers spend over $250B each year on administrative overhead just to get paid by insurance. Medical billing is expensive because it’s nuanced and hard - maybe ~100x harder than credit card payment processing - and because it’s traditionally done by armies of humans who track and manage complex rules and processes specific to individual insurance companies with little or no supporting software. We’re rethinking medical billing from the ground up, building software backed by best-in-class data science (and, soon, a dash of machine learning) to automate much of this complexity so healthcare providers can get paid dramatically more easily and inexpensively. We were in the Y Combinator W20 batch and have since been well funded by a world-class group of funds (8VC, First Round Capital, BoxGroup) + angel investors. We're now helping our customers treat opioid addiction, provide holistic care for women, lose weight, increase access to mental health care, and much more. This is such important and gratifying work; we can't wait for you to join our team and help support some of the most important innovation happening in healthcare today! Our values We spend at least as much time with our coworkers as we do with our closest friends + family - if we intend to do the most important + challenging work of our lives, it’s important that these folks energize us, support us, inspire us, and push us to do our best work. This is what you can expect of your teammates at Candid (in no particular order): - We put our customers first - We take care of each other and ourselves - We anchor on outcomes and work relentlessly and creatively to achieve them - We collectively prioritize building a diverse and inclusive workspace - We believe humility is our greatest strength - We are candid, kind, and committed - We strive to be the most prepared person in the room - We are truth seekers What you’ll be doing You’ll conceptualize, design, build, and maintain complex services/platforms/features and develop ownership over large swaths of our product + infrastructure. You’ll interact closely with our current + prospective customers, developing intuition around their biggest pain points and thinking of creative ways to address them. You’ll play a critical role in shaping our engineering + broader company culture and help make this the best place we’ve ever worked. Who you are You have Bachelors of Science or Bachelors of Art in Computer Science, Computer Engineering, Math or other similar degree. Experience with the technologies we currently use is a plus, but by no means required: Python, PostgreSQL, Docker, Kubernetes, React, Typescript, Google Cloud Platform, Auth0, Terraform. You enjoy building high-quality software, but you also anchor on outcomes and have good intuition around which corners are worth cutting and which aren’t. You enjoy owning features end-to-end and are comfortable learning new tools or moving across the stack to do so. You have a customer-first and learner’s mindset, and value teaching others. You’re a clear and concise communicator; you enjoy the challenge of explaining complicated ideas in simple terms, both in-person and in writing. Pay Transparency The estimated starting annual salary range for this position is $135,000 to $230,000 USD. The listed range is a guideline from Pave data, and the actual base salary may be modified based on factors including job-related skills, experience/qualifications, interview performance, market data, etc. Total compensation for this position may also include equity, sales incentives (for sales roles), and employee benefits. Given Candid Health’s funding and size, we heavily value the potential upside from equity in our compensation package. Further note that Candid Health has minimal hierarchy and titles, but has broad ranges of experience represented within roles. We are an equal opportunity employer. We don’t discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. We also consider for employment qualified applicants with arrest and conviction records, consistent with applicable federal, state and local law, including but not limited to the San Francisco Fair Chance Ordinance.",
        "url": "https://www.linkedin.com/jobs/view/3953795669",
        "summary": "Candid Health is a software company focused on automating medical billing for healthcare providers, using data science and machine learning. They aim to reduce administrative overhead and simplify the payment process for providers. They are looking for a Software Engineer to conceptualize, design, build, and maintain complex services/platforms/features, interact with customers, and contribute to the engineering culture.",
        "industries": [
            "Healthcare",
            "Software",
            "Technology",
            "Data Science",
            "Machine Learning"
        ],
        "soft_skills": [
            "Customer-first",
            "Learner's mindset",
            "Teaching",
            "Clear and concise communication",
            "Problem-solving",
            "Teamwork",
            "Creativity",
            "Humility",
            "Ownership",
            "Collaboration",
            "Adaptability",
            "Communication"
        ],
        "hard_skills": [
            "Python",
            "PostgreSQL",
            "Docker",
            "Kubernetes",
            "React",
            "Typescript",
            "Google Cloud Platform",
            "Auth0",
            "Terraform"
        ],
        "tech_stack": [
            "Python",
            "PostgreSQL",
            "Docker",
            "Kubernetes",
            "React",
            "Typescript",
            "Google Cloud Platform",
            "Auth0",
            "Terraform"
        ],
        "programming_languages": [
            "Python",
            "Typescript"
        ],
        "experience": 0,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Computer Science",
                "Computer Engineering",
                "Math"
            ]
        },
        "salary": {
            "max": 230000,
            "min": 135000
        },
        "benefits": [
            "Equity",
            "Sales incentives",
            "Employee benefits"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Ontario, CA",
        "job_id": 3944072752,
        "company": "Capgemini",
        "title": "Software Engineer - BI Engineer",
        "created_on": 1720635852.610044,
        "description": "Job Description We are looking for a Tableau BI Expert to work with the business and IT teams. The responsibilities include creating technical solutions, data storage tools, and conducting tests. An ideal candidate should have a broad understanding of the business technology landscape, the ability to design reports, and strong analytical skills. You should have proficiency in reporting analysis tools! Key Responsibilities Excellent analytical skills to forecast and predict trends and insights using past and current data. Performing and documenting data analysis, data validation, and data mapping. Create Complex Tableau Dashboards and attractive visual stories from a variety of data source types. Present findings in Tableau in a way that compels and enables decision making. Participate in planning, analysis, design, development, and testing activities. Responsible for requirements gathering and creating business requirement documents. Ability to setting up extracts in Tableau. Ability to build visually-stunning and interactive dashboards. Ability to manipulate and blend data in order to design dashboards and visualization. Knowledge of data migration, Tableau integration, and extensions. Manage Tableau-driven implementations, architecture, and administration. Developing, maintaining, and managing advanced reporting, analytics, dashboards and other BI solutions. Required Skills Minimum 5 years of recent work experience as BI expert. A solid understanding of SQL, relational databases, and normalization. Proficient in the and reporting analysis tools. Proficiency in programming languages such as Python and use of query. Sound knowledge and experience in Excel, PL/SQL, ETL, and Tableau/Sigma platform. Experience in ETL orchestration and workflow management tools Airflow. Extensive experience in developing, maintaining and managing Tableau driven dashboards & analytics and working knowledge of Tableau administration/architecture. Should be detail oriented and possess problem-solving skills and analytical skills. Experience with snowflake data source. Experience with Sigma a cloud-based Business Intelligence (BI) platform. Life at Capgemini Capgemini Supports All Aspects Of Your Well-being Throughout The Changing Stages Of Your Life And Career. For Eligible Employees, We Offer Collaborating with teams of creative, fun, and driven colleagues Flexible work options enabling time and location-based flexibility Company-provided home office equipment Virtual collaboration and productivity tools to enable hybrid teams Comprehensive benefits program (Health, Welfare, Retirement and Paid time off) Other perks and wellness benefits like discount programs, and gym/studio access. Paid Parental Leave and coaching, baby welcome gift, and family care/illness days Back-up childcare/elder care, childcare discounts, and subsidized virtual tutoring Tuition assistance and weekly hot skill development opportunities Experiential, high-impact learning series events Access to mental health resources and mindfulness programs Access to join Capgemini Employee Resource Groups around communities of interest About Capgemini Capgemini is a global business and technology transformation partner, helping organizations to accelerate their dual transition to a digital and sustainable world, while creating tangible impact for enterprises and society. It is a responsible and diverse group of 340,000 team members in more than 50 countries. With its strong over 55-year heritage, Capgemini is trusted by its clients to unlock the value of technology to address the entire breadth of their business needs. It delivers end-to-end services and solutions leveraging strengths from strategy and design to engineering, all fueled by its market leading capabilities in AI, cloud and data, combined with its deep industry expertise and partner ecosystem. The Group reported 2023 global revenues of €22.5 billion. Get The Future You Want | www.capgemini.com Disclaimer Capgemini is an Equal Opportunity Employer encouraging diversity in the workplace. All qualified applicants will receive consideration for employment without regard to race, national origin, gender identity/expression, age, religion, disability, sexual orientation, genetics, veteran status, marital status or any other characteristic protected by law. This is a general description of the Duties, Responsibilities and Qualifications required for this position. Physical, mental, sensory or environmental demands may be referenced in an attempt to communicate the manner in which this position traditionally is performed. Whenever necessary to provide individuals with disabilities an equal employment opportunity, Capgemini will consider reasonable accommodations that might involve varying job requirements and/or changing the way this job is performed, provided that such accommodations do not pose an undue hardship. Capgemini is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to your recruiting contact. Click the following link for more information on your rights as an Applicant http://www.capgemini.com/resources/equal-employment-opportunity-is-the-law Please be aware that Capgemini may capture your image (video or screenshot) during the interview process and that image may be used for verification, including during the hiring and onboarding process. Applicants for employment in Canada must have valid work authorization that does not now and/or will not in the future require sponsorship of a visa for employment authorization in Canada by Capgemini. Capgemini discloses salary range information in compliance with state and local pay transparency obligations. The disclosed range represents the lowest to highest salary we, in good faith, believe we would pay for this role at the time of this posting, although we may ultimately pay more or less than the disclosed range and the range may be modified in the future. The disclosed range takes into account the wide range of factors that are considered in making compensation decisions including, but not limited to, geographic location, relevant education, qualifications, certifications, experience, skills, seniority, performance, sales or revenue-based metrics, and business or organizational needs. At Capgemini, it is not typical for an individual to be hired at or near the top of the range for their role. The base salary range for the tagged location is $ 65,200 - $157,040 /yr. This role may be eligible for other compensation including variable compensation, bonus, or commission. Full-time regular employees are eligible for paid time off, medical/dental/vision insurance, and any other benefits to eligible employees. Note: No amount of pay is considered to be wages or compensation until such amount is earned, vested, and determinable. The amount and availability of any bonus, commission, or any other form of compensation that is allocable to a particular employee remains in the Company's sole discretion unless and until paid and may be modified at the Company’s sole discretion, consistent with the law.",
        "url": "https://www.linkedin.com/jobs/view/3944072752",
        "summary": "Capgemini is seeking a Tableau BI Expert to work with business and IT teams. The role involves creating technical solutions, data storage tools, and conducting tests. Ideal candidates possess strong analytical skills, proficiency in reporting tools, and experience with Tableau dashboards and visualizations. The responsibilities include performing data analysis, data validation, data mapping, building dashboards, and presenting findings to enable decision-making. This role requires a minimum of 5 years of experience as a BI expert, proficiency in SQL, relational databases, and programming languages like Python. Familiarity with ETL, Tableau/Sigma platform, Snowflake, and Airflow is also essential.",
        "industries": [
            "Information Technology and Services",
            "Business Intelligence",
            "Data Analytics"
        ],
        "soft_skills": [
            "Analytical Skills",
            "Problem-Solving Skills",
            "Communication Skills",
            "Decision-Making Skills",
            "Presentation Skills",
            "Collaboration Skills"
        ],
        "hard_skills": [
            "Tableau",
            "SQL",
            "Relational Databases",
            "Python",
            "ETL",
            "Excel",
            "PL/SQL",
            "Airflow",
            "Snowflake",
            "Sigma",
            "Data Analysis",
            "Data Validation",
            "Data Mapping",
            "Dashboard Development",
            "Visualization",
            "Reporting Analysis"
        ],
        "tech_stack": [
            "Tableau",
            "SQL",
            "Python",
            "ETL",
            "Excel",
            "PL/SQL",
            "Airflow",
            "Snowflake",
            "Sigma"
        ],
        "programming_languages": [
            "Python",
            "SQL"
        ],
        "experience": 5,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 157040,
            "min": 65200
        },
        "benefits": [
            "Flexible work options",
            "Company-provided home office equipment",
            "Virtual collaboration and productivity tools",
            "Comprehensive benefits program (Health, Welfare, Retirement and Paid time off)",
            "Discount programs",
            "Gym/studio access",
            "Paid Parental Leave",
            "Coaching",
            "Baby welcome gift",
            "Family care/illness days",
            "Back-up childcare/elder care",
            "Childcare discounts",
            "Subsidized virtual tutoring",
            "Tuition assistance",
            "Weekly hot skill development opportunities",
            "Experiential, high-impact learning series events",
            "Access to mental health resources",
            "Mindfulness programs",
            "Access to join Capgemini Employee Resource Groups"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Foster City, CA",
        "job_id": 3781637530,
        "company": "Conviva",
        "title": "Senior Software Engineer, Backend",
        "created_on": 1720635856.5919783,
        "description": "If you've used Disney+, Peacock, or other top streaming platforms, you've already benefited from Conviva's technology. We empower the world’s leading B2C companies, including the biggest names in streaming media, to deliver exceptional digital experiences and optimize the “moments that matter” to their customers and their business. As the global leader in experience-centric operational analytics, Conviva has redefined big data analytics with our paradigm-shifting Time-State Analytics model. Our platform does what legacy observability and monitoring tools can’t: we eliminate the gaps between system performance, user experience, and engagement, enabling issue identification, impact assessment, and root cause resolution in seconds. This isn’t just important, its game-changing! Our platform processes over 5 trillion daily events, providing real-time, cost-effective, stateful computation across diverse data sets. This empowers operations teams for the first time to precisely and directly impact real-world business outcomes, including customer satisfaction and revenue. As Conviva continues to grow, we are actively looking for highly motivated and talented distributed software engineers at all levels to join our dynamic backend development teams. You will work with some of the best engineers in building various portions of a large end to end, distributed real time streaming data processing platform in handling internet scale massive amount data in real time. At more senior levels, you will lead technology innovation, collaborating across teams, setting technical standards, and steering the overall architectural framework. What Success Will Look Like Design, build, maintain and improve a range of algorithms and their underlying systems. Develop and implement end-to-end solutions for real-time algorithms, including monitoring, testing, and A/B testing tooling. Create a new scalable, real-time big data query engine, emphasizing scalable solutions across diverse domains. Lead critical technical decisions collaboratively, guiding and training team members to tackle engineering challenges. Design and deploy ETL pipelines using various open-source frameworks, ensuring robustness and scalability. Foster a positive team culture, promoting code quality, driving initiatives, and ensuring impeccable execution. Who You Are & What You've Done Solid foundation in Computer Science, Computer Engineering or related fields. Highly motivated, passionate about technology, eager to learn, and proving ability to learn and master fast pacing innovative technologies. Demonstrate effective collaboration within teams while being open to receiving feedback, embracing a learning mindset, and actively supporting others. At least 6+ years of industry experience in software development utilizing modern software development processes, tool chains and infrastructure. Proficiency in major big data frameworks, such as Akka, Spark, Flink, Hadoop/Hive and others. Hands-on development experience with query engines and database systems, such as Clickhouse, Druid, Presto, BigQuery. Strong programming skills in Rust, Scala, Java, Python, or a similar language. Experience in performance tuning, analysis over distributed systems. Experience building massively scalable data infrastructure using commercial and open-source tools. Adaptable to ambiguity and comfortable in a fast-paced work environment. Preferred Experience in a SaaS space Cloud experience with AWS, GCP, Azure in development and deployment Track record of contributing to open-source projects The expected salary range for this full-time position is $160,000 - $200,000 + equity + benefits. Compensation is determined by numerous factors such as your qualifications, experience, relevant education or training, and work location. Underpinning the Conviva platform is a rich history of innovation. More than 60 patents represent award-winning technologies and standards, including first-of-its kind-innovations like Time-State analytics and AI-automated data modeling, that surface actionable insights. By understanding real-world human experiences and having the ability to act within seconds of observation, our customers can solve business-critical issues and focus on growing their business ahead of the competition. Examples of the brands Conviva has helped innovate, adapt, and scale at unprecedented speed include: DAZN, Disney+, HBO, Hulu, NBCUniversal, Paramount+, Peacock, Sky, Sling TV, Univision and Warner Bros Discovery. Privately held, Conviva is headquartered in Silicon Valley, California with offices and people around the globe. For more information, visit us at www.conviva.com. Join us to help extend our leadership position in big data streaming analytics to new audiences and markets!",
        "url": "https://www.linkedin.com/jobs/view/3781637530",
        "summary": "Conviva, a leading provider of experience-centric operational analytics for streaming media companies, is seeking highly motivated and talented distributed software engineers to join their backend development teams. The role involves designing, building, and maintaining real-time streaming data processing platforms at internet scale. Senior engineers will also lead technology innovation, set technical standards, and guide team members.",
        "industries": [
            "Streaming Media",
            "Software",
            "Technology",
            "Big Data",
            "Data Analytics",
            "SaaS",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Motivated",
            "Passionate",
            "Eager to Learn",
            "Collaborative",
            "Open to Feedback",
            "Learning Mindset",
            "Supporting",
            "Adaptable",
            "Comfortable in Fast-Paced Environment"
        ],
        "hard_skills": [
            "Software Development",
            "Big Data Frameworks",
            "Akka",
            "Spark",
            "Flink",
            "Hadoop",
            "Hive",
            "Query Engines",
            "Database Systems",
            "Clickhouse",
            "Druid",
            "Presto",
            "BigQuery",
            "Rust",
            "Scala",
            "Java",
            "Python",
            "Performance Tuning",
            "Distributed Systems",
            "Scalable Data Infrastructure",
            "AWS",
            "GCP",
            "Azure",
            "Open-Source"
        ],
        "tech_stack": [
            "Akka",
            "Spark",
            "Flink",
            "Hadoop",
            "Hive",
            "Clickhouse",
            "Druid",
            "Presto",
            "BigQuery",
            "Rust",
            "Scala",
            "Java",
            "Python",
            "AWS",
            "GCP",
            "Azure"
        ],
        "programming_languages": [
            "Rust",
            "Scala",
            "Java",
            "Python"
        ],
        "experience": 6,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Computer Engineering",
                "Related Fields"
            ]
        },
        "salary": {
            "max": 200000,
            "min": 160000
        },
        "benefits": [
            "Equity"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3862412441,
        "company": "People Data Labs",
        "title": "Senior Data Engineer",
        "created_on": 1720635858.3276074,
        "description": "About Us At People Data Labs, we're committed to democratizing access to high-quality B2B data and leading the emerging DaaS economy. We empower developers, engineers, and data scientists to create innovative, compliant data products at scale with our clean, easy-to-use datasets of resume, company, location, and education data consumed through our suite of APIs. PDL is an innovative, fast-growing, global team backed by world-class investors, including Craft Ventures, Flex Capital, and Founders Fund. We scour the world for people hungry to improve, curious about how things work, and willing to challenge the status quo to build something new and better. Roles & Responsibilities: Build infrastructure for ingestion, transformation, and loading an exponentially increasing volume of data from a variety of sources using Spark, SQL, AWS, and Databricks Building an organic entity resolution framework capable of correctly merging hundreds of billions of individual entities into a number of clean, consumable datasets. Developing CI/CD pipelines and anomaly detection systems capable of continuously improving the quality of data we're pushing into production. Devising solutions to largely-undefined data engineering and data science problems. Work with stakeholders in Engineering and Product to assist with data-related technical issues and support their infrastructure needs Technical Requirements 5-7+ years industry experience with clear examples of strategic technical problem solving and implementation Strong software development fundamentals. Experience with Python Expertise with Apache Spark (Java, Scala, and/or Python-based) Experience with SQL Experience building scalable data processing systems (e.g., cleaning, transformation) from the ground up. Experience using developer-oriented data pipeline and workflow orchestration (e.g., Airflow (preferred), dbt, dagster or similar) Knowledge of modern data design and storage patterns (e.g., incremental updating, partitioning and segmentation, rebuilds and backfills) Experience working in Databricks (including delta live tables, data lakehouse patterns, etc.) Experience with cloud computing services (AWS (preferred), GCP, Azure or similar) Experience with data warehousing (e.g., Databricks, Snowflake, Redshift, BigQuery, or similar) Understanding of modern data storage formats and tools (e.g., parquet, ORC, Avro, Delta Lake) Professional Requirements Must thrive in a fast paced environment and be able to work independently Can work effectively remotely (able to be proactive about managing blockers, proactive on reaching out and asking questions, and participating in team activities) Strong written communication skills on Slack/Chat and in documents You are experienced in writing data design docs (pipeline design, dataflow, schema design) You can scope and breakdown projects, communicate and collaborate progress and blockers effectively with your manager, team, and stakeholders Nice To Haves: Degree in a quantitative discipline such as computer science, mathematics, statistics, or engineering Experience working with entity data (entity resolution / record linkage) Experience working with data acquisition / data integration Expertise with Python and the Python data stack (e.g., numpy, pandas) Experience with streaming platforms (e.g., Kafka) Experience evaluating data quality and maintaining consistently high data standards across new feature releases (e.g., consistency, accuracy, validity, completeness) Our Benefits Stock Competitive Salaries Unlimited paid time off Medical, dental, & vision insurance Health, fitness, and office stipends The permanent ability to work wherever and however you want No C2C, 1099, or Contract-to-Hire. Recruiters need not apply. People Data Labs does not discriminate on the basis of race, sex, color, religion, age, national origin, marital status, disability, veteran status, genetic information, sexual orientation, gender identity or any other reason prohibited by law in provision of employment opportunities and benefits.",
        "url": "https://www.linkedin.com/jobs/view/3862412441",
        "summary": "People Data Labs is looking for a Data Engineer to build and improve their data infrastructure. This role involves building ingestion, transformation, and loading pipelines for large datasets using Spark, SQL, AWS, and Databricks. The ideal candidate will have 5+ years of experience in data engineering, be proficient in Python and Spark, and have experience with cloud computing and data warehousing.",
        "industries": [
            "Data",
            "Technology",
            "Software Development",
            "Data Science"
        ],
        "soft_skills": [
            "Problem Solving",
            "Communication",
            "Collaboration",
            "Proactive",
            "Independent",
            "Strategic Thinking",
            "Organization",
            "Project Management"
        ],
        "hard_skills": [
            "Spark",
            "SQL",
            "AWS",
            "Databricks",
            "Python",
            "Java",
            "Scala",
            "Airflow",
            "dbt",
            "dagster",
            "Parquet",
            "ORC",
            "Avro",
            "Delta Lake",
            "Kafka",
            "Numpy",
            "Pandas"
        ],
        "tech_stack": [
            "Spark",
            "SQL",
            "AWS",
            "Databricks",
            "Python",
            "Java",
            "Scala",
            "Airflow",
            "dbt",
            "dagster",
            "Parquet",
            "ORC",
            "Avro",
            "Delta Lake",
            "Kafka",
            "Numpy",
            "Pandas"
        ],
        "programming_languages": [
            "Python",
            "Java",
            "Scala"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Computer Science",
                "Mathematics",
                "Statistics",
                "Engineering"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Stock",
            "Competitive Salaries",
            "Unlimited paid time off",
            "Medical, dental, & vision insurance",
            "Health, fitness, and office stipends",
            "Remote work"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Jose, CA",
        "job_id": 3954541152,
        "company": "Steneral Consulting",
        "title": "Hybrid Work - Need Data Engineer :: HANA in San Jose CA",
        "created_on": 1720635860.032963,
        "description": "Domain: Software Development and Digital media products & Services Client: Adobe Job title: Data Engineer - HANA Workplace type: Hybrid (1-2 days onsite) - Locals Only Worksite location: 345 Park Ave, San Jose, California, 95110-2704, United States Duration: 1-year contract Top Skills: HANA, SQL, Data modeling, HANA modeling Top Skills' Details **Please note that the position is hybrid onsite in San Jose, CA. Candidates must be already local, or be onsite on day 1.** Strong demonstrated skill working with SQL programming Experience in HANA database Performance Tuning of Database Schemas, Databases, SQL, ETL Jobs, and related scripts Demonstrated skills in data modeling (HANA data modeling), SQL Stored Procedures Functional knowledge of SAP S/4 and/or ECC (specifically SD) a big plus Experience with troubleshooting/ production support Experience with Big Data (Azure, Databricks) desired",
        "url": "https://www.linkedin.com/jobs/view/3954541152",
        "summary": "Data Engineer position at Adobe requiring expertise in HANA, SQL, data modeling, and performance tuning.  Hybrid onsite in San Jose, CA. Experience with SAP S/4 and/or ECC (specifically SD), Big Data (Azure, Databricks) is desired. ",
        "industries": [
            "Software Development",
            "Digital Media",
            "Technology",
            "Data Engineering"
        ],
        "soft_skills": [
            "Troubleshooting",
            "Problem Solving",
            "Communication",
            "Teamwork"
        ],
        "hard_skills": [
            "HANA",
            "SQL",
            "Data Modeling",
            "HANA Modeling",
            "Performance Tuning",
            "ETL",
            "SAP S/4",
            "ECC",
            "SD",
            "Big Data",
            "Azure",
            "Databricks"
        ],
        "tech_stack": [
            "HANA",
            "SQL",
            "Azure",
            "Databricks",
            "SAP S/4",
            "ECC"
        ],
        "programming_languages": [
            "SQL"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Los Angeles, CA",
        "job_id": 3578845415,
        "company": "JBA International",
        "title": "Database Engineer",
        "created_on": 1720635861.79387,
        "description": "Job Summary: The Database Engineer, Senior will work closely with the engineering team members to design, develop and enhance Microsoft SQL Server and PostgreSQL database solutions. Duties & Responsibilities Strong SQL, experience writing, tuning queries based on application requirements and performing debugging on database scripts and programs, as well as resolving conflicts. Highly skilled problem-solver and communicator that is fluent in most data manipulation languages. Design and code a high volume of SQL Queries, stored procedures, and SSIS packages Actively participate as part of matrixed product development teams providing advice on design and tuning of database objects, queries, and overall data architecture. Use of tools and utilities to monitor, load and unload data, generate, and edit test data Effectively plan and organize daily work following priorities, ensuring timely completion of projects and user support Work with the different Technology and business team members to ensure that the associated compute resources are allocated to the databases and to ensure high availability and optimum performance. Provide trend analysis to the service management team to enable them to make informed decisions regarding resource management. Develops a continually growing knowledge of Company's internal business practices, processes and the daily IT operational needs of the users Provide query and performance expertise in support of code development Monitor long running transactions and optimize query executions with index tuning and optimized T-SQL coding technique Problem escalation to development team and third parties as appropriate. Implementation and release of database changes as submitted by the development team Documenting technical environments and processes as necessary Participate in the on-call rotation Competencies Perform all work and activities with honesty and integrity. Take personal responsibility for productivity, quality and timeliness of work. Effectively communicate (and listen) clearly, professionally, politely and persuasively in all situations; respond well and in a reasonable, timely manner. Challenge conventional practices and use creativity and information to lead, innovate, problem solve, and implement ideas to contribute to the growth of the organization. Support and meet company/department goals and core values. Collaborate with co-workers to achieve common goals. Problem Solving/Analysis. Qualifications & Requirements In-depth SQL knowledge is required, including advanced tuning skills. Expertise in Microsoft SQL Server or PostgreSQL. Experience with clustering and log shipping. Strong communication skills and the ability to share ideas and work well in small teams. Possesses excellent problem-solving capabilities. Independent and self-motivated. Bachelor's degree in computer science, engineering, business, or the equivalent is preferred and 5 plus years of database development experience is preferred. Experience with Microsoft SQL Server 2008 and SSIS is required. Knowledge of other programming languages is a plus.",
        "url": "https://www.linkedin.com/jobs/view/3578845415",
        "summary": "The Database Engineer, Senior will design, develop and enhance Microsoft SQL Server and PostgreSQL database solutions, working closely with the engineering team to ensure high availability, optimum performance, and timely project completion.",
        "industries": [
            "Information Technology",
            "Software Development",
            "Database Management"
        ],
        "soft_skills": [
            "Problem-Solving",
            "Communication",
            "Collaboration",
            "Teamwork",
            "Creativity",
            "Time Management",
            "Organization",
            "Analytical Skills",
            "Problem-Solving"
        ],
        "hard_skills": [
            "SQL",
            "T-SQL",
            "SSIS",
            "Microsoft SQL Server",
            "PostgreSQL",
            "Database Tuning",
            "Query Optimization",
            "Index Tuning",
            "Clustering",
            "Log Shipping",
            "Data Manipulation Languages",
            "Data Architecture",
            "Data Modeling",
            "Data Loading",
            "Data Unloading",
            "Performance Monitoring",
            "Resource Management",
            "Trend Analysis",
            "Code Version Control"
        ],
        "tech_stack": [
            "Microsoft SQL Server",
            "PostgreSQL",
            "SSIS"
        ],
        "programming_languages": [
            "SQL",
            "T-SQL"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Computer Science",
                "Engineering",
                "Business"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3966708560,
        "company": "SPECTRAFORCE",
        "title": "(Remote) Sr. Data Engineer",
        "created_on": 1720635863.4846716,
        "description": "Position: Sr. Data Engineer Location: Remote Duration: 12 Months Responsibilities Collaborate with cross-functional stakeholders to collect data requirements Develop data pipelines using python to import data into snowflake Required: Strong python and SQL skills Experience building data pipelines in python Experience with extracting data from Rest APIs and ingesting it to a cloud data warehouse Experience working with S3 buckets and writing DAGs on Airflow Experience programmatically working with any cloud data warehouse Notice to California Applicants: SPECTRA FORCE ® is committed to complying with the California Privacy Rights Act (“CPRA”) effective January 1, 2023; and all data privacy laws in the jurisdictions in which it recruits and hires employees. A Notice to California Job Applicants Regarding the Collection of Personal Information can be located on our website. Applicants with disabilities may access this notice in an alternative format by contacting NAHR@spectraforce.com About Us: Established in 2004, SPECTRA FORCE ® is one of the largest and fastest-growing diversity-owned staffing firms in the US. The growth of our company is a direct result of our global client service delivery model that is powered by our state-of-the-art A.I. proprietary talent acquisition platform, robust ISO 9001:2015/ISO 27001 certified processes, and strong and passionate client engaged teams. We have built our business by providing talent and project-based solutions, including Contingent, Permanent, and Statement of Work (SOW) services to over 140 clients in the US, Canada, Puerto Rico, Costa Rica, and India. Key industries that we service include Technology, Financial Services, Life Sciences, Healthcare, Telecom, Retail, Utilities and Transportation. SPECTRA FORCE is built on a concept of “human connection,” defined by our branding attitude of NEWJOBPHORIA®, which is the excitement of bringing joy and freedom to the work lifestyle so our people and clients can reach their highest potential. Learn more at: http://www.spectraforce.com Benefits: SPECTRA FORCE offers ACA compliant health benefits as well as dental, vision, accident, critical illness, voluntary life, and hospital indemnity insurances to eligible employees. Additional benefits offered to eligible employees include commuter benefits, 401K plan with matching, and a referral bonus program. SPECTRA FORCE provides unpaid leave as well as paid sick leave when required by law. Equal Opportunity Employer: SPECTRA FORCE is an equal opportunity employer and does not discriminate against any employee or applicant for employment because of race, religion, color, sex, national origin, age, sexual orientation, gender identity, genetic information, disability or veteran status, or any other category protected by applicable federal, state, or local laws. Please contact Human Resources at LOA@spectraforce.com if you require reasonable accommodation.",
        "url": "https://www.linkedin.com/jobs/view/3966708560",
        "summary": "Senior Data Engineer needed for a 12-month contract to develop data pipelines using Python and Snowflake. Responsibilities include collaborating with stakeholders to gather data requirements and building data pipelines to ingest data from Rest APIs into a cloud data warehouse. Experience with S3 buckets, Airflow DAGs, and cloud data warehouses is essential.",
        "industries": [
            "Technology",
            "Financial Services",
            "Life Sciences",
            "Healthcare",
            "Telecom",
            "Retail",
            "Utilities",
            "Transportation"
        ],
        "soft_skills": [
            "Collaboration",
            "Communication"
        ],
        "hard_skills": [
            "Python",
            "SQL",
            "REST APIs",
            "S3",
            "Airflow",
            "Cloud Data Warehousing"
        ],
        "tech_stack": [
            "Python",
            "SQL",
            "Snowflake",
            "REST APIs",
            "S3",
            "Airflow"
        ],
        "programming_languages": [
            "Python",
            "SQL"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Health Insurance",
            "Dental Insurance",
            "Vision Insurance",
            "Accident Insurance",
            "Critical Illness Insurance",
            "Voluntary Life Insurance",
            "Hospital Indemnity Insurance",
            "Commuter Benefits",
            "401K Plan with Matching",
            "Referral Bonus Program",
            "Unpaid Leave",
            "Paid Sick Leave"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Port Hueneme, CA",
        "job_id": 3964404211,
        "company": "Data Intelligence, LLC",
        "title": "Sr. Database Engineer",
        "created_on": 1720635865.1488268,
        "description": "Data Intelligence, LLC (DI) is searching for a full time Senior Database Administrator in NSWC PHD operations in Port Hueneme, CA. . Responsibilities Support in the development, deployment, and maintenance of backend applications, relational databases, such as MSSQL, Azure SQL Database or Azure SQL Managed Instances, and associated business intelligence systems to improve database performance, scalability, and reliability while delivering foundational database solutions that meet best practices Design and run reports, establish and maintain backup and recovery processes, and maintain associated non-referential data Support cloud-based server, platform and database infrastructure required to maintain operational availability and full compliance with Navy cybersecurity requirements Conduct requirements gathering, analysis, design, development, documentation, testing, integration, implementation and support of reports and dashboards in accordance with stakeholder specifications and best practices. Reports and dashboards include advanced out-of-the-box visualizations, custom visualizations, queries, interactive connections, paginated reports, and data refreshes (e.g. Power Query, DAX, SQL) Ensure database server environments meet security requirements in accordance with DISA Security Technical Implementation Guidelines (STIGs) Ensure that the cloud computing environment is monitored and maintained and will report any deviations to the Government Required Skills/Experience BS Degree Minimum of five years in the development and deployment of distributed relational databases and associated BI systems Minimum of five years of experience with the Microsoft SQL server stack, Microsoft SQL, Azure SQL Database or Azure SQL Managed Instances Minimum of five years securing a SQL environment in accordance with DISA STIGs Minimum of five years SQL Server configuration management, optimization, administration, development in client/server architecture, business object modeling and relational database design using MS SQL Server Minimum of five years writing complex Transact SQL queries, stored-procedures and designing relational databases Minimum of five years implementing reports and dashboards including advanced out-of-the-box visualizations, custom visualizations, queries, interactive connections, paginated reports, and data refreshes using tools such as Power Query, DAX, SQL Current U.S. Government Secret Security Clearance Desire Skills/Experiences Experience with IBM Cognos and other On-Line Analytical Processing (OLAP) tools Understanding of Navy Working Capital Fund (WCF) accounting principles and generation of reports / OLAP analysis from WCF funding and cost information Experience leading and designing Service Oriented Architecture (SOA) based application Experience in Navy Security requirements implementation and their impacts to overall software architecture Experience with Microsoft PowerBI Stack Certifications IAT 3 Data Intelligence, DI is an established small business that has supported the critical missions of our government clients since 2005. We provide full life cycle system development, systems engineering, cybersecurity, and supporting analytical and logistics support to C4ISR and other complex systems. We are an equal opportunity employer that offers competitive salaries, comprehensive benefits, a team-oriented environment, and opportunities for advancement. Our excellent employee retention record reflects our employee focus. We work with Veteran’s organization to proactively hire those who have served our country. We offer medical, dental and vision insurance, 401k, PTO and 11 paid holidays. Data Intelligence is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, age, color, religion, creed, sex, sexual orientation, gender identity, national origin, disability, or protected Veteran status.",
        "url": "https://www.linkedin.com/jobs/view/3964404211",
        "summary": "Data Intelligence, LLC is seeking a Senior Database Administrator to support the development, deployment, and maintenance of backend applications, relational databases (MSSQL, Azure SQL), and business intelligence systems at NSWC PHD in Port Hueneme, CA. Responsibilities include database performance optimization, scalability, reliability, security, and reporting.  The candidate will work with cloud-based server and database infrastructure while adhering to Navy cybersecurity regulations.",
        "industries": [
            "Information Technology",
            "Government",
            "Defense",
            "Aerospace",
            "Cybersecurity"
        ],
        "soft_skills": [
            "Problem-solving",
            "Communication",
            "Collaboration",
            "Teamwork",
            "Analytical",
            "Organizational",
            "Attention to detail",
            "Time management",
            "Leadership",
            "Project Management"
        ],
        "hard_skills": [
            "MSSQL",
            "Azure SQL Database",
            "Azure SQL Managed Instances",
            "Power Query",
            "DAX",
            "SQL",
            "Transact SQL",
            "Stored Procedures",
            "Relational Database Design",
            "Business Intelligence",
            "Reporting",
            "Data Analysis",
            "Data Visualization",
            "Cloud Computing",
            "Cybersecurity",
            "DISA STIGs",
            "IBM Cognos",
            "OLAP",
            "SOA",
            "Microsoft PowerBI",
            "Navy Working Capital Fund (WCF)"
        ],
        "tech_stack": [
            "MSSQL",
            "Azure SQL Database",
            "Azure SQL Managed Instances",
            "Power Query",
            "DAX",
            "SQL",
            "Transact SQL",
            "Stored Procedures",
            "IBM Cognos",
            "Microsoft PowerBI"
        ],
        "programming_languages": [
            "SQL",
            "Transact SQL",
            "DAX"
        ],
        "experience": 5,
        "education": {
            "min_degree": "BS",
            "fields": [
                "Computer Science",
                "Information Technology",
                "Database Administration",
                "Related Fields"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Medical Insurance",
            "Dental Insurance",
            "Vision Insurance",
            "401k",
            "Paid Time Off (PTO)",
            "Paid Holidays"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Diego, CA",
        "job_id": 3941856242,
        "company": "Diverse Lynx",
        "title": "Data Engineer � GCP, python core data engineer",
        "created_on": 1720635866.8886175,
        "description": "Role: Data Engineer GCP, python core data engineer Contract Location : San Diego, CA Onsite day 1 Description Essential Skills: Strong SQL and PythonExperience with ETL/ELT, data integrationsExperience with Big Query and handling large data sets Desirable Skills: Strong SQL and PythonExperience with ETL/ELT, data integrationsExperience with Big Query and handling large data sets Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.",
        "url": "https://www.linkedin.com/jobs/view/3941856242",
        "summary": "Data Engineer role focused on GCP and Python with experience in ETL/ELT, data integrations, and BigQuery. Requires strong SQL and Python skills for handling large datasets. Onsite in San Diego, CA.",
        "industries": [
            "Data Engineering",
            "Software Development",
            "Technology"
        ],
        "soft_skills": [],
        "hard_skills": [
            "SQL",
            "Python",
            "ETL/ELT",
            "Data Integrations",
            "BigQuery",
            "Large Data Handling"
        ],
        "tech_stack": [
            "GCP",
            "BigQuery",
            "Python"
        ],
        "programming_languages": [
            "Python",
            "SQL"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Mountain View, CA",
        "job_id": 3964475818,
        "company": "Damco Solutions",
        "title": "Database Engineer/Developer",
        "created_on": 1720635868.6277983,
        "description": "Mountain View, CA (Hybrid 2 days a week) Interview Process: 3 Rounds of Interview Please note we need Database Engineer/Developer not looking for Data Engineer. Requirements Overall Experience of 10-12 Yrs and Bachelor's degree in computer science, Engineering, or related field. Proven experience working with Amazon Aurora and/or PostgreSQL in a production environment. Strong SQL skills and experience with SQL tuning techniques. Proficiency in AWS services such as EC2, Route 53, VPC, IAM, and CloudFormation. Hands-on experience with scripting languages (e.g., Python, Bash) for automation. Familiarity with database security concepts and best practices. Excellent problem-solving skills and attention to detail. Strong communication and collaboration skills, with the ability to work effectively in a team environment. Preferred Qualifications AWS Certification Experience with other AWS database services such as RDS.. Knowledge of containerization technologies (e.g., Docker, Kubernetes). Experience with DevOps practices and tools (e.g., CI/CD pipelines, Git).",
        "url": "https://www.linkedin.com/jobs/view/3964475818",
        "summary": "A Database Engineer/Developer with 10-12 years experience is needed to work with Amazon Aurora and/or PostgreSQL in a production environment. Strong SQL skills, AWS experience (EC2, Route 53, VPC, IAM, CloudFormation), scripting languages (Python, Bash), and database security knowledge are required. AWS certification, experience with other AWS database services, containerization technologies (Docker, Kubernetes), and DevOps practices are preferred.",
        "industries": [
            "Software Development",
            "Technology",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Problem Solving",
            "Attention to Detail",
            "Communication",
            "Collaboration",
            "Teamwork"
        ],
        "hard_skills": [
            "Amazon Aurora",
            "PostgreSQL",
            "SQL",
            "SQL Tuning",
            "AWS",
            "EC2",
            "Route 53",
            "VPC",
            "IAM",
            "CloudFormation",
            "Python",
            "Bash",
            "Database Security",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Git"
        ],
        "tech_stack": [
            "Amazon Aurora",
            "PostgreSQL",
            "AWS",
            "EC2",
            "Route 53",
            "VPC",
            "IAM",
            "CloudFormation",
            "Python",
            "Bash",
            "Docker",
            "Kubernetes",
            "Git"
        ],
        "programming_languages": [
            "Python",
            "Bash"
        ],
        "experience": 10,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Computer Science",
                "Engineering"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3928630076,
        "company": "Reflex",
        "title": "Software Engineer",
        "created_on": 1720635870.2931817,
        "description": "The Role We’re looking for a fantastic SF/Bay Area-based senior software engineer comfortable working on a very early product in a quickly changing codebase and role. Our office is in San Francisco and we work in person M-F. This role will involve working on some of our open-source projects and customer-facing applications. You should be comfortable with a high level of ownership over product decisions and the tech stack. Responsibilities Help optimize and improve our open-source project. Design and implement new features and ensure that Reflex is well-tested and maintained. Collaborate with design, product management, and fellow engineers to build and plan new features Take an active role in the Reflex community by seeking feedback, integrating contributions, and fostering positive and collaborative relationships. What We Look For Very comfortable writing Python. Strong intuition in system design. Passion for open source software and contributing to open source communities. (Nice to have) Strong knowledge of frontend web technologies such as React, JavaScript, HTML, and CSS.",
        "url": "https://www.linkedin.com/jobs/view/3928630076",
        "summary": "Senior software engineer role in San Francisco focused on open source project development and customer-facing applications. Responsibilities include optimizing and improving the open-source project, designing and implementing new features, and collaborating with other teams. Ideal candidate has strong Python skills, system design intuition, and passion for open source.",
        "industries": [
            "Software Development",
            "Technology",
            "Open Source"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Ownership",
            "Problem Solving",
            "Decision Making",
            "Passion",
            "Teamwork"
        ],
        "hard_skills": [
            "Python",
            "System Design",
            "React",
            "JavaScript",
            "HTML",
            "CSS",
            "Testing",
            "Code Maintenance",
            "Open Source"
        ],
        "tech_stack": [
            "Python",
            "React",
            "JavaScript",
            "HTML",
            "CSS"
        ],
        "programming_languages": [
            "Python",
            "JavaScript"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Sunnyvale, CA",
        "job_id": 3968430342,
        "company": "Futran Solutions",
        "title": "Data engineers - Graph DB",
        "created_on": 1720635874.2755187,
        "description": "contract/fulltime Deep hands on in Graph DB both Design Graph DB and Optimizing Graph DB (KG Area) Have Strong Background in Data warehouse (Relational and no-sql Data Models) (DataWarehouse / BI Area) Have strong data Science background specially in Natural Language Processing space (Data Science Area) Location: Bay Area, CA(Sunnyvale) Job Description: We're seeking data engineers to work on scalable data workflows to enrich knowledge graph. As an expert in developing software to manage large, dynamic datasets, you'll be building and optimizing pipelines for data ingestion, cleaning, transformation and evaluation to support a rapidly scaling organization. Key Qualifications 8 years of experience as a Software Engineer processing large-scale datasets Excellent programming skills - e.g. Python, Go, Java, Scala. Excellent problem-solving, analytic, and debugging skills Solid computer science and systems foundations; ability to quickly learn new domains Proven software development skills in UNIX-type OS (e.g. Linux, Mac OS) Experience working with large data sets and pipelines, ideally using the Apache software stack (e.g. Spark, HBase) Experience with continuous integration and continuous development solutions (e.g. Jenkins, etc.) Experience with cloud-native deployment is a good plus (e.g. Kubernetes) Good communication skills and teamwork Passion for building great products Experience in tooling and streamlining workflows in complex processes Description You'll be working at the frontier of AI, crunching massive amounts of data for our knowledge graph. In a fast-paced, continuously-integrated environment, you'll design and implement robust, scalable data pipeline and solutions capable of processing an ever-growing data set while keeping latency low and quality high. You'll work closely with fellow engineers to integrate new data, and deliver our knowledge products to cross-functional teams in Apple to power customer-facing scenarios. Your responsibilities will also include developing tools and tests to ensure quality and help diagnose issues.",
        "url": "https://www.linkedin.com/jobs/view/3968430342",
        "summary": "Data Engineer needed to design and implement scalable data pipelines for a knowledge graph at Apple.  Experience with graph databases, data warehousing, data science, and big data technologies is required. This role focuses on data ingestion, cleaning, transformation, and evaluation for a rapidly growing company.",
        "industries": [
            "Technology",
            "Artificial Intelligence",
            "Software Development",
            "Data Science",
            "Data Engineering",
            "Big Data",
            "Knowledge Graph"
        ],
        "soft_skills": [
            "Problem Solving",
            "Analytical",
            "Debugging",
            "Communication",
            "Teamwork",
            "Passion for Building Great Products",
            "Tooling",
            "Workflow Streamlining"
        ],
        "hard_skills": [
            "Graph Database Design",
            "Graph Database Optimization",
            "Data Warehousing",
            "Relational Data Models",
            "NoSQL Data Models",
            "Data Science",
            "Natural Language Processing",
            "Python",
            "Go",
            "Java",
            "Scala",
            "Unix",
            "Linux",
            "Mac OS",
            "Apache Spark",
            "Apache HBase",
            "Continuous Integration",
            "Continuous Development",
            "Jenkins",
            "Kubernetes"
        ],
        "tech_stack": [
            "Python",
            "Go",
            "Java",
            "Scala",
            "Spark",
            "HBase",
            "Jenkins",
            "Kubernetes"
        ],
        "programming_languages": [
            "Python",
            "Go",
            "Java",
            "Scala"
        ],
        "experience": 8,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Data Science",
                "Engineering"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "California, United States",
        "job_id": 3948282258,
        "company": "EVONA",
        "title": "Junior Software Engineer",
        "created_on": 1720635875.8235295,
        "description": "Software Engineer Location: Sacramento, California Salary: $65,000 - $95,000 Employment Type: Full-time, On-Site Company Overview: Join an innovative engineering company at the forefront of developing cutting-edge propulsion and ISR (intelligence, surveillance, and reconnaissance) technologies. Due to continued growth, we are seeking a talented Software Engineer to join our team in Sacramento. Responsibilities: Perform software analysis, design, development, and testing of software products with minimal supervision. Collaborate closely with software and systems engineers through all phases of software development. Evaluate commercial and open-source software (FOSS) technologies for integration into our software products. Conceptualize and prototype alternative architectural approaches. Contribute to the creation of briefing materials for customer reviews and technical interchange meetings. Perform other related duties as assigned. Experience & Qualifications: U.S. Citizenship or status as a U.S. Person as defined by 8 USC 1324b (a) (3) is required. Bachelor’s degree in Engineering or Computer Science from an accredited university. Minimum of two years of practical experience in a similar role. Proficiency in C, C++, or similar systems programming languages targeting Linux platforms. Comfortable using command line interfaces. Experience writing shell and/or Python scripts for task automation. Understanding of network protocols, IP addressing, and subnet masking. Basic knowledge of object-oriented software design principles, distributed computing concepts, and embedded software concepts. Ability to multitask in a highly collaborative team environment. Strong verbal and written communication skills. Additional Requirements: Due to the nature of the client’s customer base, U.S. Citizenship, U.S. Permanent Residency, or other status as a U.S. Person is required. Must be able to obtain and maintain a U.S. Security Clearance at the appropriate level. Why Join Us: This is an exciting opportunity to join a leading engineering company and contribute to the development of advanced technologies. We offer a competitive salary range of $65,000 - $95,000 and the chance to work on groundbreaking projects in a collaborative and dynamic environment. If you are a driven software engineer with the required experience and qualifications, we encourage you to apply and be a part of our innovative team. Apply now to make a significant impact in the field of propulsion and ISR technologies!",
        "url": "https://www.linkedin.com/jobs/view/3948282258",
        "summary": "This position involves software analysis, design, development, and testing of software products for a company specializing in propulsion and ISR (intelligence, surveillance, and reconnaissance) technologies. The role requires experience in C/C++, Linux, scripting (shell/Python), network protocols, and object-oriented design. The ideal candidate will possess strong communication skills and be able to work effectively in a collaborative team environment.",
        "industries": [
            "Aerospace",
            "Defense",
            "Technology",
            "Engineering"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Analytical",
            "Teamwork",
            "Self-Management",
            "Multitasking"
        ],
        "hard_skills": [
            "Software Analysis",
            "Software Design",
            "Software Development",
            "Software Testing",
            "C",
            "C++",
            "Linux",
            "Shell Scripting",
            "Python Scripting",
            "Network Protocols",
            "IP Addressing",
            "Subnet Masking",
            "Object-Oriented Design",
            "Distributed Computing",
            "Embedded Software"
        ],
        "tech_stack": [
            "C",
            "C++",
            "Linux",
            "Python",
            "Shell",
            "Git"
        ],
        "programming_languages": [
            "C",
            "C++",
            "Python",
            "Shell"
        ],
        "experience": 2,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Engineering",
                "Computer Science"
            ]
        },
        "salary": {
            "max": 95000,
            "min": 65000
        },
        "benefits": [
            "Competitive Salary",
            "Opportunity to Work on Groundbreaking Projects",
            "Collaborative Environment"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3914084799,
        "company": "VARITE INC",
        "title": "Sr Data Engineer",
        "created_on": 1720635877.513376,
        "description": "Pay rate range: $70-90/hr. True Job Title-Senior Infrastructure Automation Engineer Location-this is a hybrid position but we can consider remote candidates with exceptional technical expertise. This role will be subject to level 2 screening. In this role, you will get an opportunity to broadly apply your engineering skills across various technology solutions, as well as build your skills in other areas by being exposed to various aspects of product delivery from inception, through design, build, and deployment. You will be working multi-functionally with Product Managers, Architects, Engineers, and Customer teams in a rapidly evolving environment. You will be developing Infrastructure as Code to launch server instances, install and configure software, amongst other things. You will provide technical leadership in the planning, design, and implementation of cloud-based infrastructure systems with both traditional and non-traditional infrastructures. • Demonstrated technical expertise across a variety of technology platforms and solutions • Ability to work effectively independently or in a team environment • Strong analytical, collaboration, leadership, critical thinking, multitasking, and time management skills • Ability to influence successfully in a highly matrix or virtual organization • Demonstrates independent thinking and decision-making abilities • Ability to quickly receive and process information • Proven ability to write clear and concise communications: technical documents, design documents, specifications • 3 or more years of relevant technical experience, at senior level • Bachelor's degree in Computer Science, Information Systems, or other related technical field or equivalent work • Must be a US Citizen or a Green Card holder for over 3 years and with the intent to become a US Citizen Required Skills: • Experience in using Terraform to manage AWS Programmable Infrastructures • Must have architected and implemented the Cloud Infrastructure Automation scripts to create and maintain various target environments like Dev, Stage, QA, Integration and Production in AWS environments • The Infrastructure must include security roles and permissions, Cloud networking assets like VPC, Subnets, Routing Tables, Access Controls lists, storage assets like S3 buckets, creating lambda functions & layers, provisioning other AWS services like Redshift, DynamoDB etc. • Experience with advanced features like S3 backends and State file locks in Terraform. • Experienced in implementing Data and Advanced Analytics solutions, or related experience in the Cloud • Experience in developing an end to end AWS native platform for building Data lakes ( S3, Glue (Crawlers, ETL, Catalog), IAM, CodePipeline, CodeCommit, CloudTrail, CloudWatch, AWS Config, Guard Duty, Secrets Manager, KMS, EC2, Data Visualization Tool like Tableau run on an EC2 or AWS Quicksight, Athena • Hands on programming skills in JAVA or Python or Scala or other scripting language. • Working knowledge of Amazon Web Services • Experience in Continuous Integration, Continuous Delivery, and Continuous Deployment software tools to support, enhance and grow our CI and CD capabilities • Understanding of security design for enterprise software systems Preferred Skills: • Knowledge of high-availability, load-balancing and failover configurations across application, infrastructure, and platform • Experience with and/or working knowledge the Financial Industry, Government Agencies, Client Bank Lines of Business (LoBs) Applications • Working experience with Kubernetes, ConnectDirect, etc. • Practical experience and knowledge of Service Oriented Architecture (SOA), Mircoservices and API Management • Background in data security, governance and cybersecurity solutions.",
        "url": "https://www.linkedin.com/jobs/view/3914084799",
        "summary": "Senior Infrastructure Automation Engineer responsible for designing, implementing, and maintaining cloud-based infrastructure systems using AWS and Terraform.  This role involves working with various teams to build, deploy, and manage infrastructure, as well as developing Infrastructure as Code for automating server provisioning and software configuration.",
        "industries": [
            "Technology",
            "Cloud Computing",
            "Data Analytics",
            "Financial Services",
            "Government"
        ],
        "soft_skills": [
            "Analytical",
            "Collaboration",
            "Leadership",
            "Critical Thinking",
            "Multitasking",
            "Time Management",
            "Influence",
            "Independent Thinking",
            "Decision Making",
            "Communication"
        ],
        "hard_skills": [
            "Terraform",
            "AWS",
            "Infrastructure as Code",
            "Security Roles and Permissions",
            "Cloud Networking",
            "VPC",
            "Subnets",
            "Routing Tables",
            "Access Control Lists",
            "S3",
            "Lambda Functions",
            "Redshift",
            "DynamoDB",
            "S3 Backends",
            "State File Locks",
            "Data Analytics",
            "Data Lakes",
            "Glue",
            "ETL",
            "IAM",
            "CodePipeline",
            "CodeCommit",
            "CloudTrail",
            "CloudWatch",
            "AWS Config",
            "Guard Duty",
            "Secrets Manager",
            "KMS",
            "EC2",
            "Tableau",
            "Quicksight",
            "Athena",
            "Java",
            "Python",
            "Scala",
            "Continuous Integration",
            "Continuous Delivery",
            "Continuous Deployment",
            "Security Design",
            "High Availability",
            "Load Balancing",
            "Failover",
            "Kubernetes",
            "ConnectDirect",
            "Service Oriented Architecture",
            "Microservices",
            "API Management",
            "Data Security",
            "Governance",
            "Cybersecurity"
        ],
        "tech_stack": [
            "AWS",
            "Terraform",
            "S3",
            "Lambda",
            "Redshift",
            "DynamoDB",
            "Glue",
            "IAM",
            "CodePipeline",
            "CodeCommit",
            "CloudTrail",
            "CloudWatch",
            "AWS Config",
            "Guard Duty",
            "Secrets Manager",
            "KMS",
            "EC2",
            "Tableau",
            "Quicksight",
            "Athena",
            "Kubernetes",
            "ConnectDirect"
        ],
        "programming_languages": [
            "Java",
            "Python",
            "Scala"
        ],
        "experience": 3,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Computer Science",
                "Information Systems",
                "Related Technical Field"
            ]
        },
        "salary": {
            "max": 90,
            "min": 70
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Jose, CA",
        "job_id": 3891684433,
        "company": "Dew Software",
        "title": "Software Engineer (Front End)",
        "created_on": 1720635879.036114,
        "description": "Dew Software is seeking a talented Front End Software Engineer to join our team. As a Front End Software Engineer, you will be responsible for developing and maintaining user interfaces for web applications. You will work closely with designers, product managers, and backend developers to create highly functional and visually appealing web applications. If you are passionate about building intuitive and responsive user interfaces and enjoy working in a collaborative environment, we'd love to hear from you. Responsibilities Develop and maintain responsive and highly functional user interfaces for web applications using HTML, CSS, and JavaScript Collaborate with designers and backend developers to ensure a seamless and visually appealing user experience Optimize web applications for maximum performance and scalability Write clean and maintainable code using best practices Perform code reviews and provide constructive feedback to peers Stay up to date with the latest front end development trends and technologies Requirements Bachelor's degree in Computer Science or a related field Strong proficiency in HTML, CSS, and JavaScript Experience with front-end frameworks such as React, Angular, or Vue.js Knowledge of responsive design principles and best practices Experience with version control systems such as Git Ability to work collaboratively in a team environment Strong problem-solving and communication skills",
        "url": "https://www.linkedin.com/jobs/view/3891684433",
        "summary": "Dew Software is hiring a Front End Software Engineer to develop and maintain user interfaces for web applications. Responsibilities include developing responsive UIs, collaborating with designers and backend developers, optimizing for performance and scalability, writing clean code, and staying up-to-date with front-end trends. Required skills include HTML, CSS, JavaScript, front-end frameworks (React, Angular, Vue.js), responsive design, version control (Git), and strong communication and problem-solving skills.",
        "industries": [
            "Software Development",
            "Web Development",
            "Technology"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem-solving"
        ],
        "hard_skills": [
            "HTML",
            "CSS",
            "JavaScript",
            "React",
            "Angular",
            "Vue.js",
            "Responsive Design",
            "Git"
        ],
        "tech_stack": [
            "HTML",
            "CSS",
            "JavaScript",
            "React",
            "Angular",
            "Vue.js",
            "Git"
        ],
        "programming_languages": [
            "HTML",
            "CSS",
            "JavaScript"
        ],
        "experience": 0,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Computer Science",
                "Related field"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Foster City, CA",
        "job_id": 3965546605,
        "company": "HireArt",
        "title": "Data Engineer (Databricks)",
        "created_on": 1720635880.7372026,
        "description": "Expected compensation: $72.00 - $77.00 per hour HireArt is helping an on-demand, autonomous ride-hailing company hire an experienced Databricks Administrator to manage and maintain company data stack tools. We are looking for a Databricks Administrator who will be responsible for administering and maintaining different workspaces on Databricks running on AWS. Responsibilities Run Databricks at a workspace and account level. Set up and advise on architecture and scaling for the Databricks environment, including administering, configuring and installing libraries. Collaborate with data scientists, analysts, and other stakeholders to understand and deliver data solutions. Troubleshoot end user and platform-level issues. Document and maintain Databricks best practices and standards. Manage/monitor the following: Tenants, including workspace creation, user management, cloud resources, and account usage. Cluster and jobs, policies, templates, and pools configuration options. Auto-scaling to ensure resources are allocated efficiently. Workspace users and groups, including single sign-on, provisioning, and access control to workspace storage accounts across a large user base. Requirements 5+ years of experience working as Databricks administrator/architect Strong fluency with Python or Java Advanced understanding of SQL to extract data from the databases Must have hands-on experience with the following: Serving as the Databricks account owner, including security and privacy setup, marketplace plugins and integration with other tools Unity Catalog migration, workspaces and audit logs Amazon Web Services (AWS) accounts and high-level usage monitoring Hadoop and EMR administration Preferred Qualifications Experience optimizing usage for performance, including monitoring cluster health checks and cost Experience with Looker/LookML Benefits Pre-tax commuter benefits Employer (HireArt) Subsidized healthcare benefits Flexible Spending Account for healthcare-related costs HireArt covers all costs for short and long term disability and life insurance 401k package Commitment: This is a full-time, 6-month, ongoing contract position staffed via HireArt. This role is hybrid requiring at least 3 days onsite. It will be available to candidates who are local to the Foster City, CA area. HireArt values diversity and is an Equal Opportunity Employer. We are interested in every qualified candidate who is eligible to work in the United States. Unfortunately, we are not able to sponsor visas or employ corp-to-corp .",
        "url": "https://www.linkedin.com/jobs/view/3965546605",
        "summary": "HireArt is seeking an experienced Databricks Administrator to manage and maintain an on-demand, autonomous ride-hailing company's data stack tools. This role involves administering and maintaining Databricks workspaces on AWS, collaborating with data scientists and analysts, troubleshooting issues, documenting best practices, and managing various aspects of the Databricks environment.",
        "industries": [
            "Ride-hailing",
            "Transportation",
            "Data",
            "Technology"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem-solving",
            "Troubleshooting",
            "Documentation",
            "Teamwork"
        ],
        "hard_skills": [
            "Databricks Administration",
            "Databricks Architecture",
            "Python",
            "Java",
            "SQL",
            "Unity Catalog",
            "AWS",
            "Hadoop",
            "EMR",
            "Looker",
            "LookML"
        ],
        "tech_stack": [
            "Databricks",
            "AWS",
            "Hadoop",
            "EMR",
            "Unity Catalog",
            "Looker",
            "LookML"
        ],
        "programming_languages": [
            "Python",
            "Java",
            "SQL"
        ],
        "experience": 5,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 77,
            "min": 72
        },
        "benefits": [
            "Pre-tax commuter benefits",
            "Employer (HireArt) Subsidized healthcare benefits",
            "Flexible Spending Account for healthcare-related costs",
            "Short and long term disability insurance",
            "Life insurance",
            "401k package"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Oakland, CA",
        "job_id": 3919928159,
        "company": "EA Team Inc",
        "title": "Azure Data Factory Engineer",
        "created_on": 1720635884.9009671,
        "description": "Minimum 8+ years of developing and deploy data oriented solutions in Azure Cloud Minimum 5+ years of Creation of data transformations using Azure Data Factory, Synapse Analytics, Databricks Design and implement data ingestion to Azure Storage services Azure Data Factory Azure Data Lake Storage SQL Database Azure Data certifications will be a big advantage Apply cloud and data engineering skills to solve problems and design approaches Translate requirements, design and develop programs Willingness to participate in all aspects of the software development life cycle Communicate effectively in a collaborative, complex and high performing team environment.",
        "url": "https://www.linkedin.com/jobs/view/3919928159",
        "summary": "Data Engineer with 8+ years of experience in developing and deploying data-oriented solutions in Azure Cloud. Must have 5+ years of experience in creating data transformations using Azure Data Factory, Synapse Analytics, and Databricks. Proven ability to design and implement data ingestion to Azure Storage services. Experience with Azure Data Factory, Azure Data Lake Storage, and SQL Database is required. Azure Data certifications are a plus.",
        "industries": [
            "Information Technology",
            "Data Science",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Problem Solving",
            "Design",
            "Communication",
            "Teamwork",
            "Collaboration"
        ],
        "hard_skills": [
            "Azure Data Factory",
            "Synapse Analytics",
            "Databricks",
            "Data Ingestion",
            "Azure Storage",
            "Azure Data Lake Storage",
            "SQL Database"
        ],
        "tech_stack": [
            "Azure Cloud",
            "Azure Data Factory",
            "Synapse Analytics",
            "Databricks",
            "Azure Storage",
            "Azure Data Lake Storage",
            "SQL Database"
        ],
        "programming_languages": [],
        "experience": 8,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Sunnyvale, CA",
        "job_id": 3797074030,
        "company": "LinkedIn",
        "title": "Staff Software Engineer - Data Applications",
        "created_on": 1720635886.6750288,
        "description": "LinkedIn is the world’s largest professional network, built to help members of all backgrounds and experiences achieve more in their careers. Our vision is to create economic opportunity for every member of the global workforce. Every day our members use our products to make connections, discover opportunities, build skills and gain insights. We believe amazing things happen when we work together in an environment where everyone feels a true sense of belonging, and that what matters most in a candidate is having the skills needed to succeed.  It inspires us to invest in our talent and support career growth. Join us to challenge yourself with work that matters. This role will be based in Sunnyvale. At LinkedIn, we trust each other to do our best work where it works best for us and our teams. This role offers a hybrid work option, meaning you can both work from home and commute to a LinkedIn office, depending on what’s best for you and when it is important for your team to be together. LinkedIn’s Data Science team leverages big data to empower business decisions and deliver data-driven insights, metrics, and tools in order to drive member engagement, business growth, and monetization efforts. We are now looking for a talented and driven individual to accelerate our efforts and be a major part of our data-centric culture. This person will work with the rest of data science team and other cross function partners to work on a wide list of projects from generalized tools, implementations of complex analyses, prototypes of new methodologies from our Applied Research partner team, or collaborations with other engineering teams to enhance products for Data Science specific needs.You will be responsible for guiding and building the applications that empower our Data Scientists. This is a unique opportunity to work end-to-end on 0 to 1 efforts as opposed to just incremental features of large established products. Responsibilities: ● Work with a team of high-performing data science professionals, and cross-functional teams to identify business opportunities and build scalable data solutions. ● Establish efficient design and programming patterns for engineers as well as for non-technical partners. ● Build web applications and platforms that enable producers and consumers of data insights to work smarter and more efficiently. ● Lead the architecture and design of both front-end and back-end for novel data applications. ● Own the application development for one or more of our internal products and collaborate with other engineers, data scientists, and product managers to launch new products, iterate on existing features, and build a world-class user experience. ● Engage with internal data platform teams to prototype and validate tools developed in-house to derive insight from very large datasets or automate complex algorithms. ● Contribute to engineering innovations that fuel LinkedIn’s vision and mission. Basic Qualifications: ● Bachelor’s Degree in a quantitative discipline: Computer Science, Statistics, Operations Research, Informatics, Engineering, Applied Mathematics, Economics, etc. ● 5+ years of industry experience Preferred Qualifications: ● Experience writing RESTful APIs with modern frameworks (Spring/Flask). ● 7+ years of relevant work experience. ● MS or PhD in Computer Science or related technical discipline. ● Experience with data products and basic statistics. ● Experience creating data visualizations and UX design. ● Experience building data science or machine learning platforms. ● Familiarity with source control, testing frameworks, and all aspects of developing in large, distributed software teams. ● Excellent communication skills, with the ability to synthesize, simplify and explain complex problems to different types of audiences. Suggested Skills : ● Spring/Flask ● Data Visualization ● UX Designs ● Technical Leadership You will Benefit from our Culture: We strongly believe in the well-being of our employees and their families. That is why we offer generous health and wellness programs and time away for employees of all levels. LinkedIn is committed to fair and equitable compensation practices. The pay range for this role is $144,000 to $235,000. Actual compensation packages are based on a wide array of factors unique to each candidate, including but not limited to skill set, years & depth of experience, certifications and specific office location. This may differ in other locations due to cost of labor considerations. The total compensation package for this position may also include annual performance bonus, stock and benefits. For additional information, visit: https://careers.linkedin.com/benefits. Equal Opportunity Statement LinkedIn is committed to diversity in its workforce and is proud to be an equal opportunity employer. LinkedIn considers qualified applicants without regard to race, color, religion, creed, gender, national origin, age, disability, veteran status, marital status, pregnancy, sex, gender expression or identity, sexual orientation, citizenship, or any other legally protected class. LinkedIn is an Affirmative Action and Equal Opportunity Employer as described in our equal opportunity statement here: https://microsoft.sharepoint.com/:b:/t/LinkedInGCI/EeE8sk7CTIdFmEp9ONzFOTEBM62TPrWLMHs4J1C_QxVTbg?e=5hfhpE. Please reference https://www.eeoc.gov/sites/default/files/2023-06/22-088_EEOC_KnowYourRights6.12ScreenRdr.pdf and https://www.dol.gov/ofccp/regs/compliance/posters/pdf/OFCCP_EEO_Supplement_Final_JRF_QA_508c.pdf for more information. LinkedIn is committed to offering an inclusive and accessible experience for all job seekers, including individuals with disabilities. Our goal is to foster an inclusive and accessible workplace where everyone has the opportunity to be successful. If you need a reasonable accommodation to search for a job opening, apply for a position, or participate in the interview process, connect with us at accommodations@linkedin.com and describe the specific accommodation requested for a disability-related limitation. Reasonable accommodations are modifications or adjustments to the application or hiring process that would enable you to fully participate in that process. Examples of reasonable accommodations include but are not limited to: -Documents in alternate formats or read aloud to you -Having interviews in an accessible location -Being accompanied by a service dog -Having a sign language interpreter present for the interview A request for an accommodation will be responded to within three business days. However, non-disability related requests, such as following up on an application, will not receive a response. LinkedIn will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by LinkedIn, or (c) consistent with LinkedIn's legal duty to furnish information. Pay Transparency Policy Statement As a federal contractor, LinkedIn follows the Pay Transparency and non-discrimination provisions described at this link: https://lnkd.in/paytransparency. Global Data Privacy Notice for Job Candidates This document provides transparency around the way in which LinkedIn handles personal data of employees and job applicants: https://lnkd.in/GlobalDataPrivacyNotice",
        "url": "https://www.linkedin.com/jobs/view/3797074030",
        "summary": "LinkedIn is seeking a Data Science Platform Engineer to build scalable data solutions, design efficient programming patterns, and develop web applications for data scientists. The role involves leading the architecture and design of data applications, owning application development, and collaborating with cross-functional teams.  This is a unique opportunity to work end-to-end on new data products. ",
        "industries": [
            "Information Technology",
            "Data Science",
            "Software Development",
            "Internet",
            "Networking"
        ],
        "soft_skills": [
            "Communication",
            "Problem Solving",
            "Leadership",
            "Collaboration",
            "Technical Leadership"
        ],
        "hard_skills": [
            "RESTful APIs",
            "Spring",
            "Flask",
            "Data Visualization",
            "UX Design",
            "Data Science",
            "Machine Learning",
            "Source Control",
            "Testing Frameworks"
        ],
        "tech_stack": [
            "Spring",
            "Flask",
            "RESTful APIs"
        ],
        "programming_languages": [
            "Python",
            "Java"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor’s Degree",
            "fields": [
                "Computer Science",
                "Statistics",
                "Operations Research",
                "Informatics",
                "Engineering",
                "Applied Mathematics",
                "Economics"
            ]
        },
        "salary": {
            "max": 235000,
            "min": 144000
        },
        "benefits": [
            "Health and wellness programs",
            "Time away",
            "Annual performance bonus",
            "Stock",
            "Benefits"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3929354518,
        "company": "Worldcoin",
        "title": "Senior Data Engineer",
        "created_on": 1720635888.3482504,
        "description": "About the Company: Worldcoin ( www.worldcoin.org ) is an open-source protocol, supported by a global community of developers, individuals, economists and technologists committed to expanding participation in, and access to, the global economy. Its community is united around core beliefs in the inherent worth and equality of every individual, the right to personal privacy, and open and public collaboration. These beliefs are reflected in what the community is building: a public utility to connect everyone to the global economy. The Worldcoin Foundation ( www.worldcoin.foundation ) is the protocol’s steward and will support and grow the Worldcoin community until it becomes self-sufficient. Tools for Humanity ( www.toolsforhumanity.com ) is a global hardware and software development company. It helped launch Worldcoin and continues to provide support to the Foundation, in addition to operating the World App. About the Team: The data team is responsible for deriving data-driven insights to support TFH’s key decisions. Because of our unique growth model of using a global network of Orb Operators to give a free share of Worldcoin to everyone on Earth, we require a high-performing Data team that is able to manage and analyze large amounts of data generated by the Orb, by the Worldcoin app, and by blockchain activity. In particular, the Data team’s responsibilities include data and analytics engineering, graph analysis, business intelligence, and statistical analysis. About the Opportunity: As a Senior Data Engineer, you will be responsible for developing the data architecture and pipelines that support our data-driven initiatives. In addition, you will also take on significant Analytics Engineering work, applying engineering best practices to provide clean, reliable, and up-to-date data, ready for analysis. You will work closely with data scientists, analysts, and other stakeholders to ensure data quality, accessibility, and performance across various data storage systems and platforms. You will be part of a team that serves and collaborates with many other teams, including Fraud and Risk Analytics, Product, Market Operations, Blockchain, and the wider Engineering team. In this role, you will: Work closely with other data engineers and analytics engineers and contribute to the building of a new data architecture, including data models, database structures, and storage solutions Build, maintain, and optimize scalable and efficient data pipelines to enable data ingestion, processing, and transformation from various internal and external sources Assist with project planning, estimation, and resource allocation for data engineering initiatives, ensuring timely delivery of high-quality solutions Monitor, troubleshoot, and optimize data pipeline performance; identifying and resolving issues to ensure high levels of reliability and availability Collaborate with data scientists, analysts, and other stakeholders to understand their data needs and develop solutions to address them About You: Degree in a quantitative field like computer science, engineering or physics 5+ years of hands-on work experience as a data engineer or in a similar role Experience in database design and data warehousing concepts, with specific expertise in Snowflake Strong analytical and problem-solving skills, with the ability to understand complex data systems and develop innovative solutions Excellent programming skills in SQL Significant programming experience in at least one high-level programming language (e.g., Python, Java) Experience with DBT and data modeling techniques Experience with AWS a plus Familiarity with Spark Familiarity with data integration and ETL (Stitch, Airbyte or Fivetran) or data replication (AWS DMS) tools and techniques Familiarity with workflow management tools, such as Airflow, Prefect or Luigi a plus Excellent communication and collaboration skills and experienced working in cross-functional teams Proven track record of engineering systems in a fast-paced and continuously evolving environment; start-up experience a plus Pay transparency statement (for CA and NY based roles): The reasonably estimated salary for this role at TFH ranges from $193,000 - $215,000 , plus a competitive long term incentive package. Actual compensation is based on factors such as the candidate's skills, qualifications, and experience. In addition, TFH offers a wide range of best in class, comprehensive and inclusive employee benefits for this role including healthcare, dental, vision, 401(k) plan and match, life insurance, flexible time off, commuter benefits, professional development stipend and much more! By submitting your application, you consent to the processing and internal sharing of your CV within the company, in compliance with the GDPR",
        "url": "https://www.linkedin.com/jobs/view/3929354518",
        "summary": "Worldcoin Foundation is looking for a Senior Data Engineer to build and maintain data architecture and pipelines. This role requires 5+ years of experience in data engineering and expertise in Snowflake, SQL, Python/Java, DBT, and AWS. The data team is responsible for analyzing data generated by the Orb, Worldcoin app, and blockchain activity.",
        "industries": [
            "Technology",
            "Software Development",
            "Blockchain",
            "Fintech",
            "Data Engineering"
        ],
        "soft_skills": [
            "Analytical",
            "Problem-solving",
            "Communication",
            "Collaboration",
            "Teamwork",
            "Time Management",
            "Project Planning",
            "Resource Allocation",
            "Troubleshooting",
            "Optimization",
            "Problem Solving"
        ],
        "hard_skills": [
            "Snowflake",
            "SQL",
            "Python",
            "Java",
            "DBT",
            "AWS",
            "Spark",
            "Data Integration",
            "ETL",
            "Data Replication",
            "Airflow",
            "Prefect",
            "Luigi"
        ],
        "tech_stack": [
            "Snowflake",
            "SQL",
            "Python",
            "Java",
            "DBT",
            "AWS",
            "Spark",
            "Stitch",
            "Airbyte",
            "Fivetran",
            "AWS DMS",
            "Airflow",
            "Prefect",
            "Luigi"
        ],
        "programming_languages": [
            "SQL",
            "Python",
            "Java"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Engineering",
                "Physics"
            ]
        },
        "salary": {
            "max": 215000,
            "min": 193000
        },
        "benefits": [
            "Healthcare",
            "Dental",
            "Vision",
            "401(k)",
            "Life Insurance",
            "Flexible Time Off",
            "Commuter Benefits",
            "Professional Development Stipend"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Aliso Viejo, CA",
        "job_id": 3970521060,
        "company": "Ambry Genetics",
        "title": "Software Engineer II- Remote USA",
        "created_on": 1720635890.1405241,
        "description": "Compensation : $104,000-$120,000 per year (CA minimum= $115,763, WA minimum= $118,514). You are eligible to a Short-Term Incentive Plan with the target at 7.5% of your annual earnings, terms and conditions apply. Software Engineer II-Remote, USA Software engineers design and develop custom software, systems, and integrations. Software engineers work with a team of fellow software engineers, QA engineers, and business systems analysts in an agile/scrum process to deliver enhanced product value. In addition to writing code, the successful candidate will design technical solutions with IT operations teams, participate in scrum ceremonies, and work with stakeholders to understand workflows and requests. As level 2 engineers, candidates are expected to complete tasks assigned to them, but are also expected to utilize critical thinking to identify design issues and propose technical or functional enhancements. Essential Functions Design and develop software according to ticket specifications Participate on both sides of code review – giving and receiving feedback Participate in scrum ceremonies, including backlog grooming, sprint planning, and daily standups Meet with stakeholders to understand workflows Help leads and systems analysts curate tickets, providing workflow analysis Monitor process and log information and troubleshoot problems identified Other duties as assigned Qualifications Bachelor’s in computer science or related discipline required, or equivalent experience. Confident ability to code in object-oriented programming (OOP) languages specifically Java, then Python. Experience with frontend web development using HTML, CSS, and JavaScript Solid experience with frontend frameworks like ReactJS Strong understanding of relational databases, ex: MySQL, PostgreSQL, or MSSQL Experience with using non-relational databases, ex: MongoDB Ability to effectively use version control, including ability to branch and merge in git Ability to programmatically read and write a variety of structured and unstructured formats, such as JSON, CSV, and XML Experience with production and consumption of web services (ex: REST, SOAP, AMQP) Basic understanding and usage of Docker containers Critical thinking: successful candidate is able to review monitor and log info to identify issues and trends, as well as thinking through implementation details during grooming and development 3+ years previous experience in software engineering Preferred Certifications in cloud technologies, containers, security, or other technology may be beneficial (AWS) About Us: Ambry Genetics Corporation is a CAP-accredited and CLIA-licensed molecular genetics laboratory based in Aliso Viejo, California. We are a genetics-based healthcare company that is dedicated to open scientific exchange so we can work together to understand and treat all human disease faster. At Ambry, everyone is welcome. A career at Ambry Genetics is a chance to be part of a dynamic company that aims to improve health by understanding the relationships between genetics and human disease. We earned our reputation as industry leaders by responsibly introducing cutting-edge genetic testing solutions and continually sharing what we learn with the global scientific community. At Ambry you will be learning, challenging yourself, and having fun while collaborating with teammates through the open exchange of ideas. Our outstanding benefits program includes medical, dental, vision, 401k with a 4% employer match, FSA, paid sick leave and generous paid time off (PTO) program. You can learn more about the benefits here. Ambry Genetics is an Equal Opportunity Employer (EOE) and we maintain a drug-free work environment. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. All qualified applicants will receive consideration for employment without regard to race (and traits historically associated with race, including, but not limited to hair texture and protective hairstyles such as braids, locks, and twists), color, creed, religion, sex, sexual orientation, gender identity, gender expression (including transgender status), national origin, ancestry, age, marital status or protected veteran status and will not be discriminated against on the basis of disability, protected medical condition as defined by applicable state or local law, genetic information, or any other characteristic protected by applicable federal, state, or local laws and ordinances. If you have a disability or special need that requires accommodation, please contact us at careers@ambrygen.com Ambry does not accept unsolicited resumes from individual recruiters, third party recruiting agencies, outside recruiters or firms without an executed contract in place. We are not responsible for any fees related to resumes that are unsolicited or are received by Ambry. Such resumes will be deemed the sole property of Ambry and will be processed accordingly. PRIVACY NOTICES To review Ambry’s Privacy Notice, Click here: https://www.ambrygen.com/legal/privacy-policy To review the California privacy notice, click here : California Privacy Notice | Ambry Genetics To review the UKG privacy notice, click here: California Privacy Notice | UKG",
        "url": "https://www.linkedin.com/jobs/view/3970521060",
        "summary": "Ambry Genetics is seeking a Software Engineer II to design and develop custom software, systems, and integrations. The role involves working in an agile/scrum environment with a team of engineers, QA, and business analysts. Responsibilities include writing code, designing technical solutions, participating in scrum ceremonies, and collaborating with stakeholders to understand workflows. The ideal candidate will have 3+ years of experience in software engineering, a strong understanding of OOP languages (Java, Python), experience with frontend development, relational and non-relational databases, version control (Git), and various data formats. Experience with Docker containers is preferred. The position is remote and offers a competitive salary, benefits, and a dynamic work environment.",
        "industries": [
            "Healthcare",
            "Genetics",
            "Biotechnology",
            "Software Development",
            "IT"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Critical Thinking",
            "Problem Solving",
            "Teamwork",
            "Time Management",
            "Organization",
            "Analytical Skills"
        ],
        "hard_skills": [
            "Java",
            "Python",
            "HTML",
            "CSS",
            "JavaScript",
            "ReactJS",
            "MySQL",
            "PostgreSQL",
            "MSSQL",
            "MongoDB",
            "Git",
            "JSON",
            "CSV",
            "XML",
            "REST",
            "SOAP",
            "AMQP",
            "Docker"
        ],
        "tech_stack": [
            "Java",
            "Python",
            "ReactJS",
            "MySQL",
            "PostgreSQL",
            "MSSQL",
            "MongoDB",
            "Git",
            "Docker"
        ],
        "programming_languages": [
            "Java",
            "Python"
        ],
        "experience": 3,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Computer Science",
                "Related Discipline"
            ]
        },
        "salary": {
            "max": 120000,
            "min": 104000
        },
        "benefits": [
            "Medical",
            "Dental",
            "Vision",
            "401k with 4% employer match",
            "FSA",
            "Paid Sick Leave",
            "Paid Time Off"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Diego, CA",
        "job_id": 3962970489,
        "company": "Mosaic.tech",
        "title": "Senior Data Engineer",
        "created_on": 1720635894.3549948,
        "description": "Mosaic, a leading company providing powerful financial planning and business performance solutions, is on the lookout for a Senior Data Engineer. Our team is dedicated to building scalable, high-performance solutions. If you are an individual who thrives in a collaborative environment, enjoys managing the end-to-end data process, and is passionate about shaping how data is consumed within an organization, this could be your next role! What You'll Be Responsible For: Design, develop, and maintain scalable and efficient data pipelines to process large volumes of data from various sources Collaborate with stakeholders and other backend engineers to understand data requirements and deliver high-quality data solutions Optimize and maintain data infrastructure, ensuring reliability, scalability, and performance Implement best practices for data management, including data governance and data quality Develop and maintain ETL processes to integrate data from multiple heterogeneous sources into a unified data warehouse Monitor and troubleshoot data pipeline issues, ensuring data integrity and availability Requirements: Strong communication and collaboration skills, with the ability to work effectively in a distributed team across various time zones Demonstrated ability to manage data projects from start to finish, effectively negotiating requirements and deliverables with key stakeholders 5+ years of experience in data engineering or a related field working with data in a high-volume environment Proficiency in programming languages such as Python, Java, or Scala Extensive experience with SQL and database technologies (e.g., PostgreSQL, MySQL, Oracle) Familiarity with data orchestration tools (e.g. Apache Airflow), data transformation tools (e.g. Spark), dimensional modeling (e.g. star schema), metadata, indexing, dependencies, and data workflows to support data analytics and data science Experience with big data technologies (e.g., Hadoop, Spark, Kafka) and cloud platforms (e.g., AWS, Azure, Google Cloud) Familiarity with data warehousing solutions (e.g., Redshift, BigQuery, Snowflake) Solid understanding of data modeling, data architecture, and database design principles Excellent problem-solving skills and attention to detail Bachelor’s degree in Computer Science, Engineering, Information Technology, or a related field Preferred Qualifications: Subject Matter Expertise (SME) on data structure and datasets in the Financial Planning and Analysis space Experience with containerization and orchestration tools (e.g., Docker, Kubernetes) Experience in a fast-paced, agile development environment Understanding of data lake and data lakehouse architectures and Delta Lake or Apache Iceberg table formats At Mosaic, we take immense pride in our diverse workforce, continuously nurturing a cohesive team of talented, independent, and compassionate individuals who are revolutionizing the realm of corporate finance. As a staunch believer in equal opportunities, Mosaic ensures the inclusion of all intersectional identities and maintains a strict policy against any form of discrimination. Our team members are equipped with the tools they require to enjoy a high degree of autonomy, and we encourage everyone to chase both professional and personal growth. While our headquarters are located in the picturesque Del Mar, California, we adopted a remote-friendly work environment early in our journey. Our teams make biannual trips to San Diego, taking a break from digital screens to engage in in-person interactions and team-building activities. The target salary for this position is $160,000 - $190,000 and is part of a competitive total rewards package including stock options, benefits, and additional opportunities for incentives and bonuses for performance beyond goals. Individual pay may vary from the target range and is determined by a number of factors including experience, location, internal pay equity, and other relevant business considerations. We review all employee pay and compensation programs annually at a minimum to ensure competitive and fair pay.",
        "url": "https://www.linkedin.com/jobs/view/3962970489",
        "summary": "Mosaic, a financial planning and business performance solutions company, is seeking a Senior Data Engineer to design, develop, and maintain scalable data pipelines. This role involves collaborating with stakeholders, optimizing data infrastructure, implementing data governance, and developing ETL processes.  The ideal candidate will have 5+ years of experience in data engineering, proficiency in Python, Java, or Scala, extensive experience with SQL and databases, and familiarity with big data technologies, cloud platforms, and data warehousing solutions.",
        "industries": [
            "Financial Services",
            "Software",
            "Technology",
            "Data Engineering"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem-solving",
            "Attention to detail",
            "Project Management",
            "Negotiation"
        ],
        "hard_skills": [
            "Python",
            "Java",
            "Scala",
            "SQL",
            "PostgreSQL",
            "MySQL",
            "Oracle",
            "Apache Airflow",
            "Spark",
            "Hadoop",
            "Kafka",
            "AWS",
            "Azure",
            "Google Cloud",
            "Redshift",
            "BigQuery",
            "Snowflake",
            "Docker",
            "Kubernetes",
            "Delta Lake",
            "Apache Iceberg"
        ],
        "tech_stack": [
            "Python",
            "Java",
            "Scala",
            "SQL",
            "PostgreSQL",
            "MySQL",
            "Oracle",
            "Apache Airflow",
            "Spark",
            "Hadoop",
            "Kafka",
            "AWS",
            "Azure",
            "Google Cloud",
            "Redshift",
            "BigQuery",
            "Snowflake",
            "Docker",
            "Kubernetes",
            "Delta Lake",
            "Apache Iceberg"
        ],
        "programming_languages": [
            "Python",
            "Java",
            "Scala",
            "SQL"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Computer Science",
                "Engineering",
                "Information Technology"
            ]
        },
        "salary": {
            "max": 190000,
            "min": 160000
        },
        "benefits": [
            "Stock options",
            "Benefits",
            "Incentives",
            "Bonuses"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Menlo Park, CA",
        "job_id": 3916423227,
        "company": "Character.AI",
        "title": "Staff Data Engineer - Analytics",
        "created_on": 1720635901.851824,
        "description": "Responsibilities: As Staff Data Engineer - Analytics, you will work on a diverse set of initiatives spanning the data engineering and data science domains to help Character grow its product through analytical excellence. You will partner with our Data Platform and Data Science teams to optimize warehouse design and performance, evolve critical product analytics systems, enable and expand use cases of product data and help develop a world-class data culture. Your initial focus will be on these key areas: Evolve our A/B testing tooling: Conduct a thorough analysis of our current A/B testing framework and prepare it for the next generation of data-driven product growth. Evolve our analytics event collection systems: Conduct a thorough analysis of the product landscape for a scalable and robust solution that prepares the company for its next phase of growth. Enable product team insights: Design, implement, and maintain a robust data warehousing design for consistent and reliable reporting and ease of data exploration as we continue to better understand our users. Requirements: 5+ years experience in data engineering within a consumer-facing technology company (chat, social media, or UGC) Experience building data warehousing and pipelines with BigQuery (or similar), dbt and Airflow Experience modeling and scaling event telemetry systems for analytical use-cases Experience implementing and supporting product experimentation data platforms Proven track record of cross-functional execution and collaboration Strong Python and SQL experience About Character.AI Founded in 2021 by AI pioneers Noam Shazeer and Daniel De Freitas, Character is a leading AI company offering personalized experiences through customizable AI 'Characters.' As one of the most widely used AI platforms worldwide, Character enables users to interact with AI tailored to their unique needs and preferences. Noam co-invented core LLM tech and was recently honored as one of TIME's 100 Most Influential in AI. Daniel created LaMDA, the breakthrough conversational AI now powering Google's Bard. In just two years, we achieved unicorn status and were named Google Play's AI App of the Year – a testament to our groundbreaking technology and vision. Ready to shape the future of AGI? 🚀 At Character, we value diversity and welcome applicants from all backgrounds. As an equal opportunity employer, we firmly uphold a non-discrimination policy based on race, religion, national origin, gender, sexual orientation, age, veteran status, or disability. Your unique perspectives are vital to our success.",
        "url": "https://www.linkedin.com/jobs/view/3916423227",
        "summary": "Character.AI is looking for a Staff Data Engineer - Analytics to work on a diverse set of initiatives related to data engineering and data science. Responsibilities include optimizing warehouse design and performance, evolving product analytics systems, and enabling use cases of product data. This role will focus on evolving A/B testing tooling, analytics event collection systems, and enabling product team insights. The ideal candidate has 5+ years of experience in data engineering within a consumer-facing technology company, experience with BigQuery, dbt, Airflow, event telemetry systems, and product experimentation data platforms, and strong Python and SQL skills.",
        "industries": [
            "Artificial Intelligence",
            "Technology",
            "Software",
            "Consumer-facing"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem-solving",
            "Analytical thinking",
            "Teamwork",
            "Cross-functional"
        ],
        "hard_skills": [
            "Data Engineering",
            "BigQuery",
            "dbt",
            "Airflow",
            "Event Telemetry",
            "Product Experimentation",
            "Python",
            "SQL",
            "A/B Testing",
            "Data Warehousing",
            "Data Pipelines",
            "Data Modeling",
            "Data Analysis"
        ],
        "tech_stack": [
            "BigQuery",
            "dbt",
            "Airflow",
            "Python",
            "SQL"
        ],
        "programming_languages": [
            "Python",
            "SQL"
        ],
        "experience": 5,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Los Angeles, CA",
        "job_id": 3945654394,
        "company": "FOX Tech",
        "title": "Senior Data Engineer",
        "created_on": 1720635903.4564397,
        "description": "Overview Of The Company Fox Corporation Under the FOX banner, we produce and distribute content through some of the world’s leading and most valued brands, including: FOX News Media, FOX Sports, FOX Entertainment, FOX Television Stations and Tubi Media Group. We empower a diverse range of creators to imagine and develop culturally significant content, while building an organization that thrives on creative ideas, operational expertise and strategic thinking. Job Description Under the FOX banner, we produce and distribute content through some of the world’s leading and most valued brands, including FOX News Media, FOX Sports, FOX Entertainment, FOX Television Stations and Tubi Media Group. We empower a diverse range of creators to imagine and develop culturally significant content, while building an organization that thrives on creative ideas, operational expertise, and strategic thinking. About The Role The FOX Data and Commercial Technology Team is looking for Senior Data Engineers with a passion for building robust, scalable, efficient, and high-quality Data Engineering solutions to join our Data Engineering team. This is a great opportunity to join a data-first media company and be part of our Enterprise Data Platform team that prides itself in making Fox a data-driven organization. If you enjoy designing and building innovative data engineering solutions using the latest tech stack in a fast-paced environment, this role is for you. About You Strong curiosity about our business: you are willing to ask questions, do research, and build a deep understanding of the FOX landscape Innovative: you stay on top of industry developments, are excited about innovation, and look for ways to bring new ideas to our business. You are passionate about technology Enthusiasm about operational products and the opportunity they provide for creating outstanding employee experiences: you enjoy delivering products that make work easier for colleagues Perseverance in the face of ambiguity or roadblocks: you are a problem solver who uses all resources available to understand the issues and propose a path forward Emotional intelligence: you can read the room, know when to speak vs. when to listen, and know that connecting personally with your colleagues delivers positive outcomes Bridge building skills - you establish trusted and productive relationships with internal and external stakeholders, both business and technical A self-starter mentality - you are oriented to growth and are a quick learner a Snapshot Of Your Responsibilities Collaborate with and across agile teams to design and develop data engineering solutions Build distributed, low latency, reliable data pipelines ensuring high availability and timely delivery of data Design and develop highly optimized data engineering solutions for Big Data workloads to efficiently handle continuous increase in data volume and complexity Build highly performing real-time data ingestion solutions for streaming workloads Adhere to best practices and agreed upon design patterns across all Data Engineering solutions Ensure the code is elegantly designed, efficiently coded, and effectively tuned for performance Focus on data quality and consistency, implement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it Create design (Data Flow Diagrams, Technical Design Specs, Source to Target Mapping documents) and test (unit/integration tests) documentation Perform data analysis required to troubleshoot data related issues and assist in the resolution of data issues Focus on end-to-end automation of data engineering pipelines and data validations (audit, balance controls) without any manual intervention Focus on data security and privacy by implementing proper access controls, key management, and encryption techniques Take a proactive approach in learning new technologies, stay on top of tech trends, experimenting with new tools & technologies and educate other team members Collaborate with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization Communicate clearly and effectively to technical and non-technical leadership What You Will Need Education: Bachelor’s degree in Computer Science, Computer Engineering, or relevant field Work Experience: Senior Data Engineer: 4+ years of experience in architecting, designing and building Data Engineering solutions and Data Platforms Experience in building Data Warehouses/Data Platforms on Redshift, Snowflake, or Data Bricks Experience working with data acquisition and transformation tools such as Fivetran and DBT Experience building highly optimized & efficient data engineering pipelines using Python, PySpark Experience working with distributed data processing frameworks such as Apache Hadoop, or Apache Spark or Flink Experience working with real-time data streams processing using Apache Kafka, Kinesis or Flink Experience working with various AWS Services (S3, EC2, EMR, Lambda, RDS, DynamoDB, Redshift, Glue Catalog) Expertise in Advanced SQL programming and SQL Performance Tuning Experience with version control tools such as GitHub or Bitbucket. Expert level understanding of dimensional modeling techniques. Excellent communication, adaptability, and collaboration skills. Excellent analytical skills, strong attention to detail with emphasis on accuracy, consistency, and quality Strong logical and problem-solving skills with critical thinking NICE TO HAVE, BUT NOT A DEAL BREAKER Experience in designing and building applications using Container and serverless technologies Experience working with fully automated workflow scheduling and orchestration services such as Apache Airflow Experience working with semi-structured, unstructured data, No SQL databases Experience with CI/CD using GitHub Actions or Jenkins Experience designing and building APIs Learn more about Fox Tech at #foxtech We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, gender identity, disability, protected veteran status, or any other characteristic protected by law. We will consider for employment qualified applicants with criminal histories consistent with applicable law. At FOX, we foster a culture and environment where everyone feels welcome and can thrive. We are deeply committed to diversity, equity, and inclusion, including attracting, retaining, and promoting diverse talent across our company. We live in a diverse world, with different ideas and different perspectives that come together to spark new ideas and make great things happen. That means reflecting the diversity of the world around us is critical to our company’s success. We ensure that our viewers, communities and employees feel heard, represented, and celebrated both on screen and off. Pursuant to state and local pay disclosure requirements, the pay range for this role, with final offer amount dependent on education, skills, experience, and location is: $121,500.00-170,000.00 annually for California. This role is also eligible for an annual discretionary bonus, various benefits, including medical/dental/vision, insurance, a 401(k) plan, paid time off, and other benefits in accordance with applicable plan documents. Benefits for Union represented employees will be in accordance with the applicable collective bargaining agreement.",
        "url": "https://www.linkedin.com/jobs/view/3945654394",
        "summary": "FOX Data and Commercial Technology team is looking for a Senior Data Engineer to build robust, scalable, efficient, and high-quality Data Engineering solutions. The role involves designing and building data engineering solutions for Big Data workloads, real-time data ingestion solutions, and ensuring data quality and consistency. The ideal candidate will have experience in building Data Warehouses/Data Platforms on Redshift, Snowflake, or Data Bricks, and working with data acquisition and transformation tools such as Fivetran and DBT.",
        "industries": [
            "Media",
            "Entertainment",
            "Technology",
            "Data",
            "Data Engineering",
            "Data Analytics"
        ],
        "soft_skills": [
            "Strong Curiosity",
            "Innovative",
            "Enthusiasm",
            "Perseverance",
            "Emotional Intelligence",
            "Bridge Building Skills",
            "Self-Starter Mentality"
        ],
        "hard_skills": [
            "Data Engineering",
            "Data Warehousing",
            "Data Platforms",
            "Data Pipelines",
            "Data Acquisition",
            "Data Transformation",
            "Python",
            "PySpark",
            "Apache Hadoop",
            "Apache Spark",
            "Apache Flink",
            "Apache Kafka",
            "Kinesis",
            "AWS Services",
            "S3",
            "EC2",
            "EMR",
            "Lambda",
            "RDS",
            "DynamoDB",
            "Redshift",
            "Glue Catalog",
            "SQL",
            "SQL Performance Tuning",
            "GitHub",
            "Bitbucket",
            "Dimensional Modeling",
            "Communication",
            "Adaptability",
            "Collaboration",
            "Analytical Skills",
            "Attention to Detail",
            "Logical Thinking",
            "Problem Solving",
            "Critical Thinking",
            "Container Technologies",
            "Serverless Technologies",
            "Apache Airflow",
            "No SQL Databases",
            "CI/CD",
            "GitHub Actions",
            "Jenkins",
            "API Design"
        ],
        "tech_stack": [
            "Redshift",
            "Snowflake",
            "Data Bricks",
            "Fivetran",
            "DBT",
            "Python",
            "PySpark",
            "Apache Hadoop",
            "Apache Spark",
            "Apache Flink",
            "Apache Kafka",
            "Kinesis",
            "AWS Services",
            "S3",
            "EC2",
            "EMR",
            "Lambda",
            "RDS",
            "DynamoDB",
            "Redshift",
            "Glue Catalog",
            "SQL",
            "GitHub",
            "Bitbucket",
            "Container Technologies",
            "Serverless Technologies",
            "Apache Airflow",
            "No SQL Databases",
            "GitHub Actions",
            "Jenkins"
        ],
        "programming_languages": [
            "Python",
            "PySpark",
            "SQL"
        ],
        "experience": 4,
        "education": {
            "min_degree": "Bachelor’s degree",
            "fields": [
                "Computer Science",
                "Computer Engineering",
                "Relevant Field"
            ]
        },
        "salary": {
            "max": 170000,
            "min": 121500
        },
        "benefits": [
            "Medical",
            "Dental",
            "Vision",
            "Insurance",
            "401(k) Plan",
            "Paid Time Off",
            "Discretionary Bonus"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Mountain View, CA",
        "job_id": 3925326915,
        "company": "Google",
        "title": "Software Engineer III, Machine Learning, Search",
        "created_on": 1720635905.2218096,
        "description": "Note: By applying to this position you will have an opportunity to share your preferred working location from the following: Mountain View, CA, USA; New York, NY, USA . Minimum qualifications: Bachelor’s degree or equivalent practical experience. 2 years of experience with software development in one or more programming languages, or 1 year of experience with an advanced degree in an industry setting. 2 years of experience with data structures or algorithms in either an academic or industry setting. 2 years of experience with machine learning algorithms and tools (e.g., TensorFlow), artificial intelligence, deep learning or natural language processing. Preferred qualifications: Master's degree or PhD in Computer Science or related technical field. 2 years of experience with performance, large scale systems data analysis, visualization tools, and/or debugging. Experience developing accessible technologies. Proficiency in code and system health, diagnosis and resolution, and software test engineering. About The Job Google's software engineers develop the next-generation technologies that change how billions of users connect, explore, and interact with information and one another. Our products need to handle information at massive scale, and extend well beyond web search. We're looking for engineers who bring fresh ideas from all areas, including information retrieval, distributed computing, large-scale system design, networking and data storage, security, artificial intelligence, natural language processing, UI design and mobile; the list goes on and is growing every day. As a software engineer, you will work on a specific project critical to Google’s needs with opportunities to switch teams and projects as you and our fast-paced business grow and evolve. We need our engineers to be versatile, display leadership qualities and be enthusiastic to take on new problems across the full-stack as we continue to push technology forward. With your technical expertise you will manage project priorities, deadlines, and deliverables. You will design, develop, test, deploy, maintain, and enhance software solutions. In Google Search, we're reimagining what it means to search for information – any way and anywhere. To do that, we need to solve complex engineering challenges and expand our infrastructure, while maintaining a universally accessible and useful experience that people around the world rely on. In joining the Search team, you'll have an opportunity to make an impact on billions of people globally. The US base salary range for this full-time position is $136,000-$200,000 + bonus + equity + benefits. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process. Please note that the compensation details listed in US role postings reflect the base salary only, and do not include bonus, equity, or benefits. Learn more about benefits at Google . Responsibilities Write product or system development code. Participate in, or lead design reviews with peers and stakeholders to decide amongst available technologies. Review code developed by other developers and provide feedback to ensure best practices (e.g., style guidelines, checking code in, accuracy, testability, and efficiency). Contribute to existing documentation or educational content and adapt content based on product/program updates and user feedback. Triage product or system issues and debug/track/resolve by analyzing the sources of issues and the impact on hardware, network, or service operations and quality. Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing our Accommodations for Applicants form .",
        "url": "https://www.linkedin.com/jobs/view/3925326915",
        "summary": "Google is seeking a Software Engineer to join their Search team, focusing on developing next-generation technologies for billions of users. The role involves managing project priorities, designing and developing software solutions, and participating in code reviews. This position offers the opportunity to work on impactful projects and contribute to a globally accessible product. The ideal candidate will have experience with software development, data structures, algorithms, and machine learning.",
        "industries": [
            "Technology",
            "Software Development",
            "Information Retrieval",
            "Artificial Intelligence",
            "Search Engines"
        ],
        "soft_skills": [
            "Leadership",
            "Versatility",
            "Communication",
            "Problem-solving",
            "Collaboration"
        ],
        "hard_skills": [
            "Software Development",
            "Programming Languages",
            "Data Structures",
            "Algorithms",
            "Machine Learning",
            "TensorFlow",
            "Artificial Intelligence",
            "Deep Learning",
            "Natural Language Processing",
            "Performance",
            "Data Analysis",
            "Visualization Tools",
            "Debugging",
            "Accessibility",
            "Code Review",
            "Documentation",
            "Issue Triaging",
            "Debugging",
            "System Health"
        ],
        "tech_stack": [
            "TensorFlow"
        ],
        "programming_languages": [],
        "experience": 2,
        "education": {
            "min_degree": "Bachelor’s degree",
            "fields": [
                "Computer Science",
                "Related Technical Field"
            ]
        },
        "salary": {
            "max": 200000,
            "min": 136000
        },
        "benefits": [
            "Bonus",
            "Equity",
            "Benefits"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3889323873,
        "company": "Prominence Advisors",
        "title": "Epic Data Engineer",
        "created_on": 1720635906.9149165,
        "description": "Prominence is looking for a Data Engineer to assist with dimensional data modeling development for healthcare data architecture project. Who We Are Prominence is a healthcare technology strategy and implementation firm, focused on helping the nation's leading healthcare organizations to do more with their data. Founded by former Epic managers, we understand the technology landscape in healthcare and provide IT staffing, advisory services, and analytics solutions to create robust data ecosystems that support clinical workflows, automate operational processes, and expedite research. Whether it's guiding a technology implementation, establishing governance principles, or developing leading edge analytics, we help our customers make sense out of the mountain of data at their fingertips in order to deliver higher quality care at a lower cost. Ranked as a best place to work over 27 times (and counting!), Prominence's culture provides consultants with a supportive environment that allows you to innovate and grow your career in healthcare IT. Additional information is available on our website. Your Role Our consultants guide our customers through complex technology requirements to summit the challenge at hand. You will need to be able to create order out of chaos, and compile ambiguous information into tactical action plans. Our ideal team members are humble, smart, and driven to ensure our customer's success. This includes a passion to deliver high-quality results, while teaching our counterparts how to fish and grow the skills needed to support and expand upon the deliverables of our projects. If this sounds like you, and you meet the requirements below, we encourage you to apply. If you know of someone else how would be a great fit, let us know! Requirements As a member of our Epic Consulting team, you'll work closely with our customers to implement and optimize their Epic workflows. In addition to your Epic project work, you will help mentor and grow our customer's teams, escalate issues, and guide projects to a successful outcome. Key Responsibilities Perform Epic-related consulting and advisory services, including but not limited to the following: Apply technical expertise to implement and optimize EMR workflows and data capture Mentor customers to up-level their system knowledge and analyst skills Analyze operational and business requirements, and translate into system configuration Create build documentation and workflow diagrams Track and resolve project risks and issues Lead meetings and participate in ongoing work-product coordination. Transparently report on project status and deliverables. Develop robust knowledge transfer documentation to hand-off deliverables to customer teams. Additional duties as may be required to successfully deliver a project May be invited to participate in corporate functions, events, and meetings Desired Qualifications Active Certification(s): Cogito Data Model (Clinical or Revenue Cycle), Cogito Tools, Caboodle Developer 5+ years of experience as an Epic BI Developer or Data Engineer Dimensional data modeling experience Caboodle development experience preferred SSIS, Azure Data Factory, or Data Lake development preferred Demonstrated ability to deliver successful projects remotely Success Criteria Successful team members at Prominence display the following: High degree of professionalism; treats others with respect, keeps commitments, builds trust within a team, works with integrity, and upholds organizational values. Highly organized; able to manage multi-faceted work streams Self-motivated; able to maintain schedule, meet deadlines, and monitor your personal work product Highly adaptable; able to acclimate quickly to new project assignments and work environments. Creative; not paralyzed by problems and able to work collaboratively to find novel solutions Clear communication skills; ability to clearly convey messaging that resonates with your audience, in clear and concise written and verbal communications Can smell smoke and anticipate issues before they arise, ability to escalate effectively Passion to mentor and guide others Benefits Prominence is dedicated to hiring the best and brightest minds in healthcare and maintaining a culture that rewards our employees for following their passion. We are excited to offer the following benefits for this position: Competitive Salaried and Hybrid Compensation Plans Health Care Plan (Medical, HSAs, Dental & Vision) Retirement Plan (401k) Life Insurance (Basic, Voluntary & AD&D) Dependent & Health Savings Accounts Short Term & Long Term Disability Paid Time Off (Vacation/Sick & Public Holidays) Training & Development Fund Technology Stipends (for Qualifying Roles) Work From Home Charitable Giving to Causes You Believe In Employment Eligibility Must be legally authorized to work in the United States without sponsorship. Commitment to Equal Opportunity The world's most talented professionals come from every background. All applicants will be considered for employment without attention to age, race, color, religion, gender identity and/or expression, sexual orientation, national origin, marital status, veteran or disability status, or any other characteristic protected by law. In addition, Prominence will provide reasonable accommodations for qualified individuals with disabilities. If you are smart and good at what you do, come as you are. All qualified candidates are encouraged to apply. Partnership Eligibility Our partnerships are extremely important to us. This online application is not intended for anyone who is currently under a non-compete agreement or has an arrangement that precludes employment at Prominence. We appreciate your help in respecting our partners. Interested in learning more? Apply below to connect with our Talent team about immediate openings and future consulting projects.",
        "url": "https://www.linkedin.com/jobs/view/3889323873",
        "summary": "Prominence, a healthcare technology firm, is looking for a Data Engineer to help with dimensional data modeling for a healthcare data architecture project. Responsibilities include applying technical expertise for EMR workflows and data capture, mentoring customers, analyzing operational and business requirements, creating documentation, tracking and resolving project risks, leading meetings, and reporting on project status.  Ideal candidates possess 5+ years of experience as an Epic BI Developer or Data Engineer with dimensional data modeling and Caboodle development experience. Preferred skills include SSIS, Azure Data Factory, or Data Lake development.",
        "industries": [
            "Healthcare",
            "Technology",
            "Healthcare IT",
            "Data Engineering"
        ],
        "soft_skills": [
            "Communication",
            "Problem-solving",
            "Leadership",
            "Mentorship",
            "Teamwork",
            "Organization",
            "Adaptability",
            "Creativity",
            "Professionalism",
            "Analytical",
            "Critical thinking"
        ],
        "hard_skills": [
            "Epic",
            "Cogito Data Model",
            "Cogito Tools",
            "Caboodle",
            "SSIS",
            "Azure Data Factory",
            "Data Lake",
            "Dimensional Data Modeling",
            "EMR Workflows",
            "Data Capture",
            "System Configuration",
            "Project Management",
            "Risk Management",
            "Meeting Facilitation",
            "Documentation",
            "Reporting"
        ],
        "tech_stack": [
            "Epic",
            "Cogito",
            "Caboodle",
            "SSIS",
            "Azure Data Factory",
            "Data Lake"
        ],
        "programming_languages": [],
        "experience": 5,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Competitive Salaried and Hybrid Compensation Plans",
            "Health Care Plan",
            "Retirement Plan",
            "Life Insurance",
            "Dependent & Health Savings Accounts",
            "Short Term & Long Term Disability",
            "Paid Time Off",
            "Training & Development Fund",
            "Technology Stipends",
            "Work From Home",
            "Charitable Giving"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco Bay Area",
        "job_id": 3956367114,
        "company": "Fractal",
        "title": "Software Engineer",
        "created_on": 1720635908.5619466,
        "description": "Fractal Analytics is a strategic AI partner to Fortune 500 companies with a vision to power every human decision in the enterprise. Fractal is building a world where individual choices, freedom, and diversity are the greatest assets. An ecosystem where human imagination is at the heart of every decision. Where no possibility is written off, only challenged to get better. We believe that a true Fractalite empowers imagination with intelligence. And that it will be such Fractalites that will continue to build the company for the next 100 years. **Please Note: This role is specifically located in the North Bay area of San Francisco. You'll need to be onsite or have the ability to move.** Role Overview:We're looking for a Software Development Engineer to play a pivotal role in developing and deploying cutting-edge, AI-driven enterprise applications using our client’s proprietary AI Platform. This role involves crafting and enhancing applications and diving deep into problem-solving, performance optimization, and comprehensive documentation. You'll be expected to work closely with our client, offering technical expertise and innovative solutions to meet their needs. Responsibilities: Design, develop, and deploy full-stack, AI-centric enterprise applications on the client AI Platform, ensuring their integration with advanced data solutions.Lead the architecture, development, and maintenance of comprehensive data integration systems, employing efficient ETL processes using Python, Pandas, and NumPy for large-scale data manipulation.Apply JavaScript or another object-oriented language (e.g., Python, C#) in client environments to seamlessly integrate various system functionalities.Proactively test, diagnose, and refine software applications alongside clients, aiming for high quality and optimal functionality.Identify and rectify performance issues within applications and integrated data systems, focusing on efficiency, optimization, and data integrity.Utilize shell scripting and cron job scheduling for automating routine data operations, ETL tasks, and ensuring data accuracy and harmonization.Actively participate in and lead the design and review processes for both internal and client software applications.Implement and manage version control using Git, adeptly handling repository operations including rebase, pull, push, and branch management.Create detailed application specifications and maintain precise documentation throughout the software lifecycle, guaranteeing the integrity and harmonization of all integrated data Qualifications: Have interest and ability to become certified on the end client AI platform. (We will provide all the necessary training and support)A bachelor’s degree in computer science, a related field, or an equivalent combination of education and experience.Minimum of 5 years of relevant experience.Proficient in software development with JavaScript and Python.Experience with version control systems, preferably Git.Strong experience in shell scripting and cron job scheduling for automating data processes.Solid background in working with various JavaScript frameworks (e.g., React, Redux, Vue, Backbone, Angular).Experience in deploying software on leading cloud computing platforms (such as GCP, AWS, Azure).Understanding of both SQL and NoSQL database technologies.Strong skills in data structures, algorithm design, and implementation.Proficiency in handling and analyzing time-series data, including its cleansing and normalization.Familiarity with Agile software development methodologies. Pay:The wage range for this role takes into account the wide range of factors that are considered in making compensation decisions, including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. At Fractal, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. A reasonable estimate of the current range is: $90,000 to $208,000. In addition, you may be eligible for a discretionary bonus for the current performance period. Benefits:As a full-time employee of the company or as an hourly employee working more than 30 hours per week, you will be eligible to participate in the health, dental, vision, life insurance, and disability plans in accordance with the plan documents, which may be amended from time to time. You will be eligible for benefits on the first day of employment with the Company. In addition, you are eligible to participate in the Company 401(k) Plan after 30 days of employment, in accordance with the applicable plan terms. The Company provides for 11 paid holidays and 12 weeks of Parental Leave. We also follow a “free time” PTO policy, allowing you the flexibility to take the time needed for either sick time or vacation. Fractal provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.",
        "url": "https://www.linkedin.com/jobs/view/3956367114",
        "summary": "Fractal Analytics seeks a Software Development Engineer to build and deploy AI-driven enterprise applications using their client's proprietary platform. The role involves full-stack development, data integration, performance optimization, and collaboration with clients. Experience with Python, JavaScript, ETL processes, cloud platforms, and Agile methodologies is required.",
        "industries": [
            "Artificial Intelligence",
            "Software Development",
            "Data Science",
            "Technology",
            "Consulting"
        ],
        "soft_skills": [
            "Problem Solving",
            "Performance Optimization",
            "Technical Expertise",
            "Communication",
            "Collaboration",
            "Client Interaction",
            "Agile",
            "Documentation",
            "Teamwork"
        ],
        "hard_skills": [
            "Python",
            "Pandas",
            "NumPy",
            "JavaScript",
            "C#",
            "ETL",
            "Shell Scripting",
            "Cron Job Scheduling",
            "Git",
            "React",
            "Redux",
            "Vue",
            "Backbone",
            "Angular",
            "GCP",
            "AWS",
            "Azure",
            "SQL",
            "NoSQL",
            "Data Structures",
            "Algorithms",
            "Time-Series Data",
            "Data Cleansing",
            "Data Normalization"
        ],
        "tech_stack": [
            "AI Platform",
            "Python",
            "Pandas",
            "NumPy",
            "JavaScript",
            "React",
            "Redux",
            "Vue",
            "Backbone",
            "Angular",
            "GCP",
            "AWS",
            "Azure",
            "SQL",
            "NoSQL",
            "Git"
        ],
        "programming_languages": [
            "Python",
            "JavaScript",
            "C#"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Related Fields"
            ]
        },
        "salary": {
            "max": 208000,
            "min": 90000
        },
        "benefits": [
            "Health Insurance",
            "Dental Insurance",
            "Vision Insurance",
            "Life Insurance",
            "Disability Insurance",
            "401(k)",
            "Paid Holidays",
            "Parental Leave",
            "Paid Time Off"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Pleasanton, CA",
        "job_id": 3950997410,
        "company": "10x Genomics",
        "title": "Senior Software Engineer, Data Platform",
        "created_on": 1720635910.2590637,
        "description": "About The Role As a Senior Software Engineer, you will bring a full-stack software engineering mindset and practices to designing, building, and maintaining our central data and business intelligence platform and ensuring we have robust, high-quality data pipelines and infrastructure. The Ideal Candidate Enjoys Thinking About How columnar data formats are laid out on block devices Creative ways to educate business analysts on the nuances of ERP data models Usability design of column names and visualization software How to obviate “data extracts” and caches by making things really fast The ideal UX/UI for human-in-the-loop data cleaning & labeling Building custom applets for integrating ad-hoc data sources like Excel workbooks Whether an EC2 instance can fit, in memory, all the data we care about What You Will Be Doing Designing and building data infrastructure to serve a variety of uses: business intelligence exploration, real-time operational cockpits, enterprise reporting, and “reverse ETL” to feed back to customer-facing applications. Evaluating when to buy off-the-shelf tech versus when to build in-house. Designing & implementing data models in collaborating with data scientists and business analysts Mentoring business teams to create advanced visualizations and dashboards using the best tool for the job, whether it’s a custom app, Tableau, Retool, or something else. Implementing software engineering practices like version control, CI/CD, automated testing, and code reviews. Implementing Infrastructure as Code (IaC) for automated and efficient management of data infrastructure. Developing elegant & robust ETL/ELT processes for a multitude of sources.Partnering with business functions and data governance leaders to support data quality and data ownership. Evaluating emerging technologies and approaches to data warehousing, such as DuckDB and Parquet. Minimum Qualifications Bachelor's degree in computer science, engineering, math, or scientific discipline and 3 years of software development experience; OR 5 years of professional experience building software Experience with SQL. Preferred Skills & Experience Excellent understanding of computer science fundamentals. Significant experience with backend (eg. Python, GraphQL, Postgres, Go). Experience in data engineering with a focus on data warehousing and modeling. Experience with implementing data platform technologies, such as Snowflake, Redshift, BigQuery, DuckDB, or others. Experience with cloud infrastructure, such as AWS or GCP. Experience in project management and communication, with the ability to articulate complex data issues clearly.Experience in implementing as-code practices in data engineering.Experience with data governance and quality improvement initiatives. Below is the base pay range for this full time position. The actual base pay will depend on several factors unique to each candidate, including one’s skills, qualifications, and experience. At 10x, base pay is also just one component of the Company’s total compensation package. This role is also eligible for 10x’s equity grants, its comprehensive health and retirement benefit programs, and its annual bonus program or sales incentive program. Your 10x recruiter can share more about the Company’s total compensation package during the hiring process. Pay Range $177,000—$239,000 USD About 10x Genomics At 10x Genomics, accelerating our understanding of biology is more than a mission for us. It is a commitment. This is the century of biology, and the breakthroughs we make now have the potential to change the world. We enable scientists to advance their research, allowing them to address scientific questions they did not even know they could ask. Our tools have enabled fundamental discoveries across biology including cancer, immunology, and neuroscience. Our teams are empowered and encouraged to follow their passions, pursue new ideas, and perform at their best in an inclusive and dynamic environment. We know that behind every scientific breakthrough, there is a deep infrastructure of talented people driving the life sciences industry and making it possible for scientists and clinicians to make new strides. We are dedicated to finding the very best person for every aspect of our work because the innovations and discoveries that we enable together will lead to better technologies, better treatments, and a better future. Find out how you can make a 10x difference. Individuals seeking employment at 10x Genomics are considered without regards to race, color, religion, national origin, age, sex, marital status, ancestry, physical or mental disability, veteran status, gender identity, or sexual orientation, or any other characteristic protected by applicable law. 10x does not accept unsolicited applicants submitted by third-party recruiters or agencies. Any resume or application submitted to 10x without a vendor agreement in place will be considered unsolicited and property of 10x, and 10x will not pay a placement fee.",
        "url": "https://www.linkedin.com/jobs/view/3950997410",
        "summary": "Senior Software Engineer responsible for designing, building, and maintaining a central data and business intelligence platform. This includes building robust data pipelines and infrastructure, evaluating data models, mentoring business teams on advanced visualizations, and implementing software engineering best practices. The role involves experience with data warehousing, cloud infrastructure, and emerging technologies like DuckDB and Parquet.",
        "industries": [
            "Biotechnology",
            "Healthcare",
            "Data Science",
            "Software Development"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Mentorship",
            "Problem-solving",
            "Project Management",
            "Data Governance"
        ],
        "hard_skills": [
            "SQL",
            "Python",
            "GraphQL",
            "Postgres",
            "Go",
            "Data Warehousing",
            "Data Modeling",
            "Snowflake",
            "Redshift",
            "BigQuery",
            "DuckDB",
            "AWS",
            "GCP",
            "CI/CD",
            "Automated Testing",
            "Code Reviews",
            "Infrastructure as Code",
            "ETL/ELT",
            "Data Quality",
            "Data Ownership"
        ],
        "tech_stack": [
            "Python",
            "GraphQL",
            "Postgres",
            "Go",
            "Snowflake",
            "Redshift",
            "BigQuery",
            "DuckDB",
            "AWS",
            "GCP",
            "Tableau",
            "Retool"
        ],
        "programming_languages": [
            "Python",
            "Go",
            "SQL"
        ],
        "experience": 3,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Computer Science",
                "Engineering",
                "Math",
                "Scientific discipline"
            ]
        },
        "salary": {
            "max": 239000,
            "min": 177000
        },
        "benefits": [
            "Equity Grants",
            "Health Insurance",
            "Retirement Benefits",
            "Annual Bonus",
            "Sales Incentive Program"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3224222224,
        "company": "Forward",
        "title": "Software Engineer, Data Science",
        "created_on": 1720635911.9902132,
        "description": "Forward is on a bold mission to make high quality healthcare available to a billion people across the globe. We’re building the world’s most advanced healthcare platform from the ground up, combining hardware, software and doctors under one roof. We are building our IT team and looking for a world-class IT Engineer with expertise in improving and scaling IT systems and infrastructure. As an early member of our IT team, you’ll have a key role in building the future of healthcare from first principles. Forward was founded in January 2016 by former executives and engineering leaders from Google and Uber. We are funded by some of the world's best investors and entrepreneurs including Founder's Fund, Khosla Ventures, First Round Capital, Eric Schmidt (Google/Alphabet Chairman), Marc Benioff (Salesforce Founder), Joe Lonsdale (Palantir Founder), Joshua Kushner (Oscar co-Founder) and Garrett Camp (Uber co-Founder). Press And Videos Health Moves Forward [CEO Blog Post] Forward Health Launched CarePods [Tech Crunch] An AI Doctor In A Box Coming To A Mall [Forbes] Former Googler Adrian Aoun raises $100 million for walk-in AI healthcare pods [Fortune] Forward Raises $100M and Announces Forward CarePod™ [Forward] What You'll Do Develop algorithms and design highly scalable software solutions for real world problems using data science and machine learning. Collaborate with domain experts in fields like AI and NLP. Our engineers made major contributions to projects such as Amazon Go and TensorFlow. Work with top-flight software and hardware engineering talent from places like Google, Amazon, Nvidia, Palantir, and NASA JPL. Own entire projects while working alongside cross-functional teams of doctors, designers, and operators. Your work will directly contribute to saving and improving people’s lives. For real. :) What We're Looking For Impact - You’re deeply mission-driven and you think there’s more to life than software that enables puppy ears to be superimposed on photos. Although we concede those are cute. Data Science - You have experience developing production software involving building data models for consumer or industrial applications. You are familiar with robust and scalable infrastructure for visualization and presentation. Product passion - You care about the bits you ship ending up in users’ hands, and work backwards from user needs to come up with solutions to problems. Entrepreneurship - You’re a self-starter who loves to own things end-to-end. You don’t ask for permission - you’re too busy making things happen. Team player - You know how to make those around you better and feed off their energy. You take care of your teammates. Curiosity - You like to prototype mobile apps based on the latest psychological research in habit formation in your free time? You open sourced your own neural network library? Boy have we got some fun things for you… You have a BS, MS or PhD in computer science or a related field with focus on data science or machine learning. The base salary range for this full-time position is $100,000-$220,000, plus equity and benefits. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all locations. Within the range, individual pay is determined by factors including job-related skills, experience, relevant education or training, and location. WHY JOIN FORWARD? We don’t want to just move dollars around the healthcare industry - we want to rebuild it and fix it. All of it. You’d be a major part of the story behind one of the most ambitious startup attempts of the past decade and you’d work with a team of people who want to use their talents for good. We are an equal opportunity employer. In accordance with anti-discrimination law, it is the purpose of this policy to effectuate these principles and mandates. We prohibit discrimination and harassment of any type and afford equal employment opportunities to employees and applicants without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law. We conform to the spirit as well as to the letter of all applicable laws and regulations. Pursuant to the San Francisco Fair Chance Ordinance and the Los Angeles Fair Chance Initiative for Hiring, we will consider for employment qualified applicants with arrest and conviction records. Our Commitment To Diversity And Inclusion We deeply understand the value of bringing together a team with different perspectives, educational backgrounds, and life experiences, and we prioritize diversity within our team. We encourage people from underrepresented backgrounds to apply. Information collected and processed as part of any job application you choose to submit is subject to Forward’s Privacy Notice to California Job Applicants .",
        "url": "https://www.linkedin.com/jobs/view/3224222224",
        "summary": "Forward is seeking a world-class IT Engineer to join their team and build the future of healthcare from first principles. The ideal candidate will have experience developing production software involving building data models, a passion for product development, and a strong entrepreneurial spirit. They will also be a team player with a strong sense of curiosity. This role will involve developing algorithms, designing highly scalable software solutions, and collaborating with domain experts in AI and NLP. The position offers a base salary range of $100,000-$220,000, plus equity and benefits.",
        "industries": [
            "Healthcare",
            "Technology",
            "Software Development",
            "Machine Learning",
            "Data Science",
            "Artificial Intelligence"
        ],
        "soft_skills": [
            "Mission-driven",
            "Data Science",
            "Product Passion",
            "Entrepreneurship",
            "Team Player",
            "Curiosity"
        ],
        "hard_skills": [
            "Data Modeling",
            "Machine Learning",
            "Visualization",
            "Presentation",
            "Software Development",
            "AI",
            "NLP"
        ],
        "tech_stack": [
            "Amazon Go",
            "TensorFlow"
        ],
        "programming_languages": [],
        "experience": 0,
        "education": {
            "min_degree": "BS",
            "fields": [
                "Computer Science",
                "Data Science",
                "Machine Learning"
            ]
        },
        "salary": {
            "max": 220000,
            "min": 100000
        },
        "benefits": [
            "Equity"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3971293024,
        "company": "Continue",
        "title": "Software Engineer",
        "created_on": 1720635913.5388396,
        "description": "Continue is seeking an intense and fast-learning engineer to help us build our open-source product, starting by resolving and learning about user problems on GitHub and Discord. In this role, you will shape our open-source IDE extensions by designing, building, and maintaining a product loved by many, including yourself. About Continue We believe there is an opportunity to create a future where developers are amplified, not automated. This is why we are building the leading open-source AI code assistant and layering an enterprise product on top of it to enable organizations to build their own AI code assistant. Headquartered in San Francisco, Continue (YC S23) is funded by Heavybit and angels like Julien Chaumond (co-founder of Hugging Face), Lisha Li (founder of Rosebud AI), and Florian Leibert (co-founder of Mesosphere). With 12k+ GitHub stars, 200k+ downloads, and many large organizations like Siemens rolling out Continue, we are building a team to tackle the biggest challenges at the intersection of AI and developer tools. About you Please keep in mind that we are describing the background that we imagine would best fit the role. If you don’t meet all the requirements, but you are confident that you are up for the task, we absolutely want to get to know you! You are proficient in TypeScript and have a track record of building side-projects You are constantly learning something new and are excited to bring this attitude to problems at the intersection of developer tools and AI You are extremely detail-oriented and care deeply about building a product that you are proud to share with others Your natural instinct is to jump into the code, which you can do already, since a lot of the codebase is open source What you will do We’re a startup, so you’ll have to be ready to do whatever is required to accomplish our mission. However, you can definitely expect to: Play a key role in our GitHub and Discord communities by answering questions and resolving problems Iterate on the smallest details to make our chat and autocomplete experiences awesome Rapidly experiment to improve key metrics for autocomplete and codebase retrieval Design and implement new affordances for coding with LLMs as new model capabilities emerge",
        "url": "https://www.linkedin.com/jobs/view/3971293024",
        "summary": "Continue is seeking an engineer to build their open-source IDE extensions. The role involves resolving user problems on GitHub and Discord, designing, building, and maintaining the product. The ideal candidate is proficient in TypeScript, constantly learning, detail-oriented, and comfortable working in an open-source environment.",
        "industries": [
            "Software Development",
            "Artificial Intelligence",
            "Open Source"
        ],
        "soft_skills": [
            "Problem Solving",
            "Communication",
            "Detail-Oriented",
            "Teamwork",
            "Learning",
            "Passion for Technology"
        ],
        "hard_skills": [
            "TypeScript",
            "GitHub",
            "Discord",
            "AI Code Assistant",
            "LLMs",
            "Autocomplete",
            "Codebase Retrieval",
            "Open Source"
        ],
        "tech_stack": [
            "TypeScript",
            "GitHub",
            "Discord",
            "AI Code Assistant",
            "LLMs"
        ],
        "programming_languages": [
            "TypeScript"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3915651521,
        "company": "OpenAI",
        "title": "Software Engineer, Frontend",
        "created_on": 1720635917.4017031,
        "description": "About The Team The Applied team works across research, engineering, product, and design to bring OpenAI’s technology to the world. We seek to learn from deployment and broadly distribute the benefits of AI, while ensuring that this powerful tool is used responsibly and safely. We aim to make our innovative tools globally accessible, transcending geographic, economic, or platform barriers. Our commitment is to facilitate the use of AI to enhance lives, fostered by rigorous insights into how people use our products. About The Role We are looking for an experienced frontend engineer to join our Applied Engineering team. You’ll work in a deeply iterative, collaborative, fast-paced environment to bring our technology to millions of users around the world, and ensure it’s delivered with safety and reliability in mind. We value engineers who are self-starters, care deeply about the end user experience, and take pride in building products to solve customer needs. In This Role, You Will Partner closely with research, product, and design to bring new features and research capabilities to the world Design and build efficient and reusable front-end systems that drive complex web applications Plan and deploy front end infrastructure necessary to build, test, and deploy our products Create a diverse and inclusive culture that makes all feel welcome while enabling radical candor and the challenging of group think You Might Thrive In This Role If You Have experience building production web apps at scale using TypeScript, React and other web technologies Are comfortable building complex applications and systems from the ground up, with limited development infrastructure available to you Have a voracious and intrinsic desire to learn and fill in missing skills. An equally strong talent for sharing that information clearly and concisely with others Are comfortable with ambiguity and rapidly changing conditions. You view changes as an opportunity to add structure and order when necessary About OpenAI OpenAI is an AI research and deployment company dedicated to ensuring that general-purpose artificial intelligence benefits all of humanity. We push the boundaries of the capabilities of AI systems and seek to safely deploy them to the world through our products. AI is an extremely powerful tool that must be created with safety and human needs at its core, and to achieve our mission, we must encompass and value the many different perspectives, voices, and experiences that form the full spectrum of humanity. We are an equal opportunity employer and do not discriminate on the basis of race, religion, national origin, gender, sexual orientation, age, veteran status, disability or any other legally protected status. For US Based Candidates: Pursuant to the San Francisco Fair Chance Ordinance, we will consider qualified applicants with arrest and conviction records. We are committed to providing reasonable accommodations to applicants with disabilities, and requests can be made via this link. OpenAI Global Applicant Privacy Policy At OpenAI, we believe artificial intelligence has the potential to help people solve immense global challenges, and we want the upside of AI to be widely shared. Join us in shaping the future of technology.",
        "url": "https://www.linkedin.com/jobs/view/3915651521",
        "summary": "OpenAI seeks an experienced frontend engineer to join their Applied Engineering team. The role involves collaborating with research, product, and design teams to build scalable web applications using TypeScript, React, and other web technologies. The ideal candidate is comfortable with ambiguity and rapid change, has a strong desire to learn and share knowledge, and values building inclusive work environments.",
        "industries": [
            "Artificial Intelligence",
            "Software Development",
            "Technology",
            "Research"
        ],
        "soft_skills": [
            "Collaboration",
            "Communication",
            "Problem Solving",
            "Self-Starter",
            "Teamwork",
            "Ambiguity Tolerance",
            "Adaptability",
            "Learning Agility",
            "Inclusion",
            "Candor"
        ],
        "hard_skills": [
            "TypeScript",
            "React",
            "Frontend Development",
            "Web Technologies",
            "Web Application Development",
            "Production Web Apps",
            "System Building",
            "Infrastructure Deployment",
            "Software Engineering"
        ],
        "tech_stack": [
            "TypeScript",
            "React"
        ],
        "programming_languages": [
            "TypeScript",
            "JavaScript"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Jose, CA",
        "job_id": 3967445262,
        "company": "Intelliswift Software",
        "title": "Senior Data Developer",
        "created_on": 1720635919.1398768,
        "description": "$98.59 per hour on W2 San Jose, CA Hybrid. Open to remote 6 Months You will translate the business requirements into technical specifications using logical and critical thinking skills. You will profile data, join them, cleanse them, and build data products that meet the business requirements. You will troubleshoot and resolve issues related to data quality, data incompleteness, and data pipeline. You will have the ability to identify the root cause of poor code performance and resolve them through code optimization, modifying data structures, and configuration changes. You will support multiple ongoing projects in a fast-paced environment across a global company You should be comfortable with the design and implementation of any stage of an end-to-end data pipeline, including data ingestion, processing, transformation, and storage. A willingness to learn business processes, systems, and the technologies used. Strong communication and collaboration skills essential. The ability to work with Product Owners and Stakeholders to reach consensus during design and development is crucial. You will need to clearly explain data nuances, data quality issues, and limitations to non-technical colleagues. The ability to read complex SQL or Python queries, understand them, and modify them for efficiency and readability. A minimum of 8 years of experience in data engineering or related roles, with a solid background in data warehousing, data modeling, data access, and data storage techniques. Proficiency in SQL or Python with at least 8 years of experience. Experience with cloud platforms such as Databricks, Azure and proficiency in cloud services such as Azure Data Factory.",
        "url": "https://www.linkedin.com/jobs/view/3967445262",
        "summary": "Data Engineer needed to translate business requirements into technical specifications. This role involves profiling, joining, and cleansing data to build data products that meet business needs. You'll troubleshoot data quality and pipeline issues, optimize code performance, and support multiple ongoing projects in a fast-paced environment. Experience with cloud platforms like Databricks and Azure is essential.",
        "industries": [
            "Data Engineering",
            "Technology",
            "Software Development",
            "Analytics",
            "Data Science"
        ],
        "soft_skills": [
            "Logical Thinking",
            "Critical Thinking",
            "Troubleshooting",
            "Problem Solving",
            "Communication",
            "Collaboration",
            "Teamwork",
            "Stakeholder Management",
            "Business Acumen",
            "Time Management",
            "Adaptability",
            "Flexibility",
            "Learning Agility"
        ],
        "hard_skills": [
            "Data Engineering",
            "Data Warehousing",
            "Data Modeling",
            "Data Access",
            "Data Storage",
            "SQL",
            "Python",
            "Databricks",
            "Azure",
            "Azure Data Factory"
        ],
        "tech_stack": [
            "Databricks",
            "Azure",
            "Azure Data Factory"
        ],
        "programming_languages": [
            "SQL",
            "Python"
        ],
        "experience": 8,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 9859,
            "min": 9859
        },
        "benefits": [
            "Hybrid Work",
            "Remote Work"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Orinda, CA",
        "job_id": 3888430783,
        "company": "Seven Hills Group Technologies Inc.",
        "title": "Cloud Data Engineer",
        "created_on": 1720635920.6942601,
        "description": "Title: Cloud Data Engineer Client: Palo Alto Networks Location: Bay Area, CA (Hybrid, not fully remote) Only independent contract for USC/GC Expert in data engineering and GCP data technologies. Work with client teams to design and implement modern, scalable data solutions using a range of new and emerging technologies from the Google Cloud Platform. Work with Agile and DevOps techniques and implementation approaches in the delivery Job Description 10+ Year experience in BI and Analytics Hands on and deep experience (at least 2 years) working with Google Data Products (e.g., BigQuery, Dataflow, Dataproc,] etc.). Experience in Spark (Scala/Python/Java) and Kafka, Airflow Data Engineering and Lifecycle (including non-functional requirements and operations) management. E2E Solution Design skills - Prototyping, Usability testing and data visualization literacy. Experience with SQL and NoSQL modern data stores.",
        "url": "https://www.linkedin.com/jobs/view/3888430783",
        "summary": "Cloud Data Engineer specializing in Google Cloud Platform (GCP) technologies like BigQuery, Dataflow, Dataproc, Spark, Kafka, and Airflow. Responsible for designing, implementing, and managing scalable data solutions, leveraging Agile and DevOps practices.",
        "industries": [
            "Technology",
            "Software",
            "Data & Analytics",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Analytical Thinking",
            "Design Thinking",
            "Project Management",
            "Agile",
            "DevOps"
        ],
        "hard_skills": [
            "Data Engineering",
            "GCP",
            "BigQuery",
            "Dataflow",
            "Dataproc",
            "Spark",
            "Scala",
            "Python",
            "Java",
            "Kafka",
            "Airflow",
            "SQL",
            "NoSQL",
            "Data Visualization",
            "Prototyping",
            "Usability Testing"
        ],
        "tech_stack": [
            "Google Cloud Platform (GCP)",
            "BigQuery",
            "Dataflow",
            "Dataproc",
            "Spark",
            "Kafka",
            "Airflow",
            "SQL",
            "NoSQL"
        ],
        "programming_languages": [
            "Scala",
            "Python",
            "Java"
        ],
        "experience": 10,
        "education": {
            "min_degree": null,
            "fields": [
                "Computer Science",
                "Data Science",
                "Engineering"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco Bay Area",
        "job_id": 3925718646,
        "company": "Imbue",
        "title": "Software Engineer, Data",
        "created_on": 1720635922.4181993,
        "description": "Summary This role can either be fully in-person or remote. We believe that high quality data is the most important part of creating high performance machine learning systems, regardless of whether they are simple classifiers or state of the art reasoning agents. Unlike many other organizations, we view this work, and this role, as one of the most important at the company. In this role, you will work on the most important part of our system--the software infrastructure for collecting, preprocessing, generating, analyzing, and distilling the wide variety of data sources that go into both our primary pretraining data corpus, as well as the datasets for all of the other ancillary and secondary models and system. You will make a meaningful, measurable impact on the performance of our systems, and experience the joy of spending time to make high quality software that makes high quality data. Example projects Discover, reprocess, and clean open source datasets that are applicable to our targeted LLM capabilities Utilize or develop helper models to accurately classify, tag, or preprocess various forms of text Create a high quality OCR pipeline for pulling pre-training text from images and scans Integrate with third parties that can manually generate, label, or fix existing datasets, and do so in an efficient, reliable manner Work with researchers to find creative ways to leverage existing LLM tooling into data pipelines Collaborate with product team to discover what types of data have the most downstream impact on end-to-end product power Create statistical methods and analyses to investigate various quality metrics about our datasets, and compare and contrast various data cleaning or generation methodologies In summary: create robust systems for making sure all input data into our LLMs is 100% good data You are Detail oriented. Data mistakes are easy to make and hard to catch Passionate about data. You should be happy to look at and deeply engage with the raw data An excellent software engineer. We care about engineering best practices Familiar with Python Compensation And Benefits Work on the most important part of our system Work at a place that deeply cares about data quality Work directly on creating software with human-like intelligence Very generous compensation Flexible working hours Work remotely Time and budget for learning and self improvement Compensation packages are highly variable based on a variety of factors. If your salary requirements fall outside of the stated range, we still encourage you to apply. The range for this role is $170,000–$350,000 cash, $10,000–$2,000,000 in equity How To Apply All submissions are reviewed by a person, so we encourage you to include notes on why you're interested in working with us. If you have any other work that you can showcase (open source code, side projects, etc.), certainly include it! We know that talent comes from many backgrounds, and we aim to build a team with diverse skillsets that spike strongly in different areas. We try to reply either way within a week or two at most (usually much sooner). Learn more about our full interview process here. About Us Imbue builds AI systems that reason and code, enabling AI agents to accomplish larger goals and safely work in the real world. We train our own foundation models optimized for reasoning and prototype agents on top of these models. By using these agents extensively, we gain insights into improving both the capabilities of the underlying models and the interaction design for agents. We aim to rekindle the dream of the *personal* computer, where computers become truly intelligent tools that empower us, giving us freedom, dignity, and agency to pursue the things we love.",
        "url": "https://www.linkedin.com/jobs/view/3925718646",
        "summary": "This role focuses on data infrastructure for collecting, preprocessing, analyzing, and distilling data for large language models (LLMs). The ideal candidate is detail-oriented, passionate about data, an excellent software engineer, and proficient in Python. Responsibilities include discovering, cleaning, and reprocessing open source datasets, utilizing or developing helper models for data classification and preprocessing, creating OCR pipelines, integrating with third parties for data generation and labeling, collaborating with researchers and product teams, and creating statistical methods for evaluating data quality. ",
        "industries": [
            "Artificial Intelligence",
            "Machine Learning",
            "Software Development"
        ],
        "soft_skills": [
            "Detail-oriented",
            "Passionate about data",
            "Excellent communication",
            "Collaboration",
            "Problem-solving",
            "Creative thinking"
        ],
        "hard_skills": [
            "Python",
            "OCR",
            "Data Cleaning",
            "Data Preprocessing",
            "Data Analysis",
            "Statistical Methods",
            "Data Quality Evaluation",
            "Software Engineering"
        ],
        "tech_stack": [
            "Python",
            "OCR",
            "LLMs",
            "Foundation Models"
        ],
        "programming_languages": [
            "Python"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 350000,
            "min": 170000
        },
        "benefits": [
            "Very generous compensation",
            "Flexible working hours",
            "Work remotely",
            "Time and budget for learning and self improvement"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Sunnyvale, CA",
        "job_id": 3971398684,
        "company": "Softworld, a Kelly Company",
        "title": "Sr. Data Verification Engineer",
        "created_on": 1720635923.9883194,
        "description": "Sr Data Verification Engineer Location: Sunnyvale, CA (hybrid) Duration: 1 year We are currently recruiting for a Sr Data Verification Engineer to work in hybrid model in Sunnyvale. The ideal candidate will have strong python and SQL. There will be a coding challenge during interview. Candidates should also have experience in building and automating test processes to ensure data quality and reliability within a data science platform. They should possess expertise in developing automated tests for cloud-based applications and demonstrate a deep understanding of database concepts and complex data relationships. The Senior Data Verification Engineer is responsible for building and automating test processes and workflows to ensure data quality and support data validation across interdependent components of a central data science platform. In this role, you will have the opportunity to: Develop and implement automated test cases to ensure reliability of applications, data pipelines, deployment and migration processes. Perform technical troubleshooting, identify bottlenecks, and conduct initial root cause analysis for data issues, collaborate to identify the scope of the issue, and path to resolution. Build models that measure and evaluate the quality, consistency, resilience, accuracy, and availability of the current data ecosystem. Analyze and communicate impacts of data updates, changes, additions/removals to technical and non-technical partners. Create design (data flow diagrams, technical design specs, source to target mapping documents) and test (unit/integration tests) documentation. The essential requirements of the job include: Bachelors degree in software engineering, computer science or a similar field with 5+ years of related work experience, OR Masters degree with 3+ years of related work experience. Working knowledge of Python and SQL. Experience building automated tests for production-grade cloud software applications, pipelines, and deployment processes. Experience with database and data relation concepts such as mapping documents and complex data relationships. Excellent communication, collaboration and problem-solving skills and ability to address complex data issues and investigate systemic defects.",
        "url": "https://www.linkedin.com/jobs/view/3971398684",
        "summary": "A Sr Data Verification Engineer is needed to work in a hybrid model in Sunnyvale, CA for a 1 year contract. The ideal candidate will have strong Python and SQL skills and experience building and automating test processes for cloud-based applications. They should also have a deep understanding of database concepts and complex data relationships.",
        "industries": [
            "Software Engineering",
            "Computer Science",
            "Data Science"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem-solving"
        ],
        "hard_skills": [
            "Python",
            "SQL",
            "Data Quality",
            "Data Reliability",
            "Automated Testing",
            "Cloud Applications",
            "Database Concepts",
            "Data Relationships",
            "Technical Troubleshooting",
            "Root Cause Analysis",
            "Data Modeling",
            "Data Analysis",
            "Data Flow Diagrams",
            "Technical Design Specs",
            "Source to Target Mapping",
            "Unit Testing",
            "Integration Testing"
        ],
        "tech_stack": [
            "Python",
            "SQL",
            "Cloud Applications",
            "Data Pipelines",
            "Data Science Platform"
        ],
        "programming_languages": [
            "Python",
            "SQL"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelors",
            "fields": [
                "Software Engineering",
                "Computer Science"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Jose, CA",
        "job_id": 3893969732,
        "company": "TikTok",
        "title": "Data Engineer - Applied AI (DCC)",
        "created_on": 1720635925.6608372,
        "description": "Responsibilities TikTok is the leading destination for short-form mobile video. Our mission is to inspire creativity and bring joy. TikTok has global offices including Los Angeles, New York, London, Paris, Berlin, Dubai, Singapore, Jakarta, Seoul and Tokyo. Creation is the core of TikTok's purpose. Our platform is built to help imaginations thrive. This is doubly true of the teams that make TikTok possible. Together, we inspire creativity and bring joy - a mission we all believe in and aim towards achieving every day. To us, every challenge, no matter how difficult, is an opportunity; to learn, to innovate, and to grow as one team. Status quo? Never. Courage? Always. At TikTok, we create together and grow together. That's how we drive impact - for ourselves, our company, and the communities we serve. Join us. About the team The Applied AI (AAI) Team is a data science team that is part of the Monetization Integrity team in the Global Business Solutions of TikTok. It supports the Data Cycling Center, which focuses on empowering businesses with affordable and trusted data & models. Responsibilities: - Construct, maintain, and test architectures, such as databases and large-scale processing systems. - Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. - Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources. - Work with stakeholders including the PMs, Data Scientists, and Machine Learning Engineer teams to assist with data-related technical issues and support their data infrastructure needs. - Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader. Qualifications Minimum Qualifications: - BS or above in Computer Science, Software Engineering, Data Science, or a related field - 2 years of experience with big data tools: Hadoop, Spark, Kafka (Hadoop, M/R, Hive, Spark, Metastore, Presto, Flume, Kafka, ClickHouse, Flink) - Experience with ETL (Extraction, Transformation & Loading) and architecting data systems - Experience with schema design and data modeling, performing data analysis, data ingestion, and data integration - Experience supporting and working with cross-functional teams in a dynamic environment. - Experience in CI/CD such as git and cloud services such as AWS/GCP/Azure will be highly desirable - Excellent XFN communications skills; able to communicate analytical and technical content in an easy-to-understand way to both technical and non-technical audiences - Intellectual curiosity, along with excellent problem-solving and quantitative skills, including the ability to desegregate issues, identify root causes and recommend solutions Preferred Qualifications: -5 years of experience with Big Data tools TikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too. TikTok is committed to providing reasonable accommodations in our recruitment processes for candidates with disabilities, pregnancy, sincerely held religious beliefs or other reasons protected by applicable laws. If you need assistance or a reasonable accommodation, please reach out to us at https://shorturl.at/cdpT2 Job Information: 【For Pay Transparency】Compensation Description (annually) The base salary range for this position in the selected city is $128600 - $290000 annually. ​ Compensation may vary outside of this range depending on a number of factors, including a candidate’s qualifications, skills, competencies and experience, and location. Base pay is one part of the Total Package that is provided to compensate and recognize employees for their work, and this role may be eligible for additional discretionary bonuses/incentives, and restricted stock units. ​ Our company benefits are designed to convey company culture and values, to create an efficient and inspiring work environment, and to support our employees to give their best in both work and life. We offer the following benefits to eligible employees: ​ We cover 100% premium coverage for employee medical insurance, approximately 75% premium coverage for dependents and offer a Health Savings Account(HSA) with a company match. As well as Dental, Vision, Short/Long term Disability, Basic Life, Voluntary Life and AD&D insurance plans. In addition to Flexible Spending Account(FSA) Options like Health Care, Limited Purpose and Dependent Care. ​ Our time off and leave plans are: 10 paid holidays per year plus 17 days of Paid Personal Time Off (PPTO) (prorated upon hire and increased by tenure) and 10 paid sick days per year as well as 12 weeks of paid Parental leave and 8 weeks of paid Supplemental Disability. ​ We also provide generous benefits like mental and emotional health benefits through our EAP and Lyra. A 401K company match, gym and cellphone service reimbursements. The Company reserves the right to modify or change these benefits programs at any time, with or without notice. ​",
        "url": "https://www.linkedin.com/jobs/view/3893969732",
        "summary": "TikTok is seeking a Data Engineer to join its Applied AI team, focusing on data infrastructure and automation within the Monetization Integrity team. The role involves building and maintaining data systems, extracting, transforming, and loading data, collaborating with data scientists and engineers, and developing tools for data analysis. Ideal candidates will have strong experience with big data tools (Hadoop, Spark, Kafka), ETL processes, data modeling, and working with cross-functional teams. Excellent communication skills and experience with CI/CD and cloud services are highly desirable. This is a full-time position at TikTok's Global Business Solutions with an annual salary range of $128,600 - $290,000.",
        "industries": [
            "Technology",
            "Social Media",
            "Data Science",
            "Machine Learning",
            "Artificial Intelligence",
            "Big Data",
            "Cloud Computing",
            "Monetization",
            "Business Solutions"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Quantitative Skills",
            "Analytical Skills",
            "Teamwork",
            "Communication Skills",
            "Cross-Functional Collaboration"
        ],
        "hard_skills": [
            "Hadoop",
            "Spark",
            "Kafka",
            "ETL",
            "Data Modeling",
            "Data Analysis",
            "Data Ingestion",
            "Data Integration",
            "Schema Design",
            "CI/CD",
            "Git",
            "AWS",
            "GCP",
            "Azure"
        ],
        "tech_stack": [
            "Hadoop",
            "Spark",
            "Kafka",
            "ClickHouse",
            "Flink",
            "Hive",
            "Metastore",
            "Presto",
            "Flume",
            "AWS",
            "GCP",
            "Azure"
        ],
        "programming_languages": [],
        "experience": 2,
        "education": {
            "min_degree": "BS",
            "fields": [
                "Computer Science",
                "Software Engineering",
                "Data Science",
                "Related Field"
            ]
        },
        "salary": {
            "max": 290000,
            "min": 128600
        },
        "benefits": [
            "Medical Insurance",
            "Dental Insurance",
            "Vision Insurance",
            "Short-Term Disability",
            "Long-Term Disability",
            "Life Insurance",
            "AD&D Insurance",
            "Flexible Spending Account (FSA)",
            "Health Savings Account (HSA)",
            "Paid Time Off (PTO)",
            "Sick Leave",
            "Parental Leave",
            "Supplemental Disability",
            "Employee Assistance Program (EAP)",
            "Mental Health Benefits",
            "401k Match",
            "Gym Reimbursement",
            "Cellphone Service Reimbursement"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Oakland, CA",
        "job_id": 3956557833,
        "company": "Ava Community Energy",
        "title": "Sr. Data Engineer",
        "created_on": 1720635927.2094285,
        "description": "About Ava Community Energy Ava Community Energy (Ava), Formerly East Bay Community Energy, is a Joint Powers Authority comprising Alameda County and its incorporated areas, and the cities of Lathrop, Stockton, and Tracy. Ava’s charter is to provide its customers with low carbon, cost-effective electricity that integrates innovative energy products and maximizes local benefits through the development of local solutions, ranging from increasing access to rooftop solar to supporting the adoption of electric vehicles. Culture and Commitment to Diversity Ava believes in a vibrant culture that supports and nurtures a workplace that offers every individual the opportunity to attain professional goals and contribute to accomplishing our mission. We foster a culture of open communication, responsibility, curiosity, accountability, and teamwork. We want creative problem solvers that are excited to work in an entrepreneurial environment and grow our organization! As an equal opportunity employer, we are committed to diversity, equity, and inclusion. We are committed to a diverse and inclusive workforce that is reflective of our service area’s rich culture and communities. We welcome a diversity of experiences, perspectives, and skills. We strongly encourage people of all characteristics to apply and prohibit unlawful discrimination and harassment of any type based on age, religion, color, gender identity or expression, race, ancestry, or national origin, religion or creed, marital status, military or veteran status, sexual orientation, genetic information or characteristics, and those with a disability as protected by federal, state and local laws. Position Summary Ava is seeking to hire a full-time Senior Data Engineer to join our growing Analytics team, reporting to the Head of Technology and Analytics. Ava is looking for a software engineer with experience in data pipelines and data wrangling who can expand and optimize Ava’s existing data infrastructure, extensively collaborating with our Salesforce, Data Science, Modeling and Analytics team. We are looking for someone who can navigate both the realms of Data and Software Engineering. Ideally, we are looking for a software engineer who communicates effectively and is collaborative in nature, and who will be excited about contributing to Ava’s technology stack and broader mission. Position Details Essential Duties And Responsibilities Create and maintain an optimal engineering tech stack and reliable data pipeline operations, including but not limited to: Further developing and maintaining the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources (e.g., ETL, APIs). Innovating internal processes by automating manual tasks, fine-tuning data delivery mechanisms, and revamping infrastructure to ensure scalability. Designing and implementing best practices for data storage and query optimization, with a focus on minimizing costs within GCP. Designing robust error logging and process monitoring solutions while adhering to best practices and leveraging technologies available on the Google Cloud Platform. Establishing and maintaining engineering, analytical, and monitoring protocols to uphold data quality standards. Playing a key role in transitioning Ava's bash processes to a containerized framework utilizing Google Cloud Platform's CloudRun service. Assisting in the adoption of top-tier DevOps practices, which includes refining version control infrastructure to better suit DevOps workflows, implementing GitHub Actions for automated process orchestration, and establishing robust CI/CD pipelines for seamless integration and delivery of software updates. Having a “tackle challenges head-on” mentality for jumping and solving problems as they arise, supporting the team with process testing, bug fixes, and code reviews to ensure software reliability and quality. Innovation in Data Engineering and Analytics: Further enhance the Ava API using best practices in data architecture, engineering efficiency, data and cybersecurity. Help implement best practices in machine Learning infrastructure in partnership with the Analytics team Help maintain open source software that has been developed to foster the growth of clean and renewable energy. Data and Cyber Security: Enhancing IAM (Identity and Access Management) policies related to data and cybersecurity to ensure compliance with industry standards and best practices. Collaborating on network security measures to fortify Ava's infrastructure against potential cyber threats, ensuring data integrity and confidentiality are preserved. Conducting regular security audits and risk assessments to identify vulnerabilities and prioritize mitigation efforts. Monitoring and analyzing security logs and alerts to detect and respond to suspicious activities. Other duties as required Minimum Qualifications A bachelor’s degree in computer science or another engineering and/or data-intensive discipline (MS preferred). > 5 years of work experience at the intersection of data and software engineering in a cross-functional collaborative environment > 5 years of experience in building, maintaining, and optimizing ‘big data’ pipelines (e.g., architecture, engineering systems, processes), implementing DevOps processes for efficient asynchronous collaboration, and strong analytical skills related to working with unstructured datasets. A successful history of maintaining or developing software and data engineering infrastructure for internal and external data sharing, analytical processes, and machine learning infrastructure Extensive experience with Bash Scripting, Linux operating systems Extensive experience with relational SQL (GCP BigQuery preferred) databases Extensive experience with Python Experience with frontend (TypeScript, Vue3, Vuetify3, Google Maps / DeckGL) and backend (Golang) software development Extensive experience with Google Cloud Services (e.g., Google Cloud Platform, Amazon). Preferred: Google Cloud Platform Extensive experience with CRMs and their backend infrastructure (e.g., Salesforce, Salesforce Bulk Python API). Preferred: Salesforce Extensive experience with batch-processing systems Experience with data pipeline and workflow management tools (e.g. Luigi, Airflow) Extensive experience in containerized applications and container based orchestration environments (e.g., Docker, Kubernetes) Extensive experience with version control (e.g., git, Github) Experience with infrastructure as code (e.g. Terraform) Compensation And Benefits The annual salary for this position is $173,567 with a Total Compensation of up to $190,924 annually based on performance. Ava offers a generous benefits package which includes: Zero cost Individual, family, and domestic partner health insurance (medical, dental, vision) Life and AD&D, EAP, STD, and LTD Retirement with Employer Matching Social Security deferment plan Paid Time Off and Holidays Commuter Benefits Fertility & Adoption Benefits Pet Insurance Mental Health Support Flexible Spending Accounts (health and dependent care accounts) Paid Volunteer Days And More! Location and Working Conditions This position will be based in Ava headquarters in Oakland, near BART. Ava's team has a hybrid approach inclusive of in-office and remote work. Ava’s team is expected to go into the office Tuesday, Thursday, and the first Wednesday of the month. For this position, based on role fit, Ava may be flexible on the number of days expected in the office. We are not providing relocation or sponsorships currently. The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Ava is dedicated to reasonably accommodate an applicant for known physical or mental disabilities so that the applicant may participate in the application process. We will engage in a timely, good-faith interactive process with any employee with a known physical or mental disability to identify potential reasonable accommodations, if any, to enable the employee to perform the essential functions of their position. General office environment is primarily sedentary work which requires the following physical activities: standing, sitting, walking, reaching, lifting, finger dexterity, grasping, repetitive motions, talking, hearing and visual acuity. The employee must occasionally lift and/or move up to 10 pounds. The noise level is usually moderate. Licenses/Certificates Possession and continued maintenance of a valid class C California driver’s license or the ability to provide alternate transportation and a safe driving record. Working at Ava Community Energy Ava is committed to complying with applicable laws, including the Americans with Disability Act and Fair Employment and Housing Act, ensuring equal employment opportunities to qualified individuals with a disability. Ava prohibits unlawful discrimination based on age, sex or gender (including pregnancy, childbirth, breastfeeding or related medical conditions), genetic information or characteristics, gender identity, gender expression, race, color, ancestry, national origin, religion, creed, marital status, military or veteran status, sexual orientation, physical or mental disability, medical condition, or on any other basis prohibited by federal, state, or local laws. Ava strives to maintain a COVID-free workplace, being fully vaccinated is a condition of employment (exemptions will be evaluated by legal counsel). The information contained herein has been designed to indicate the general nature and level of work performed by employees within this classification. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications required of employees assigned to this job. The information also does not constitute either an expressed or implied contract, and these provisions are subject to change.",
        "url": "https://www.linkedin.com/jobs/view/3956557833",
        "summary": "Ava Community Energy is seeking a Senior Data Engineer to join their growing Analytics team. The role will focus on building and maintaining data pipelines, optimizing data infrastructure, and collaborating with the Salesforce, Data Science, Modeling and Analytics team. The ideal candidate will have experience in data and software engineering, strong communication and collaboration skills, and a passion for contributing to Ava's mission.",
        "industries": [
            "Energy",
            "Renewable Energy",
            "Clean Energy",
            "Technology",
            "Analytics",
            "Software Engineering",
            "Data Science",
            "Data Engineering"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Analytical Skills",
            "Teamwork",
            "Curiosity",
            "Accountability",
            "Entrepreneurial",
            "Adaptability",
            "Innovation"
        ],
        "hard_skills": [
            "Data Pipelines",
            "ETL",
            "APIs",
            "Automation",
            "Data Delivery Mechanisms",
            "Data Storage",
            "Query Optimization",
            "Google Cloud Platform",
            "Error Logging",
            "Process Monitoring",
            "Data Quality Standards",
            "Bash Scripting",
            "Linux",
            "SQL",
            "GCP BigQuery",
            "Python",
            "TypeScript",
            "Vue3",
            "Vuetify3",
            "Google Maps",
            "DeckGL",
            "Golang",
            "Salesforce",
            "Salesforce Bulk Python API",
            "Batch Processing Systems",
            "Luigi",
            "Airflow",
            "Docker",
            "Kubernetes",
            "Git",
            "GitHub",
            "Terraform",
            "DevOps"
        ],
        "tech_stack": [
            "Google Cloud Platform (GCP)",
            "Salesforce",
            "Bash",
            "Linux",
            "SQL",
            "BigQuery",
            "Python",
            "TypeScript",
            "Vue3",
            "Vuetify3",
            "Google Maps",
            "DeckGL",
            "Golang",
            "Docker",
            "Kubernetes",
            "Git",
            "GitHub",
            "Terraform",
            "Luigi",
            "Airflow"
        ],
        "programming_languages": [
            "Python",
            "Bash",
            "SQL",
            "TypeScript",
            "Golang"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Engineering",
                "Data-Intensive Disciplines"
            ]
        },
        "salary": {
            "max": 190924,
            "min": 173567
        },
        "benefits": [
            "Health Insurance",
            "Dental Insurance",
            "Vision Insurance",
            "Life Insurance",
            "AD&D",
            "EAP",
            "STD",
            "LTD",
            "Retirement with Employer Matching",
            "Social Security Deferment Plan",
            "Paid Time Off",
            "Holidays",
            "Commuter Benefits",
            "Fertility & Adoption Benefits",
            "Pet Insurance",
            "Mental Health Support",
            "Flexible Spending Accounts",
            "Paid Volunteer Days"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3957761675,
        "company": "Akraya, Inc.",
        "title": "Python Data Engineer - Content Operations: 24-01732",
        "created_on": 1720635928.9258103,
        "description": "Primary Skills: Python, Data Analysis, LLMs, Cloud Platforms, Git Contract Type: W2 Duration: 2+ Months (Possible Extension) Location: San Francisco, CA Pay Range: $52 - $55 Per hour on W2 #DP Job Responsibilities We are on the hunt for a dedicated Data Engineer with a passion for data and expertise in Python, to join our team part-time (3 days/week) in our San Francisco office. The successful candidate will be pivotal in executing projects focused on processing, analyzing, and interpreting vast datasets within the global financial markets. This position involves building and managing data pipelines, leveraging large language models for automation, and enhancing productivity through tool integration. Extract, transform, join, and load data to support internal project needs and operational management of customer-ready help content. Build, format, and publish aggregated datasets containing customer-facing help content. Design, develop, and document data pipelines for ingesting and managing data from various sources. Utilize Large Language Models (LLMs) to automate content-related tasks, increasing efficiency. Integrate AI-based and other tools to improve processes and productivity within content authoring teams. JOB REQUIREMENTS: Proficient in Python coding with experience in data structures and web service APIs. Familiarity with cloud storage and platforms (AWS or Azure) and proficient use of Git. Demonstrated ability to perform exploratory data analysis, uncovering insights within datasets. ABOUT AKRAYA Akraya is an award-winning IT staffing firm consistently recognized for our commitment to excellence and a positive work environment. Voted the #1 Best Place to Work in Silicon Valley (2023) and a Glassdoor Best Places to Work (2023 & 2022), Akraya prioritizes a culture of inclusivity and fosters a sense of belonging for all team members. We are staffing solutions providers for Fortune 100 companies, and our industry recognitions solidify our leadership position in the IT staffing space. Let us lead you to your dream career, join Akraya today!",
        "url": "https://www.linkedin.com/jobs/view/3957761675",
        "summary": "Part-time Data Engineer role (3 days/week) in San Francisco, CA focusing on processing, analyzing, and interpreting large datasets within the global financial markets. Responsibilities include building and managing data pipelines, utilizing LLMs for automation, and integrating AI-based tools for productivity enhancement within content authoring teams.",
        "industries": [
            "Finance",
            "Technology",
            "Data Engineering",
            "AI"
        ],
        "soft_skills": [
            "Dedicated",
            "Passionate",
            "Organized",
            "Analytical",
            "Detail-Oriented",
            "Problem-Solving",
            "Communication",
            "Teamwork"
        ],
        "hard_skills": [
            "Python",
            "Data Analysis",
            "LLMs",
            "Cloud Platforms",
            "Git",
            "Data Structures",
            "Web Service APIs",
            "Exploratory Data Analysis"
        ],
        "tech_stack": [
            "Python",
            "LLMs",
            "AWS",
            "Azure",
            "Git"
        ],
        "programming_languages": [
            "Python"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 55,
            "min": 52
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Los Angeles, CA",
        "job_id": 3970500763,
        "company": "Bright Digital Solutions",
        "title": "Software Engineer (Remote)",
        "created_on": 1720635930.7668524,
        "description": "Bright Digital Solutions is a female-led digital marketing agency based in LA. We specialize in crafting tailored campaigns that resonate with your audience and drive meaningful results, from marketing strategy to SEO optimization. Leveraging our diverse perspectives, we deliver personalized, data-driven strategies that elevate your brand's online presence. At Bright, we're committed to innovation and inclusivity, staying ahead in the ever-evolving digital landscape while upholding integrity and creativity in all our endeavors. About The Role We are seeking a passionate and driven Junior Software Engineer to join our dynamic team. As a Software Engineer, you will be involved in all stages of software development, from brainstorming ideas to deployment. You will work closely with our experienced team members to design, develop, test, and maintain high-quality software solutions that meet customer needs and enhance user experience. Some of what you will do: Collaborate with cross-functional teams to gather and define software requirements. Design, develop, and implement software solutions following best practices and coding standards. Write clean, maintainable, and efficient code. Perform thorough testing to ensure software quality, including writing and executing test cases. Troubleshoot and debug issues reported by users or detected during testing. Participate in code reviews to provide and receive constructive feedback. Stay updated on emerging technologies and trends in software development. Contribute to the continuous improvement of our development processes and practices. Document software designs, implementation, and maintenance procedures. What will help you be successful: Bachelor's degree in Computer Science, Software Engineering, or a related field. Strong understanding of software development principles, data structures, and algorithms. Proficiency in at least one programming language (e.g., Java, Python, C++, etc.). Familiarity with web development technologies such as HTML, CSS, JavaScript, and frameworks like React or Angular is a plus. Ability to work effectively in a team environment and collaborate with others. Excellent problem-solving and analytical skills. Strong communication skills, both verbal and written. Ability to adapt to new technologies and learn quickly. What we offer: Highly competitive salary and extensive employee benefits package. An impactful role offering genuine influence and contribution opportunities. A nurturing culture that prizes creativity, learning, and empowerment within a supportive team environment. Ample opportunities for professional growth in a company at the forefront of digital innovation. Wellness Days dedicated to prioritizing physical and mental well-being. Additional leave allowance of 2 weeks annually Two paid volunteer days each year to support our affiliated charities and give back to the community. How to Apply: At Bright Digital Solutions, we are dedicated to fostering a workplace that celebrates diversity and inclusivity, where every employee's unique qualities are embraced and valued for their significant contribution. We firmly believe that a diverse and inclusive environment brings out the best in each individual and enables us to deliver a truly exceptional client experience. We welcome applications from candidates of all cultural backgrounds, genders, and sexual orientations. If you believe this role aligns with your aspirations, we warmly invite you to apply today. We look forward to connecting with you!",
        "url": "https://www.linkedin.com/jobs/view/3970500763",
        "summary": "Bright Digital Solutions, a female-led digital marketing agency, seeks a Junior Software Engineer to contribute to all stages of software development, including gathering requirements, design, implementation, testing, and maintenance. The ideal candidate possesses a strong understanding of software development principles, proficiency in a programming language (Java, Python, C++), and familiarity with web development technologies. Bright Digital Solutions offers competitive salary and benefits, a supportive team environment, opportunities for professional growth, wellness days, additional leave, and volunteer days.",
        "industries": [
            "Digital Marketing",
            "Software Development",
            "Marketing"
        ],
        "soft_skills": [
            "Passionate",
            "Driven",
            "Collaborative",
            "Problem-Solving",
            "Analytical",
            "Communication",
            "Adaptable",
            "Teamwork"
        ],
        "hard_skills": [
            "Software Development",
            "Data Structures",
            "Algorithms",
            "Java",
            "Python",
            "C++",
            "HTML",
            "CSS",
            "JavaScript",
            "React",
            "Angular"
        ],
        "tech_stack": [
            "Java",
            "Python",
            "C++",
            "HTML",
            "CSS",
            "JavaScript",
            "React",
            "Angular"
        ],
        "programming_languages": [
            "Java",
            "Python",
            "C++"
        ],
        "experience": 0,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Computer Science",
                "Software Engineering"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Competitive Salary",
            "Employee Benefits",
            "Creative Culture",
            "Learning Opportunities",
            "Professional Growth",
            "Wellness Days",
            "Additional Leave",
            "Volunteer Days"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Los Angeles, CA",
        "job_id": 3947133753,
        "company": "DoorDash",
        "title": "Software Engineer, Backend (All Teams)",
        "created_on": 1720635932.3627672,
        "description": "About The Role DoorDash is building the world's most reliable on-demand, logistics engine for delivery. We are continuing to grow rapidly and expanding our Engineering offices globally! We are looking for Backend Engineers to build and maintain a large scale 24x7 global infrastructure system that powers DoorDash's 3-sided marketplace of Consumers, Merchants and Dashers. We’re looking for Backend Engineers to work on both Product and Product Platform based teams. Product focused Engineers work at the intersection of product and infrastructure to solve key business problems with elegant technical solutions. You'll operate our backend services and architecture that support all product functionality and will be challenged to consider the big picture -- collaborating cross-functionally, as well as evaluating and executing on trade-offs to maximize business impact for the company. This role is hybrid with some in-office time expected and will report to an Engineering Manager. You're Excited About This Opportunity Because You Will… Develop, release and run large-scale web applications Develop and define the backend architecture and tech stack for a product area Improve performance, reliability, scalability and security for our backend systems Be involved in transitioning our monolithic codebase to a microservice-based architecture Completely disrupt logistics by tackling bleeding-edge, technical problems We're Excited About You Because You Have… B.S., M.S., or PhD. in Computer Science or equivalent Prior experience (2+ years industry experience) working with backend tech stacks Ability to analyze and improve efficiency, scalability, and stability of various system resources Experience with service oriented architecture, writing REST API’s, unit testing, and architectural design Understanding of modern web stacks and architecture (HTTP, REST) Experience with SQL and NoSQL databases and other technologies (e.g. Postgres, Redis, Elasticsearch, RabbitMQ) About DoorDash At DoorDash, our mission to empower local economies shapes how our team members move quickly, learn, and reiterate in order to make impactful decisions that display empathy for our range of users—from Dashers to merchant partners to consumers. We are a technology and logistics company that started with door-to-door delivery, and we are looking for team members who can help us go from a company that is known for delivering food to a company that people turn to for any and all goods. DoorDash is growing rapidly and changing constantly, which gives our team members the opportunity to share their unique perspectives, solve new challenges, and own their careers. We're committed to supporting employees’ happiness, healthiness, and overall well-being by providing comprehensive benefits and perks including premium healthcare, wellness expense reimbursement, paid parental leave and more. Our Commitment to Diversity and Inclusion We’re committed to growing and empowering a more inclusive community within our company, industry, and cities. That’s why we hire and cultivate diverse teams of people from all backgrounds, experiences, and perspectives. We believe that true innovation happens when everyone has room at the table and the tools, resources, and opportunity to excel. Statement of Non-Discrimination: In keeping with our beliefs and goals, no employee or applicant will face discrimination or harassment based on: race, color, ancestry, national origin, religion, age, gender, marital/domestic partner status, sexual orientation, gender identity or expression, disability status, or veteran status. Above and beyond discrimination and harassment based on “protected categories,” we also strive to prevent other subtler forms of inappropriate behavior (i.e., stereotyping) from ever gaining a foothold in our office. Whether blatant or hidden, barriers to success have no place at DoorDash. We value a diverse workforce – people who identify as women, non-binary or gender non-conforming, LGBTQIA+, American Indian or Native Alaskan, Black or African American, Hispanic or Latinx, Native Hawaiian or Other Pacific Islander, differently-abled, caretakers and parents, and veterans are strongly encouraged to apply. Thank you to the Level Playing Field Institute for this statement of non-discrimination. Pursuant to the San Francisco Fair Chance Ordinance, Los Angeles Fair Chance Initiative for Hiring Ordinance, and any other state or local hiring regulations, we will consider for employment any qualified applicant, including those with arrest and conviction records, in a manner consistent with the applicable regulation. If you need any accommodations, please inform your recruiting contact upon initial connection. Compensation The location-specific base salary range for this position is listed below. Compensation in other geographies may vary. Actual compensation within the pay range will be decided based on factors including, but not limited to, skills, prior relevant experience, and specific work location. For roles that are available to be filled remotely, base salary is localized according to employee work location. Please discuss your intended work location with your recruiter for more information. DoorDash cares about you and your overall well-being, and that’s why we offer a comprehensive benefits package, for full-time employees, that includes healthcare benefits, a 401(k) plan including an employer match, short-term and long-term disability coverage, basic life insurance, wellbeing benefits, paid time off, paid parental leave, and several paid holidays, among others. In addition to base salary, the compensation package for this role also includes opportunities for equity grants. We use Covey as part of our hiring and / or promotional process for jobs in NYC and certain features may qualify it as an AEDT. As part of the evaluation process we provide Covey with job requirements and candidate submitted applications. We began using Covey Scout for Inbound on August 21, 2023. Please see the independent bias audit report covering our use of Covey here. California Pay Range: $140,100—$302,900 USD New York Pay Range: $140,100—$302,900 USD Washington Pay Range: $140,100—$302,900 USD",
        "url": "https://www.linkedin.com/jobs/view/3947133753",
        "summary": "DoorDash is seeking Backend Engineers to build and maintain their global infrastructure system supporting their 3-sided marketplace.  Engineers will work on both product and platform-based teams, developing and maintaining backend services, architecture, and systems.  The role involves working with microservices, REST APIs, and improving system performance, reliability, scalability, and security. ",
        "industries": [
            "Technology",
            "Logistics",
            "Food Delivery",
            "E-commerce",
            "Marketplace"
        ],
        "soft_skills": [
            "Problem Solving",
            "Collaboration",
            "Communication",
            "Technical Leadership",
            "Analytical Skills",
            "Decision Making",
            "Time Management"
        ],
        "hard_skills": [
            "Backend Engineering",
            "Microservices Architecture",
            "REST API Development",
            "Unit Testing",
            "Architectural Design",
            "HTTP",
            "SQL",
            "NoSQL Databases",
            "Postgres",
            "Redis",
            "Elasticsearch",
            "RabbitMQ"
        ],
        "tech_stack": [
            "Microservices",
            "REST API",
            "Postgres",
            "Redis",
            "Elasticsearch",
            "RabbitMQ",
            "HTTP",
            "SQL",
            "NoSQL"
        ],
        "programming_languages": [],
        "experience": 2,
        "education": {
            "min_degree": "B.S.",
            "fields": [
                "Computer Science"
            ]
        },
        "salary": {
            "max": 302900,
            "min": 140100
        },
        "benefits": [
            "Healthcare Benefits",
            "401(k) Plan",
            "Employer Match",
            "Short-Term Disability",
            "Long-Term Disability",
            "Life Insurance",
            "Wellbeing Benefits",
            "Paid Time Off",
            "Paid Parental Leave",
            "Paid Holidays",
            "Equity Grants"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3787339093,
        "company": "Lever Middleware Test Company 2",
        "title": "Software Engineer",
        "created_on": 1720635933.9380705,
        "description": "Disproportionately impact a product-driven startup that is transforming the way companies grow their teams. We’re building next-generation collaboration software that helps companies to bring more transparency, participation, and engagement to their hiring. Jump into an empowered role doing end-to-end feature development. You’ll be familiar with all parts of our stack—most notably our homegrown, open source web framework, DerbyJS. Not only will you learn how to develop on a modern, real-time framework, you’ll be building enterprise-grade software on top of it. To do so, you’ll exercise judgment in making tradeoffs between design and feasibility. Choose when to hack and when to invest. You’ll engineer your features to be scalable and resilient in a large, single-page app. Lever is an incredibly design-driven company and you’ll be an active voice in shaping our product and user experience, down to the last detail when shipping your features. Our close-knit, cross-functional team is a great chance to grow your knowledge of different domains. Level-up your knowledge of design theory and UX by working closely with our Design and Customer Success teams. Share your knowledge with them so that they’re more empowered to hook up their front-end code to to support users with more internal tools. You’ll join a team where everyone—including you—is knowledgeable about development patterns and opinionated about product development process. We pitch in to help when things are crazy. We build tools and scripts to ease the lives of our fellow developers. And we’re passionate about lots of random things, and share those passions with each other. Core Technologies JavaScript, Node.js, MongoDB, Redis, Solr, Elasticsearch, DerbyJS, ShareJS, IMAP, SMTP, Gmail and Google Calendar, Microsoft Exchange, AWS",
        "url": "https://www.linkedin.com/jobs/view/3787339093",
        "summary": "Software Engineer role at a product-driven startup building collaboration software for hiring.  You'll develop features using DerbyJS, work with design and customer success teams, contribute to a close-knit, cross-functional team, and contribute to a knowledge-sharing environment.",
        "industries": [
            "Software",
            "Technology",
            "SaaS",
            "Human Resources",
            "Recruitment"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem-solving",
            "Decision-making",
            "Teamwork",
            "Design Thinking",
            "UX",
            "Knowledge Sharing"
        ],
        "hard_skills": [
            "JavaScript",
            "Node.js",
            "MongoDB",
            "Redis",
            "Solr",
            "Elasticsearch",
            "DerbyJS",
            "ShareJS",
            "IMAP",
            "SMTP",
            "Gmail",
            "Google Calendar",
            "Microsoft Exchange",
            "AWS"
        ],
        "tech_stack": [
            "JavaScript",
            "Node.js",
            "MongoDB",
            "Redis",
            "Solr",
            "Elasticsearch",
            "DerbyJS",
            "ShareJS",
            "IMAP",
            "SMTP",
            "Gmail",
            "Google Calendar",
            "Microsoft Exchange",
            "AWS"
        ],
        "programming_languages": [
            "JavaScript"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3897978524,
        "company": "Unreal Staffing, Inc",
        "title": "Lead Cloud Data Engineer",
        "created_on": 1720635935.5301113,
        "description": "Company Overview: Welcome to the forefront of data-driven innovation! Our company is a trailblazer in leveraging the power of data to drive transformative change and solve complex problems across industries. We're committed to building cutting-edge cloud data solutions that harness the scalability, flexibility, and power of cloud platforms. Join us and lead our efforts in shaping the future of cloud data engineering. Position Overview: As the Lead Cloud Data Engineer, you'll spearhead our initiatives in designing, building, and optimizing cloud data solutions. You'll lead a team of skilled engineers, collaborating closely with cross-functional teams to deliver high-quality, scalable, and cost-effective data solutions. If you're a seasoned engineer with expertise in cloud data technologies and a proven track record of leadership in delivering successful data projects, we invite you to lead our team in this exciting opportunity. Requirements Key Responsibilities: Technical Leadership: Provide strategic guidance, mentorship, and technical leadership to a team of cloud data engineers, fostering a culture of excellence, innovation, and collaboration Cloud Data Architecture: Lead the design and implementation of cloud data architectures on platforms such as AWS, Azure, or Google Cloud Platform (GCP), leveraging cloud-native services to build scalable, reliable, and cost-effective data solutions Data Ingestion: Architect and develop robust data ingestion pipelines to collect data from diverse sources, including databases, APIs, files, and streaming sources, ensuring efficient and reliable data ingestion into cloud storage Data Storage: Design and optimize data storage solutions using cloud-native storage services such as Amazon S3, Azure Data Lake Storage, or Google Cloud Storage, balancing performance, cost, and scalability requirements Data Processing: Architect and implement data processing pipelines using cloud-native data processing frameworks such as Apache Spark, Apache Flink, or Google Dataflow, enabling real-time and batch data processing and analysis Data Integration: Architect and implement data integration solutions to seamlessly integrate data from diverse sources and systems into cloud data solutions, ensuring interoperability and data consistency Data Governance: Establish and enforce data governance policies and procedures to ensure data quality, security, and compliance with regulatory requirements in cloud data environments Performance Optimization: Lead efforts to optimize data pipelines and processing workflows for performance, scalability, and cost-effectiveness, leveraging cloud-native features and techniques Monitoring and Alerting: Implement robust monitoring and alerting solutions to track data pipeline performance and health, proactively identifying and resolving issues to minimize downtime and data loss Documentation and Best Practices: Define and promote best practices for cloud data engineering, ensuring clear and comprehensive documentation to facilitate understanding and collaboration among team members Collaboration: Collaborate closely with cross-functional teams, including data scientists, business analysts, and infrastructure teams, to understand requirements and deliver cloud data solutions that meet business needs Mentorship and Development: Mentor and coach junior engineers, providing guidance, support, and opportunities for skill development and career growth, and foster a culture of continuous learning and improvement within the team Qualifications: Bachelor's degree or higher in Computer Science, Engineering, Mathematics, or related field 8+ years of experience in data engineering, with a focus on cloud data technologies Proven leadership experience, with a track record of successfully leading cloud data engineering teams and delivering complex data projects Expertise in cloud platforms such as AWS, Azure, or GCP, and proficiency in cloud-native services and tools for data storage, processing, and analytics Strong programming skills in languages such as Python, Java, or Scala, with experience in data processing frameworks like Apache Spark or Apache Flink Experience with cloud-native data processing services such as AWS Glue, Azure Data Factory, or Google Dataflow Strong understanding of data integration concepts and techniques, with experience integrating data from diverse sources and systems in cloud environments Strong problem-solving skills and analytical thinking, with the ability to design and troubleshoot complex cloud data solutions Excellent communication and collaboration skills, with the ability to effectively communicate technical concepts to non-technical stakeholders Benefits Competitive salary: The industry standard salary for Lead Cloud Data Engineers typically ranges from $200,000 to $300,000 per year, depending on experience and qualifications Comprehensive benefits package, including health insurance, retirement plans, and wellness programs Flexible work arrangements, including remote work options and flexible hours Generous vacation and paid time off Professional development opportunities, including access to training programs, conferences, and workshops State-of-the-art technology environment with access to cutting-edge tools and resources Vibrant and inclusive company culture with opportunities for growth and advancement Exciting projects with real-world impact at the forefront of data-driven innovation Join Us: Ready to lead the charge in cloud data engineering? Apply now to join our team and be part of the data revolution!",
        "url": "https://www.linkedin.com/jobs/view/3897978524",
        "summary": "We are looking for a Lead Cloud Data Engineer to lead a team of skilled engineers in designing, building, and optimizing cloud data solutions on AWS, Azure, or GCP. You will be responsible for architecting and developing data ingestion pipelines, data storage solutions, data processing pipelines, data integration solutions, and data governance policies. You will also need to optimize data pipelines for performance, scalability, and cost-effectiveness, implement monitoring and alerting solutions, and mentor and coach junior engineers. The ideal candidate will have 8+ years of experience in data engineering, with a focus on cloud data technologies, and proven leadership experience.",
        "industries": [
            "Cloud Computing",
            "Data Engineering",
            "Technology",
            "Software Development"
        ],
        "soft_skills": [
            "Leadership",
            "Collaboration",
            "Communication",
            "Mentorship",
            "Problem Solving",
            "Analytical Thinking"
        ],
        "hard_skills": [
            "AWS",
            "Azure",
            "GCP",
            "Python",
            "Java",
            "Scala",
            "Apache Spark",
            "Apache Flink",
            "AWS Glue",
            "Azure Data Factory",
            "Google Dataflow",
            "Data Integration"
        ],
        "tech_stack": [
            "AWS",
            "Azure",
            "GCP",
            "Apache Spark",
            "Apache Flink",
            "AWS Glue",
            "Azure Data Factory",
            "Google Dataflow"
        ],
        "programming_languages": [
            "Python",
            "Java",
            "Scala"
        ],
        "experience": 8,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Computer Science",
                "Engineering",
                "Mathematics"
            ]
        },
        "salary": {
            "max": 300000,
            "min": 200000
        },
        "benefits": [
            "Competitive salary",
            "Comprehensive benefits package",
            "Flexible work arrangements",
            "Generous vacation and paid time off",
            "Professional development opportunities",
            "State-of-the-art technology environment",
            "Vibrant and inclusive company culture"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Irvine, CA",
        "job_id": 3839728685,
        "company": "Kia America",
        "title": "Field Data Analysis Engineer",
        "created_on": 1720635937.1171644,
        "description": "At Kia, we’re creating award-winning products and redefining what value means in the automotive industry. It takes a special group of individuals to do what we do, and we do it together. Our culture is fast-paced, collaborative, and innovative. Our people thrive on thinking differently and challenging the status quo. We are creating something special here, a culture of learning and opportunity, where you can help Kia achieve big things and most importantly, feel passionate and connected to your work every day. Kia provides team members with competitive benefits including premium paid medical, dental and vision coverage for you and your dependents, 401(k) plan matching of 100% up to 6% of the salary deferral, and time off starting at 14 days per year. Kia also offers company lease and purchase programs, company-wide holiday shutdown, paid volunteer hours, and premium lifestyle amenities at our corporate campus in Irvine, California. Status Exempt Summary The purpose of this position is to support the Field Data Analysis Manager and Data Evaluation Team (DET) by identifying at an early stage in a model’s life cycle, any emerging Potential Safety Issues (PSIs) through data searches and sophisticated analysis of Kia America (KUS) data from all primary data sources, including warranty, Techline, Consumer Affairs, customer pay, NHTSA Vehicle Owner Questionnaires (VOQs), as well as other data sources, as applicable. This position will also support safety and noncompliance recall decision-making and requests by NHTSA for Defect Petitions, Preliminary Evaluations, investigations, and pre-investigative requests for information. This position will also support KUS Legal Department for defense of class action suits and in-house and outside counsel requests. Major Responsibilities 1st Priority - 40% Review data sources as assigned for new PSIs. Work from analytic dashboards and generate reports showing emerging field issues using primary field data sources, as well as other data sources as appropriate. Investigate PSIs across data sources to determine current complaint counts, rates, and any trends from the data, from emerging field issues stage through safety evaluation list stage and to field action decision stage, if applicable. Identify PSIs and potential noncompliance issues and inform Field Data Analysis Manager of them at the earliest point in time. 2nd Priority - 25% Coordinate with Forensic Engineering and Investigation team to hand over and collaborate on analyses and updates. Provide analysis explanations to management, coordinators, other departments, and Kia HQ staff. 3rd Priority - 20% Provide data analysis support for NHTSA inquiries, investigations, monthly NHTSA meeting preparation, and KUS Legal in-house and outside counsel. 4th Priority - 15% Develop sophisticated processes for evaluating data and identifying PSIs by utilizing new technologies and analytic tools. Assist in the continuous improvement and development of Safety Data Analytics Infrastructure (SDAI). Share new data analysis techniques with team. Education/Certification BS degree in Engineering or Automotive Technology or equivalent work experience required. Overall Experience 3-7 years of experience in technical positions such as Safety Analysis, Product Quality, or in a product engineering environment. Directly Related Experience Automotive, aerospace, or similar technical work experience required Working knowledge of statistical analysis concepts and software. Other Requirements Schedule(s) may vary due to needs of the business including but not limited to working outside of normal business hours, travel, weekends and/or holidays. Perform other duties as assigned. Skills Basic knowledge of SAS programing language and Structured Query Language (SQL). Excellent analytical skills and attention to detail. Excellent teamwork skills. Excellent written and oral communication skills. Intermediate knowledge of automotive systems and components. Knowledge of PC software such as Microsoft Excel and PowerPoint. Knowledge of SAS Visual Analytics. Knowledge of statistics. Competencies CHALLENGE - Solving Complex Problems COLLABORATION - Building and Supporting Teams CUSTOMER - Serving Customers GLOBALITY - Showing Community and Social Responsibility PEOPLE - Interacting with People at Different Levels Pay Range $69,547 - $93,113 Pay will be based on several variables that are unique to each candidate, including but not limited to, job-related skills, experience, relevant education or training, etc. Equal Employment Opportunities KUS provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, ancestry, national origin, sex, including pregnancy and childbirth and related medical conditions, gender, gender identity, gender expression, age, legally protected physical disability or mental disability, legally protected medical condition, marital status, sexual orientation, family care or medical leave status, protected veteran or military status, genetic information or any other characteristic protected by applicable law. KUS complies with applicable law governing non-discrimination in employment in every location in which KUS has offices. The KUS EEO policy applies to all areas of employment, including recruitment, hiring, training, promotion, compensation, benefits, discipline, termination and all other privileges, terms and conditions of employment. Disclaimer: The above information on this job description has been designed to indicate the general nature and level of work performed by employees within this classification and for this position. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities, and qualifications required of employees assigned to this job.",
        "url": "https://www.linkedin.com/jobs/view/3839728685",
        "summary": "This position supports the Field Data Analysis Manager and Data Evaluation Team (DET) by identifying potential safety issues (PSIs) through data analysis of various sources like warranty, Techline, Consumer Affairs, customer pay, NHTSA VOQs, etc. It also supports safety and noncompliance recall decision-making and requests by NHTSA and KUS Legal Department for defense of class action suits.",
        "industries": [
            "Automotive",
            "Engineering",
            "Data Analysis",
            "Safety"
        ],
        "soft_skills": [
            "Analytical skills",
            "Attention to detail",
            "Teamwork",
            "Communication (written and oral)",
            "Problem-solving"
        ],
        "hard_skills": [
            "SAS",
            "SQL",
            "Statistical analysis",
            "Automotive systems",
            "Microsoft Excel",
            "PowerPoint",
            "SAS Visual Analytics",
            "Statistics"
        ],
        "tech_stack": [
            "SAS",
            "SQL",
            "SAS Visual Analytics"
        ],
        "programming_languages": [
            "SAS",
            "SQL"
        ],
        "experience": 3,
        "education": {
            "min_degree": "BS",
            "fields": [
                "Engineering",
                "Automotive Technology"
            ]
        },
        "salary": {
            "max": 93113,
            "min": 69547
        },
        "benefits": [
            "Premium medical, dental, and vision coverage",
            "401(k) plan matching",
            "Time off",
            "Company lease and purchase programs",
            "Company-wide holiday shutdown",
            "Paid volunteer hours",
            "Lifestyle amenities"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Jose, CA",
        "job_id": 3603926912,
        "company": "TikTok",
        "title": "Data Engineer, TikTok Multimedia",
        "created_on": 1720635938.879481,
        "description": "Responsibilities TikTok is the leading destination for short-form mobile video. Our mission is to inspire creativity and bring joy. TikTok has global offices, including Los Angeles, New York, London, Paris, Berlin, Dubai, Mumbai, Singapore, Jakarta, Seoul, and Tokyo. Why Join Us At TikTok, our people are humble, intelligent, compassionate and creative. We create to inspire - for you, for us, and for more than 1 billion users on our platform. We lead with curiosity and aim for the highest, never shying away from taking calculated risks and embracing ambiguity as it comes. Here, the opportunities are limitless for those who dare to pursue bold ideas that exist just beyond the boundary of possibility. Join us and make impact happen with a career at TikTok. About the Team The Multimedia Data Platform team is responsible for optimizing app experience related to performance for TikTok users by providing data support. Working in collaboration with various teams throughout TikTok, the data platform team focuses on the creation and consumption of video content to provide comprehensive optimization solutions. This includes end-to-end optimization solutions such as client, video shooting, uploading, video playback, video delivery and player, etc. Responsibilities: Our Multimedia data platform team work closely with our product managers and data analysts by building state of the art streaming and batch data processing solution. The entire data pipeline is not only supporting the core business at TikTok -- short video, but also horizontal business across TikTok. In this role, you will see a direct link between your work, and the company's business success. You will have opportunities to deal with Petabyte-level data warehouse. Some of the world's most challenging technical and business problems are waiting for you to solve. - Apply broad knowledge of technology options, technology platforms, design techniques and approaches across the Data Engineering ecosystem to build systems that meet quality needs. - Build systems and datasets using software engineering best practices, data management fundamentals, data storage principles, recent advances in distributed systems, and operational and engineering excellence best practices. - Analyze systems, define transformation requirements, design suitable data models and document the design/specifications. - Demonstrate passion for quality and productivity by using efficient development techniques, standards and guidelines. - Drive the design, to build, execute, and maintain automated tests and/or manage deep data profiling runs to ensure data products and pipelines meet expectations - Partner with analysts, engineers, subject matter experts, and product managers to apply TikTok Multimedia analytical and quality methods to satisfy client needs. - Participate in the growth of the Data Quality Excellence practice by sharing knowledge and lessons learned, continually improving best practices, and contributing to methods that will systematically advance workforce capabilities - Effectively communicate through technical documentation, commented code, and interactions with stakeholders and adjacent teams - Contribute to building a vibrant workplace, where teams can thrive, and model the organization’s positive, supportive culture of respect and excellence Qualifications - BS/BA in Technical Field, Computer Science or Mathematics. - 3+ years experience in the data warehouse space. - 3+ years experience in custom ETL design, implementation and maintenance. - 2+ years experience working with big data technologies (Hadoop, Hive, Spark, Clickhouse, etc.) . -  2+ years experience with schema design and dimensional data modeling. -  3+ years experience in writing SQL statements. -  Proficient in one of Programming languages (e.g., Python, Go, C++) -  Communication skills, including the ability to identify and communicate data driven insights. - Ability in managing and communicating data warehouse plans to internal clients. TikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too. TikTok is committed to providing reasonable accommodations in our recruitment processes for candidates with disabilities, pregnancy, sincerely held religious beliefs or other reasons protected by applicable laws. If you need assistance or a reasonable accommodation, please reach out to us at gprd.accommodations@tiktok.com. Job Information: 【For Pay Transparency】Compensation Description (annually) The base salary range for this position in the selected city is $145000 - $250000 annually. ​ Compensation may vary outside of this range depending on a number of factors, including a candidate’s qualifications, skills, competencies and experience, and location. Base pay is one part of the Total Package that is provided to compensate and recognize employees for their work, and this role may be eligible for additional discretionary bonuses/incentives, and restricted stock units. ​ Our company benefits are designed to convey company culture and values, to create an efficient and inspiring work environment, and to support our employees to give their best in both work and life. We offer the following benefits to eligible employees: ​ We cover 100% premium coverage for employee medical insurance, approximately 75% premium coverage for dependents and offer a Health Savings Account(HSA) with a company match. As well as Dental, Vision, Short/Long term Disability, Basic Life, Voluntary Life and AD&D insurance plans. In addition to Flexible Spending Account(FSA) Options like Health Care, Limited Purpose and Dependent Care. ​ Our time off and leave plans are: 10 paid holidays per year plus 17 days of Paid Personal Time Off (PPTO) (prorated upon hire and increased by tenure) and 10 paid sick days per year as well as 12 weeks of paid Parental leave and 8 weeks of paid Supplemental Disability. ​ We also provide generous benefits like mental and emotional health benefits through our EAP and Lyra. A 401K company match, gym and cellphone service reimbursements. The Company reserves the right to modify or change these benefits programs at any time, with or without notice. ​",
        "url": "https://www.linkedin.com/jobs/view/3603926912",
        "summary": "TikTok is seeking a Data Engineer to join their Multimedia Data Platform team. This individual will be responsible for building state-of-the-art streaming and batch data processing solutions, working with Petabyte-level data warehouses, and collaborating with various teams to optimize app experience for users. The ideal candidate will have experience in data warehouse design and implementation, big data technologies (Hadoop, Hive, Spark, Clickhouse), schema design, SQL, and programming languages (Python, Go, C++).",
        "industries": [
            "Technology",
            "Media & Entertainment",
            "Social Media"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Data Analysis",
            "Quality Focus",
            "Passion for Technology",
            "Teamwork"
        ],
        "hard_skills": [
            "Data Warehouse",
            "ETL",
            "Hadoop",
            "Hive",
            "Spark",
            "Clickhouse",
            "Schema Design",
            "Dimensional Data Modeling",
            "SQL",
            "Python",
            "Go",
            "C++"
        ],
        "tech_stack": [
            "Hadoop",
            "Hive",
            "Spark",
            "Clickhouse",
            "SQL",
            "Python",
            "Go",
            "C++"
        ],
        "programming_languages": [
            "Python",
            "Go",
            "C++"
        ],
        "experience": 3,
        "education": {
            "min_degree": "BS/BA",
            "fields": [
                "Technical Field",
                "Computer Science",
                "Mathematics"
            ]
        },
        "salary": {
            "max": 250000,
            "min": 145000
        },
        "benefits": [
            "Medical Insurance",
            "Dental Insurance",
            "Vision Insurance",
            "Short/Long Term Disability",
            "Basic Life",
            "Voluntary Life",
            "AD&D",
            "Flexible Spending Account",
            "Paid Time Off",
            "Sick Leave",
            "Parental Leave",
            "Supplemental Disability",
            "Employee Assistance Program",
            "401K Match",
            "Gym Reimbursement",
            "Cellphone Service Reimbursement"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Costa Mesa, CA",
        "job_id": 3953577259,
        "company": "Mantek Solutions Inc.",
        "title": "Sr. Cloud Data Platform Engineer",
        "created_on": 1720635942.0325184,
        "description": "Sr Cloud Data Platform Engineer - Hybrid Remote, Costa Mesa, CA If you are passionate about making an impact utilizing cutting-edge data technology and eager to learn and grow with talented data & analytics teams, we believe you would be a great fit and help in innovation and solving challenges. Responsibilities: Design, deploy, and maintain scalable and reliable cloud infrastructure on AWS and GCP. Build scalability, fault-tolerance, security, and performance into our data platforms to meet our growing data & analytics needs Configure and optimize networking components, to ensure high performance, availability, and security. Work with our data engineering teams, data science teams, and analytical business users to review and ensure best practices are followed and high-quality code gets implemented Partner with our Information Security, Application Security, Data Center services, and Cloud Infrastructure Services teams to ensure data is secure both at rest and in-flight Ensure data platforms remain current to take advantage of the latest features and support Communicate the status of assigned work to management and follow agile practices, standard procedures & policies Seek guidance when direction is needed and speak up about technology risks identified You have worked previously with an Agile team or understand these concepts. You expect to participate in daily standup meetings, you’ll complete your projects or stories during our sprints, and you’ll be ready to meet frequent deployment deadlines. Requirements: Extensive hands-on expertise in troubleshooting logs and optimizing system performance. Deep understanding of Cloud and data security in AWS or Google Cloud Good experience with networking, Linux and Unix OS, and shell scripting. Demonstrated aptitude for identifying the underlying issues in Cloud services such as Glue, Lambda, Big Query, and Cloud Functions Good experience with Cloud platforms AWS and/or Google Cloud in supporting Data pipelines, and integrations Experience with Cloud Automation using Cloud Formation or Terraform Good understanding of network infrastructures and protocols such as TCP/IP, TLS Familiarity with data integration from different sources into Big Data systems is preferable. Familiarity with SQL and programming languages such as Python, Spark, YAML, and Bash. Cloud Certification in Dev Ops in AWS or Google Cloud a plus If qualified and interested in this opportunity, please apply with an updated resume and annual salary requirements. No Corp to Corp / No Sponsorship / W2 Only / No third party candidates considered for this position",
        "url": "https://www.linkedin.com/jobs/view/3953577259",
        "summary": "Sr Cloud Data Platform Engineer responsible for designing, deploying, and maintaining cloud infrastructure on AWS and GCP, building scalable data platforms, optimizing network components, ensuring data security, and working with data engineering and science teams.",
        "industries": [
            "Data & Analytics",
            "Cloud Computing",
            "Software Development",
            "Information Technology"
        ],
        "soft_skills": [
            "Problem-solving",
            "Communication",
            "Collaboration",
            "Teamwork",
            "Agile",
            "Troubleshooting",
            "Performance Optimization",
            "Security Awareness",
            "Critical Thinking"
        ],
        "hard_skills": [
            "AWS",
            "GCP",
            "Cloud Security",
            "Networking",
            "Linux",
            "Unix",
            "Shell Scripting",
            "Glue",
            "Lambda",
            "Big Query",
            "Cloud Functions",
            "Data Pipelines",
            "Cloud Formation",
            "Terraform",
            "TCP/IP",
            "TLS",
            "SQL",
            "Python",
            "Spark",
            "YAML",
            "Bash"
        ],
        "tech_stack": [
            "AWS",
            "GCP",
            "Glue",
            "Lambda",
            "Big Query",
            "Cloud Functions",
            "Cloud Formation",
            "Terraform",
            "Python",
            "Spark",
            "YAML",
            "Bash"
        ],
        "programming_languages": [
            "Python",
            "Spark",
            "YAML",
            "Bash"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Engineering",
                "Information Technology",
                "Data Science"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3969152863,
        "company": "Magic",
        "title": "Software Engineer",
        "created_on": 1720635943.7025614,
        "description": "Magic's mission is to build safe AGI that accelerates humanity’s progress on the world’s most important problems. We believe the most promising path to safe AGI lies in automating research and code generation to improve models and solve alignment more reliably than humans can alone. Our approach combines frontier-scale pre-training, domain-specific RL, ultra-long context, and test-time compute to achieve this goal. About the role: As a Software Engineer, you will design, implement, and maintain scalable solutions and applications to accelerate our research and deploy our latest models. What you might work on: Designing, implementing and maintaining scalable software and tools for the development and deployment of our AI models Ideating and prototyping novel product features enabled by our research Collaborating with researchers to integrate feedback and accelerate the team Optimizing applications for speed, scale and reliability What we’re looking for: Proven track record of shipping high-quality code in challenging projects Exceptional software engineering skills across multiple languages (we use Python, Typescript, Go, Rust, and C++) Experience with scalable software design and implementation Ability to navigate ambiguity and adapt to changing priorities in a fast-paced environment Magic strives to be the place where high-potential individuals can do their best work. We value quick learning and grit just as much as skill and experience. Our culture: Integrity. Words and actions should be aligned Hands-on. At Magic, everyone is building Teamwork. We move as one team, not N individuals Focus. Safely deploy AGI. Everything else is noise Quality. Magic should feel like magic Compensation, benefits and perks (US): Annual salary range: $100K - $1M Equity is a significant part of total compensation, in addition to salary 401(k) plan with 6% salary matching Generous health, dental and vision insurance for you and your dependants Unlimited paid time off Option to work in-person in SF or remotely Visa sponsorship and relocation stipend to bring you to SF A small, fast-paced, highly focused team",
        "url": "https://www.linkedin.com/jobs/view/3969152863",
        "summary": "Magic is seeking a Software Engineer to design, implement, and maintain scalable solutions and applications for AI research and deployment. The role involves working on scalable software and tools for AI model development, prototyping novel product features, collaborating with researchers, optimizing applications, and contributing to a fast-paced environment. The ideal candidate has a proven track record of shipping high-quality code, exceptional software engineering skills in Python, Typescript, Go, Rust, and C++, experience with scalable software design, and the ability to adapt to changing priorities.",
        "industries": [
            "Artificial Intelligence",
            "Machine Learning",
            "Software Development",
            "Research"
        ],
        "soft_skills": [
            "Problem Solving",
            "Collaboration",
            "Communication",
            "Adaptability",
            "Teamwork",
            "Quick Learning",
            "Grit"
        ],
        "hard_skills": [
            "Python",
            "Typescript",
            "Go",
            "Rust",
            "C++",
            "Scalable Software Design",
            "Software Development",
            "Deployment"
        ],
        "tech_stack": [
            "Python",
            "Typescript",
            "Go",
            "Rust",
            "C++",
            "AI models",
            "Product Features"
        ],
        "programming_languages": [
            "Python",
            "Typescript",
            "Go",
            "Rust",
            "C++"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 1000000,
            "min": 100000
        },
        "benefits": [
            "Equity",
            "401(k) plan with 6% salary matching",
            "Health, dental and vision insurance",
            "Unlimited paid time off",
            "Remote work option",
            "Visa sponsorship",
            "Relocation stipend"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "California, United States",
        "job_id": 3888432634,
        "company": "Enexus Global Inc.",
        "title": "[14+]Senior Data Engineer - Remote",
        "created_on": 1720635945.5439413,
        "description": "Location - Remote Contract Type - W2/C2C/1099 Minimum Experience - 14+ Years Responsibilities Develop and enhance data-processing, orchestration, monitoring, and more by leveraging popular open-source software, AWS, and GitLab automation. Collaborate with product and technology teams to design and validate the capabilities of the data platform Identify, design, and implement process improvements: automating manual processes, optimizing for usability, re-designing for greater scalability Provide technical support and usage guidance to the users of our platform's services. Drive the creation and refinement of metrics, monitoring, and alerting mechanisms to give us the visibility we need into our production services. Qualifications Experience building and optimizing data pipelines in a distributed environment Experience supporting and working with cross-functional teams Proficiency working in Linux environment 8+ years of advanced working knowledge of SQL, Python, and PySpark 5+ years of experience with using a broad range of AWS technologies Experience using tools such as: Git/Bitbucket, Jenkins/CodeBuild, CodePipeline Experience with platform monitoring and alerts tools",
        "url": "https://www.linkedin.com/jobs/view/3888432634",
        "summary": "This role requires a highly experienced data engineer with strong expertise in building and optimizing data pipelines in a distributed environment, specifically leveraging AWS technologies and open-source software. The position involves designing, developing, and maintaining data processing, orchestration, and monitoring systems. Collaboration with product and technology teams is essential, along with providing technical support and guidance to users. The ideal candidate will have a proven track record in automation, process improvement, and utilizing tools like GitLab, Jenkins, CodeBuild, CodePipeline, and platform monitoring tools.",
        "industries": [
            "Data Engineering",
            "Software Development",
            "Cloud Computing",
            "Technology"
        ],
        "soft_skills": [
            "Collaboration",
            "Problem Solving",
            "Communication",
            "Technical Support",
            "Analytical",
            "Process Improvement",
            "Automation"
        ],
        "hard_skills": [
            "SQL",
            "Python",
            "PySpark",
            "AWS",
            "Git",
            "Bitbucket",
            "Jenkins",
            "CodeBuild",
            "CodePipeline",
            "Linux",
            "Data Pipelines",
            "Distributed Systems",
            "Platform Monitoring",
            "Alerting"
        ],
        "tech_stack": [
            "AWS",
            "GitLab",
            "Jenkins",
            "CodeBuild",
            "CodePipeline",
            "PySpark",
            "SQL",
            "Python",
            "Linux"
        ],
        "programming_languages": [
            "SQL",
            "Python",
            "PySpark"
        ],
        "experience": 14,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3705238108,
        "company": "Notion",
        "title": "Software Engineer, Developer Infrastructure",
        "created_on": 1720635947.103523,
        "description": "About Us We're on a mission to make it possible for every person, team, and company to be able to tailor their software to solve any problem and take on any challenge. Computers may be our most powerful tools, but most of us can't build or modify the software we use on them every day. At Notion, we want to change this with focus, design, and craft. We've been working on this together since 2016, and have customers like Pixar, Mitsubishi, Figma, Plaid, Match Group, and thousands more on this journey with us. Today, we're growing fast and excited for new teammates to join us who are the best at what they do. We're passionate about building a company as diverse and creative as the millions of people Notion reaches worldwide. About The Role Millions of people use Notion — and this number is increasing every day. That means millions of people trust us to deliver a fast, reliable, and secure experience, and we value this more than anything. We want to keep earning trust, while also continuing to amaze our users with the tools they can build in Notion. The Developer Productivity team is responsible for architecting, implementing, and scaling our tools and frameworks that Notion developers use to build these products for our users. We are looking for engineers who are passionate about solving engineering productivity challenges and have deep empathy for other developers. What You'll Achieve Take ownership of the overall development environment and toolchain, and drive novel projects across the company to deliver impact. Minimize time spent between code commit and production deploy, via better tooling, testing, framework, and automation, without sacrificing quality or reliability. Provide top-notch observability tools for Notion engineers to scale Notion reliably. Skills You'll Need To Bring Developer productivity expert: You bring multiple years of expertise in developer productivity, developer experience or DevOps engineering. You have deep understanding of modern development practices and tools, and experience designing, building, and evaluating those tools. Developer experience mindset: You think critically about how the tools you build will be used by the rest of the engineering team. Your work emphasizes simplicity, supportability, and ease-of-use, and you communicate clearly with other teams to get both structured and unstructured feedback on where we can improve. You have experience supporting developer experience for engineers working on a web client, not just server. Startup mentality: You are comfortable navigating the fast moving, unstructured nature of a fast-growing startup. You are self-motivated to add value and bias towards action. You understand the importance of setting the engineering quality bar for the long term success of our business without shying away from difficult prioritization decisions. Pragmatic and business-oriented: You care about business impact and prioritize projects accordingly. You don't just go after cool stuff—you understand the balance between craft, speed, and the bottom line. To you, technologies and programming languages are about tradeoffs. You may be opinionated, but you're not ideological and can learn new technologies as you go. Nice To Haves You have a deep understanding of TypeScript and how to keep it running smoothly in a large codebase. You’ve untangled complex codebases with interwoven dependencies to improve developer productivity and tooling speed without compromising reliability. You have experience in scaling developer experience a hyper-growth startup You can independently lead a small team and multi-month project end to end to accomplish team goals. We hire talented and passionate people from a variety of backgrounds because we want our global employee base to represent the wide diversity of our customers. If you’re excited about a role but your past experience doesn’t align perfectly with every bullet point listed in the job description, we still encourage you to apply. If you’re a builder at heart, share our company values, and enthusiastic about making software toolmaking ubiquitous, we want to hear from you. Notion is proud to be an equal opportunity employer. We do not discriminate in hiring or any employment decision based on race, color, religion, national origin, age, sex (including pregnancy, childbirth, or related medical conditions), marital status, ancestry, physical or mental disability, genetic information, veteran status, gender identity or expression, sexual orientation, or other applicable legally protected characteristic. Notion considers qualified applicants with criminal histories, consistent with applicable federal, state and local law. Notion is also committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. If you need assistance or an accommodation due to a disability, please let your recruiter know. Notion is committed to providing highly competitive cash compensation, equity, and benefits. The compensation offered for this role will be based on multiple factors such as location, the role’s scope and complexity, and the candidate’s experience and expertise, and may vary from the range provided below. For roles based in San Francisco, the estimated base salary range for this role is $145,000 - $250,000 per year.",
        "url": "https://www.linkedin.com/jobs/view/3705238108",
        "summary": "Notion is seeking a Developer Productivity Engineer to architect, implement, and scale their tools and frameworks used by Notion developers. The ideal candidate has expertise in developer productivity, developer experience, or DevOps engineering, a deep understanding of modern development practices and tools, and experience designing, building, and evaluating those tools. They will focus on minimizing time between code commit and production deploy, improving observability tools for Notion engineers, and building a scalable development environment. Bonus points for experience with TypeScript, untangling complex codebases, and scaling developer experience in hyper-growth startups.",
        "industries": [
            "Software Development",
            "Technology",
            "SaaS",
            "Productivity"
        ],
        "soft_skills": [
            "Communication",
            "Empathy",
            "Problem Solving",
            "Critical Thinking",
            "Self-Motivation",
            "Teamwork",
            "Leadership",
            "Prioritization",
            "Business Acumen",
            "Adaptability"
        ],
        "hard_skills": [
            "Developer Productivity",
            "Developer Experience",
            "DevOps Engineering",
            "Modern Development Practices",
            "Tool Design and Development",
            "TypeScript",
            "Codebase Optimization",
            "Scaling Developer Experience",
            "Project Management"
        ],
        "tech_stack": [
            "TypeScript",
            "Observability Tools",
            "Automation Frameworks",
            "Testing Tools",
            "Deployment Tools"
        ],
        "programming_languages": [
            "TypeScript"
        ],
        "experience": 3,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Engineering",
                "Related Fields"
            ]
        },
        "salary": {
            "max": 250000,
            "min": 145000
        },
        "benefits": [
            "Competitive Cash Compensation",
            "Equity",
            "Benefits"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Mountain View, CA",
        "job_id": 3938225922,
        "company": "Aerospike",
        "title": "Senior Engineer – Graph Database",
        "created_on": 1720635948.6594284,
        "description": "About Aerospike At Aerospike, we dream big. Our focus is helping companies tackle seemingly insurmountable problems and doing what’s never been done before. That is why we developed the world's leading real-time data platform that powers mission-critical applications at the world's most innovative, category disrupting companies. Our customers have deployed extreme scale real-time applications to fight fraud, dramatically increase shopping cart size, enable global digital payments, and deliver hyper-personalized user experiences to tens of millions of customers. Customers like Airtel, Experian, Nielsen, PayPal, Snap, Verizon Media, Wayfair, and many others rely on Aerospike as the data foundation for the future to help them act in the microsecond moments that matter. Aerospike is headquartered in Mountain View, California and has a global presence with offices in London, Bangalore and Tel Aviv. Job Description Position Overview: A skilled Java developer and database engineer to join our graph database team. The ideal candidate will have a strong background in graph databases or database internals, with experience contributing features to or implementing TinkerPop for a database vendor. Experience building systems on top of distributed key-value stores is desirable. Key Responsibilities: Design, develop, and maintain our TinkerPop graph database implementation. Optimize and enhance database query performance and capability. Design new features from the end user perspective and translate that into code implementation. Collaborate with other teams to build performance tests, cloud deployment and enhance integration. Conduct rigorous testing and debugging to ensure high reliability and performance standards. Required Qualifications: Proven experience in graph database development or database internals. Previous work with distributed key-value stores. Strong proficiency in Java and familiarity with the Apache TinkerPop framework. Ability to design and implement efficient data structures and algorithms. Excellent problem-solving skills and attention to detail. Strong communication skills and ability to work effectively in a team environment. Desirable Skills: Python scripting experience, Spark experience Previous work on an OLAP system Work on query parsing and rewriting Knowledge of additional graph processing frameworks and systems. Contributions to open-source projects or published research in relevant fields. Experience with distributed systems and understanding of underlying principles of scalability and fault tolerance. What We Offer: A collaborative and innovative remote work environment. Competitive salary and comprehensive benefits package. Opportunities for professional growth and advancement. Build cutting-edge technology and solve technical problems from first principals. Aerospike is an Equal Opportunity Employer. We are committed to providing an environment free from discrimination on the basis of race, religion, color, sex, gender identity, sexual orientation, age, non-disqualifying physical or mental disability, national origin, veteran status, or any other basis covered by appropriate law.Salary Range for California Based Applicants: $170,000 - $190,000 (actual compensation will be determined based on experience, location, and other factors permitted by law).",
        "url": "https://www.linkedin.com/jobs/view/3938225922",
        "summary": "Aerospike is seeking a skilled Java developer and database engineer to join their graph database team. The ideal candidate will have experience with graph databases, TinkerPop, and distributed key-value stores. Responsibilities include designing, developing, and maintaining the TinkerPop graph database implementation, optimizing query performance, collaborating with other teams, and ensuring high reliability and performance standards. ",
        "industries": [
            "Technology",
            "Software",
            "Data Management",
            "Database"
        ],
        "soft_skills": [
            "Communication",
            "Problem-solving",
            "Collaboration",
            "Attention to Detail"
        ],
        "hard_skills": [
            "Java",
            "Database Engineering",
            "Graph Databases",
            "Apache TinkerPop",
            "Distributed Key-Value Stores",
            "Data Structures and Algorithms",
            "Performance Optimization",
            "Query Optimization",
            "Testing and Debugging"
        ],
        "tech_stack": [
            "Java",
            "Apache TinkerPop",
            "Spark",
            "OLAP",
            "Distributed Systems"
        ],
        "programming_languages": [
            "Java",
            "Python"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 190000,
            "min": 170000
        },
        "benefits": [
            "Remote Work",
            "Competitive Salary",
            "Comprehensive Benefits",
            "Professional Growth Opportunities"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Jose, CA",
        "job_id": 3967186819,
        "company": "Kognitos",
        "title": "Software Engineer Intern",
        "created_on": 1720635950.412565,
        "description": "Kognitos is a cutting-edge automation platform that combines the power of Generative AI and Natural Language Processing (NLP) to provide a conversational and intuitive experience for business users. As a fully serverless, SaaS automation platform, Kognitos enables users to build, manage, and execute automations in natural language, with the ability to process and understand domain-specific language. Kognitos provides a detailed auditable view of its runs, allowing users to gain insights into why an action happened or what might have gone wrong in case of an exceptional situation. Additionally, Kognitos's conversational exception handling, powered by Generative AI, allows for quick and easy resolution of unexpected system errors or business exceptions. We are seeking passionate and talen ted Software Engineer Int erns to join our team. As an intern at Kogntios, you will have the opportunity to work on exciting projects, collaborate with experienced engineers, and gain hands-on experience in a fast-paced and innovative environment. This is a fantastic opportunity to enhance your skills, contribute to impactful projects, and learn from industry experts. Responsibilities Collaborate with the engineering team to design, develop, and maintain software application Write clean, efficient, and well-documented code Participate in code reviews and provide constructive feedback Assist in troubleshooting and debugging software issue Contribute to the development of new features and enhancement Work on projects involving AI, machine learning, and data analysis Stay up-to-date with the latest industry trends and technologies Requirements Currently pursuing a degree in Computer Science, Software Engineering, or a related field Strong programming skills in languages such as Python, Java, C++, or similar Familiarity with software development methodologies and tools Basic understanding of algorithms, data structures, and computer science principles Excellent problem-solving and analytical skills Strong communication and teamwork abilities Eagerness to learn and adapt in a fast-paced environment Preferred Qualifications Experience with AI and machine learning frameworks (e.g., TensorFlow, PyTorch) Knowledge of web development technologies (HTML, CSS, JavaScript) Familiarity with cloud platforms (e.g., AWS, Azure, Google Cloud) Previous internship or project experience in software development",
        "url": "https://www.linkedin.com/jobs/view/3967186819",
        "summary": "Kognitos is seeking Software Engineer Interns to work on their automation platform that utilizes Generative AI and NLP. Interns will collaborate with the engineering team to design, develop, and maintain software applications, write clean and well-documented code, and participate in code reviews. Experience with AI and machine learning frameworks, web development technologies, and cloud platforms is preferred.",
        "industries": [
            "Software",
            "Artificial Intelligence",
            "Automation",
            "Machine Learning",
            "Cloud Computing",
            "SaaS"
        ],
        "soft_skills": [
            "Communication",
            "Teamwork",
            "Problem-solving",
            "Analytical Skills",
            "Eagerness to Learn",
            "Adaptability"
        ],
        "hard_skills": [
            "Python",
            "Java",
            "C++",
            "Software Development Methodologies",
            "Algorithms",
            "Data Structures",
            "Computer Science Principles",
            "TensorFlow",
            "PyTorch",
            "HTML",
            "CSS",
            "JavaScript",
            "AWS",
            "Azure",
            "Google Cloud"
        ],
        "tech_stack": [
            "Generative AI",
            "NLP",
            "Python",
            "Java",
            "C++",
            "TensorFlow",
            "PyTorch",
            "HTML",
            "CSS",
            "JavaScript",
            "AWS",
            "Azure",
            "Google Cloud"
        ],
        "programming_languages": [
            "Python",
            "Java",
            "C++"
        ],
        "experience": 0,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Computer Science",
                "Software Engineering"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Sunnyvale, CA",
        "job_id": 3940820234,
        "company": "Syntricate Technologies",
        "title": "Sr. Software Engineer (Spark & Scala)",
        "created_on": 1720635951.9338944,
        "description": "Job Title: Sr. Software Engineer (Spark & Scala) Location: Austin, TX / Sunnyvale, CA Type: Contract Desirable Skills: Customer is looking for resources with Software Engineering experience and not Data Engineering. Strong Software Engineering experience (Application Development) using Apache Spark, Scala and messaging systems as Kafka for handling/processing large datasets. Experience showcasing Spark/Scala projects. Strong in SQL Good programming experience in Scala - with Services, Jobs, Workflows, and other features like Libraries/Dataframes etc. with a primary focus on Scala and Apache Spark / good understanding and usage of REST API's Spark performance tuning of complex calculations on large datasets Good understanding of Apache Spark architecture, components, and concepts.",
        "url": "https://www.linkedin.com/jobs/view/3940820234",
        "summary": "Senior Software Engineer with a focus on application development using Apache Spark, Scala, and messaging systems like Kafka for processing large datasets. Strong experience with SQL, REST APIs, and Spark performance tuning. ",
        "industries": [
            "Software Development",
            "Data Processing",
            "Big Data"
        ],
        "soft_skills": [
            "Problem Solving",
            "Communication",
            "Teamwork"
        ],
        "hard_skills": [
            "Apache Spark",
            "Scala",
            "Kafka",
            "SQL",
            "REST APIs",
            "Spark Performance Tuning",
            "Dataframes",
            "Services",
            "Jobs",
            "Workflows",
            "Libraries"
        ],
        "tech_stack": [
            "Apache Spark",
            "Scala",
            "Kafka",
            "REST APIs"
        ],
        "programming_languages": [
            "Scala"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Engineering"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Dublin, CA",
        "job_id": 3648719677,
        "company": "KPI Partners",
        "title": "Senior AWS Data Engineer",
        "created_on": 1720635953.5711098,
        "description": "KPI Partners, A global consulting firm focused on strategy, technology, and digital transformation. We help companies tackle their most ambitious projects and build new capabilities. We provide solutions in Cloud, Data, Application Development & BI spaces. Title: Sr AWS Data Engineer – Long-term Contract Location: Dublin, CA / Santa Monica, CA (Hybrid) Description Senior Hands-on Data Engineer and Architect - Designs, develops and implements AWS eco-system-based data pipeline/engineering applications to support business requirements. Follows approved life cycle methodologies, creates design documents, and performs program coding and testing. Resolves technical issues through debugging, research, and investigation. Experience/Skills Required: Expert in Cloud Big Data Solutions using AWS, Snowflake, and SQL. DBT preferred and highly desirable. Experience in GitLab and CI/CD implementation in Data Engineering Expert in Data modeling and Data Warehousing Understand and convert business requirements into Data Models & Schema Designs Ability to communicate well with business users to understand needs and requirements, IT Stakeholders to understand data sources and access, and manage client expectations and delivery",
        "url": "https://www.linkedin.com/jobs/view/3648719677",
        "summary": "Senior AWS Data Engineer role with KPI Partners, a global consulting firm specializing in strategy, technology, and digital transformation. The role involves designing, developing, and implementing AWS data pipeline and engineering applications to support business requirements. The ideal candidate will have extensive experience with AWS, Snowflake, SQL, DBT, GitLab, CI/CD, data modeling, and data warehousing.",
        "industries": [
            "Consulting",
            "Technology",
            "Data Engineering",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Communication",
            "Problem Solving",
            "Analytical Thinking",
            "Collaboration",
            "Client Management",
            "Technical Documentation"
        ],
        "hard_skills": [
            "AWS",
            "Snowflake",
            "SQL",
            "DBT",
            "GitLab",
            "CI/CD",
            "Data Modeling",
            "Data Warehousing"
        ],
        "tech_stack": [
            "AWS",
            "Snowflake",
            "SQL",
            "DBT",
            "GitLab",
            "CI/CD"
        ],
        "programming_languages": [
            "SQL"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Mountain View, CA",
        "job_id": 3736414876,
        "company": "Databricks",
        "title": "Senior Software Engineer - Database Engine Internals",
        "created_on": 1720635957.726725,
        "description": "Our mission at Databricks is to radically simplify the whole data lifecycle from ingestion to ETL, BI, and all the way up to ML/AI with a unified platform. To achieve this goal, we believe the data warehouse architecture as we know it today will be replaced by a new architectural pattern, Lakehouse (CIDR 2021 paper), open platforms that unify data warehousing and advanced analytics. The new architecture will help address several major challenges, including data staleness, reliability, total cost of ownership, data lock-in, and limited use-case support. A critical part of realizing this vision is the next generation (decoupled) query engine and structured storage system that can outperform specialized data warehouses in relational query performance, yet retain the expressiveness and of general purpose systems such as Apache Spark™ to support diverse workloads ranging from ETL to data science. As Part Of This Team, You Will Be Working In One Or More Of The Following Areas To Design And Implement These Next Gen Systems That Leapfrog State-of-the-art Query compilation and optimization Distributed query execution and scheduling Vectorized execution engine Data security Resource management Transaction coordination Efficient storage structures (encodings, indexes) Automatic physical data optimization What We Look For A passion for database systems, storage systems, distributed systems, language design, or performance optimization Experience working towards a multi-year vision with incremental deliverables Motivated by delivering customer value and impact 5+ years of experience working in a related system (preferred) Optional: PhD in databases or distributed systems Benefits Comprehensive health coverage including medical, dental, and vision 401(k) Plan Equity awards Flexible time off Paid parental leave Family Planning Gym reimbursement Annual personal development fund Work headphones reimbursement Employee Assistance Program (EAP) Business travel accident insurance Mental wellness resources Pay Range Transparency Databricks is committed to fair and equitable compensation practices. The pay range(s) for this role is listed below and represents base salary range for non-commissionable roles or on-target earnings for commissionable roles. Actual compensation packages are based on several factors that are unique to each candidate, including but not limited to job-related skills, depth of experience, relevant certifications and training, and specific work location. Based on the factors above, Databricks utilizes the full width of the range. The total compensation package for this position may also include eligibility for annual performance bonus, equity, and the benefits listed above. For more information regarding which range your location is in visit our page here. Local Pay Range $166,000—$225,000 USD About Databricks Databricks is the data and AI company. More than 10,000 organizations worldwide — including Comcast, Condé Nast, Grammarly, and over 50% of the Fortune 500 — rely on the Databricks Data Intelligence Platform to unify and democratize data, analytics and AI. Databricks is headquartered in San Francisco, with offices around the globe and was founded by the original creators of Lakehouse, Apache Spark™, Delta Lake and MLflow. To learn more, follow Databricks on Twitter, LinkedIn and Facebook. Our Commitment to Diversity and Inclusion At Databricks, we are committed to fostering a diverse and inclusive culture where everyone can excel. We take great care to ensure that our hiring practices are inclusive and meet equal employment opportunity standards. Individuals looking for employment at Databricks are considered without regard to age, color, disability, ethnicity, family or marital status, gender identity or expression, language, national origin, physical and mental ability, political affiliation, race, religion, sexual orientation, socio-economic status, veteran status, and other protected characteristics. Compliance If access to export-controlled technology or source code is required for performance of job duties, it is within Employer's discretion whether to apply for a U.S. government license for such positions, and Employer may decline to proceed with an applicant on this basis alone.",
        "url": "https://www.linkedin.com/jobs/view/3736414876",
        "summary": "Databricks is seeking a database systems engineer to help design and implement the next generation of query engines and storage systems for their Lakehouse platform. The role will focus on areas like query compilation, distributed execution, vectorized execution, data security, resource management, and storage optimization. Ideal candidates will have a strong background in database systems, storage systems, and distributed systems, and a passion for performance optimization.",
        "industries": [
            "Software",
            "Technology",
            "Data Analytics",
            "Big Data",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Passion for database systems",
            "Passion for storage systems",
            "Passion for distributed systems",
            "Passion for language design",
            "Passion for performance optimization",
            "Experience working towards a multi-year vision",
            "Motivated by delivering customer value and impact"
        ],
        "hard_skills": [
            "Query compilation",
            "Query optimization",
            "Distributed query execution",
            "Query scheduling",
            "Vectorized execution engine",
            "Data security",
            "Resource management",
            "Transaction coordination",
            "Storage optimization",
            "Data encoding",
            "Data indexing",
            "Physical data optimization"
        ],
        "tech_stack": [
            "Apache Spark",
            "Lakehouse"
        ],
        "programming_languages": [],
        "experience": 5,
        "education": {
            "min_degree": "PhD",
            "fields": [
                "Databases",
                "Distributed Systems"
            ]
        },
        "salary": {
            "max": 225000,
            "min": 166000
        },
        "benefits": [
            "Comprehensive health coverage",
            "Dental",
            "Vision",
            "401(k) Plan",
            "Equity awards",
            "Flexible time off",
            "Paid parental leave",
            "Family Planning",
            "Gym reimbursement",
            "Annual personal development fund",
            "Work headphones reimbursement",
            "Employee Assistance Program",
            "Business travel accident insurance",
            "Mental wellness resources"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Jose, CA",
        "job_id": 3926415721,
        "company": "A5 Labs",
        "title": "Senior Analytics Engineer, Remote (Remote)",
        "created_on": 1720635959.5360408,
        "description": "Senior Analytics Engineer Europe, Remote A5 Labs is the advanced technology group for online mobile gaming companies, some of the largest mobile gaming sites in the world. The group develops leading-edge security technology and creates innovative modern game designs. A5 bets on its people. Outstanding talent and achievement should be rewarded with a superior work environment, outsized incentives, and growth opportunities. If you are the best, you deserve the best. Not later. Now. At A5 Labs, we power the future of online gambling with AI-driven experiences that set new standards in the industry. If you've ever enjoyed an online casino game, there's a good chance our technology and innovation were at play. Known as a hidden gem of innovation in the iGaming sphere, we're now seeking a Senior Analytics Engineer . The Data Foundations team plays a crucial role in our data infrastructure, focusing on transforming data from upstream teams in our data warehouse into centralised and highly reusable data models, so that models can be leveraged: By the Data Foundations to create dashboards and regular email reports By other data teams for ad hoc analysis (e.g. campaign reporting) By internal applications for presenting data to end users Given the variety of use cases for these data models, it is essential that our models, orchestrating notebooks, and dashboards are reliable, tested, understandable and well documented. Our data stack includes: Databricks for data warehousing and orchestration Dbt for data transformation Python for niche transformations and emailing custom reports Power BI for data visualization Responsibilities Take a leading role in gathering reporting requirements from stakeholders, understand their business process and create well structured, performant and reusable data models in dbt Implement complex business logic, primarily in SQL but sometimes using Python Create tests for and documentation of models Develop and maintain Databricks Notebooks used for sending email reports Develop and maintain Power BI dashboards Skills And Experience 5+ years of experience as an Analytics Engineer, Data Engineer, or related senior roles, ideally within the gaming industry SQL - Advanced proficiency in SQL and experience with cloud data warehousing solutions (e.g. Databricks, Snowflake, BigQuery, Redshift) dbt - Moderate to advanced proficiency with dbt Data Modelling - Advanced proficiency in data modelling principles, ELT processes, and data warehousing concepts Python - Intermediate proficiency in Python for data transformation and automating tasks DevOps - Demonstrable knowledge of version control (e.g., Git), testing, and continuous integration/continuous deployment (CI/CD) pipelines Cloud Platforms - Demonstrable knowledge of cloud platforms like AWS, Google Cloud Platform, or Azure, especially their data services Data Visualization - Basic or intermediate proficiency with data visualization tools (e.g. PowerBI, Tableau, Metabase). Benefits In return you’ll benefit from becoming part of a team that's shaping the future of online gaming, but it also means you'll be rewarded in ways that truly reflect your contribution and talent, which includes an industry leading base salary, plus uncapped bonuses for outstanding contributions paid out as a result of quarterly performance reviews, ensuring your hard work and impact are acknowledged and compensated. Embrace the freedom of flexible working hours and a remote-only mentality that lets you work from anywhere —be it from the comfort of your home or a café in another country, alongside our diverse team of over 250 professionals from 32 countries. We'll also equip you with a technology allowance to ensure you have the best tools at your disposal and an unlimited holiday allowance to ensure you stay fresh and rested. At A5 Labs, you'll collaborate with some of the brightest minds from leading AI and gaming companies worldwide. Plus, our engagement flexibility means we're open to various forms of professional relationships, from direct employment to contracting, all designed to attract the best talent without bureaucracy standing in the way.",
        "url": "https://www.linkedin.com/jobs/view/3926415721",
        "summary": "A5 Labs, a technology group for mobile gaming companies, is seeking a Senior Analytics Engineer to join their Data Foundations team. The role involves building and maintaining data models, orchestrating notebooks, and dashboards using technologies like Databricks, dbt, Python, and Power BI. The successful candidate will have 5+ years of experience in data engineering, proficiency in SQL, dbt, data modeling, Python, DevOps, cloud platforms, and data visualization tools. The company offers a competitive salary, uncapped bonuses, flexible working hours, a remote-only work environment, technology allowance, and unlimited holiday allowance.",
        "industries": [
            "Gaming",
            "Mobile Gaming",
            "iGaming",
            "Online Gambling",
            "Technology",
            "Data",
            "Analytics"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem-Solving",
            "Analytical Thinking",
            "Decision-Making",
            "Organization",
            "Time Management",
            "Attention to Detail",
            "Teamwork",
            "Leadership"
        ],
        "hard_skills": [
            "SQL",
            "dbt",
            "Data Modeling",
            "ELT",
            "Data Warehousing",
            "Python",
            "Git",
            "Testing",
            "CI/CD",
            "AWS",
            "Google Cloud Platform",
            "Azure",
            "Power BI",
            "Tableau",
            "Metabase"
        ],
        "tech_stack": [
            "Databricks",
            "dbt",
            "Python",
            "Power BI"
        ],
        "programming_languages": [
            "SQL",
            "Python"
        ],
        "experience": 5,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Industry Leading Base Salary",
            "Uncapped Bonuses",
            "Quarterly Performance Reviews",
            "Flexible Working Hours",
            "Remote-Only Work Environment",
            "Technology Allowance",
            "Unlimited Holiday Allowance"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Los Angeles, CA",
        "job_id": 3805446286,
        "company": "HIRECLOUT",
        "title": "Principal Data Engineer – 4265",
        "created_on": 1720635961.122793,
        "description": "HireClout Our client is a leading player in the content-creation sphere, enabling creators all over the world to reach new heights and expand their brand. What You Will Be Doing Architect and manage scalable data pipelines, encompassing both single and multi-node ETL solutions Establish data quality assurance measures for new and existing pipelines Generate derived datasets with enhanced attributes Enhance analytics-ready datasets to empower internal and creator-facing tools Address issues promptly, collaborating directly with internal data consumers Automate pipeline executions using scheduling and orchestration tools Handle extensive datasets and utilize various external APIs to enrich data Configure database tables for analytics users to access the Data Engineering team’s collected data Employ big data technologies to enhance data availability and quality in the AWS cloud Lead projects, collaborating with team members and serving as a mentor Actively engage in team discussions on technology, architecture, and solutions for both new and existing projects What You Will Need Bachelor’s degree, preferably in Computer Science or Computer Information Systems 6+ years of software engineering experience 5+ years of data engineering know-how with Apache Spark or Apache Flink 4+ years of hands-on experience running software and services in the cloud Skilled in working with DataFrame APIs (Pandas and Spark) for both parallel and single-node processing Proficiency in using languages like Python, Scala, etc., with modern data file formats like Parquet and Avro Proficiency in SQL for RDBMS and data warehouse solutions, such as Redshift Hands-on experience with Data Lake technologies like Delta Lake and Iceberg Experience in data acquisition from external APIs at a large scale and parallel processing Practical experience supporting ML/AI projects, including deploying pipelines for computing features and using models for inference on large datasets Why Us Benefits And Perks Competitive Salary : $180,000 – $210,000 per Year Full Health, Vision, and Dental Coverage Applicants must be currently authorized to work in the United States on a full-time basis now and in the future. This position does not offer sponsorship. REF: JOB-4265 Share this job First name Last name Email Attach resume* Job type: Permanent Location: Date posted: Posted 6 months ago Salary:$180000 - $210000 per Year",
        "url": "https://www.linkedin.com/jobs/view/3805446286",
        "summary": "This is a data engineering role focused on building and maintaining scalable data pipelines, ensuring data quality, and supporting ML/AI projects. Responsibilities include architecting ETL solutions, generating derived datasets, enhancing analytics datasets, and collaborating with internal data consumers. The role involves working with big data technologies in the AWS cloud and leading projects with team members.",
        "industries": [
            "Data Engineering",
            "Software Development",
            "Technology",
            "Content Creation",
            "Media"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Leadership",
            "Mentorship",
            "Project Management",
            "Time Management",
            "Organization",
            "Analytical Thinking"
        ],
        "hard_skills": [
            "Data Pipeline Architecture",
            "ETL",
            "Data Quality Assurance",
            "Data Analytics",
            "Data Modeling",
            "Data Wrangling",
            "SQL",
            "Python",
            "Scala",
            "Apache Spark",
            "Apache Flink",
            "Pandas",
            "Parquet",
            "Avro",
            "Redshift",
            "Delta Lake",
            "Iceberg",
            "API Integration",
            "Machine Learning",
            "AI",
            "AWS Cloud",
            "Data Lake"
        ],
        "tech_stack": [
            "Apache Spark",
            "Apache Flink",
            "Pandas",
            "Parquet",
            "Avro",
            "Redshift",
            "Delta Lake",
            "Iceberg",
            "AWS Cloud"
        ],
        "programming_languages": [
            "Python",
            "Scala",
            "SQL"
        ],
        "experience": 6,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Computer Science",
                "Computer Information Systems"
            ]
        },
        "salary": {
            "max": 210000,
            "min": 180000
        },
        "benefits": [
            "Competitive Salary",
            "Full Health, Vision, and Dental Coverage"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Sunnyvale, CA",
        "job_id": 3921151811,
        "company": "LinkedIn",
        "title": "Staff Software Engineer - Storage Engineering",
        "created_on": 1720635964.2120733,
        "description": "LinkedIn is the world’s largest professional network, built to create economic opportunity for every member of the global workforce. Our products help people make powerful connections, discover exciting opportunities, build necessary skills, and gain valuable insights every day. We’re also committed to providing transformational opportunities for our own employees by investing in their growth. We aspire to create a culture that’s built on trust, care, inclusion, and fun – where everyone can succeed. Join us to transform the way the world works. At LinkedIn, we trust each other to do our best work where it works best for us and our teams. This role offers a hybrid work option, meaning you can both work from home and commute to a LinkedIn office, depending on what’s best for you and when it is important for your team to be together. This role is based in our Sunnyvale, CA office location. Our team is responsible for architecting, designing, building and maintaining storage software systems for our data centers, consisting of an exabyte scale multi-tenanted disaggregated chunk storage abstraction layer and a remote block storage system. As a Staff Software Engineer, you will be a key technical leader and role model within the team. We are looking for someone who designs and develops technology to serve business and technology objectives, understands state-of-the-art storage research, aligns points of view across teams, and makes smart trade-offs. In addition to leadership skills, a successful Staff Software Engineer must exhibit strong technical skills in software engineering and storage. Responsibilities: · Responsible in designing and developing strategy for storage system design and consumption. · Additional responsibilities include influencing the open source community in white-box storage developments and software defined storage. · Utilize communication skills in interacting with peer groups & drive technical presentations. · Project lead initial storage implementations, and proof-of-concepts and pilots. · Adapt to the ever-evolving industry; learn and scrutinize new technologies, and envision its possible application towards the Linkedin mission. · Participate in a 12x7 rotation for second-tier escalations. Basic Qualifications: · BA/BS Degree in Computer Science, Electrical Engineering, or related technical discipline, or related practical experience. · 4+ years of experience in design and development of storage systems · 4+ years programming experience in object-oriented programming languages such as Python, Java, Javascript, C/C++, C#, Objective-C, or Ruby. Preferred Qualifications · Experience in distributed/clustered storage designs, multiple storage protocols, deploying, analyzing and debugging storage networks · Experience in configuring, tuning operating systems for use with storage including performance analysis · Working knowledge of the Kubernetes, CSI, Linux storage stack, both block and file-system (e.g., XFS, GPFS, Gluster, Ceph, Swift, NFS) · Software engineering skills with efficient, maintainable and testable C/C++/Python/Go · Experience deploying storage for shared-nothing applications · Experience leading cross functional teams engaged in storage system design and deployment Suggested Skills: · Distributed Systems · C/C++, Go, or Rust · Storage Software You will Benefit from our Culture: We strongly believe in the well-being of our employees and their families. That is why we offer generous health and wellness programs and time away for employees of all levels. LinkedIn is committed to fair and equitable compensation practices. The pay range for this role is $144,000 - $235,000. Actual compensation packages are based on a wide array of factors unique to each candidate, including but not limited to skill set, years & depth of experience, certifications and specific office location. This may differ in other locations due to cost of labor considerations. The total compensation package for this position may also include annual performance bonus, stock, benefits and/or other applicable incentive compensation plans. For additional information, visit: https://careers.linkedin.com/benefits. Equal Opportunity Statement LinkedIn is committed to diversity in its workforce and is proud to be an equal opportunity employer. LinkedIn considers qualified applicants without regard to race, color, religion, creed, gender, national origin, age, disability, veteran status, marital status, pregnancy, sex, gender expression or identity, sexual orientation, citizenship, or any other legally protected class. LinkedIn is an Affirmative Action and Equal Opportunity Employer as described in our equal opportunity statement here: https://microsoft.sharepoint.com/:b:/t/LinkedInGCI/EeE8sk7CTIdFmEp9ONzFOTEBM62TPrWLMHs4J1C_QxVTbg?e=5hfhpE. Please reference https://www.eeoc.gov/sites/default/files/2023-06/22-088_EEOC_KnowYourRights6.12ScreenRdr.pdf and https://www.dol.gov/ofccp/regs/compliance/posters/pdf/OFCCP_EEO_Supplement_Final_JRF_QA_508c.pdf for more information. LinkedIn is committed to offering an inclusive and accessible experience for all job seekers, including individuals with disabilities. Our goal is to foster an inclusive and accessible workplace where everyone has the opportunity to be successful. If you need a reasonable accommodation to search for a job opening, apply for a position, or participate in the interview process, connect with us at accommodations@linkedin.com and describe the specific accommodation requested for a disability-related limitation. Reasonable accommodations are modifications or adjustments to the application or hiring process that would enable you to fully participate in that process. Examples of reasonable accommodations include but are not limited to: -Documents in alternate formats or read aloud to you -Having interviews in an accessible location -Being accompanied by a service dog -Having a sign language interpreter present for the interview A request for an accommodation will be responded to within three business days. However, non-disability related requests, such as following up on an application, will not receive a response. LinkedIn will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by LinkedIn, or (c) consistent with LinkedIn's legal duty to furnish information. Pay Transparency Policy Statement As a federal contractor, LinkedIn follows the Pay Transparency and non-discrimination provisions described at this link: https://lnkd.in/paytransparency. Global Data Privacy Notice for Job Candidates This document provides transparency around the way in which LinkedIn handles personal data of employees and job applicants: https://lnkd.in/GlobalDataPrivacyNotice",
        "url": "https://www.linkedin.com/jobs/view/3921151811",
        "summary": "LinkedIn is seeking a Staff Software Engineer to design and develop storage systems for their data centers. This role involves architecting, designing, building, and maintaining exabyte-scale storage systems. The ideal candidate will have strong technical skills in software engineering and storage, as well as experience with distributed/clustered storage designs, multiple storage protocols, and Kubernetes.",
        "industries": [
            "Information Technology",
            "Software Development",
            "Cloud Computing",
            "Data Management",
            "Storage"
        ],
        "soft_skills": [
            "Leadership",
            "Communication",
            "Technical Presentation",
            "Project Management",
            "Adaptability",
            "Learning",
            "Problem Solving"
        ],
        "hard_skills": [
            "Storage System Design",
            "Open Source Software",
            "Software Defined Storage",
            "Object-Oriented Programming",
            "Python",
            "Java",
            "Javascript",
            "C/C++",
            "C#",
            "Objective-C",
            "Ruby",
            "Distributed Systems",
            "Clustered Storage",
            "Storage Protocols",
            "Kubernetes",
            "CSI",
            "Linux Storage",
            "XFS",
            "GPFS",
            "Gluster",
            "Ceph",
            "Swift",
            "NFS",
            "C/C++",
            "Go",
            "Rust"
        ],
        "tech_stack": [
            "Kubernetes",
            "CSI",
            "XFS",
            "GPFS",
            "Gluster",
            "Ceph",
            "Swift",
            "NFS",
            "C/C++",
            "Go",
            "Rust"
        ],
        "programming_languages": [
            "Python",
            "Java",
            "Javascript",
            "C/C++",
            "C#",
            "Objective-C",
            "Ruby",
            "Go",
            "Rust"
        ],
        "experience": 4,
        "education": {
            "min_degree": "BA/BS",
            "fields": [
                "Computer Science",
                "Electrical Engineering",
                "Related Technical Discipline"
            ]
        },
        "salary": {
            "max": 235000,
            "min": 144000
        },
        "benefits": [
            "Health and Wellness Programs",
            "Time Away",
            "Annual Performance Bonus",
            "Stock",
            "Other Applicable Incentive Compensation Plans"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3960842148,
        "company": "Refuel",
        "title": "Software Engineer - Frontend",
        "created_on": 1720635965.9325674,
        "description": "About Refuel.ai The Mission The next wave of ML applications (video security, conversational AI and autonomous vehicles) is going to look very different from the last one (search and recommender systems). Teams working in these domains have to deal with large streams of unstructured data (images, video, text and audio), exorbitant labeling costs, and broken feedback loops. For this next wave of ML applications, getting to production takes months and adapting to the long tail is an existential struggle. That’s what we’re solving - Refuel.ai accelerates time to production for ML applications built on top of unstructured data, and enables them to continually improve in the real world. The team Refuel.ai was founded in 2021 by Rishabh Bhargava and Nihit Desai. Our founders bring machine learning and product experience from the world's leading technology companies (Facebook, Instagram, Primer.ai and Cloudera) and institutions (Stanford, Cambridge and UIUC). We are backed by General Catalyst, XYZ Ventures and a host of successful and influential angel investors from places like Stanford, OpenAI, Datadog and Facebook. What you will do Build our web application frontend. Frameworks/languages you will use on a regular basis include React, NextJS, Javascript and CSS Work closely with our design and engineering teams to implement new features and own them across the product life cycle. Prototype and rapidly experiment with new product features and UI improvements in response to customer feedback Help define and evolve our engineering stack as we scale the product Help us hire a world-class team and mentor new team members You will be a great fit if you have Expertise with building frontend applications using React and Javascript The ability and passion to lead product-defining initiatives from ideation to execution while maintaining a high bar for design and correctness Strong communication skills and enjoy working in a collaborative environment. We encourage open discussion and want the best ideas to win A curiosity about the company building journey Bonus: prior experience with building interactive frontend interfaces for data-intensive software applications Our current tech stack Infrastructure - AWS + Serverless + Kubernetes + secret sauce infra to power modern ML Backend - Python + FastAPI + Postgres + DynamoDB Frontend - React + Typescript What we offer We have raised funding from top-tier investors, including General Catalyst, XYZ Ventures and a host of successful angel investors from places such as Stanford, OpenAI, Facebook and Datadog. We will offer a competitive salary and meaningful equity in the company. Most importantly, you will have the opportunity to create a massive impact on the product and company as a member of our early team. We care a lot about making a difference for our customers while working on a team where we are empowered to do the best work of our lives.",
        "url": "https://www.linkedin.com/jobs/view/3960842148",
        "summary": "Refuel.ai is a company developing tools to accelerate the development and deployment of machine learning applications based on unstructured data (images, video, text, and audio).  They are looking for a Frontend Engineer to build and maintain their web application using React, NextJS, Javascript, and CSS. The role involves collaborating with design and engineering teams, prototyping features, and contributing to hiring and mentoring.",
        "industries": [
            "Machine Learning",
            "Artificial Intelligence",
            "Software Development",
            "Technology",
            "Data Science"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Leadership",
            "Curiosity",
            "Passion"
        ],
        "hard_skills": [
            "React",
            "NextJS",
            "Javascript",
            "CSS",
            "Typescript"
        ],
        "tech_stack": [
            "AWS",
            "Serverless",
            "Kubernetes",
            "Python",
            "FastAPI",
            "Postgres",
            "DynamoDB",
            "React",
            "Typescript"
        ],
        "programming_languages": [
            "Javascript",
            "Python",
            "Typescript"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Competitive salary",
            "Equity in the company"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Sunnyvale, CA",
        "job_id": 3968136736,
        "company": "eTek IT Services, Inc.",
        "title": "Data Engineer- W2",
        "created_on": 1720635968.0173051,
        "description": "TRequired Skills 5 years experience in computer programming, software development or related 2. 3+ years of solid Java and 2+ years experience in design, implementation, and support of solutions big data solution in Hadoop using Hive, Spark, Drill, Impala, HBase 3. Hands on experience with Unix, Teradata and other relational databases. Experience with @Scale a plus. 4. Strong communication and problem-solving skills 5. Candidates must have 5+ years of data experience, 2+ years of experience with Google Cloud platform, and hands on experience process large scale distributed data. Skills Additional Skills Job Description Description: Designs, develops, and implements Hadoop eco-system based applications to support business requirements. Follows approved life cycle methodologies, creates design documents, and performs program coding and testing. Resolves technical issues through debugging, research, and investigation. Experience/Skills Required:1. Bachelor?s degree in Computer Science, Information Technology, or related field and 5 years experience in computer programming, software development or related2. 3+ years of solid Java and 2+ years experience in design, implementation, and support of solutions big data solution in Hadoop using Hive, Spark, Drill, Impala, HBase3. Hands on experience with Unix, Teradata and other relational databases. Experience with @Scale a plus.4. Strong communication and problem-solving skills Candidates must have 5+ years of data experience, 2+ years of experience with Google Cloud platform, and hands on experience process large scale distributed data. Skills: cloud,big data,hadoop,hive,hbase,spark,scales,gcp",
        "url": "https://www.linkedin.com/jobs/view/3968136736",
        "summary": "Designs, develops, and implements Hadoop eco-system based applications to support business requirements. Follows approved life cycle methodologies, creates design documents, and performs program coding and testing. Resolves technical issues through debugging, research, and investigation.",
        "industries": [
            "Software Development",
            "Data Analytics",
            "Big Data",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Communication",
            "Problem-solving"
        ],
        "hard_skills": [
            "Java",
            "Hadoop",
            "Hive",
            "Spark",
            "Drill",
            "Impala",
            "HBase",
            "Unix",
            "Teradata",
            "Relational Databases",
            "Google Cloud Platform",
            "Distributed Data"
        ],
        "tech_stack": [
            "Hadoop",
            "Hive",
            "Spark",
            "Drill",
            "Impala",
            "HBase",
            "Google Cloud Platform",
            "Unix",
            "Teradata"
        ],
        "programming_languages": [
            "Java"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Computer Science",
                "Information Technology"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3945768508,
        "company": "Vellum",
        "title": "Software Engineer - Backend / Fullstack",
        "created_on": 1720635969.748598,
        "description": "What it means to be an early engineer Joining Vellum this early means that you’re going to play a huge part in guiding both the technical and cultural evolution of the company. Together, we’re going to face the hardest technical challenges this journey has to offer head-on. As the team grows, you’ll be the battle hardened expert that everyone looks towards for guidance. With great power comes great responsibility 🙂 What you’re signing up for Tackling our most difficult (and interesting) problems - prompt chaining, fine-tuning infrastructure, and multimodality, just to name a few Designing, architecting, and building robust critical infrastructure Owning our most important backend systems Defining and scoping green-field projects that move the company forward Working closely with customers to understand what the need, figuring out how to make them successful, and owning the solutions end to end Serving as a cultural pillar of the team Navigating a fast-moving, highly competitive landscape Our stack In Case You’re Curious Google Cloud Platform - as our cloud provider Postgres - as our primary database Python - what we write our backend in Django - for our webservers Flask - for our microservices Typescript - what we write our frontend in React - as our frontend framework Working at Vellum We’re laser focused on helping companies bring AI into production and providing real business value. Decision making is simple: if what we do allows our users to increase how many use cases of AI they’re able to successfully put into production, then we do it! Don’t just cut corners. Cut the right corners (i.e. reduce scope, not quality). What can you do with 10% of your time to get 90% of the desired impact? Now that you only spent 10% of the time on this task, imagine how much more can be done with the rest of your time! Everyone is a 1000x engineer/designer/pm when they’re passionate about the problems they’re solving and love the people they’re working with Because we trust each other, we can move fast and have fun doing it 😄",
        "url": "https://www.linkedin.com/jobs/view/3945768508",
        "summary": "Early engineer at Vellum will play a key role in shaping the company's technical and cultural landscape, tackling challenging problems like prompt chaining, fine-tuning infrastructure, and multimodality. Responsibilities include designing and building robust backend systems, defining green-field projects, understanding customer needs, and fostering a strong team culture.",
        "industries": [
            "Artificial Intelligence",
            "Software Development",
            "Technology"
        ],
        "soft_skills": [
            "Problem Solving",
            "Decision Making",
            "Communication",
            "Teamwork",
            "Leadership",
            "Adaptability"
        ],
        "hard_skills": [
            "Prompt Engineering",
            "Fine-Tuning",
            "Multimodality",
            "Backend Development",
            "System Architecture",
            "Microservices",
            "Frontend Development"
        ],
        "tech_stack": [
            "Google Cloud Platform",
            "Postgres",
            "Python",
            "Django",
            "Flask",
            "Typescript",
            "React"
        ],
        "programming_languages": [
            "Python",
            "Typescript"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3931982861,
        "company": "Akraya, Inc.",
        "title": "Cloud Data Engineer II: 24-01406",
        "created_on": 1720635971.4466116,
        "description": "Primary Skills: Cloud Data Engineering, SQL, Data Modeling, ETL, AWS, Python Contract Type: W2 Only Duration: 12+ months contract with possible extension Location: San Francisco, CA (Hybrid 2 days in office) Pay Range:$70.55 - $75.55 per hour Grow your network by interacting with people Job Responsibilities Contribute to the design, development, testing, implementation, and review of complex data warehouse and business intelligence solutions. Develop software from established requirements, build reports and dashboards, and coordinate with fellow programmers to meet delivery commitments. Recommend process improvements and contribute to the design of technology infrastructure and configurations. Implement complex software packages and deploy code, ensuring optimal performance and data integrity. Act as a key participant in cross-functional team initiatives and process improvement projects. JOB REQUIREMENT S: Expertise in SQL and data modeling for various RDBMS (e.g., Microsoft SQL Server, Oracle). Practical experience with ETL tools (preferably Informatica) and AWS cloud data warehousing technologies. Strong programming skills in Python and familiarity with Agile development methodologies. ABOUT AKRAYA \"Akraya is an award-winning IT staffing firm consistently recognized for our commitment to excellence and a positive work environment. Voted the #1 Best Place to Work in Silicon Valley (2023) and a Glassdoor Best Places to Work (2023 & 2022), Akraya prioritizes a culture of inclusivity and fosters a sense of belonging for all team members. We are staffing solutions providers for Fortune 100 companies, and our industry recognitions solidify our leadership position in the IT staffing space. Let us lead you to your dream career, join Akraya today!\"",
        "url": "https://www.linkedin.com/jobs/view/3931982861",
        "summary": "This is a 12+ month W2 contract opportunity for a Cloud Data Engineer in San Francisco, CA. The role requires expertise in SQL, data modeling, ETL, AWS, and Python.  You will contribute to the design, development, testing, implementation, and review of complex data warehouse and business intelligence solutions. ",
        "industries": [
            "Technology",
            "Information Technology",
            "Software Development",
            "Data Engineering",
            "Business Intelligence"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem-Solving",
            "Teamwork",
            "Process Improvement",
            "Analytical Thinking"
        ],
        "hard_skills": [
            "Cloud Data Engineering",
            "SQL",
            "Data Modeling",
            "ETL",
            "AWS",
            "Python",
            "RDBMS",
            "Microsoft SQL Server",
            "Oracle",
            "Informatica",
            "Agile Development"
        ],
        "tech_stack": [
            "AWS",
            "Informatica",
            "Microsoft SQL Server",
            "Oracle"
        ],
        "programming_languages": [
            "Python",
            "SQL"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 7555,
            "min": 7055
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "California, United States",
        "job_id": 3888433198,
        "company": "Enexus Global Inc.",
        "title": "Senior Data Engineer - Remote - Not Less than 12+ Years",
        "created_on": 1720635975.6384318,
        "description": "Job Title - Senior Data Engineer (Not less than 12+ Years) Location - Remote Contract Type - W2/C2C/1099 Experience -- 12+ Years only Responsibilities Develop and enhance data-processing, orchestration, monitoring, and more by leveraging popular open-source software, AWS, and GitLab automation. Collaborate with product and technology teams to design and validate the capabilities of the data platform Identify, design, and implement process improvements: automating manual processes, optimizing for usability, re-designing for greater scalability Provide technical support and usage guidance to the users of our platform's services. Drive the creation and refinement of metrics, monitoring, and alerting mechanisms to give us the visibility we need into our production services. Qualifications Experience building and optimizing data pipelines in a distributed environment Experience supporting and working with cross-functional teams Proficiency working in Linux environment 8+ years of advanced working knowledge of SQL, Python, and PySpark 5+ years of experience with using a broad range of AWS technologies Experience using tools such as: Git/Bitbucket, Jenkins/CodeBuild, CodePipeline Experience with platform monitoring and alerts tools",
        "url": "https://www.linkedin.com/jobs/view/3888433198",
        "summary": "Senior Data Engineer with 12+ years of experience. Responsibilities include developing data pipelines, collaborating with product & tech teams, optimizing processes, providing technical support, and building monitoring systems. The role requires experience with AWS, GitLab automation, SQL, Python, PySpark, and various AWS technologies, as well as proficiency in Linux and tools like Git/Bitbucket, Jenkins/CodeBuild, and CodePipeline.",
        "industries": [
            "Data Engineering",
            "Software Development",
            "Technology",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Collaboration",
            "Communication",
            "Problem Solving",
            "Technical Support",
            "Process Improvement",
            "Optimization",
            "Scalability",
            "Automation"
        ],
        "hard_skills": [
            "Data Pipelines",
            "Distributed Systems",
            "SQL",
            "Python",
            "PySpark",
            "AWS",
            "Linux",
            "Git",
            "Bitbucket",
            "Jenkins",
            "CodeBuild",
            "CodePipeline"
        ],
        "tech_stack": [
            "AWS",
            "GitLab",
            "SQL",
            "Python",
            "PySpark",
            "Linux",
            "Git",
            "Bitbucket",
            "Jenkins",
            "CodeBuild",
            "CodePipeline"
        ],
        "programming_languages": [
            "SQL",
            "Python",
            "PySpark"
        ],
        "experience": 12,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3745504358,
        "company": "Mixpanel",
        "title": "Software Engineer, Analysis",
        "created_on": 1720635977.2955108,
        "description": "About Mixpanel Mixpanel is an event analytics platform for builders who need answers from their data at their fingertips—no SQL required. When everyone in the organization can see and learn from the impact of their work on product, marketing, and company revenue metrics, they are poised to make better decisions. Over 9,000 paid customers, including companies like Netflix, Pinterest, Sweetgreen, Samsara, and Uber, use Mixpanel to understand their customers and measure progress. Our commitment is to provide the most comprehensive and reliable analytics platform accessible and trusted by all. We are actively recruiting for multiple Software Engineers across different levels for our org! About The Role Mixpanel engineering is a small team that works quickly, prioritizing getting value to customers and iterating to make those experiences even better. We offer the most powerful product analytics tool while still focusing on making the user experience delightful. The full stack engineers have the freedom to move about the stack to deliver value to customers without being blocked by ownership silos. This means doing everything from building snappy UIs and fast and reliable services to leveraging our powerful query engine to get the customers answers that help them deliver great products. We are looking for folks who seek to understand and own customer problems, participating in every step of the development life cycle from ideation to delivery and maintenance. We encourage ownership and autonomy—if you see customer pain points, you will be empowered to fix them. There is no \"not my problem\" at Mixpanel. All customer problems are ours to fix. We believe that our UX is a key differentiator, and we invest heavily in UI, aiming for a snappy and intuitive experience. We believe in Product-Led Growth, that even enterprise adoption of software tools starts with individual users and the way to get there is through creating a user experience that gets customers to answers quickly. Responsibilities At Mixpanel Engineering, Product, and Design (EPD) work closely in pods to define the product and experiences. You'll be collaborating closely with your EPD team to create a vision for delightful and intuitive UX, then work to deliver the most impactful features for our customers. You'll be owning key pieces of the product, maintaining them, and always advocating for improvements to make the user experience even better, adding to our growing shared components library and expanding our design system. You’ll be working on our web app, which is built in Python, running Django, with an open source JavaScript framework running the front end. Everything is run on Google Cloud Platform, using Kubernetes and Docker for orchestration and containerization of our services. A typical project requires working across our stack, creating endpoints for your feature as well as developing and deploying the UI. Here are some projects we've worked on in the past to give you an idea of what to expect. A way for customers to share their metrics publicly, a pain point for customers who want to easily share their data on social media or with executive leadership. This feature required keeping an eye on security so unintentional data would not be leaked for customers, but would enable easier collaboration. We rebuilt our Live View from the ground up and rebranded to Events. Our goal was to improve users’ trust and make debugging their data and query results easier. With these goals in mind, we removed a number of limitations from Live View, improved key work flows, and tightened the UX. We also drastically simplified both the back-end and front-end architecture resulting in a more performant and maintainable product. The end result is a more powerful tool for our users and a simpler codebase for our developers. We revamped our dashboards UI to improve the workflow for creating and saving analyses that also had the nice side effect of making our data model clearer. This required us to iterate and polish while learning more about how customers used and worked around our product in their day to day. We applied these learnings to smooth our workflows even more while continuing to set the stage for more series of improvements. We redesigned our query builder to allow users to instantly see the results of their changes on one screen. This was required balancing competing requirements of visual space for the query builder and the ability to see two things at once, resulting in a more intuitive UX. We made segmentation faster allowing users to customize what they can compare at the query time vs deciding everything they will ever need at the time they develop your product. What We're Looking For We are hiring full stack engineers at all levels across all teams. You take ownership of problems with a focus on delivering value to users/customers. You can deliver solutions that are pragmatic in approach. You have experience with web application architecture and principles. Additional Resources Engineering Life Page Engineering Blog Compensation The amount listed below is the total target cash compensation (TTCC) and includes base compensation and variable compensation in the form of either a company bonus or commissions. Variable compensation type is determined by your role and level. In addition to the cash compensation provided, this position is also eligible for equity consideration and other benefits including medical, vision, and dental insurance coverage. You can view our benefits offerings here. Our salary ranges are determined by role and level and are benchmarked to the SF Bay Area Technology data cut released by Radford, a global compensation database. The range displayed represents the minimum and maximum TTCC for new hire salaries for the position across all of our US locations. To stay on top of market conditions, we refresh our salary ranges twice a year so these ranges may change in the future. Within the range, individual pay is determined by experience, job-related skills, qualifications, and other factors. If you have questions about the specific range, your recruiter can share this information. Mixpanel Compensation Range $188,500—$230,500 USD Benefits And Perks Comprehensive Medical, Vision, and Dental Care Mental Wellness Benefit Generous Vacation Policy & Additional Company Holidays Enhanced Parental Leave Volunteer Time Off Additional US Benefits: Pre-Tax Benefits including 401(K), Wellness Benefit, Holiday Break please note that benefits and perks for contract positions will vary* Culture Values Be Open: When knowledge becomes open, we can come together as a team to collaborate around a shared purpose Customer Focus: Our customers’ success is our success Lead Change: Everyone at Mixpanel has the capacity to make an impact on the business Results Oriented: Driving results in a measurable way ensures we stay focused on the highest impact initiatives One Team: We can’t win without each other Why choose Mixpanel? We’re a leader in analytics with over 9,000 customers and $277M raised from prominent investors: like Andreessen-Horowitz, Sequoia, YC, and, most recently, Bain Capital. Mixpanel’s pioneering event-based data analytics platform offers a powerful yet simple solution for companies to understand user behaviors and easily track overarching company success metrics. Our accomplished teams continuously facilitate our expansion by tackling the ever-evolving challenges tied to scaling, reliability, design, and service. Choosing to work at Mixpanel means you’ll be helping the world’s most innovative companies learn from their data so they can make better decisions. Mixpanel is an equal opportunity employer supporting workforce diversity. At Mixpanel, we are focused on things that really matter—our people, our customers, our partners—out of a recognition that those relationships are the most valuable assets we have. We actively encourage women, people with disabilities, veterans, underrepresented minorities, and LGBTQ+ people to apply. We do not discriminate on the basis of race, religion, color, national origin, gender, gender identity or expression, sexual orientation, age, marital status, veteran status, or disability status. Pursuant to the San Francisco Fair Chance Ordinance or other similar laws that may be applicable, we will consider for employment qualified applicants with arrest and conviction records. We’ve immersed ourselves in our Culture and Values as our guiding principles for the impact we want to have and the future we are building.",
        "url": "https://www.linkedin.com/jobs/view/3745504358",
        "summary": "Mixpanel is seeking Full Stack Engineers at all levels for their growing team.  As a Full Stack Engineer, you will work closely with product and design teams to build intuitive user experiences on Mixpanel's web app. Mixpanel is a leader in event-based data analytics and its web app is built with Python, Django, and an open source JavaScript framework. The company emphasizes customer focus, ownership, and autonomy, and is known for its fast-paced, iterative development process.",
        "industries": [
            "Software",
            "Technology",
            "Analytics",
            "Data"
        ],
        "soft_skills": [
            "Ownership",
            "Problem Solving",
            "Collaboration",
            "Communication",
            "Customer Focus",
            "Autonomy",
            "Pragmatism",
            "Product-Led Growth",
            "UX Design",
            "Iterative Development"
        ],
        "hard_skills": [
            "Python",
            "Django",
            "JavaScript",
            "Google Cloud Platform",
            "Kubernetes",
            "Docker",
            "Web Application Architecture"
        ],
        "tech_stack": [
            "Python",
            "Django",
            "JavaScript",
            "Google Cloud Platform",
            "Kubernetes",
            "Docker"
        ],
        "programming_languages": [
            "Python",
            "JavaScript"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 230500,
            "min": 188500
        },
        "benefits": [
            "Comprehensive Medical, Vision, and Dental Care",
            "Mental Wellness Benefit",
            "Generous Vacation Policy",
            "Additional Company Holidays",
            "Enhanced Parental Leave",
            "Volunteer Time Off",
            "401(K)",
            "Wellness Benefit",
            "Holiday Break",
            "Equity"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Mountain View, CA",
        "job_id": 3851867707,
        "company": "Google",
        "title": "Software Engineer III, Google Ads",
        "created_on": 1720635978.9273078,
        "description": "Note: By applying to this position you will have an opportunity to share your preferred working location from the following: Mountain View, CA, USA; Pittsburgh, PA, USA; Los Angeles, CA, USA . Minimum qualifications: Bachelor’s degree or equivalent practical experience. 2 years of experience with software development in one or more programming languages, or 1 year of experience with an advanced degree in an industry setting. 2 years of experience with data structures or algorithms in either an academic or industry setting. Preferred qualifications: Master's degree or PhD in Computer Science or related technical field. 2 years of experience with performance, large scale systems data analysis, visualization tools, and/or debugging. Experience developing accessible technologies. Proficiency in code and system health, diagnosis and resolution, and software test engineering. About The Job Google's software engineers develop the next-generation technologies that change how billions of users connect, explore, and interact with information and one another. Our products need to handle information at massive scale, and extend well beyond web search. We're looking for engineers who bring fresh ideas from all areas, including information retrieval, distributed computing, large-scale system design, networking and data storage, security, artificial intelligence, natural language processing, UI design and mobile; the list goes on and is growing every day. As a software engineer, you will work on a specific project critical to Google’s needs with opportunities to switch teams and projects as you and our fast-paced business grow and evolve. We need our engineers to be versatile, display leadership qualities and be enthusiastic to take on new problems across the full-stack as we continue to push technology forward. The Google Ads Mission is “Powering the open and free internet with the best technology that connects users and businesses everywhere.” Google Ads operates across several countries and is composed of the following engineering teams: Search Ads, Display, Video Ads and Apps (AViD), YouTube Ads, Analytics, Insights & Measurements (AIM), Ads Privacy & Safety (APaS), Commerce, Travel and Customer Engagement, and two Departments: Reach UX and Ads Engineering Productivity. Google Ads is helping power the open internet with the best technology that connects and creates value for people, publishers, advertisers, and Google. We’re made up of multiple teams, building Google’s Advertising products including search, display, shopping, travel and video advertising, as well as analytics. Our teams create trusted experiences between people and businesses with useful ads. We help grow businesses of all sizes from small businesses, to large brands, to YouTube creators, with effective advertiser tools that deliver measurable results. We also enable Google to engage with customers at scale. The US base salary range for this full-time position is $136,000-$200,000 + bonus + equity + benefits. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process. Please note that the compensation details listed in US role postings reflect the base salary only, and do not include bonus, equity, or benefits. Learn more about benefits at Google . Responsibilities Design, develop, test, deploy, maintain, and improve software. Manage individual project priorities, deadlines, and deliverables. Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing our Accommodations for Applicants form .",
        "url": "https://www.linkedin.com/jobs/view/3851867707",
        "summary": "Google is seeking Software Engineers to develop and maintain their advertising products, including search, display, shopping, travel, and video advertising. Responsibilities include designing, developing, testing, deploying, maintaining, and improving software, as well as managing individual project priorities, deadlines, and deliverables. The position offers a competitive salary, bonus, equity, and benefits.",
        "industries": [
            "Technology",
            "Software Development",
            "Advertising",
            "Internet",
            "E-commerce"
        ],
        "soft_skills": [
            "Leadership",
            "Versatility",
            "Communication",
            "Problem Solving",
            "Teamwork",
            "Collaboration",
            "Time Management",
            "Organization"
        ],
        "hard_skills": [
            "Software Development",
            "Programming Languages",
            "Data Structures",
            "Algorithms",
            "Performance Optimization",
            "Large Scale Systems",
            "Data Analysis",
            "Visualization Tools",
            "Debugging",
            "Accessibility",
            "Code Health",
            "System Diagnosis",
            "Software Test Engineering"
        ],
        "tech_stack": [
            "Google Ads",
            "Search Ads",
            "Display Ads",
            "Video Ads",
            "Apps (AViD)",
            "YouTube Ads",
            "Analytics",
            "Insights & Measurements (AIM)",
            "Ads Privacy & Safety (APaS)",
            "Commerce",
            "Travel",
            "Customer Engagement",
            "Reach UX",
            "Ads Engineering Productivity"
        ],
        "programming_languages": [],
        "experience": 2,
        "education": {
            "min_degree": "Bachelor’s degree",
            "fields": [
                "Computer Science",
                "Related Technical Field"
            ]
        },
        "salary": {
            "max": 200000,
            "min": 136000
        },
        "benefits": [
            "Bonus",
            "Equity",
            "Health Insurance",
            "Paid Time Off",
            "Retirement Plan"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3955675268,
        "company": "Candid Health",
        "title": "Lead Data Engineer",
        "created_on": 1720635980.5908773,
        "description": "The Role At Candid Health, we’re searching for our founding Lead Data Engineer to design, build, and support our next generation of data infrastructure. As a key strategic investment for the company and product, you will be working on highly impactful and visible outcomes. This is an opportunity to get in at the ground floor of designing and building something exciting and new in a secure, durable, performant, and maintainable way with other top-tier talent. You will architect and build our data platform from the ground up, select tools, codify best practices, and lead and mentor others. As the first Lead Data Engineer at Candid Health, you will report to the Head of Engineering and will help to build out a world-class Data Team. What We Do We’re fixing one of the most broken and costly pieces of the US healthcare system: medical billing. Today, healthcare providers spend over $250B each year on administrative overhead just to get paid by insurance. Medical billing is expensive because it’s nuanced and hard - maybe ~100x harder than credit card payment processing - and because it’s traditionally done by armies of humans who track and manage complex rules and processes specific to individual insurance companies with little or no supporting software. We’re rethinking medical billing from the ground up, building software backed by best-in-class data science (and, soon, a dash of machine learning) to automate much of this complexity so healthcare providers can get paid dramatically more easily and inexpensively. We were in the Y Combinator W20 batch and have since been well funded by a world-class group of funds (8VC, First Round Capital, BoxGroup) + angel investors. We're now helping our customers treat opioid addiction, provide holistic care for women, lose weight, increase access to mental health care, and much more. This is such important and gratifying work; we can't wait for you to join our team and help support some of the most important innovation happening in healthcare today! What You’ll Do Collaborate with leadership, delivery, engineering, product, and customers: Collaborate with leadership and other stakeholders to understand Candid’s data needs. Work with product and engineering to understand the long-term product strategy and broader technical architecture. Work with customers to understand their needs and how best to address them in a scalable way. Expansively own Data Engineering team outcomes: Deliver on Data Team outcomes for both internal teams and external customers, which include both operational and analytics workflows. Lead design of new data infrastructure: Research and identify industry best practices and state-of-the-art tools to leverage. Design new systems to support the necessary outcomes in a secure, durable, performant, and maintainable way. Build and support new data products: Build datasets, pipelines, and other solutions to support operational and product outcomes. Develop support systems to scale the team and achievable outcomes. Who You Are You have Bachelors of Science or Bachelors of Art in Computer Science, Computer Engineering, Math or other similar degree. You have 6+ years of experience working with data pipelines, products, and tools. You’ve built and maintained complex data integrations or pipelines. You have well-developed opinions on modern data warehouse architecture, tools, and patterns. You have experience with selection and implementation of new tools and process. You have excellent technical vision; you know the right architecture and patterns to build functional, durable, and maintainable product. You have a customer-first and learner’s mindset, and value teaching others. You make the right trade-offs when considering project scope, which corners are worth cutting, and which are not. You’re a clear and concise communicator; you enjoy the challenge of explaining complicated ideas in simple terms, both in-person and in writing. Experience with the technologies we currently use is a plus, but by no means required: Google Cloud Platform, BigQuery, PostgreSQL, Metabase, Terraform, Python. Our Values We Spend At Least As Much Time With Our Coworkers As We Do With Our Closest Friends + Family - If We Intend To Do The Most Important + Challenging Work Of Our Lives, It’s Important That These Folks Energize Us, Support Us, Inspire Us, And Push Us To Do Our Best Work. This Is What You Can Expect Of Your Teammates At Candid (in No Particular Order) We put our customers first We take care of each other and ourselves We anchor on outcomes and work relentlessly and creatively to achieve them We collectively prioritize building a diverse and inclusive workspace We believe humility is our greatest strength We are candid, kind, and committed We strive to be the most prepared person in the room We are truth seekers Pay Transparency The estimated starting annual salary range for this position is $150,000 to $212,000 USD. The listed range is a guideline from Pave data, and the actual base salary may be modified based on factors including job-related skills, experience/qualifications, interview performance, market data, etc. Total compensation for this position may also include equity, sales incentives (for sales roles), and employee benefits. Given Candid Health’s funding and size, we heavily value the potential upside from equity in our compensation package. Further note that Candid Health has minimal hierarchy and titles, but has broad ranges of experience represented within roles.",
        "url": "https://www.linkedin.com/jobs/view/3955675268",
        "summary": "Candid Health is seeking a Lead Data Engineer to design, build, and support its data infrastructure. This role will be instrumental in architecting and building the data platform from the ground up, selecting tools, codifying best practices, and leading and mentoring a team. The ideal candidate will have 6+ years of experience with data pipelines, products, and tools, and a strong understanding of modern data warehouse architecture.",
        "industries": [
            "Healthcare",
            "Software",
            "Technology",
            "Data Science",
            "Machine Learning",
            "Medical Billing"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Leadership",
            "Problem Solving",
            "Decision Making",
            "Customer Focus",
            "Teamwork",
            "Mentoring",
            "Teaching",
            "Learning",
            "Adaptability",
            "Resilience",
            "Creativity",
            "Humility",
            "Integrity"
        ],
        "hard_skills": [
            "Data Pipelines",
            "Data Warehousing",
            "Data Integration",
            "Data Modeling",
            "Data Architecture",
            "Data Security",
            "SQL",
            "Python",
            "Google Cloud Platform",
            "BigQuery",
            "PostgreSQL",
            "Metabase",
            "Terraform"
        ],
        "tech_stack": [
            "Google Cloud Platform",
            "BigQuery",
            "PostgreSQL",
            "Metabase",
            "Terraform",
            "Python"
        ],
        "programming_languages": [
            "Python",
            "SQL"
        ],
        "experience": 6,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Computer Engineering",
                "Math"
            ]
        },
        "salary": {
            "max": 212000,
            "min": 150000
        },
        "benefits": [
            "Equity",
            "Sales Incentives"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Sunnyvale, CA",
        "job_id": 3960507442,
        "company": "Acubed",
        "title": "Senior Data Engineer",
        "created_on": 1720635982.3010566,
        "description": "About Acubed Founded in 2015, Acubed is the Silicon Valley innovation center of Airbus. As a global leader in aerospace, Airbus aims to make things fly. Our mission is to provide a lens into the future for the industry, transforming risk into opportunity to build the future of flight now. At Acubed, we strive to propel innovation to market faster, broaden the talent pool in emerging aerospace careers and simultaneously help drive a culture change across Airbus. WAYFINDER Our Wayfinder team is building scalable, certifiable autonomy systems to power the next generation of commercial aircraft. Our team of experts is driving the maturation of machine learning and other core technologies for autonomous flight; we are creating a reference architecture that includes hardware, software, and a data-driven development process to allow aircraft to perceive and react to their environment. Autonomous flight is transforming the transportation industry, and our team is at the heart of this revolution. The Opportunity/ Role Description We are looking for a Data Engineer who specializes in managing and optimizing data pipelines, with a specific focus on labeling for computer vision. The ideal candidate will have experience in designing and implementing data infrastructure, ensuring high-quality labeled datasets, and collaborating with cross-functional teams to support our computer vision initiatives. Key Responsibilities: Design, develop, and maintain scalable data and labeling pipelines for computer vision projects Manage and optimize data storage solutions to ensure efficient data retrieval and processing Collaborate with machine learning engineers to understand data requirements and ensure the availability of high-quality labeled datasets Develop and maintain tools and processes for data labeling, including annotation workflows, quality control, and validation procedures Implement data governance and security protocols to ensure data hygiene Monitor and troubleshoot data pipeline issues, ensuring timely resolution and minimal disruption to project timelines Stay up-to-date with the latest trends and technologies in data engineering and computer vision, and apply this knowledge to improve our data infrastructure and processes Qualifications: Bachelor’s in Computer Science, Data Engineering, and 3 to 5 years of experience in a related field Proven experience as a Data Engineer, preferably with a focus on computer vision and data labeling Strong programming skills in Python, SQL, and experience with distributed computing Proficiency in data modeling, ETL processes, and data pipeline design Experience with data annotation tools and frameworks Familiarity with cloud platforms (AWS, GCP, Azure), workflow management tools (Airflow, Dagster) and tools that simplify data movement (e.g., Airbyte) Strong understanding of OLTP databases like Postgresql is critical in this role Excellent problem-solving skills and attention to detail Strong communication skills and the ability to work effectively in a collaborative team environment Pay Transparency Notice: Depending on your work location and years of experience, the target annual salary for this position can range from $146,000 to $171,000 + target bonus + benefits (including medical, dental, vision, 401(k), and flight training). * Note that Acubed does not offer sponsorship of employment-based nonimmigrant visa petitions for this role.",
        "url": "https://www.linkedin.com/jobs/view/3960507442",
        "summary": "Acubed, Airbus's Silicon Valley innovation center, is looking for a Data Engineer to manage and optimize data pipelines, particularly for computer vision labeling.  The ideal candidate will have experience designing and implementing data infrastructure, ensuring high-quality labeled datasets, and collaborating with cross-functional teams. Key responsibilities include designing and maintaining data and labeling pipelines, managing data storage solutions, collaborating with machine learning engineers, developing tools and processes for data labeling, implementing data governance and security protocols, monitoring and troubleshooting data pipeline issues, and staying up-to-date on the latest trends in data engineering and computer vision. ",
        "industries": [
            "Aerospace",
            "Aviation",
            "Technology",
            "Computer Vision",
            "Machine Learning",
            "Data Engineering"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem-Solving",
            "Attention to Detail"
        ],
        "hard_skills": [
            "Python",
            "SQL",
            "Distributed Computing",
            "Data Modeling",
            "ETL",
            "Data Pipeline Design",
            "Data Annotation",
            "Cloud Platforms (AWS, GCP, Azure)",
            "Workflow Management Tools (Airflow, Dagster)",
            "Data Movement Tools (Airbyte)",
            "Postgresql"
        ],
        "tech_stack": [
            "Python",
            "SQL",
            "AWS",
            "GCP",
            "Azure",
            "Airflow",
            "Dagster",
            "Airbyte",
            "Postgresql"
        ],
        "programming_languages": [
            "Python",
            "SQL"
        ],
        "experience": 3,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Computer Science",
                "Data Engineering"
            ]
        },
        "salary": {
            "max": 171000,
            "min": 146000
        },
        "benefits": [
            "Medical",
            "Dental",
            "Vision",
            "401(k)",
            "Flight Training"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Irvine, CA",
        "job_id": 3971541263,
        "company": "Allergan Data Labs",
        "title": "Sr. Data Engineer (Data Product Engineering)",
        "created_on": 1720635984.0924604,
        "description": "Allergan Data Labs is on a mission to transform the Allergan Aesthetics beauty business at AbbVie, one of the largest pharmaceutical companies in the world. Our iconic brands include BOTOX® Cosmetic, CoolSculpting®, JUVÉDERM® and more. The medical aesthetics business is ripe for rapid growth and disruption, and we are looking to add to our high performing team to do just that. Our team has successfully launched a new and innovative technology platform, Allē, which serves millions of consumers, tens of thousands of aesthetics providers and thousands of colleagues throughout the US. Since its launch in November 2020, Allē has delivered curated promotions, personalized experiences and had millions of consumers use it as part of their beauty journey. We're looking to add to our team as we prepare to launch a new array of game-changing technologies on our successfully adopted platform. If you're interested in working within a startup-oriented environment, while having the backing of a very large company, please read on. As the Sr. Data Engineer , you will report to the Director, Data and Machine Learning , as well as continuously collaborate with key stakeholders across the business to solve the most important technical problems. Responsibilities Collaborate with cross functional partners (Product Managers, Data Scientists, Machine Learning Engineers, Software Engineers, Business teams) to build data products Develop, optimize, and maintain complex ETL processes for moving and transforming data. Champion code quality, reusability, scalability, security and help make strategic architecture decisions Implement processes and tools to ensure data quality, enforce data governance policies and engineering best practices Develop APIs and Microservices to expose and integrate data products with software systems Required Experience & Skills Completed BS, MS, or PhD in Computer Science, Mathematics, Statistics, Engineering, Operations Research, or other quantitative field 5+ years of experience as a Data Engineer writing code to extract, process and store data within different types of data stores (Snowflake, Postgres, DynamoDB, Kafka, Graph databases) Strong programming skills in Python and understanding of core computer science principles Experience with building batch and streaming pipelines using complex SQL, PySpark, Pandas, and similar frameworks Knowledge of relational and dimensional data modeling concepts to build data products Experience with data quality checks, data monitoring solutions Experience with orchestrating complex workflows and data pipelines using like Airflow or similar tools Experience with Git, CI/CD pipelines, Docker, Kubernetes Experience with architecting solutions on AWS or equivalent public cloud platforms. Experience with developing data APIs, Microservices and event driven systems to integrate data products Strong interpersonal and verbal communication skills Preferred Experience & Skills : Knowledge of data mesh concepts Experience in assessing and implementing new data tools to enhance the data engineering stack Knowledge in domains such as recommender systems, fraud detection, personalization, and marketing science Knowledge of vector databases, knowledge graphs, and other approaches for organizing & storing information Familiarity with Snowflake, RDS, DynamoDB, Kafka, Fivetran, dbt, Airflow, Docker, Kubernetes, EMR, Sagemaker, DataDog, PagerDuty, Data Cataloging tools, Data Observability tools and Data Governance tools Our Core Values Be Humble: You're smart yet always interested in learning from others. Work Transparently: You always deal in an honest, direct and transparent way. Take Ownership: You embrace responsibility and find joy in having the answers. Learn More: Through blog posts, newsletters, podcasts, video tutorials and meetups you regularly self-educate and improve your skill set. Show Gratitude: You show appreciation and return kindness to those you work with. Perks Competitive salary. Competitive annual bonus targets. 401k with dollar for dollar match, up to 6% of eligible earnings (base, bonus). Plus additional company contribution. RSU grants (Long Term Incentives) for approved roles. Comprehensive medical, dental, vision and life insurance. 17 paid holidays per year, including 3 floating holidays. Annual Paid Time Off (PTO), with separate sick days 12 weeks paid Parental Leave Caregiver Leave Adoption and Surrogacy Assistance Plan Flexible workplace accommodations. Free gym membership for those in our Irvine, CA WeWork office. We celebrate our wins with opportunities to attend Lakers, Knicks, Anaheim Ducks, Anaheim Angels and NY Rangers games. Opportunities to attend concerts, festivals and other live entertainment events in recognition of delivering great work. Attend AWS Re:Invent in person (Las Vegas) or virtually each year. Tuition reimbursement. Attend a tech or marketing conference of your choice each year. A MacBook Pro and accompanying hardware to do great work. A modern productivity toolset to get work done: Slack, Miro, Loom, Lucid, Google Docs, Atlassian and more. Generous discounts on SkinMedica skin care products. Applicable only to applicants applying to a position in any location with pay disclosure requirements under state or local law: The compensation range described below is the range of possible base pay compensation that the Company believes in good faith it will pay for this role at the time of this posting based on the job grade for this position. Individual compensation paid within this range will depend on many factors including geographic location, and we may ultimately pay more or less than the posted range. This range may be modified in the future. We offer a comprehensive package of benefits including paid time off. (vacation, holidays, sick), medical/dental/vision insurance and 401(k) to eligible employees. This job is eligible to participate in our short-term incentive programs. This job is eligible to participate in our long-term incentive programs. Note: No amount of pay is considered to be wages or compensation until such amount is earned, vested and determinable. The amount and availability of any bonus, commission, incentive, benefits, or any other form of compensation and benefits that are allocable to a particular employee remains in the Company's sole and absolute discretion unless and until paid and may be modified at the Company's sole and absolute discretion, consistent with applicable law. Compensation Range (Minimum - Maximum) $109,500—$208,000 USD",
        "url": "https://www.linkedin.com/jobs/view/3971541263",
        "summary": "Allergan Data Labs is seeking a Sr. Data Engineer to build data products, develop complex ETL processes, champion code quality, and implement data governance policies. The role involves collaborating with cross-functional partners and leveraging technologies like Snowflake, Postgres, DynamoDB, Kafka, and AWS.",
        "industries": [
            "Pharmaceuticals",
            "Healthcare",
            "Medical Devices",
            "Technology",
            "Data & Analytics"
        ],
        "soft_skills": [
            "Collaboration",
            "Communication",
            "Problem Solving",
            "Analytical Thinking",
            "Strategic Thinking",
            "Teamwork"
        ],
        "hard_skills": [
            "Python",
            "SQL",
            "PySpark",
            "Pandas",
            "Data Modeling",
            "Data Quality",
            "Data Governance",
            "API Development",
            "Microservices",
            "Airflow",
            "Git",
            "CI/CD",
            "Docker",
            "Kubernetes",
            "AWS",
            "Data Mesh",
            "Recommender Systems",
            "Fraud Detection",
            "Personalization",
            "Marketing Science",
            "Vector Databases",
            "Knowledge Graphs",
            "Snowflake",
            "RDS",
            "DynamoDB",
            "Kafka",
            "Fivetran",
            "dbt",
            "EMR",
            "Sagemaker",
            "DataDog",
            "PagerDuty",
            "Data Cataloging Tools",
            "Data Observability Tools",
            "Data Governance Tools"
        ],
        "tech_stack": [
            "Snowflake",
            "Postgres",
            "DynamoDB",
            "Kafka",
            "Graph databases",
            "PySpark",
            "Pandas",
            "Airflow",
            "Git",
            "CI/CD",
            "Docker",
            "Kubernetes",
            "AWS",
            "Fivetran",
            "dbt",
            "EMR",
            "Sagemaker",
            "DataDog",
            "PagerDuty",
            "Data Cataloging tools",
            "Data Observability tools",
            "Data Governance tools"
        ],
        "programming_languages": [
            "Python"
        ],
        "experience": 5,
        "education": {
            "min_degree": "BS",
            "fields": [
                "Computer Science",
                "Mathematics",
                "Statistics",
                "Engineering",
                "Operations Research"
            ]
        },
        "salary": {
            "max": 208000,
            "min": 109500
        },
        "benefits": [
            "Competitive Salary",
            "Annual Bonus",
            "401k Match",
            "RSU Grants",
            "Medical, Dental, Vision, Life Insurance",
            "Paid Holidays",
            "Paid Time Off",
            "Parental Leave",
            "Caregiver Leave",
            "Adoption and Surrogacy Assistance",
            "Flexible Workplace",
            "Gym Membership",
            "Lakers, Knicks, Ducks, Angels, Rangers Games",
            "Concerts, Festivals",
            "AWS Re:Invent",
            "Tuition Reimbursement",
            "Tech/Marketing Conference",
            "MacBook Pro",
            "Slack, Miro, Loom, Lucid, Google Docs, Atlassian",
            "SkinMedica Discounts"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Palo Alto, CA",
        "job_id": 3920386361,
        "company": "EarnIn",
        "title": "Software Engineer",
        "created_on": 1720635985.7389822,
        "description": "About Earnin As one of the first pioneers of earned wage access, our passion at EarnIn is building products that deliver real-time financial flexibility for those with the unique needs of living paycheck to paycheck. Our community members access their earnings as they earn them, with options to spend, save, and grow their money without mandatory fees, interest rates, or credit checks. Since our founding, our app has been downloaded over 13M times and we have provided access to over $15 billion in earnings. We’re fortunate to have an incredibly experienced leadership team, combined with world-class funding partners like A16Z, Matrix Partners, DST, Ribbit Capital, and a very healthy core business with a tremendous runway. We’re growing fast and are excited to continue bringing world-class talent onboard to help shape the next chapter of our growth journey. Position Summary We are looking for an experienced, passionate, and resourceful software engineer to join our Core Services org. In our team, you’ll work cross-functionally with various teams and contribute to the design and development of key platform services. This person will need to be strong in JVM programming languages and event-driven architecture on top of AWS. This position is a hybrid position and requires in-person work from our Palo Alto HQ office 2-3 days a week . The Palo Alto base salary range for this full-time position is $166,500 - $203,500 + equity + benefits. Our salary ranges are determined by role, level, and location. What You'll Do Drive the design & implementation of new features - break down complex problems into their bare essentials, translate this complexity into elegant design, and create high-quality, clean code Make a meaningful impact in the lives of our community members Collaborate and mentor other engineers while providing thoughtful guidance using code, design, and architecture reviews Contribute to defining technical direction, planning the roadmap, escalating issues, and synthesizing feedback to ensure team success Care passionately about producing high-quality, efficient designs and code Constantly learning about new technologies and industry standards What We're Looking For 3+ years of development experience in Software Engineering Bachelor's, Master’s, or PhD degree in computer science, computer engineering, or a related technical discipline or equivalent industry experience. Proficient in at least one JVM programming language such as Java, Kotlin, Scala Hands-on experience working with SQL databases like Postgres or MySQL Hands-on experience with data technologies such as Kafka, AWS SNS/SQS, AWS Kinesis Experience with continuous integration and delivery tools. Experienced in developing and executing functional and integration tests. Excellent written and verbal communication skills. Ability to thrive in a fast-paced, dynamic environment and have a bias towards action and results. Experience with Kubernetes and microservice architecture is a strong plus. At EarnIn, we believe that the best way to build a financial system that works for everyday people is by hiring a team that represents our diverse community. Our team is diverse not only in background and experience but also in perspective. We celebrate our diversity and strive to create a culture of belonging. EarnIn does not unlawfully discriminate based on race, color, religion, sex (including pregnancy, childbirth, breastfeeding, or related medical conditions), gender identity, gender expression, national origin, ancestry, citizenship, age, physical or mental disability, legally protected medical condition, family care status, military or veteran status, marital status, registered domestic partner status, sexual orientation, genetic information, or any other basis protected by local, state, or federal laws. EarnIn is an E-Verify participant. EarnIn does not accept unsolicited resumes from individual recruiters or third-party recruiting agencies in response to job postings. No fee will be paid to third parties who submit unsolicited candidates directly to our hiring managers or HR team",
        "url": "https://www.linkedin.com/jobs/view/3920386361",
        "summary": "EarnIn is seeking a Software Engineer to join their Core Services org. This role involves developing key platform services, collaborating with other teams, and driving the design and implementation of new features. The ideal candidate has 3+ years of experience in Software Engineering, strong JVM programming skills, and experience with event-driven architecture and AWS. This is a hybrid position requiring 2-3 days a week in the Palo Alto HQ.",
        "industries": [
            "FinTech",
            "Technology",
            "Software Development",
            "Financial Services"
        ],
        "soft_skills": [
            "Problem-solving",
            "Communication",
            "Collaboration",
            "Mentoring",
            "Time management",
            "Results-oriented",
            "Passionate",
            "Resourceful"
        ],
        "hard_skills": [
            "Java",
            "Kotlin",
            "Scala",
            "SQL",
            "Postgres",
            "MySQL",
            "Kafka",
            "AWS SNS",
            "AWS SQS",
            "AWS Kinesis",
            "Continuous Integration",
            "Continuous Delivery",
            "Functional Testing",
            "Integration Testing",
            "Kubernetes",
            "Microservice Architecture"
        ],
        "tech_stack": [
            "JVM",
            "AWS",
            "Kafka",
            "AWS SNS",
            "AWS SQS",
            "AWS Kinesis",
            "Kubernetes",
            "Microservices",
            "SQL",
            "Postgres",
            "MySQL"
        ],
        "programming_languages": [
            "Java",
            "Kotlin",
            "Scala",
            "SQL"
        ],
        "experience": 3,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Computer Science",
                "Computer Engineering",
                "Related Technical Disciplines"
            ]
        },
        "salary": {
            "max": 203500,
            "min": 166500
        },
        "benefits": [
            "Equity",
            "Benefits"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3915648723,
        "company": "OpenAI",
        "title": "Software Engineer, Backend",
        "created_on": 1720635987.5159338,
        "description": "About The Team We bring OpenAI's technology to the world through products like ChatGPT and the OpenAI API. We seek to learn from deployment and distribute the benefits of AI, while ensuring that this powerful tool is used responsibly and safely. Safety is more important to us than unfettered growth. About The Role As OpenAI scales, we’re looking for experienced, problem-solving engineers to build new products and scale our systems. Our success depends on our ability to quickly iterate on products while also ensuring that they are performant and reliable. You’ll work in a deeply iterative, collaborative, fast-paced environment to bring our technology to millions of users around the world, and ensure it’s delivered with safety and reliability in mind. In This Role, You Will Design and build the development and production platforms that power ChatGPT, enabling reliability and security at scale Partner with researchers, engineers, product managers, and designers to bring new features and research capabilities to the world Accelerate engineering productivity by empowering your fellow engineers with excellent tooling and systems Help create a diverse, equitable, and inclusive culture that makes all feel welcome while enabling radical candor and the challenging of group think Like all other teams, we are responsible for the reliability of the systems we build. This includes an on-call rotation to respond to critical incidents as needed You Might Thrive In This Role If You Have meaningful experience with building (and rebuilding) production systems to deliver new product capabilities and to handle increasing scale Care deeply about the end user experience and take pride in building products to solve customer needs Have a humble attitude, an eagerness to help your colleagues, and a desire to do whatever it takes to make the team succeed Own problems end-to-end, and are willing to pick up whatever knowledge you're missing to get the job done Build tools to accelerate your own (and your teammates’) workflows, but only when off-the-shelf solutions won’t do Have been a startup founder or an early-stage engineer About OpenAI OpenAI is an AI research and deployment company dedicated to ensuring that general-purpose artificial intelligence benefits all of humanity. We push the boundaries of the capabilities of AI systems and seek to safely deploy them to the world through our products. AI is an extremely powerful tool that must be created with safety and human needs at its core, and to achieve our mission, we must encompass and value the many different perspectives, voices, and experiences that form the full spectrum of humanity. We are an equal opportunity employer and do not discriminate on the basis of race, religion, national origin, gender, sexual orientation, age, veteran status, disability or any other legally protected status. For US Based Candidates: Pursuant to the San Francisco Fair Chance Ordinance, we will consider qualified applicants with arrest and conviction records. We are committed to providing reasonable accommodations to applicants with disabilities, and requests can be made via this link. OpenAI Global Applicant Privacy Policy At OpenAI, we believe artificial intelligence has the potential to help people solve immense global challenges, and we want the upside of AI to be widely shared. Join us in shaping the future of technology.",
        "url": "https://www.linkedin.com/jobs/view/3915648723",
        "summary": "OpenAI is looking for experienced engineers to build new products and scale their systems. This role involves designing and building development and production platforms for ChatGPT, partnering with researchers and engineers, accelerating engineering productivity, and contributing to a diverse and inclusive culture. The ideal candidate has experience with building production systems, cares about user experience, has a humble attitude, owns problems end-to-end, and has experience with building tools and workflows. OpenAI is a company dedicated to ensuring that general-purpose artificial intelligence benefits all of humanity. They push the boundaries of AI capabilities and deploy them safely to the world through their products.",
        "industries": [
            "Artificial Intelligence",
            "Technology",
            "Software Development"
        ],
        "soft_skills": [
            "Problem Solving",
            "Collaboration",
            "Iteration",
            "Communication",
            "Leadership",
            "Teamwork",
            "Humility",
            "Ownership",
            "Adaptability",
            "Initiative",
            "Passion",
            "Creativity"
        ],
        "hard_skills": [
            "Development Platform Design",
            "Production System Building",
            "System Reliability",
            "Security at Scale",
            "Research Capabilities",
            "Engineering Productivity",
            "Tooling and Systems Development",
            "Workflow Optimization",
            "Startup Experience"
        ],
        "tech_stack": [
            "ChatGPT",
            "OpenAI API"
        ],
        "programming_languages": [],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Mountain View, CA",
        "job_id": 3938225434,
        "company": "ICONMA",
        "title": "Software Engineer",
        "created_on": 1720635989.1110084,
        "description": "Skill/Experience/Education Mandatory Bachelor's degree or equivalent practical experience. 4 years of experience in software development in multiple programming languages, and with data structures/algorithms. 2-4 years of experience testing, auditing and analyzing data. Proficiency in at least 5 out of these programming languages: Java Python C/C++ SQL Javascript HTML/CSS Scala Ruby Desired Master’s degree or PhD in Engineering, Computer Science, or a related technical field. 3 years of experience in a technical leadership role leading project teams and setting technical direction. Experience working on improving data quality for Large language models Ability to convince business stakeholders and communicate complex analysis insights to non-technical audiences. Background in AI/LLM industry As an equal opportunity employer, ICONMA pride itself on creating an employment environment that supports and encourages the abilities of all persons regardless of race, colour, gender, age, Sexual orientation, citizenship, or disability",
        "url": "https://www.linkedin.com/jobs/view/3938225434",
        "summary": "Software development role requiring strong programming skills (Java, Python, C/C++, SQL, Javascript, HTML/CSS, Scala, Ruby), data analysis experience, and experience with Large Language Models. Leadership experience and communication skills are desired. ",
        "industries": [
            "Software Development",
            "Data Analysis",
            "Artificial Intelligence",
            "Large Language Models"
        ],
        "soft_skills": [
            "Communication",
            "Leadership",
            "Problem Solving",
            "Teamwork",
            "Analytical Thinking"
        ],
        "hard_skills": [
            "Java",
            "Python",
            "C/C++",
            "SQL",
            "Javascript",
            "HTML",
            "CSS",
            "Scala",
            "Ruby",
            "Data Structures",
            "Algorithms",
            "Data Testing",
            "Data Auditing",
            "Data Analysis",
            "Large Language Models",
            "AI"
        ],
        "tech_stack": [
            "Large Language Models",
            "AI"
        ],
        "programming_languages": [
            "Java",
            "Python",
            "C/C++",
            "SQL",
            "Javascript",
            "HTML",
            "CSS",
            "Scala",
            "Ruby"
        ],
        "experience": 4,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Engineering",
                "Computer Science"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3787341033,
        "company": "Lever Middleware Test Company 2",
        "title": "Software Engineer",
        "created_on": 1720635991.0132709,
        "description": "Disproportionately impact a product-driven startup that is transforming the way companies grow their teams. We’re building next-generation collaboration software that helps companies to bring more transparency, participation, and engagement to their hiring. Jump into an empowered role doing end-to-end feature development. You’ll be familiar with all parts of our stack—most notably our homegrown, open source web framework, DerbyJS. Not only will you learn how to develop on a modern, real-time framework, you’ll be building enterprise-grade software on top of it. To do so, you’ll exercise judgment in making tradeoffs between design and feasibility. Choose when to hack and when to invest. You’ll engineer your features to be scalable and resilient in a large, single-page app. Lever is an incredibly design-driven company and you’ll be an active voice in shaping our product and user experience, down to the last detail when shipping your features. Our close-knit, cross-functional team is a great chance to grow your knowledge of different domains. Level-up your knowledge of design theory and UX by working closely with our Design and Customer Success teams. Share your knowledge with them so that they’re more empowered to hook up their front-end code to support users with more internal tools. You’ll join a team where everyone—including you—is knowledgeable about development patterns and opinionated about product development process. We pitch in to help when things are crazy. We build tools and scripts to ease the lives of our fellow developers. And we’re passionate about lots of random things, and share those passions with each other. Core Technologies JavaScript, Node.js, MongoDB, Redis, Solr, Elasticsearch, DerbyJS, ShareJS, IMAP, SMTP, Gmail and Google Calendar, Microsoft Exchange, AWS WITHIN 3 MONTHS, YOU’LL… Build, launch, and support your first big feature Master Derby’s development patterns Participate in on-call rotation, diagnose, and resolve production issues Participate in support engineering rotation, understand and resolve customer issues Write internal documentation for your features and systems WITHIN 6 MONTHS, YOU’LL... Own major feature areas of the application Touch most of our stack/infrastructure Be able to own the entire planning, scoping, design, and implementation of new features Contend for our bug hunt hackathon crown Participate in team-wide project estimation and scoping WITHIN 12 MONTHS, YOU’LL… Build within reliable estimates Train and mentor your peers Lever builds software for teams to find, interview, and hire top talent. Our team strives to set a new bar for enterprise software with modern, well-designed, realtime apps. After going through Y-Combinator (Summer 2012) and raising a seed round in 2012, we have the support of many fantastic advisors and investors and are gearing up for an incredible, fast-paced year. Lever is proud to be an equal opportunity workplace dedicated to pursuing and hiring a diverse workforce. A look into life at Lever: https://inside.lever.co/",
        "url": "https://www.linkedin.com/jobs/view/3787341033",
        "summary": "Lever is a product-driven startup that builds next-generation collaboration software to improve company hiring processes. They are looking for a Full Stack Engineer to develop features using their DerbyJS framework and contribute to their product and user experience.",
        "industries": [
            "Software",
            "Technology",
            "SaaS",
            "Human Resources",
            "Recruitment",
            "Talent Acquisition"
        ],
        "soft_skills": [
            "Collaboration",
            "Communication",
            "Problem-solving",
            "Decision-making",
            "Time management",
            "Prioritization",
            "Teamwork",
            "Mentorship",
            "Training"
        ],
        "hard_skills": [
            "JavaScript",
            "Node.js",
            "MongoDB",
            "Redis",
            "Solr",
            "Elasticsearch",
            "DerbyJS",
            "ShareJS",
            "IMAP",
            "SMTP",
            "Gmail",
            "Google Calendar",
            "Microsoft Exchange",
            "AWS"
        ],
        "tech_stack": [
            "JavaScript",
            "Node.js",
            "MongoDB",
            "Redis",
            "Solr",
            "Elasticsearch",
            "DerbyJS",
            "ShareJS",
            "IMAP",
            "SMTP",
            "Gmail",
            "Google Calendar",
            "Microsoft Exchange",
            "AWS"
        ],
        "programming_languages": [
            "JavaScript"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Los Angeles, CA",
        "job_id": 3898755949,
        "company": "Intratek Computer Inc.",
        "title": "Sr. Data Engineer/Developer",
        "created_on": 1720635994.9641435,
        "description": "Apply Now Intratek Computer, Inc., an Irvine, California based company, was founded and incorporated in 1989 as a computer service, support, and networking firm to provide state-of-the-art customized information technology solutions for Federal, State and County agencies, as well as leading private commercial accounts. We have comprehensive experience providing IT support services, including hardware and software support, maintenance and repair, programming, professional staffing, networking, web design and development, and helpdesk implementation and management. Intratek Computer, Inc. is seeking a highly skilled and experienced Senior Data Engineer / Developer (see skills below) to support us in Los Angeles, CA 90012. The candidate should have demonstrated skills with WhereScape RED automation tools and the ability to design and implement fully operational solutions on Snowflake Data Warehouse. Additionally, the ideal candidate will have a strong background in delivering enterprise data warehouses, data lakes, with experience in designing and engineering end-to-end data analytics solutions. This is a remote position; however, they may have to travel on site or conferences on rare occasions. Senior Data Engineer / Developer 12 Months contract Remote with occasional onsite Los Angeles, CA 90012 Monday through Friday – day Shift Pay rate depends on experience Medical benefits Paid vacation Paid holidays Duties and requirements: Required Skills Proficiency in WhereScape RED for data warehouse automation, including designing, building, and managing data warehouses. Expertise in Snowflake’s cloud data platform, including data loading, transformation, and querying using Snowflake SQL. Experience with SQL-based development, optimization, and tuning for large-scale data processing. Strong understanding of dimensional modeling concepts and experience in designing and implementing data models for analytics and reporting purposes. Ability to optimize data pipelines and queries for performance and scalability. Familiarity with Snowflake’s features such as virtual warehouses, data sharing, and data governance capabilities. Knowledge of WhereScape scripting language (WSL) for customizing and extending automation processes. Experience with data integration tools and techniques to ingest data from various sources into Snowflake. Understanding of data governance principles and experience implementing data governance frameworks within Snowflake. Ability to implement data quality checks and ensure data integrity within the data warehouse environment. Strong SQL skills for data manipulation, optimization, and performance tuning. Experience with data visualization tools such as Power BI. Equal Opportunity Employer: Intratek Computer Inc. is an equal opportunity employer. “All qualified applicants will receive consideration for employment without regard to their race, religion, ancestry, national origin, sex, sexual orientation, age, disability, marital status, domestic partner status, or medical condition.” Veterans Preference: Special preference will be given returning war veterans when hiring new employees in an attempt to recognize their service, sacrifice, and skills Apply Now Tagged as: Sr. Data Engineer/Developer",
        "url": "https://www.linkedin.com/jobs/view/3898755949",
        "summary": "Intratek Computer, Inc. is seeking a Senior Data Engineer / Developer to support their Los Angeles, CA office. This is a 12-month remote contract position with occasional onsite work. The ideal candidate will have strong experience with WhereScape RED automation tools and Snowflake Data Warehouse. They will be responsible for designing and implementing fully operational data warehouse solutions, including data loading, transformation, querying, and optimization.  Experience with data integration, visualization tools like Power BI, and data governance are also important.",
        "industries": [
            "Information Technology",
            "Software Development",
            "Data Management",
            "Cloud Computing",
            "Data Analytics",
            "Data Warehousing",
            "Data Engineering"
        ],
        "soft_skills": [
            "Strong communication skills",
            "Problem-solving skills",
            "Analytical skills",
            "Teamwork",
            "Time management",
            "Adaptability"
        ],
        "hard_skills": [
            "WhereScape RED",
            "Snowflake",
            "Snowflake SQL",
            "SQL",
            "Dimensional Modeling",
            "Data Modeling",
            "Data Pipelines",
            "Data Governance",
            "Data Quality",
            "Power BI",
            "Data Integration",
            "WSL (WhereScape Scripting Language)"
        ],
        "tech_stack": [
            "WhereScape RED",
            "Snowflake",
            "Snowflake Data Warehouse",
            "Power BI"
        ],
        "programming_languages": [
            "SQL",
            "WSL (WhereScape Scripting Language)"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Medical benefits",
            "Paid vacation",
            "Paid holidays"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Sunnyvale, CA",
        "job_id": 3954549955,
        "company": "Chelsoft Solutions Co.",
        "title": "Software Engineer (Java, Spring Boot, SQL )_Hybrid in Sunnyvale, CA_Only on W2_No Third Parties",
        "created_on": 1720635996.9589105,
        "description": "Description The Backend Software Engineer position will be responsible for hands on development as well as lead the development of Next Generation Enterprise solutions for Global eCommerce. SE will be responsible for application development in support of the multiplatform roadmap. Responsible for application architecture, ensure high performance, scalability and availability for those applications. Areas Of Responsibility Include Involved in planning of system and development deployment as well as responsible for meeting software compliance standards Drives technical vision and influences product roadmap and vision Implement largescale, complex, cross functional projects Leads the discovery phase of large projects to develop high level design Supervise the work of cross functional groups of engineers, including offshore associates Directs root cause analysis of critical business and production issues Documents testing and maintenance of application corrections and improvements Influence decisions, builds consensus, and resolves conflicts constructively and proactively Supports business objectives and ensures the business needs are being met Minimum Qualifications Bachelor's degree in Computer Science or Computer Information Systems 7+ years of experience in eCommerce Software development 7+ years of programming experience in architecting highly scalable & performing Web Applications 5+ years of experience with Open Source technologies 4+ years of developing / architecting high transaction high throughput systems is a must 2+ years of experience with integrating 3rd party solutions in existing architecture 4+ years of developing / architecting high transaction high throughput systems is a must 2+ years of experience in agile ways of working in a distributed geographical model Additional Preferred Qualifications Java, Spring Boot, SQL & NoSQL Databases Bachelor's degree in computer science or related discipline.",
        "url": "https://www.linkedin.com/jobs/view/3954549955",
        "summary": "This is a Backend Software Engineer position at a global eCommerce company. You will be responsible for developing and leading the development of next-generation enterprise solutions, working on application architecture, ensuring high performance, scalability and availability for those applications. This role involves planning and deployment, driving technical vision, influencing product roadmaps, implementing large-scale projects, leading discovery phases, supervising cross-functional teams, directing root cause analysis, and ensuring alignment with business objectives.",
        "industries": [
            "Ecommerce",
            "Technology",
            "Software Development"
        ],
        "soft_skills": [
            "Communication",
            "Leadership",
            "Problem Solving",
            "Collaboration",
            "Technical Vision",
            "Influence",
            "Decision Making",
            "Conflict Resolution",
            "Strategic Thinking",
            "Teamwork",
            "Time Management",
            "Organization"
        ],
        "hard_skills": [
            "Application Development",
            "Application Architecture",
            "Scalability",
            "Performance Optimization",
            "Availability",
            "Compliance",
            "Project Management",
            "Root Cause Analysis",
            "Technical Documentation",
            "Agile Methodology",
            "Distributed Systems"
        ],
        "tech_stack": [
            "Java",
            "Spring Boot",
            "SQL",
            "NoSQL Databases",
            "Open Source Technologies",
            "Third-Party Integrations"
        ],
        "programming_languages": [
            "Java"
        ],
        "experience": 7,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Computer Science",
                "Computer Information Systems"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Fremont, CA",
        "job_id": 3669289917,
        "company": "DeepRoute.ai",
        "title": "Software Engineer",
        "created_on": 1720635998.878449,
        "description": "Job Duties: Design and implement control systems for in-vehicle autonomous driving functionalities; Develop planning algorithms for trajectories and designing policies and plans to manage multi-actor interactions and plans under uncertainty; Design and test vehicle dynamic models; and integrate and test the control system using the test plan in order to determine compliance with functional requirements Requirements Master's degree in Computer Science, Electrical Engineering or Mechanical Engineering. Knowledge of machine learning is required Benefits Salary Offered: $ 132,870.00 per year Application Method: Send your resume to 3125 Skyway Ct, Fremont, CA 94539 or email to qizhuang@deeproute.ai No. of Openings: Five (5)",
        "url": "https://www.linkedin.com/jobs/view/3669289917",
        "summary": "DeepRoute is seeking talented engineers with a Master's degree in Computer Science, Electrical Engineering, or Mechanical Engineering to design and implement control systems for autonomous driving functionalities. Responsibilities include developing planning algorithms for trajectories, designing policies for managing multi-actor interactions under uncertainty, designing and testing vehicle dynamic models, and integrating/testing the control system to ensure compliance with functional requirements.",
        "industries": [
            "Automotive",
            "Autonomous Vehicles",
            "Software Development",
            "Artificial Intelligence"
        ],
        "soft_skills": [
            "Problem Solving",
            "Analytical Skills",
            "Communication Skills",
            "Teamwork",
            "Attention to Detail"
        ],
        "hard_skills": [
            "Control Systems",
            "Autonomous Driving",
            "Machine Learning",
            "Planning Algorithms",
            "Trajectory Planning",
            "Multi-Actor Interactions",
            "Vehicle Dynamics",
            "Testing"
        ],
        "tech_stack": [],
        "programming_languages": [],
        "experience": 0,
        "education": {
            "min_degree": "Master's Degree",
            "fields": [
                "Computer Science",
                "Electrical Engineering",
                "Mechanical Engineering"
            ]
        },
        "salary": {
            "max": 132870,
            "min": 132870
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Los Angeles, CA",
        "job_id": 3644022453,
        "company": "Executive Staff Recruiters / ESR Healthcare",
        "title": "BI/Data Engineer    Los Angeles, CA",
        "created_on": 1720636000.6360428,
        "description": "Company Profile esrhealthcare.com.mysmartjobboard.com BI/Data Engineer > > Location: Los Angeles, CA onsite at least for 2-3 days. > > Position: 2 Nos > > Duration: 6 – 12 Months+ > > Only local candidates to CA– 2/3 days onsite is a must. > > JD > > Minimum 5-7 years of experience > > Data warehouse basics, Experience in requirements gathering > and writing functional & Design Specs > > ETL development using functional specs/design specs and > inputs from business user feedback > > Experience using ETL tools (Wherescape, Informatica, Talend > etc ) and MS SQL Stored Procedures > > Data validation experience (ERP source to DW target facts > and dimensions) > > BI tool Experience (Tableau) > > Building and automating data flows from external sources > (Cloud data sources such as HR datasets, TMS, etc) > > Working with a distributed team environment. > > Regards...!!!! > > Aravind Powered by Webbtree",
        "url": "https://www.linkedin.com/jobs/view/3644022453",
        "summary": "We are seeking two experienced BI/Data Engineers with a minimum of 5-7 years of experience for a 6-12 month+ contract position in Los Angeles, CA.  This is an onsite role, requiring 2-3 days in the office each week.  The ideal candidate will have a strong understanding of data warehousing principles, ETL development, and BI tools like Tableau. Experience with MS SQL Stored Procedures, Wherescape, Informatica, Talend, and cloud data sources like HR datasets and TMS is highly valued.",
        "industries": [
            "Healthcare",
            "Information Technology",
            "Data Analytics"
        ],
        "soft_skills": [
            "Requirements Gathering",
            "Communication",
            "Teamwork",
            "Problem Solving"
        ],
        "hard_skills": [
            "Data Warehousing",
            "ETL Development",
            "MS SQL Stored Procedures",
            "Data Validation",
            "BI Tools",
            "Tableau",
            "Cloud Data Sources"
        ],
        "tech_stack": [
            "Wherescape",
            "Informatica",
            "Talend",
            "MS SQL",
            "Tableau"
        ],
        "programming_languages": [],
        "experience": 5,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Alameda, CA",
        "job_id": 3933730297,
        "company": "Triunity Software, Inc.",
        "title": "Snowflake Data Engineer",
        "created_on": 1720636002.3628292,
        "description": "Job Description A Data Engineer with experience using AWS and Snowflake typically has a strong background in working with large datasets and knowledge of database design and implementation. They are responsible for designing, building, and maintaining the data pipelines and architectures that support the organization's data analytics and business intelligence efforts Responsibilities: Designing and implementing data pipelines using AWS technologies, such as S3, EMR, and Redshift, to extract, transform, and load data from a variety of sources Building and optimizing data models in Snowflake to support the organization's data analysis and reporting needs Developing and maintaining ETL processes to ensure data integrity and accuracy Collaborating with data analysts and data scientists to support the development of machine learning and predictive modeling efforts Working with other members of the engineering team to ensure that data infrastructure is scalable, reliable, and secure Must Skills: 8+ year of relevant experience ,Have strong technical skills, including experience with SQL, Python, and cloud technologies Experience with data warehousing, big data technologies, and data visualization tools Have excellent problem-solving and communication skills Ability to work effectively in a team environment",
        "url": "https://www.linkedin.com/jobs/view/3933730297",
        "summary": "A Data Engineer with experience using AWS and Snowflake is responsible for designing, building, and maintaining data pipelines and architectures for data analytics and business intelligence. Key responsibilities include designing data pipelines with AWS technologies like S3, EMR, and Redshift, building and optimizing data models in Snowflake, developing ETL processes, collaborating with data analysts and scientists, and ensuring data infrastructure scalability, reliability, and security.",
        "industries": [
            "Data Analytics",
            "Business Intelligence",
            "Machine Learning",
            "Cloud Computing",
            "Software Engineering"
        ],
        "soft_skills": [
            "Problem-solving",
            "Communication",
            "Teamwork"
        ],
        "hard_skills": [
            "SQL",
            "Python",
            "AWS",
            "Snowflake",
            "S3",
            "EMR",
            "Redshift",
            "ETL",
            "Data Warehousing",
            "Big Data",
            "Data Visualization"
        ],
        "tech_stack": [
            "AWS",
            "Snowflake",
            "S3",
            "EMR",
            "Redshift"
        ],
        "programming_languages": [
            "SQL",
            "Python"
        ],
        "experience": 8,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Menlo Park, CA",
        "job_id": 3945852259,
        "company": "Crystal Equation Corporation",
        "title": "Data Engineer V",
        "created_on": 1720636004.0463886,
        "description": "Job Description Data Engineer V Onsite Summary: Data Engineer with Technical Project Manager skills The main function of the Data Engineer is to develop, evaluate, test and maintain architectures and data solutions within our organization. The typical Data Engineer executes plans, policies, and practices that control, protect, deliver, and enhance the value of the organization’s data assets. Responsibilities: Manage data engineering projects through the full cycle. Identify and underline business initiatives from a data engineering perspective Design, construct, install, test and maintain highly scalable data management systems. Ensure systems meet business requirements and industry practices. Design, implement, automate and maintain large scale enterprise data ETL processes. Build high-performance algorithms, prototypes, predictive models and proof of concepts. Skills: Ability to work as part of a team, as well as work independently or with minimal direction. Excellent written, presentation, and verbal communication skills. Collaborate with data architects, modelers and IT team members on project goals. Strong PC skills including knowledge of Microsoft SharePoint. Education/Experience: Bachelor's degree in a technical field such as computer science, computer engineering or related field required. Process certification, such as, Six Sigma, CBPP, BPM, ISO 20000, ITIL, CMMI. Pay range is $102 - $107 per hour with full benefits available, including paid time off, medical/dental/vision/life insurance, 401K, parental leave, and more. Our compensation reflects the cost of labor across several US geographic markets. Pay is based on several factors including market location and may vary depending on job-related knowledge, skills, and experience. THE PROMISES WE MAKE: At Crystal Equation, we empower people and advance technology initiatives by building trust. Your recruiter will prep you for the interview, obtain feedback, guide you through any necessary paperwork and provide everything you need for a successful start. We will serve to empower you along the way and provide the path for your professional journey.",
        "url": "https://www.linkedin.com/jobs/view/3945852259",
        "summary": "This role involves managing data engineering projects from conception to completion, analyzing business needs from a data perspective, designing and maintaining scalable data management systems, ensuring compliance with business requirements and industry standards, and developing data ETL processes. The ideal candidate will have strong technical project management skills, excellent communication skills, and experience with data architecture, modeling, and IT systems.",
        "industries": [
            "Information Technology",
            "Data Management",
            "Software Development",
            "Consulting"
        ],
        "soft_skills": [
            "Teamwork",
            "Independent Work",
            "Communication (Written)",
            "Communication (Verbal)",
            "Presentation Skills",
            "Collaboration"
        ],
        "hard_skills": [
            "Data Engineering",
            "Project Management",
            "Data Architecture",
            "Data Modeling",
            "Data Management",
            "ETL",
            "Algorithms",
            "Prototyping",
            "Predictive Modeling",
            "Proof of Concepts",
            "Microsoft SharePoint"
        ],
        "tech_stack": [],
        "programming_languages": [],
        "experience": 0,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Computer Engineering"
            ]
        },
        "salary": {
            "max": 107,
            "min": 102
        },
        "benefits": [
            "Paid Time Off",
            "Medical Insurance",
            "Dental Insurance",
            "Vision Insurance",
            "Life Insurance",
            "401K",
            "Parental Leave"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Berkeley, CA",
        "job_id": 3952238449,
        "company": "Osmosis.org from Elsevier",
        "title": "Senior Data Engineer",
        "created_on": 1720636005.8713887,
        "description": "About Us Do you dream of a future where finance is open, accessible, and user-driven? Are you passionate about Decentralized Exchanges (DEXs) and the potential of DeFi to revolutionize financial markets? If so, then we want to hear from you! Osmosis is the leading interchain DEX built on the Cosmos ecosystem. We're on a mission to build the future of DeFi, and we're searching for a talented and visionary Data Engineer to join our growing team. Role Description We are seeking a Senior Backend Engineer with a strong interest in blockchain technology to join our Osmosis Data team. As a Backend Engineer, you will play a critical role in building data pipelines and developing API-based backend services. You will collaborate closely with other engineering teams to support their data requirements and ensure our data systems are robust, scalable, and efficient, enabling all the product innovation we have planned to deliver in Osmosis. What you could work on: Helping design, implement, and operate data-intensive microservices from the ground up. Building and maintaining data processing pipelines that extract and process data from the osmosis blockchain. Developing and maintaining secure and scalable backend services and APIs. Designing a real-time streaming system capable of listening to and reacting to real-time blockchain data and events. Contributing to improving our protocol's observability by developing tools for monitoring, analyzing, and reporting on key metrics and behaviors (e.g., incentives). Debugging and troubleshooting production issues with our data platform. You may be a fit for this role if you: Have at least 4 years of professional software development experience in building and maintaining data-intensive applications. Possess a solid understanding of Backend and Data Engineering principles and best practices. Have strong experience with relational and non-relational databases, time-series databases, queueing systems (such as Kafka), and API design. Have strong programming skills, preferably in Go, Python, and/or Rust. Are familiar with cloud data services and infrastructure (e.g., AWS, GCP). Have a strong understanding of distributed systems and how they can be operated at scale. Have experience in running and operating production workloads. Are passionate about blockchains and decentralized technology. Possess excellent communication skills and the ability to work collaboratively. Demonstrate a proactive approach to problem-solving and ownership of tasks. Experience that will set you apart: Previous experience working on high-scale, highly critical data systems, preferably in DeFi or TradFi. Proven record of driving processes from start to production, demonstrating the ability to lead initiatives from the conceptual phase through to successful operational deployment. Familiarity with the Cosmos SDK stack. Contributions to open-source data projects. Experience working in remote and globally distributed teams. At Osmosis, you'll join a passionate and talented team working at the forefront of DeFi innovation. We offer a competitive compensation package, a collaborative work environment, and the opportunity to make a real impact on the future of finance. Ready to join the DeFi revolution? We look forward to hearing from you! Legal Stuff Employees and contractors are engaged through contributing entities, including but not limited to Osmurica, the Osmosis Foundation, and Chainapsis. Contributing entities provide equal employment opportunities to all employees, contractors, and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.",
        "url": "https://www.linkedin.com/jobs/view/3952238449",
        "summary": "Osmosis, a leading interchain DEX on the Cosmos ecosystem, seeks a Senior Backend Engineer to join their Data team. The role involves building data pipelines, developing backend services, and ensuring robust, scalable, and efficient data systems.  Responsibilities include designing microservices, building data processing pipelines, developing secure APIs, designing real-time streaming systems, and improving protocol observability through monitoring tools. Ideal candidates have 4+ years of experience in building data-intensive applications, strong backend and data engineering knowledge, experience with databases, queueing systems, API design, and programming skills in Go, Python, or Rust. Additional qualifications include experience with cloud data services, distributed systems, production workloads, blockchain technology, and strong communication and problem-solving skills.",
        "industries": [
            "Blockchain",
            "Decentralized Finance (DeFi)",
            "Technology",
            "Software Development"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem-solving",
            "Proactive",
            "Ownership",
            "Passionate",
            "Visionary",
            "Talented"
        ],
        "hard_skills": [
            "Backend Engineering",
            "Data Engineering",
            "Data Pipelines",
            "API Design",
            "Microservices",
            "Relational Databases",
            "Non-Relational Databases",
            "Time-Series Databases",
            "Queueing Systems",
            "Kafka",
            "Go",
            "Python",
            "Rust",
            "Cloud Data Services",
            "AWS",
            "GCP",
            "Distributed Systems",
            "Production Workloads",
            "Blockchain Technology",
            "Cosmos SDK",
            "Open-Source Data Projects"
        ],
        "tech_stack": [
            "Cosmos Ecosystem",
            "Osmosis",
            "Go",
            "Python",
            "Rust",
            "AWS",
            "GCP",
            "Kafka"
        ],
        "programming_languages": [
            "Go",
            "Python",
            "Rust"
        ],
        "experience": 4,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Competitive Compensation Package",
            "Collaborative Work Environment"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Alameda, CA",
        "job_id": 3954494731,
        "company": "IT Solutions Inc",
        "title": "Data Infrastructure Engineer",
        "created_on": 1720636007.5932536,
        "description": "Position: Data Infrastructure Engineer Duration: 6+ Months Location: Onsite at Alameda (3 Days), CA. Job Description The client is adopting ‘data mesh’ concepts to build a data platform that can support federated data products with governance and security. Data Platform provides all the data infrastructure needs of data engineers in developing data products. Key Responsibilities: Architect, design, and implement unified data access for various personas like data engineer, data analyst, and data steward by simplified access control mechanism that supports governance on AWS and non-AWS assets. Design, build & optimize CI/CD pipelines for data engineers to deploy and push code across multiple environments. Develop and maintain infrastructure as code (IaaC) with Terraform for reproducible and scalable deployments. Design & implementation of AWS cloud infrastructure using best practices and industry standards by working closely with internal & external stakeholders like Information Security, Cloud Infrastructure, Data Engineering teams, etc. Work closely with data governance teams in implementing data governance policies, periodic review, and optimization of repository from a security & governance perspective. Continuous improvement of Cloud Data platform to enhance speed, agility, and cost efficient Skills and Qualifications Expertise in reducing costs and increasing speed and efficiency in large-scale data platform deployments. Proficiency in Terraform, and Python Scripting along with GitHub Actions for the deployment of infrastructure as code. Deep understanding of data governance policies, implementation best practices and hands-on experience in implementing Policies as Code Architecture, design & implementation of unified data access using AWS Lake Formation, AWS Data Zone, IAM, OKTA, and others. Experience in architecture & security of AWS EC2, EBS, S3, EKS, Athena, Redshift, RDS, Kafka, Glue, and other data management tools Experience in in design and implementation of granular access to data without losing the speed & agility of delivering the access provisioning. Expertise in CI/CD process and deployment technologies using tools like GitHub, GitHub Actions Expertise in logging, monitoring, reliability engineering Strong knowledge of networking concepts and experience managing network-related issues",
        "url": "https://www.linkedin.com/jobs/view/3954494731",
        "summary": "This role involves architecting, designing, and implementing a unified data access platform for various personas, including data engineers, data analysts, and data stewards, using AWS Lake Formation, AWS Data Zone, IAM, and OKTA. The engineer will also design, build, and optimize CI/CD pipelines for data engineers to deploy code across multiple environments using Terraform and GitHub Actions. The ideal candidate will have expertise in reducing costs, increasing speed and efficiency in large-scale data platform deployments, and possess a deep understanding of data governance policies.",
        "industries": [
            "Data",
            "Technology",
            "Cloud Computing",
            "Information Security"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Organization",
            "Time Management",
            "Decision Making",
            "Continuous Improvement"
        ],
        "hard_skills": [
            "Terraform",
            "Python",
            "GitHub Actions",
            "AWS Lake Formation",
            "AWS Data Zone",
            "IAM",
            "OKTA",
            "AWS EC2",
            "EBS",
            "S3",
            "EKS",
            "Athena",
            "Redshift",
            "RDS",
            "Kafka",
            "Glue",
            "CI/CD",
            "GitHub",
            "Logging",
            "Monitoring",
            "Reliability Engineering",
            "Networking"
        ],
        "tech_stack": [
            "AWS",
            "Terraform",
            "Python",
            "GitHub Actions",
            "AWS Lake Formation",
            "AWS Data Zone",
            "IAM",
            "OKTA",
            "AWS EC2",
            "EBS",
            "S3",
            "EKS",
            "Athena",
            "Redshift",
            "RDS",
            "Kafka",
            "Glue",
            "GitHub"
        ],
        "programming_languages": [
            "Python"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3912064382,
        "company": "Unreal Staffing, Inc",
        "title": "Software Engineer",
        "created_on": 1720636011.1183653,
        "description": "About The Role We're seeking seasoned engineers across various domains (Product, Full-Stack, Backend, Applied AI/LLM) to drive the development of both new and existing products. Our success hinges on our ability to iterate rapidly while ensuring top-notch performance and reliability. You'll operate in a highly iterative, collaborative, and fast-paced setting to deploy our technology to a broad user base, ensuring it's delivered with finesse and reliability at its core. This position offers a hybrid work model: you'll work in person at our San Francisco office two days a week. Remote work is available for the remaining days, though the office space is always open for use! Unfortunately, we cannot provide visa sponsorship at this time. Requirements Depending On The Role, You Will Architect and construct the platforms powering our new products, focusing on scalability, reliability, and security Lead the development of new generative AI features and product experiences from inception to delivery Collaborate closely with researchers, engineers, product managers, and designers to introduce new features and research capabilities Enhance engineering efficiency by providing robust tooling and systems Foster a diverse, equitable, and inclusive culture while encouraging open communication and challenging groupthink Like all teams, you'll share responsibility for system reliability and participate in an on-call rotation to address critical incidents promptly You Might Thrive In This Role, If You Bring 5+ years of engineering experience across diverse tech environments, particularly in product-driven companies Possess proficiency in TypeScript/React (frontend), Python (backend), and have a penchant for learning new technologies Embrace ownership in a dynamic, fast-paced startup environment Have a solid track record of designing, building, and scaling production systems to meet evolving product needs and growing demand Prioritize exceptional user experiences and take pride in delivering customer-centric solutions Stay abreast of advancements in AI and LLM technology and are enthusiastic about their potential Exhibit humility, collaboration, and a willingness to support teammates for collective success Take initiative in tackling challenges, proactive in skill development, and adept at building custom tools when necessary Bonus points if you have startup experience or familiarity with healthcare regulatory structures like HIPAA Tech Stack LLMs: OpenAI & similar Backend: Python, Django, Postgres Frontend: React, Typescript Infrastructure: GCP, Docker Benefits Flexible work hours & generous PTO High autonomy and support from a collaborative team New MacBook Pro Work from home stipend Personal development stipend Salary Range: $170,000 - $220,000 USD annually",
        "url": "https://www.linkedin.com/jobs/view/3912064382",
        "summary": "We are looking for experienced engineers in Product, Full-Stack, Backend, and Applied AI/LLM to build and improve our products. You will work in a fast-paced environment, collaborating with researchers, engineers, product managers, and designers to deliver new features and capabilities. We offer a hybrid work model, with two days a week in our San Francisco office and the remaining days remote. We are looking for someone with 5+ years of experience, proficiency in TypeScript/React (frontend), Python (backend), and a passion for learning new technologies. You should be able to design, build, and scale production systems to meet evolving product needs and growing demand.",
        "industries": [
            "Technology",
            "Software Development",
            "Artificial Intelligence",
            "Healthcare"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Ownership",
            "Problem Solving",
            "Initiative",
            "Humility",
            "Adaptability",
            "Teamwork",
            "Proactive"
        ],
        "hard_skills": [
            "TypeScript",
            "React",
            "Python",
            "Django",
            "Postgres",
            "GCP",
            "Docker",
            "AI",
            "LLM"
        ],
        "tech_stack": [
            "OpenAI",
            "Python",
            "Django",
            "Postgres",
            "React",
            "Typescript",
            "GCP",
            "Docker"
        ],
        "programming_languages": [
            "TypeScript",
            "React",
            "Python",
            "Django"
        ],
        "experience": 5,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 220000,
            "min": 170000
        },
        "benefits": [
            "Flexible work hours",
            "Generous PTO",
            "High autonomy",
            "Collaborative team",
            "New MacBook Pro",
            "Work from home stipend",
            "Personal development stipend"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3895819857,
        "company": "Unreal Staffing, Inc",
        "title": "Data Pipeline Developer",
        "created_on": 1720636012.891656,
        "description": "Company Overview: Welcome to the forefront of data-driven innovation! At our company, we're committed to harnessing the power of data to drive insights and empower decision-making. Our mission is to develop scalable and efficient data pipeline solutions that enable organizations to streamline data processing and analysis. Join us and be part of a dynamic team dedicated to shaping the future of data pipeline development. Position Overview: As a Data Pipeline Developer, you'll play a crucial role in designing, building, and maintaining our data pipeline infrastructure and workflows. Working closely with cross-functional teams of data engineers, analysts, and business stakeholders, you'll ensure the reliability, scalability, and efficiency of our data pipeline solutions. If you're passionate about data engineering and eager to drive innovation through scalable data solutions, we want you on our team. Requirements Key Responsibilities: Data Pipeline Design: Design and architect scalable and reliable data pipeline solutions to automate data ingestion, processing, and transformation tasks Data Integration: Integrate data from diverse sources, including databases, files, APIs, and streaming platforms, into our data pipeline ecosystem, ensuring data consistency and coherence Data Processing: Develop and maintain data processing workflows to cleanse, enrich, and transform raw data into formats suitable for analysis and reporting Data Orchestration: Orchestrate and schedule data pipeline workflows using workflow management tools and frameworks (e.g., Apache Airflow, Luigi, Prefect) Performance Optimization: Optimize data pipeline performance for speed, efficiency, and scalability, tuning data processing tasks and optimizing resource utilization Data Quality and Governance: Implement data quality checks, validation rules, and error handling mechanisms to ensure the accuracy, completeness, and consistency of data in the pipeline Monitoring and Maintenance: Monitor data pipeline performance and health, proactively identifying and addressing issues to minimize downtime and optimize resource utilization Documentation and Collaboration: Document data pipeline architecture, processes, and best practices, and collaborate with cross-functional teams to ensure alignment and transparency Qualifications: Bachelor's degree or higher in Computer Science, Engineering, or related field Strong background in data engineering, with hands-on experience in designing, building, and optimizing data pipeline solutions Proficiency in programming languages such as Python, Java, or Scala, and experience with data processing frameworks such as Apache Spark, Apache Beam, or Apache Flink Experience with workflow management tools and frameworks such as Apache Airflow, Luigi, or Prefect Familiarity with data integration techniques and technologies, including ETL (Extract, Transform, Load) processes and streaming data processing Strong problem-solving abilities and analytical thinking, with a keen attention to detail and a passion for tackling complex technical challenges Excellent communication and collaboration skills, with the ability to work effectively in cross-functional teams and communicate technical concepts to non-technical stakeholders Benefits Competitive salary: The industry standard salary for Data Pipeline Developers typically ranges from $140,000 to $210,000 per year, depending on experience and qualifications. Exceptional candidates may be eligible for higher compensation packages Comprehensive health, dental, and vision insurance plans Flexible work hours and remote work options Generous vacation and paid time off Professional development opportunities, including access to training programs, conferences, and workshops State-of-the-art technology environment with access to cutting-edge tools and resources Vibrant and inclusive company culture with team-building activities and social events Opportunities for career growth and advancement within the company Exciting projects with real-world impact across diverse industries Chance to work alongside top talent and industry experts in the field of data engineering Join Us: Ready to shape the future of data pipeline development? Apply now to join our team and be part of an exciting journey of innovation and discovery!",
        "url": "https://www.linkedin.com/jobs/view/3895819857",
        "summary": "This job description is for a Data Pipeline Developer.  You will design, build, and maintain data pipeline infrastructure and workflows, working closely with data engineers, analysts, and business stakeholders to ensure the reliability, scalability, and efficiency of data pipeline solutions.",
        "industries": [
            "Data Engineering",
            "Software Development",
            "Technology",
            "Data Analytics",
            "Data Science"
        ],
        "soft_skills": [
            "Problem-solving",
            "Analytical thinking",
            "Communication",
            "Collaboration",
            "Attention to detail",
            "Passion for complex technical challenges"
        ],
        "hard_skills": [
            "Python",
            "Java",
            "Scala",
            "Apache Spark",
            "Apache Beam",
            "Apache Flink",
            "Apache Airflow",
            "Luigi",
            "Prefect",
            "ETL",
            "Data Integration",
            "Data Processing",
            "Data Orchestration",
            "Performance Optimization",
            "Data Quality",
            "Data Governance",
            "Monitoring",
            "Maintenance",
            "Documentation",
            "Workflow Management"
        ],
        "tech_stack": [
            "Python",
            "Java",
            "Scala",
            "Apache Spark",
            "Apache Beam",
            "Apache Flink",
            "Apache Airflow",
            "Luigi",
            "Prefect"
        ],
        "programming_languages": [
            "Python",
            "Java",
            "Scala"
        ],
        "experience": 0,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Computer Science",
                "Engineering"
            ]
        },
        "salary": {
            "max": 210000,
            "min": 140000
        },
        "benefits": [
            "Competitive salary",
            "Health, dental, and vision insurance",
            "Flexible work hours",
            "Remote work options",
            "Vacation and paid time off",
            "Professional development opportunities",
            "State-of-the-art technology environment",
            "Vibrant and inclusive company culture",
            "Career growth opportunities",
            "Exciting projects",
            "Work with top talent"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Jose, CA",
        "job_id": 3900084967,
        "company": "Zortech Solutions",
        "title": "Software Engineer – Opensource - US",
        "created_on": 1720636014.6236787,
        "description": "Job Title: Software Engineer – Opensource Location: Hybrid in CA, TX, UT Job Overview eBay has a rich history in the Open-Source Community. Open Source is a significant component of most of our applications, and we contribute back to the community in many ways. We are seeking an experienced Open-Source Software expert to join our team. As an Open-Source Software expert, you will be responsible for managing and contributing to the development of our open-source projects, evaluating and implementing new open-source technologies, and collaborating with our development team to ensure the successful implementation of open source software solutions. Additionally, you will be required to manage the technical aspects of enforcing Open-Source policy in our software development lifecycle. You will also be responsible for internal customer support at any level from coding to strategic consultation. Responsibilities Manage and contribute to the development of open-source projects to maximize value to eBay for the least effort. Search for, evaluate and implement new open-source technologies to improve existing software solutions throughout the company. Collaborate with internal development teams to ensure the successful implementation of open-source software solutions. This could be anywhere from the L1 to the L3+ level (of traditional customer support models). Provide technical expertise and guidance on open-source software usage and best practices Help the open-source team ensure software is built in compliance with open-source licenses and legal requirements Manage the technical aspects of integrating Snyk and/or other automated tools that enforce policy in the software supply chain. Participate in the open-source community by contributing to projects, attending conferences, and staying up to date on industry trends Create and maintain documentation for open-source projects and software solutions Help identify and resolve open-source software related issues Required Qualifications Education and Experience combined: 5+ years in computer science or related field. Experience needs to demonstrate proficiency in: Experience in full-stack web development Java Development including Spring Boot Experience with one of the major client-side web frameworks Restful APIs Demonstrated contributor to open-source projects 3+ years’ experience in open-source software development, either personally or professionally Strong communication and collaboration skills, including collaborative tools like GitHub. Knowledge of basic economics like Cost of Delay and software development best-practices like DevOps Research and Assessment. Desired Qualifications Knowledge of open-source licenses and their appropriate/inappropriate usages in commercial software. Experience and/or knowledge of CI/CD utilities. Experience and/or knowledge of cloud native technologies Customer support experience, either in software development or another field. Software development in a fortune 500 company. Contributions to public discourse on open-source software, software development, or other topic such as journal articles, published research, conference talks, blogs, etc. Open API and JSON API Standards Python",
        "url": "https://www.linkedin.com/jobs/view/3900084967",
        "summary": "eBay seeks an Open-Source Software expert to manage and contribute to the development of open-source projects, evaluate and implement new open-source technologies, collaborate with development teams on open-source software solutions, and ensure compliance with open-source licenses and legal requirements. This role involves internal customer support, technical expertise on open-source software usage and best practices, and participation in the open-source community.",
        "industries": [
            "Technology",
            "E-commerce",
            "Software Development"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem-solving",
            "Customer Support",
            "Documentation",
            "Technical Expertise",
            "Leadership"
        ],
        "hard_skills": [
            "Full-stack web development",
            "Java Development",
            "Spring Boot",
            "Client-side web frameworks",
            "Restful APIs",
            "Open Source Development",
            "GitHub",
            "DevOps",
            "Cost of Delay",
            "Open-Source Licenses",
            "CI/CD",
            "Cloud Native Technologies",
            "JSON API",
            "Python"
        ],
        "tech_stack": [
            "Java",
            "Spring Boot",
            "GitHub",
            "Snyk",
            "Restful APIs",
            "Open API",
            "JSON API",
            "CI/CD",
            "Cloud Native Technologies",
            "Python"
        ],
        "programming_languages": [
            "Java",
            "Python"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Computer Science",
                "Related Field"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Cerritos, CA",
        "job_id": 3902052205,
        "company": "REVOLVE",
        "title": "Software Engineer",
        "created_on": 1720636016.325075,
        "description": "Meet REVOLVE REVOLVE is the next-generation fashion retailer for Millennial and Generation Z consumers. As a trusted, premium lifestyle brand, and a go-to online source for discovery and inspiration, we deliver an engaging customer experience from a vast yet curated offering totaling over 45,000 apparel, footwear, accessories and beauty styles. Our dynamic platform connects a deeply engaged community of millions of consumers, thousands of global fashion influencers, and more than 500 emerging, established and owned brands. Through 20 years of continued investment in technology, data analytics, and innovative marketing and merchandising strategies, we have built a powerful platform and brand that we believe is connecting with the next generation of consumers and is redefining fashion retail for the 21st century. For more information please visit www.revolve.com. At REVOLVE the most successful team members have the thirst and creativity to redefine fashion retail for the 21st century, making REVOLVE the leading online retail destination targeted towards Millennial and Generation Z consumers seeking premium fashion. With a team of 1,000 strong, we are a dynamic bunch that are motivated by getting the company to the next level. It’s our goal to hire high-energy, diverse, bright, creative, and flexible individuals who thrive in a fast-paced work environment. Some of the sweetest perks we offer aren’t in a typical benefit package like hefty discount on items we carry – as in 50% or more off retail prices, free weekly lunches, and pretty rad company parties. To take a behind the scenes look at the REVOLVE “corporate” lifestyle check out our Instagram @REVOLVEcareers or . Are you ready to set the standard for Premium apparel? Main Purpose Of The Software Engineer Role This is a full-time role for a candidate who will develop and maintain software services and reporting software used by Finance and Planning teams. Major Responsibilities Essential Duties and Responsibilities include the following. Other duties may be assigned. Analyze user requirement and develop new modules for external customers and internal business processes. Implement, maintain and troubleshoot database driven internal applications using SQL, Java, JSP, Spring MVC, Javascript and Redis. Troubleshoot issues, perform analyses and provide efficient solutions for operations, warehouse and international marketing teams. Required Competencies To perform the job successfully, an individual should demonstrate the following competencies: Develop and deploy bug free code to production on a daily basis Communicate effectively with internal end users (Business Owners) and working with them to refine specs for tasks. Must be comfortable coding applications from start to finish on a fast pace. Minimum Qualifications 2 Years of professional work experience Bachelors in Computer Science/Computer Engineering or equivalent degree Java, SQL, Redis, Java Spring Framework, Javascript Preferred Qualifications Bachelors or Masters Degree in Computer Science/Computer Engineering Java, Sql, Redis, Java Spring Framework, Webpack, Javascript A successful candidate works well in a dynamic environment with minimal supervision. At REVOLVE we all roll up our sleeves to pitch-in and do whatever it takes to get the job done. Each day is a little different, it’s what keeps us on our toes and excited to come to work every day. For individuals assigned and/or hired to work in California, Revolve includes a reasonable estimate of the salary or hourly rate range for this role. This takes into account the wide range of factors that are considered in making compensation decisions; including but not limited to business or organizational needs, skill sets, experience and training, licensure, and certifications. A reasonable estimate of the current base hourly/salary range is $110,000 /year - $140,000 /year Attention After submitting your application, please check your spam folder for emails on your application status. Emails are sent from an ADP email address. The following job description contains representative examples of work that will be performed in positions allocated to this classification. It is not required that any position perform all of the duties listed, so long as primary responsibilities are consistent with the work as described. Roles and responsibilities can often be expanded to accommodate changing business conditions and goals, as well as to tap into the skills and talents of the individuals in the company. Accordingly, associates may be asked to perform duties that are outside the specific functions that are listed.",
        "url": "https://www.linkedin.com/jobs/view/3902052205",
        "summary": "REVOLVE, a fashion retailer, is looking for a Software Engineer to develop and maintain software services and reporting software used by Finance and Planning teams. The role involves analyzing user requirements, developing new modules, implementing and maintaining database-driven internal applications, troubleshooting issues, and providing solutions for various teams. The ideal candidate will have 2 years of experience, a Bachelor's degree in Computer Science/Computer Engineering, and expertise in Java, SQL, Redis, Java Spring Framework, and Javascript.",
        "industries": [
            "Retail",
            "Fashion",
            "E-commerce"
        ],
        "soft_skills": [
            "Communication",
            "Problem-solving",
            "Analytical Skills",
            "Teamwork",
            "Adaptability",
            "Self-Motivation"
        ],
        "hard_skills": [
            "Java",
            "SQL",
            "Redis",
            "Java Spring Framework",
            "Javascript",
            "Webpack"
        ],
        "tech_stack": [
            "SQL",
            "Java",
            "JSP",
            "Spring MVC",
            "Javascript",
            "Redis",
            "Webpack"
        ],
        "programming_languages": [
            "Java",
            "Javascript"
        ],
        "experience": 2,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Computer Science",
                "Computer Engineering"
            ]
        },
        "salary": {
            "max": 140000,
            "min": 110000
        },
        "benefits": [
            "Employee Discount",
            "Free Lunch",
            "Company Parties"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3965747094,
        "company": "Andreessen Horowitz",
        "title": "Partner 20, Analytics Engineer",
        "created_on": 1720636018.4533224,
        "description": "Founded in Silicon Valley in 2009 by Marc Andreessen and Ben Horowitz, Andreessen Horowitz (aka a16z) is a venture capital firm that backs bold entrepreneurs building the future through technology. We are stage agnostic. We invest in seed to venture to growth-stage technology companies, across AI, bio + healthcare, consumer, crypto, enterprise, fintech, games, and companies building toward American dynamism. a16z has $42B in assets under management across multiple funds. We’ve established a team that is defined by respect for the entrepreneur and the company-building process; we know what it’s like to be in the founder’s shoes. We’ve invested in companies like Affirm, Airbnb, Coinbase, Databricks, Devoted Health, Insitro, Figma, GitHub, Instacart, OpenSea, Roblox, Stripe, and Substack. Our team is at the forefront of new technology, helping founders and their companies impact and change the world. The Role At a16z, we believe data is at the core of making informed decisions and effectively supporting our portfolio companies. Our Data & Analytics team builds and manages the infrastructure that supports our data lake (Databricks), data pipelines (dbt), data ingestion (Fivetran & custom Python scripts), and data visualizations (Looker/Hex). We leverage cutting-edge GenAI alongside more traditional data & analytics to empower our firm. As an Analytics Engineer, you will be a key player in architecting and developing our data infrastructure and analytics solutions. You will work closely with cross-functional teams to design, implement, and optimize data pipelines, ensuring seamless data integration and high-quality data delivery. This role requires advanced technical skills, a deep understanding of data management practices, and a passion for leveraging data to drive business insights. Additionally, you will play a key part in ensuring our data pipelines enable effective, trustred GenAI applications. You will have the opportunity to own key parts of our data infrastructure, learn about implementing GenerativeAI, and have access to a plethora of learning and development resources. To Join Our Team, You Should Be Excited To Collaborate with data analysts and data scientists to design and build data models that support analytical, reporting, GenAI needs. Strong communication skills and the ability to collaborate effectively with cross-functional teams. Architect, design, and implement scalable and robust data pipelines using dbt, Fivetran, and custom Python scripts. Manage and optimize our data lake infrastructure in Databricks, ensuring data integrity, security, and accessibility. Develop and maintain ETL processes to support data ingestion from various sources into our data lake. Create and maintain comprehensive documentation for data pipelines, ETL processes, and data models. Implement best practices for data governance, quality, and compliance. Develop and maintain data visualizations and dashboards in Looker to provide actionable insights to stakeholders. Minimum Qualifications 6+ years of experience in data engineering, analytics engineering, or a related field. Advanced proficiency in SQL. Experience with Databricks and dbt preferred. Strong understanding of data warehousing concepts, ETL processes, and data modeling techniques. Proven experience in designing and building scalable data pipelines and infrastructure. Experience with data visualization tools, particularly Looker, and the ability to translate complex data into actionable insights. Excellent problem-solving skills and the ability to work independently in a fast-paced environment. Experience writing data pipelines in python is preferred. Knowledge of data governance and compliance best practices preferred. The anticipated salary range for this role is between $225,000-$263,000, actual starting pay may vary based on a range of factors which can include experience, skills, and scope. This role is eligible to participate in the a16z Carry program and various discretionary bonus programs as well as benefit and perquisite plans including health, dental, vision, disability, life insurance, 401K plan, vacation, and sick leave. a16z culture We do only first class business and only in a first class way We take a long view of relationships, because we are in the relationship business We believe in the future and bet the firm that way We are all different, we recognize that, and we win We celebrate the good times We do it for the team We play to win At a16z we are always looking to hire the absolute best talent and recognize that diversity in our experiences and backgrounds is what makes us stronger. We hire candidates of any race, color, ancestry, religion, sex, national origin, sexual orientation, gender identity, age, marital or family status, disability, Veteran status, and any other status. These differences are what enables us to work towards the future we envision for ourselves, our portfolio companies, and the World. Our organization participates in E-Verify. Click here to learn about E-Verify. Andreessen Horowitz hereby reserves the right to make use of any unsolicited resumes received from outside recruiting agencies and / or individual recruiters without being responsible for payment of any fees asserted from the use of unsolicited resumes.",
        "url": "https://www.linkedin.com/jobs/view/3965747094",
        "summary": "Andreessen Horowitz (a16z) is seeking an Analytics Engineer to architect and develop data infrastructure and analytics solutions. You'll work with cross-functional teams on data pipelines, ensuring seamless data integration and high-quality data delivery. This role requires advanced technical skills, data management expertise, and a passion for leveraging data to drive business insights. You will have the opportunity to own key parts of our data infrastructure, learn about implementing GenerativeAI, and have access to a plethora of learning and development resources.",
        "industries": [
            "Venture Capital",
            "Technology",
            "Fintech",
            "Data & Analytics",
            "Artificial Intelligence"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem-Solving",
            "Independent Work"
        ],
        "hard_skills": [
            "SQL",
            "Databricks",
            "dbt",
            "Python",
            "Fivetran",
            "Looker",
            "ETL",
            "Data Warehousing",
            "Data Modeling",
            "Data Visualization",
            "Data Governance",
            "Data Compliance"
        ],
        "tech_stack": [
            "Databricks",
            "dbt",
            "Fivetran",
            "Python",
            "Looker",
            "GenAI",
            "Hex"
        ],
        "programming_languages": [
            "SQL",
            "Python"
        ],
        "experience": 6,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 263000,
            "min": 225000
        },
        "benefits": [
            "Carry program",
            "Discretionary bonus programs",
            "Health insurance",
            "Dental insurance",
            "Vision insurance",
            "Disability insurance",
            "Life insurance",
            "401K plan",
            "Vacation",
            "Sick leave"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Santa Barbara County, CA",
        "job_id": 3961887977,
        "company": "Pearly",
        "title": "Software Engineer (Data Platform)",
        "created_on": 1720636020.0796103,
        "description": "About the Role We're looking for a Santa Barbara based Software Engineer that will own the evolution and scaling of our best-in-class data platform. Pearly maintains synchronized dental records for over 25 million patients across the United States. These records form the foundation of our software capabilities, allowing for workflows, reporting, and attribution that are trusted by the largest dental organizations in the country. You will be responsible for building the next generation of our data systems, including deploying ETLs across our integration partners, employing novel warehousing techniques at scale, and developing clean APIs for exposing data across a variety of application and reporting requirements. What You'll Do Deliver data at scale: You will build mission-critical ETL processes that structure and warehouse high-volume medical and financial data, emphasizing performance, integrity, and accuracy . Integrate with a modern stack: Pearly operates a hyper-modern stack with a focus on clean, functional programming and thoughtful DX. Your primary tools will be Typescript and PostgreSQL, and your code will interact with broader platform components built on top of GCP Serverless, GraphQL, Prisma, and React. Collaborate effectively: We operate as a flat, mutually supportive, highly connected team where frequent input and collaboration is expected and best ideas always win. Architect for the future: We believe simplicity is the greatest virtue, and any system you develop will be built with a focus on extensibility, modularity, and readability. Qualifications Sense of Craft: You take pride in and are consistently honing your technical and creative abilities. You seek out opportunities to introduce simplicity and elegance, and enjoy composing solutions to business requirements in a way that is both innovative and effective. 2+ Years Deploying to Prod: You have an understanding of the requirements and stakes involved in deploying live systems. You exhibit exceptional attention to detail and reliability, and are prepared to operate in a security and accuracy-sensitive domain. Customer-Driven Mindset : While your focus will be data, our mission at Pearly is to build software that serves the customer. You understand how data ultimately powers business use cases, and how to leverage data to deliver novel capabilities. Willing to Roll Up Your Sleeves: Taking the initiative is second nature to you. You will operate with a high degree of autonomy, take projects to the finish line, and own the outcome. Based in Santa Barbara, CA: Our team is in-office first, based out of downtown Santa Barbara, California, with hybrid flexibility following onboarding. Benefits Competitive salary, equity, and healthcare benefits Meeting-light culture Work with an A+ smart and passionate team Flexible vacation/time-off policy Opportunity to make your mark at an early stage company with great product-market fit",
        "url": "https://www.linkedin.com/jobs/view/3961887977",
        "summary": "Pearly is seeking a Software Engineer to build and scale their data platform. You'll develop ETL processes, integrate with a modern tech stack (Typescript, PostgreSQL, GCP Serverless, GraphQL, Prisma, React), collaborate with the team, and architect for the future. The role requires 2+ years of production experience, a customer-driven mindset, and a willingness to take initiative.",
        "industries": [
            "Healthcare",
            "Software",
            "Data Management",
            "Technology",
            "Medical",
            "Finance"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem-Solving",
            "Creativity",
            "Attention to Detail",
            "Initiative",
            "Autonomy",
            "Teamwork",
            "Customer Focus"
        ],
        "hard_skills": [
            "ETL",
            "Data Warehousing",
            "Data Integration",
            "Data Modeling",
            "Data Analysis",
            "Data Architecture",
            "Typescript",
            "PostgreSQL",
            "GCP Serverless",
            "GraphQL",
            "Prisma",
            "React",
            "API Development",
            "Software Engineering",
            "Systems Design"
        ],
        "tech_stack": [
            "Typescript",
            "PostgreSQL",
            "GCP Serverless",
            "GraphQL",
            "Prisma",
            "React"
        ],
        "programming_languages": [
            "Typescript"
        ],
        "experience": 2,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Competitive salary",
            "Equity",
            "Healthcare benefits",
            "Meeting-light culture",
            "Flexible vacation/time-off policy"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Palo Alto, CA",
        "job_id": 3956739869,
        "company": "EV.Careers",
        "title": "Software Engineer, Backend, AI Evaluation",
        "created_on": 1720636021.9603086,
        "description": "What To Expect The AI Evaluation team is the main line of defense in ensuring customer safety. We are looking for an experienced backend developer to take ownership of the tooling we use to determine the best model for release. These tools have high visibility within the organization and help drive key decisions about model architecture, data integrity, and exported model performance. This engineer will collaborate with AI researchers, infrastructure & fleet telemetry teams, and release managers to propel these efforts forward. What You'll Do Create, maintain and expand internal tools for evaluation of Autopilot vehicle behaviorDesign and implement tools and dashboards to accelerate the evaluation cycle of our model trainingWork with AI researchers to support the evaluation of evolving projects and implement new production featuresDrive projects from inception to completion & present projects to org leadership What You'll Bring Bachelor's Degree in Computer Science, or proof of exceptional skills in related fieldStrong Python coding skillsExperience with web frameworks such as Django or FlaskStrong knowledge of relational database systems (PostgreSQL, MySQL)Strong knowledge of AWS, Splunk or other infrastructure toolsExperience with TypeScript and React is a plusExcellent interpersonal, communication, and collaboration skills Benefits Compensation and Benefits Along With Competitive Pay, As a Full-time Tesla Employee, You Are Eligible For The Following Benefits At Day 1 Of Hire: Aetna PPO and HSA plans > 2 medical plan options with $0 payroll deduction Family-building, fertility, adoption and surrogacy benefits Dental (including orthodontic coverage) and vision plans, both have options with a $0 paycheck contribution Company Paid (Health Savings Account) HSA Contribution when enrolled in the High Deductible Aetna medical plan with HSA Healthcare and Dependent Care Flexible Spending Accounts (FSA) LGBTQ+ care concierge services 401(k) with employer match, Employee Stock Purchase Plans, and other financial benefits Company paid Basic Life, AD&D, short-term and long-term disability insurance Employee Assistance Program Sick and Vacation time (Flex time for salary positions), and Paid Holidays Back-up childcare and parenting support resources Voluntary benefits to include: critical illness, hospital indemnity, accident insurance, theft & legal services, and pet insurance Weight Loss and Tobacco Cessation Programs Tesla Babies program Commuter benefits Employee discounts and perks program Expected Compensation $104,000 - $348,000/annual salary + cash and stock awards + benefits Pay offered may vary depending on multiple individualized factors, including market location, job-related knowledge, skills, and experience. The total compensation package for this position may also include other elements dependent on the position offered. Details of participation in these benefit plans will be provided if an employee receives an offer of employment. , Tesla",
        "url": "https://www.linkedin.com/jobs/view/3956739869",
        "summary": "Tesla is seeking an experienced backend developer to join their AI Evaluation team. This role will focus on creating, maintaining, and expanding internal tools for evaluating Autopilot vehicle behavior, accelerating model training evaluation, and supporting evolving AI projects. The ideal candidate will have strong Python, Django/Flask, database (PostgreSQL/MySQL), and AWS/Splunk experience, and excellent communication skills.",
        "industries": [
            "Automotive",
            "Technology",
            "Artificial Intelligence"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Project Management",
            "Ownership",
            "Decision Making"
        ],
        "hard_skills": [
            "Python",
            "Django",
            "Flask",
            "PostgreSQL",
            "MySQL",
            "AWS",
            "Splunk",
            "TypeScript",
            "React"
        ],
        "tech_stack": [
            "Python",
            "Django",
            "Flask",
            "PostgreSQL",
            "MySQL",
            "AWS",
            "Splunk",
            "TypeScript",
            "React"
        ],
        "programming_languages": [
            "Python",
            "TypeScript"
        ],
        "experience": 0,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science"
            ]
        },
        "salary": {
            "max": 348000,
            "min": 104000
        },
        "benefits": [
            "Medical Insurance",
            "Dental Insurance",
            "Vision Insurance",
            "Health Savings Account",
            "401k",
            "Employee Stock Purchase Plan",
            "Life Insurance",
            "Disability Insurance",
            "Employee Assistance Program",
            "Sick Leave",
            "Vacation Time",
            "Paid Holidays",
            "Childcare",
            "Weight Loss Program",
            "Tobacco Cessation Program",
            "Employee Discounts",
            "Perks Program"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Fremont, CA",
        "job_id": 3818359207,
        "company": "Info Way Solutions",
        "title": "Python Engineer",
        "created_on": 1720636023.769796,
        "description": "Hi Friends, I am sending requirements, kindly get back to me if the job description suits you. Python Engineer Location : Sunnyvale ,CA ( Onsite ) Job Description In this role you will be responsible for engineering, integrating and maintaining an analytics data platform based on a hybrid cloud infrastructure. Work collaboratively with our Data Engineering team to engineer custom solutions when required. Participate and provide support to our stakeholders and platform users when required. Key Qualifications 3+ years of professional experience with the Python programming language. 2+ years of professional software engineering experience (APIs, services, tooling and automation). Knowledge and experience in working with Docker, Kubernetes, and related technologies. Knowledge and experience with DevOps methodologies. Excellent time management skills with the ability to manage work to tight deadlines. Experience developing solid and scalable technical solutions to business problems. Experience with agile development and work management methodologies. Ability to pick up new technologies relatively quickly. Ability to communicate effectively (written and spoken). Ability to work with the multi-timezone development teams and self-manage own work. A strong work ethic. Nice to Have Professional experience with Java/Scala based analytics platforms (Hadoop, Spark, Hive, Trino, etc.). Professional experience with Splunk, Flink, Jupyter Notebook, Apache Airflow (and related technologies) is a big plus. Thanks and Regards Girish Tanwar Sr. Executive Recruitment & Operations Mobile: +19253019833 Email: girish.t@infowaygroup. com LinkedIn: https://www.linkedin.com/in/ girishtan007/ Info Way Solutions LLC. | 46520 Fremont Blvd, Suite 614 | Fremont, CA 94538",
        "url": "https://www.linkedin.com/jobs/view/3818359207",
        "summary": "Python Engineer responsible for engineering, integrating, and maintaining an analytics data platform based on a hybrid cloud infrastructure. Collaborate with Data Engineering team to build custom solutions. Provide support to stakeholders and platform users.",
        "industries": [
            "Software Development",
            "Data Analytics",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Collaboration",
            "Communication",
            "Problem Solving",
            "Time Management",
            "Self-Management",
            "Work Ethic",
            "Agile Development"
        ],
        "hard_skills": [
            "Python",
            "Docker",
            "Kubernetes",
            "DevOps",
            "APIs",
            "Services",
            "Tooling",
            "Automation",
            "Agile Development",
            "Java",
            "Scala",
            "Hadoop",
            "Spark",
            "Hive",
            "Trino",
            "Splunk",
            "Flink",
            "Jupyter Notebook",
            "Apache Airflow"
        ],
        "tech_stack": [
            "Python",
            "Docker",
            "Kubernetes",
            "DevOps",
            "APIs",
            "Services",
            "Tooling",
            "Automation",
            "Java",
            "Scala",
            "Hadoop",
            "Spark",
            "Hive",
            "Trino",
            "Splunk",
            "Flink",
            "Jupyter Notebook",
            "Apache Airflow"
        ],
        "programming_languages": [
            "Python",
            "Java",
            "Scala"
        ],
        "experience": 3,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3951914878,
        "company": "Greenlite",
        "title": "Software Engineer, Frontend",
        "created_on": 1720636025.4345088,
        "description": "About Us Fintechs and banks use Greenlite agents to automate workflows like suspicious activity investigations, entity due diligence and regulatory reporting. We've raised $4.8M led by Greylock, generate significant annual recurring revenue and already serve financial institutions across three continents. We're on a high-growth trajectory and we're looking for people who want to join us on the journey to building a billion-dollar company. The problem we’re trying to solve The internet revolutionized the way financial services are distributed, making them more accessible and convenient than ever before. However, this rapid expansion has also led to a new challenge: an overwhelming amount of noise and a scarcity of clear, actionable signals for making sound risk decisions. Today, teams are faced with the increasing challenge of navigating the complex landscape of compliance and onboarding. Manual due diligence processes are time-consuming and difficult to scale, hindering the ability to efficiently identify and mitigate risks. How we’re solving this problem This is where Greenlite comes in. Our mission is to boost the signal amidst the noise, empowering financial institutions to make informed decisions and expand access to financial services for the businesses and individuals who need it most. With the help of large language models, we're transforming the way compliance and onboarding teams operate, enabling them to work smarter, faster, and more effectively. The Role As a Front End Engineer at Greenlite, you’ll be responsible for creating and maintaining user interfaces that are both beautiful and functional. You’ll work closely with designers, backend engineers, and our founders to build intuitive and high-performing web applications. What To Expect Within days: Develop user interfaces using Next.JS and modern frontend technologies. Translate novel LLM capabilities into understandable, intuitive, and performant end-user experiences. Work directly with customers to understand their needs and improve user interfaces based on feedback. Collaborate with the CEO, CTO, and other founding members to help build out our team. Within Weeks Implement design systems and reusable components to ensure consistency and efficiency across all products. Lead the development of new features from concept to deployment. Participate in customer calls to gather feedback and refine front-end features. Within Months Own the architecture and development of our front-end applications. Ensure our web applications are responsive, fast, and accessible across all devices and browsers. Hire, onboard, and nurture future front-end teammates. About You Customer curious — You are interested in deeply understanding customer pain points and are motivated to engage with and solve customer problems. Experience building, and being part of, fast-growing B2B software products — You’ve worked on some of the world's best teams where you needed to ship fast and often. Bonus if you’ve built fintech or enterprise software products for large customers. Entrepreneurial drive — You’re able to work without explicit direction and feel intense ownership over the products you create. As a founding member, you’ll be expected to thrive in ambiguity that comes with an early-stage startup and can get things done without overcomplicating the solution. Strong communicator — As an early-stage startup, there will be lots of changes and ambiguity in the beginning. You should be comfortable sharing your feedback early, and often, and you should be comfortable bringing up product ideas frequently.",
        "url": "https://www.linkedin.com/jobs/view/3951914878",
        "summary": "Greenlite is a fintech company that uses AI to automate workflows like suspicious activity investigations, entity due diligence and regulatory reporting. They're looking for a Front End Engineer to create and maintain user interfaces for their web applications using Next.JS and modern frontend technologies.",
        "industries": [
            "Fintech",
            "Software",
            "AI",
            "Compliance"
        ],
        "soft_skills": [
            "Customer Curious",
            "Entrepreneurial Drive",
            "Strong Communicator"
        ],
        "hard_skills": [
            "Next.JS",
            "Frontend Development",
            "UI/UX Design",
            "Design Systems",
            "Product Development"
        ],
        "tech_stack": [
            "Next.JS",
            "LLM",
            "Frontend Technologies"
        ],
        "programming_languages": [
            "JavaScript"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Carlos, CA",
        "job_id": 3744945444,
        "company": "Electric Hydrogen",
        "title": "Senior Data Engineer",
        "created_on": 1720636026.9725385,
        "description": "Electric Hydrogen ’s mission is to make molecules to decarbonize our world! Our outstanding people are our most important asset and will allow us to deliver hydrogen from renewable electrolysis for heavy industry, at prices below fossil fuels. We are searching for an accomplished and motivated Senior Data Engineer to build and maintain mission-critical data infrastructure for the world's most powerful electrolyzer. As a Data Engineer, you will develop services to ingest, analyze, and store plant data and key performance metrics. Tools you build will monitor our electrolyzer fleet, assessing their operation and detecting issues before they lead to unplanned downtime. You will be based in San Carlos, CA [and can work on a hybrid basis], reporting to the Director of Software Engineering. Role And Responsibilities Ensure data availability to internal and external stakeholders from internal devices and deployed plants Architect streaming pipelines, analysis triggers, and notifications Building data models and develop schemas for relational databases (PostgreSQL, MySQL, MS SQL Server) Work with data from industrial PLCs and other connected devices Coalesce data from manufacturing and deployed plant data for use in analysis Design data systems for machine learning applications Qualifications Bachelor's degree in Computer Science, Engineering, or related field 6+ years working with data pipelines and industrial networks Significant experience with relational databases (PostgreSQL, MySQL, MS SQL Server) Familiarity with industrial streaming protocols such as MQTT and Kafka Familiarity with OPC servers, OPC UA, and PLC communication Experience using git and git-based workflows for version control. Experience working with time series data and time series databases Knowledge of process historians (OSI PI, FactoryTalk Historian) Experience building data pipelines in AWS, using tools such as IoT Core and Kinesis Knowledge of OSI PI ecosystem including asset framework Experience with machine learning data pipelines Compensation & Benefits | Senior Data Engineer (P4) San Carlos Zone $173,000—$190,000 USD Actual base salary offered to the hired applicant will be determined based on their work location, level, qualifications, job-related skills, as well as relevant education or training and experience. Base salary is just one part of Electric Hydrogen’s total rewards package. We feel strongly that our team should not have to worry about having quality healthcare. In addition to the base salary offered, the hired applicant may receive: an equity grant time off programs a $75/month cell phone allowance a 4% employer 401(k) match 100% fully paid premiums for employees and their families: medical, dental, vision, life insurance, short-term & long-term disability coverage a discretionary bonus Electric Hydrogen’s benefits programs are subject to eligibility requirements. About Electric Hydrogen Electric Hydrogen is a team of the world's experts in scaling technologies for the post-carbon world, with a proven record in transforming the grid and transportation sectors. Backed by some of the world's top venture capital firms, we design and manufacture electrolytic hydrogen systems matched to renewable power sources to create green hydrogen by splitting water. We are building a cost-effective and transformative path between renewable energy and multiple large industrial sectors. Abundant and low-cost renewable energy sources will power the world, and Electric Hydrogen technology will use this energy to decarbonize industry through sustainable materials. We were founded in 2020 and are based in California and Massachusetts. Electric Hydrogen is proud to be an equal opportunity employer. We are dedicated to building a diverse, inclusive, and authentic workplace for all to belong. We are aware that people from historically underrepresented groups are less likely to apply if they don't meet 100% of the job requirements. We are actively working on efforts to change this social norm. If you are excited about this role, we encourage you to apply !",
        "url": "https://www.linkedin.com/jobs/view/3744945444",
        "summary": "Electric Hydrogen is seeking a Senior Data Engineer to build and maintain data infrastructure for their powerful electrolyzer. This role involves developing services to ingest, analyze, and store plant data and key performance metrics, monitoring electrolyzer fleet operation, detecting issues, and building data systems for machine learning applications.",
        "industries": [
            "Energy",
            "Renewable Energy",
            "Hydrogen",
            "Manufacturing",
            "Technology",
            "Data Science",
            "Machine Learning"
        ],
        "soft_skills": [
            "Motivated",
            "Accomplished",
            "Problem-Solving",
            "Communication",
            "Collaboration",
            "Analytical",
            "Detail-Oriented"
        ],
        "hard_skills": [
            "Data Pipelines",
            "Industrial Networks",
            "Relational Databases",
            "PostgreSQL",
            "MySQL",
            "MS SQL Server",
            "MQTT",
            "Kafka",
            "OPC Servers",
            "OPC UA",
            "PLC Communication",
            "Git",
            "Version Control",
            "Time Series Data",
            "Time Series Databases",
            "Process Historians",
            "OSI PI",
            "FactoryTalk Historian",
            "AWS",
            "IoT Core",
            "Kinesis",
            "Machine Learning Data Pipelines",
            "Asset Framework"
        ],
        "tech_stack": [
            "PostgreSQL",
            "MySQL",
            "MS SQL Server",
            "MQTT",
            "Kafka",
            "OPC Servers",
            "OPC UA",
            "PLC",
            "Git",
            "OSI PI",
            "FactoryTalk Historian",
            "AWS",
            "IoT Core",
            "Kinesis"
        ],
        "programming_languages": [],
        "experience": 6,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Computer Science",
                "Engineering"
            ]
        },
        "salary": {
            "max": 190000,
            "min": 173000
        },
        "benefits": [
            "Equity Grant",
            "Time Off Programs",
            "Cell Phone Allowance",
            "401(k) Match",
            "Medical Insurance",
            "Dental Insurance",
            "Vision Insurance",
            "Life Insurance",
            "Short-Term Disability",
            "Long-Term Disability",
            "Discretionary Bonus"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Fremont, CA",
        "job_id": 3818360083,
        "company": "Info Way Solutions",
        "title": "Python Developer",
        "created_on": 1720636029.1127286,
        "description": "Hi Professionals, Hope you are doing good Job Description This is Jayaraman from Info Way Solutions, LLC We have job opening for Python Developer and the detailed Job description is given below: Kindly check the JD and share your views Python Developer (with Devops Experience) Sunnyvale, CA Onsite Requirrement Description In this role you will be responsible for engineering, integrating and maintaining an analytics data platform based on a hybrid cloud infrastructure. Work collaboratively with our Data Engineering team to engineer custom solutions when required. Participate and provide support to our stakeholders and platform users when required. Key Qualifications 3+ years of professional experience with the Python programming language. 2+ years of professional software engineering experience (APIs, services, tooling and automation). Knowledge and experience in working with Docker, Kubernetes, and related technologies. Knowledge and experience with DevOps methodologies. Excellent time management skills with the ability to manage work to tight deadlines. Experience developing solid and scalable technical solutions to business problems. Experience with agile development and work management methodologies. Ability to pick up new technologies relatively quickly. Ability to communicate effectively (written and spoken). Ability to work with the multi-timezone development teams and self-manage own work. A strong work ethic. Nice To Have Professional experience with Java/Scala based analytics platforms (Hadoop, Spark, Hive, Trino, etc.). Professional experience with Splunk, Flink, Jupyter Notebook, Apache Airflow (and related technologies) is a big plus. Thanks & Regards, Jayaraman Email: jayaraman@infowaygroup.com Direct: (925)-241-5719 Work: (925)-592-6160 Ext 105 Info Way Solutions LLC | 46520 Fremont Blvd, Suite 614 | Fremont, CA - 94538",
        "url": "https://www.linkedin.com/jobs/view/3818360083",
        "summary": "Python Developer with DevOps experience to build and maintain an analytics data platform on a hybrid cloud infrastructure. Responsibilities include engineering custom solutions, providing support to stakeholders, and collaborating with the Data Engineering team.",
        "industries": [
            "Software Development",
            "Data Analytics",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Collaboration",
            "Communication",
            "Problem-Solving",
            "Time Management",
            "Self-Management",
            "Work Ethic"
        ],
        "hard_skills": [
            "Python",
            "Docker",
            "Kubernetes",
            "DevOps",
            "Agile Development",
            "API Development",
            "Service Development",
            "Tooling",
            "Automation"
        ],
        "tech_stack": [
            "Python",
            "Docker",
            "Kubernetes",
            "Hadoop",
            "Spark",
            "Hive",
            "Trino",
            "Splunk",
            "Flink",
            "Jupyter Notebook",
            "Apache Airflow"
        ],
        "programming_languages": [
            "Python",
            "Java",
            "Scala"
        ],
        "experience": 3,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Port Hueneme, CA",
        "job_id": 3915760881,
        "company": "Resource Management Concepts, Inc.",
        "title": "Senior Database Engineer",
        "created_on": 1720636030.66602,
        "description": "Resource Management Concepts, Inc. (RMC) provides high-quality, professional services to government and commercial sectors. Our mission is to deliver exceptional management and technology solutions supporting the protection and preservation of the people and environment of the United States of America. RMC is hiring for a Senior Database Engineer to support a Department of the Navy (DoN) customer in Port Hueneme, CA. Position is contingent on successful award of contract. Requirements Minimum of five (5) years in the development and deployment of distributed relational databases and associated BI systems Minimum of five (5) years of experience with the Microsoft SQL server stack, Microsoft SQL, SQL Server Integration Services (SSIS), SQL Server Analysis Services (SSAS), and Microsoft SQL Server Reporting Services SSRS Minimum of five (5) years securing a SQL environment in accordance with DISA STIGs Minimum of five (5) years SQL Server configuration management, optimization, administration, development in client/server architecture, business object modeling and relational database design using MS SQL Server Minimum of five (5) years writing complex Transact SQL queries, stored-procedures and designing relational databases Master's degree in Computer, Electrical or Electronics Engineering or Mathematics with field of concentration in computer science CompTIA Security+ or equivalent (CCNA-Security; GICSP; GSEC) An Interim DoD SECRET Clearance is required to start. Personnel may be required to obtain and maintain a TS clearance. Applicant selected may be subject to a security investigation and must meet eligibility requirements for access to classified information Preferred Skills Experience with IBM Cognos and other On-Line Analytical Processing (OLAP) tools Understanding of Navy Working Capital Fund (WCF) accounting principles and generation of reports / OLAP analysis from WCF funding and cost information Experience leading and designing Service Oriented Architecture (SOA) based applications Experience in Navy Security requirements implementation and their impacts on overall software architecture Experience with Microsoft PowerBI Stack Benefits At RMC, we're committed to your career growth! RMC differentiates itself from other firms through its investment in our employees. We invest our resources to train, certify, educate, and build our employees. RMC can offer you a great place to work with a small company feel and give you the experience and certifications that will take your career to the next level. RMC also offers high-quality, low-deductible healthcare plans and a competitive 401K package. Salary at RMC is determined by various factors, including but not limited to location, a candidate's specific combination of education, knowledge, skills, competencies, and experience, as well as contract-specific requirements.",
        "url": "https://www.linkedin.com/jobs/view/3915760881",
        "summary": "Resource Management Concepts, Inc. (RMC) is seeking a Senior Database Engineer to support a Department of the Navy (DoN) customer in Port Hueneme, CA. The position requires a minimum of 5 years of experience in developing and deploying distributed relational databases and BI systems, along with extensive expertise in Microsoft SQL Server stack, security, and administration. Strong SQL skills and a Master's degree in a related field are mandatory. Preferred skills include experience with IBM Cognos, Navy Working Capital Fund (WCF) accounting, SOA, Navy security requirements, and Microsoft PowerBI. RMC offers competitive benefits like training, certifications, healthcare, and a 401K package.",
        "industries": [
            "Government",
            "Defense",
            "Technology",
            "Information Technology",
            "Data Management",
            "Analytics",
            "Consulting",
            "Engineering"
        ],
        "soft_skills": [
            "Leadership",
            "Teamwork",
            "Communication",
            "Problem Solving",
            "Analytical Skills",
            "Decision Making",
            "Time Management",
            "Organization",
            "Attention to Detail",
            "Adaptability"
        ],
        "hard_skills": [
            "SQL Server",
            "SSIS",
            "SSAS",
            "SSRS",
            "Transact SQL",
            "Relational Database Design",
            "Business Object Modeling",
            "DISA STIGs",
            "SQL Server Configuration Management",
            "SQL Server Optimization",
            "SQL Server Administration",
            "SQL Server Development",
            "Client/Server Architecture",
            "IBM Cognos",
            "OLAP",
            "Navy Working Capital Fund (WCF) Accounting",
            "SOA",
            "Navy Security Requirements",
            "Microsoft PowerBI"
        ],
        "tech_stack": [
            "Microsoft SQL Server",
            "SQL Server Integration Services (SSIS)",
            "SQL Server Analysis Services (SSAS)",
            "SQL Server Reporting Services (SSRS)",
            "IBM Cognos",
            "Microsoft PowerBI"
        ],
        "programming_languages": [
            "Transact SQL"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Master's",
            "fields": [
                "Computer Science",
                "Computer Engineering",
                "Electrical Engineering",
                "Electronics Engineering",
                "Mathematics"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Training",
            "Certifications",
            "Education",
            "Healthcare",
            "401K"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Jose, CA",
        "job_id": 3943635601,
        "company": "Adobe",
        "title": "Software Development Engineer",
        "created_on": 1720636034.7577884,
        "description": "Our Company Changing the world through digital experiences is what Adobe’s all about. We give everyone—from emerging artists to global brands—everything they need to design and deliver exceptional digital experiences! We’re passionate about empowering people to create beautiful and powerful images, videos, and apps, and transform how companies interact with customers across every screen. We’re on a mission to hire the very best and are committed to creating exceptional employee experiences where everyone is respected and has access to equal opportunity. We realize that new ideas can come from everywhere in the organization, and we know the next big idea could be yours! Adobe Experience Platform is the Adobe solution that helps customers to centralize and standardize their customer data and content across the enterprise powering 360-degree customer profiles, enabling data science and data governance to drive real-time personalized experiences. With Adobe Experience Platform, our enterprise customers are able to manage their data lifecycle. From ingestion to processing, analysis, attribution and activation, Adobe Experience Platform helps customer gain insight and use those insights in real time. We are looking for an experienced and curious engineer to be part of a core component team. As a software development engineer, we expect the candidate to contribute to one of the heavily used and technically sound feature of Adobe Experience Platform. Tasks here can range from software design and reviews to developing highly efficient enterprise software. Work closely with senior developers and architects and is able to produce good quality code. Since most of the work would involve either processing big data or managing sophisticated data pipelines, a candidate who can develop end to end view of a system or is willing to learn will be preferred. What You'll Do Design, implement, and own critical features in a micro-service architecture. Coordinate with architects, senior developers and other integrating teams. Subscribe to developments happening within Adobe ecosystem. Ensure quality of component meets high standards and ensure all appropriate validations are in place and being monitored. Work closely with product managers, asks questions and contribute to software architecture. Keep on look out for upcoming technologies and cost saving initiatives without compromising the product requirements and quality. Open to learning and experimenting with upcoming technologies in a fast paced environment. Act with clarity and precision, ensuring projects are driven with a strong sense of ownership and direction. What you'll need to succeed Bachelors' degree in Computer Science or equivalent. Prior experience working on large scale distributed systems or data processing systems. Proficiency in Java and/or Scala. Expertise in RESTful Web Services, Data Intensive applications, Data Processing pipelines, Relational and No-SQL DB. Exposure to Apache Spark, Apache Hadoop, Apache Kafka, AWS Services, Azure Services, Microservice Architecture, Databricks. Always striving to implement coding best practices and scalable design. Ability to model solutions for distributed processing problems. Our compensation reflects the cost of labor across several  U.S. geographic markets, and we pay differently based on those defined markets. The U.S. pay range for this position is $108,000 -- $198,500 annually. Pay within this range varies by work location and may also depend on job-related knowledge, skills, and experience. Your recruiter can share more about the specific salary range for the job location during the hiring process. At Adobe, for sales roles starting salaries are expressed as total target compensation (TTC = base + commission), and short-term incentives are in the form of sales commission plans. Non-sales roles starting salaries are expressed as base salary and short-term incentives are in the form of the Annual Incentive Plan (AIP). In addition, certain roles may be eligible for long-term incentives in the form of a new hire equity award. Adobe will consider qualified applicants with arrest or conviction records for employment in accordance with state and local laws and “fair chance” ordinances. Adobe is proud to be an Equal Employment Opportunity and affirmative action employer. We do not discriminate based on gender, race or color, ethnicity or national origin, age, disability, religion, sexual orientation, gender identity or expression, veteran status, or any other applicable characteristics protected by law. Learn more. Adobe aims to make Adobe.com accessible to any and all users. If you have a disability or special need that requires accommodation to navigate our website or complete the application process, email accommodations@adobe.com or call (408) 536-3015. Adobe values a free and open marketplace for all employees and has policies in place to ensure that we do not enter into illegal agreements with other companies to not recruit or hire each other’s employees.",
        "url": "https://www.linkedin.com/jobs/view/3943635601",
        "summary": "Adobe seeks an experienced Software Development Engineer to contribute to a core component of Adobe Experience Platform. This role involves designing, implementing, and owning critical features in a microservice architecture. The ideal candidate will have strong Java/Scala skills, experience with large-scale distributed systems and data processing, and familiarity with technologies like Apache Spark, Hadoop, Kafka, and cloud services (AWS, Azure). This role emphasizes quality code, collaboration with architects and product managers, and a proactive approach to learning and implementing best practices.",
        "industries": [
            "Software",
            "Technology",
            "Cloud Computing",
            "Data Management",
            "Marketing Technology",
            "Digital Marketing",
            "Customer Experience"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem-solving",
            "Teamwork",
            "Ownership",
            "Proactive",
            "Learning agility",
            "Attention to detail",
            "Critical thinking"
        ],
        "hard_skills": [
            "Java",
            "Scala",
            "RESTful Web Services",
            "Data Intensive Applications",
            "Data Processing Pipelines",
            "Relational Databases",
            "NoSQL Databases",
            "Apache Spark",
            "Apache Hadoop",
            "Apache Kafka",
            "AWS Services",
            "Azure Services",
            "Microservice Architecture",
            "Databricks",
            "Coding Best Practices",
            "Scalable Design",
            "Distributed Processing",
            "Design Patterns",
            "Software Architecture",
            "Code Review",
            "Version Control"
        ],
        "tech_stack": [
            "Java",
            "Scala",
            "RESTful Web Services",
            "Apache Spark",
            "Apache Hadoop",
            "Apache Kafka",
            "AWS Services",
            "Azure Services",
            "Microservice Architecture",
            "Databricks"
        ],
        "programming_languages": [
            "Java",
            "Scala"
        ],
        "experience": 3,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Computer Science"
            ]
        },
        "salary": {
            "max": 198500,
            "min": 108000
        },
        "benefits": [
            "Equity"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "California, United States",
        "job_id": 3888431704,
        "company": "Enexus Global Inc.",
        "title": "Lead Data Engineer - Remote",
        "created_on": 1720636036.9440415,
        "description": "Location - Remote Contract Type - W2/C2C/1099 Minimum Experience - 12+ Years Responsibilities Develop and enhance data-processing, orchestration, monitoring, and more by leveraging popular open-source software, AWS, and GitLab automation. Collaborate with product and technology teams to design and validate the capabilities of the data platform Identify, design, and implement process improvements: automating manual processes, optimizing for usability, re-designing for greater scalability Provide technical support and usage guidance to the users of our platform's services. Drive the creation and refinement of metrics, monitoring, and alerting mechanisms to give us the visibility we need into our production services. Qualifications Experience building and optimizing data pipelines in a distributed environment Experience supporting and working with cross-functional teams Proficiency working in Linux environment 8+ years of advanced working knowledge of SQL, Python, and PySpark 5+ years of experience with using a broad range of AWS technologies Experience using tools such as: Git/Bitbucket, Jenkins/CodeBuild, CodePipeline Experience with platform monitoring and alerts tools",
        "url": "https://www.linkedin.com/jobs/view/3888431704",
        "summary": "Data Engineer with 12+ years of experience needed to develop and enhance data processing, orchestration, monitoring, and automation using open-source software, AWS, and GitLab. Responsibilities include collaborating with product and technology teams, designing and validating platform capabilities, identifying and implementing process improvements, providing technical support, driving the creation of metrics and alerts, and optimizing data pipelines in a distributed environment.",
        "industries": [
            "Data & Analytics",
            "Technology",
            "Software Development",
            "Cloud Computing",
            "DevOps"
        ],
        "soft_skills": [
            "Collaboration",
            "Problem Solving",
            "Communication",
            "Technical Support",
            "Optimization",
            "Automation",
            "Process Improvement"
        ],
        "hard_skills": [
            "SQL",
            "Python",
            "PySpark",
            "AWS",
            "Linux",
            "Git",
            "Bitbucket",
            "Jenkins",
            "CodeBuild",
            "CodePipeline",
            "Platform Monitoring",
            "Alerting Tools"
        ],
        "tech_stack": [
            "AWS",
            "GitLab",
            "Open-Source Software",
            "Jenkins",
            "CodeBuild",
            "CodePipeline",
            "Bitbucket",
            "Git"
        ],
        "programming_languages": [
            "SQL",
            "Python",
            "PySpark"
        ],
        "experience": 12,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Los Angeles, CA",
        "job_id": 3957535418,
        "company": "ScoutMine",
        "title": "Software Engineer",
        "created_on": 1720636041.0195317,
        "description": "We are seeking talented and enthusiastic individuals to join our team as Remote Entry-Level Software Engineers. If you are passionate about coding, enjoy solving complex problems, and are looking for an opportunity to kickstart your career from the comfort of your own home, we want to hear from you! Our company is dedicated to innovation, growth, and creating a supportive environment where your contributions will be valued and your skills will be nurtured. Responsibilities Collaborate with senior engineers to design, develop, and maintain software applications. Write clean, efficient, and maintainable code in various programming languages. Participate in code reviews to ensure code quality and share knowledge. Debug and troubleshoot software issues to ensure optimal performance. Implement new features and enhancements based on project requirements. Contribute to the development and maintenance of technical documentation. Assist in testing and quality assurance processes. Stay current with emerging technologies and industry trends. Participate in agile development processes and sprints. Provide support and assistance to team members as needed. Ensure software is scalable, reliable, and secure. Optimize applications for maximum speed and scalability. Work closely with cross-functional teams to deliver high-quality products. Develop automated tests to ensure robustness of code. Participate in technical discussions and provide innovative solutions. Maintain version control using Git and other tools. Qualifications Bachelor's degree in Computer Science, Software Engineering, or a related field. Strong foundation in computer science principles and programming languages. Familiarity with web technologies such as HTML, CSS, and JavaScript. Basic understanding of database systems and SQL. Proficiency in at least one programming language (e.g., Python, Java, C++). Strong problem-solving skills and attention to detail. Ability to work independently and remotely. Good communication skills, both written and verbal. Familiarity with version control systems, particularly Git. Eagerness to learn and adapt to new technologies. Ability to manage time effectively and meet deadlines. Basic knowledge of software development life cycle (SDLC). Previous internship or project experience is a plus. Reliable internet connection and a suitable workspace at home. Benefits Competitive entry-level salary. Flexible remote work arrangements. Opportunities for career growth and professional development. Comprehensive training and mentorship program. Inclusive and collaborative company culture. Access to cutting-edge tools and technologies. Health, dental, and vision insurance options. Retirement savings plans. Paid time off and holiday pay. Employee recognition and rewards program. Team-building activities and virtual events. Opportunity to work on meaningful and impactful projects. Work-life balance and flexibility. Networking opportunities with industry professionals. Continuous learning and skill development resources. Supportive and approachable management team. Regular feedback and performance reviews to aid in career progression. Are you ready to embark on a rewarding career journey as a Software Engineer? Join our dynamic and innovative remote team where your passion for technology will drive real-world solutions. We are committed to your growth and success, providing you with the tools and opportunities to excel. Apply now and become an integral part of a company that values your input and invests in your future!",
        "url": "https://www.linkedin.com/jobs/view/3957535418",
        "summary": "This job listing seeks a Remote Entry-Level Software Engineer with a passion for coding and problem-solving. The role involves collaborating with senior engineers to design, develop, and maintain software applications, write clean and efficient code, participate in code reviews, debug and troubleshoot issues, implement new features, and stay updated with emerging technologies. The company offers a competitive salary, flexible work arrangements, comprehensive training, and a supportive environment focused on professional growth.",
        "industries": [
            "Software Development",
            "Technology",
            "Information Technology"
        ],
        "soft_skills": [
            "Communication",
            "Problem-solving",
            "Attention to Detail",
            "Time Management",
            "Teamwork",
            "Collaboration",
            "Adaptability",
            "Learning",
            "Eagerness"
        ],
        "hard_skills": [
            "HTML",
            "CSS",
            "JavaScript",
            "SQL",
            "Python",
            "Java",
            "C++",
            "Git",
            "Agile Development",
            "SDLC",
            "Testing",
            "Debugging",
            "Code Review"
        ],
        "tech_stack": [
            "HTML",
            "CSS",
            "JavaScript",
            "SQL",
            "Python",
            "Java",
            "C++",
            "Git"
        ],
        "programming_languages": [
            "Python",
            "Java",
            "C++"
        ],
        "experience": 0,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Software Engineering"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Competitive Entry-Level Salary",
            "Flexible Remote Work Arrangements",
            "Career Growth Opportunities",
            "Professional Development",
            "Training and Mentorship",
            "Inclusive Culture",
            "Cutting-Edge Tools and Technologies",
            "Health, Dental, and Vision Insurance",
            "Retirement Savings Plans",
            "Paid Time Off",
            "Holiday Pay",
            "Employee Recognition and Rewards",
            "Team-Building Activities",
            "Virtual Events",
            "Meaningful Projects",
            "Work-Life Balance",
            "Networking Opportunities",
            "Continuous Learning Resources",
            "Supportive Management",
            "Feedback and Performance Reviews"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Fremont, CA",
        "job_id": 3818364004,
        "company": "Info Way Solutions",
        "title": "Data Engineer with QE experience�",
        "created_on": 1720636042.6120927,
        "description": "Hi Professionals, Hope you are doing good Job Description This is Jayaraman from Info Way Solutions, LLC We have job opening for Data Engineer with QE experience and the detailed Job description is given below: Kindly check the JD and share your views Title: Data Engineer with QE experience Location: Austin or Remote Key skills required for the LPL Data Engineer with QE experience. Strong in QE(SDET) + Programming skills in python PySpark coding Snowflake or other data warehouse experience AWS Responsibilities Data Quality Management : Write Pipelines and help Automate the Quality engineering aspects in there Build new data pipelines and validate the pipelines for data accuracy -- by automating.. Thanks & Regards, Jayaraman Email: jayaraman@infowaygroup.com Direct: (925)-241-5719 Work: (925)-592-6160 Ext 105 Info Way Solutions LLC | 46520 Fremont Blvd, Suite 614 | Fremont, CA - 94538",
        "url": "https://www.linkedin.com/jobs/view/3818364004",
        "summary": "Data Engineer with QE experience needed for Info Way Solutions, LLC in Austin or remotely. Responsibilities include building and validating data pipelines, automating data quality management, and ensuring data accuracy. Requires strong programming skills in Python, PySpark, experience with Snowflake or other data warehouses, and AWS knowledge.",
        "industries": [
            "Data Engineering",
            "Quality Assurance",
            "Software Engineering",
            "Information Technology"
        ],
        "soft_skills": [
            "Automation",
            "Problem Solving",
            "Communication",
            "Teamwork"
        ],
        "hard_skills": [
            "Python",
            "PySpark",
            "Snowflake",
            "Data Warehousing",
            "AWS",
            "Data Quality Management",
            "Data Pipelines"
        ],
        "tech_stack": [
            "Python",
            "PySpark",
            "Snowflake",
            "AWS"
        ],
        "programming_languages": [
            "Python"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Diego, CA",
        "job_id": 3941631347,
        "company": "Software Technology Inc.",
        "title": "Software Engineer",
        "created_on": 1720636044.135354,
        "description": "Hello, This is Tejaswini from Software Technologies Inc . We have a job opening with our client for position of SOFTWARE ENGINEER if you are available and looking for any new opportunities, please send me your updated resume for below position ASAP. need Sr Software Engineers for a special project. They will both be working closely with my Principal Software Engineer to improve an internal tool we are using to identify and resolve misconfigurations in our AWS environment. Remote position Here Are Some Requirements For The Candidates 5+ years of Software Development Experience A very strong understanding of the AWS environment They should be able to determine what caused errors like those listed below and determine how we can resolve them All publicly accessible APIs behind an API Gateway should have at least one custom authorizer. Cloud Container should not have Sudo edit Function Parsing File Manipulation Vulnerability IAM access key must be rotated Immediately IAM Roles should not be inactive for more than 180 days S3 permissions granted to other AWS accounts in bucket policies should be restricted s3 Service Principal secrets should have a definite expiry. The VPC default security group should not allow inbound and outbound traffic sg They need to understand how to develop scripts (JavaScript - NPM/Python) or, better still, binaries (golang) that can remediate these problems Golang (preference) JavaScript - NPM Python They need to understand how Gitlab Pipelines work to block misconfigurations that a developer may push through the pipeline They should understand Infrastructure as code (IaC) concepts for AWS with a solid understanding of Cloudformation, and knowing Serverless Framework is an enormous plus! Cloudformation Terraform Serverless Framework Thanks, N.TEJASWINI NAIDU Technical recruiter Direct:404-777-9838 | Fax: 866-608-6686 Email: tejaswini.n@stiorg.com | Web: www.stiorg.com 100 Overlook Center, Suite 200 Princeton, NJ 08540.",
        "url": "https://www.linkedin.com/jobs/view/3941631347",
        "summary": "Software engineer needed to identify and resolve misconfigurations in an AWS environment. The role involves developing scripts and binaries to remediate problems and requires a strong understanding of IaC concepts (CloudFormation, Terraform, and Serverless Framework).",
        "industries": [
            "Information Technology",
            "Software Development",
            "Cloud Computing",
            "DevOps"
        ],
        "soft_skills": [
            "Problem Solving",
            "Communication",
            "Teamwork",
            "Analytical Skills",
            "Critical Thinking"
        ],
        "hard_skills": [
            "AWS",
            "JavaScript",
            "NPM",
            "Python",
            "Golang",
            "Gitlab Pipelines",
            "CloudFormation",
            "Terraform",
            "Serverless Framework"
        ],
        "tech_stack": [
            "AWS",
            "Gitlab Pipelines",
            "CloudFormation",
            "Terraform",
            "Serverless Framework",
            "JavaScript",
            "NPM",
            "Python",
            "Golang"
        ],
        "programming_languages": [
            "JavaScript",
            "Python",
            "Golang"
        ],
        "experience": 5,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Calabasas, CA",
        "job_id": 3316215273,
        "company": "EPCVIP, Inc.",
        "title": "Senior Data Engineer",
        "created_on": 1720636045.700265,
        "description": "Job Overview We are looking for a savvy Data Engineer to join our growing team of analytics experts. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The Data Engineer will support our software engineers, database architects and analysts on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. Responsibilities: Develop core data relationship, data ingest, data transformation services and search capabilities. Maintain all facets of data infrastructure, including backups/disaster recovery, software deployment/configuration, security best practices, performance tuning, and troubleshooting. Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies Review project objectives and determine best technology for implementation. Implement best practice standards for development, build and deployment automation. Design, maintain and optimize highly distributed analytics data stores Write well-abstracted, reusable and efficient code Requirements: 5+ years of relevant Data Engineering technical experience Bachelor’s degree in Technology based discipline preferred. Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases Build processes supporting data transformation, data structures, metadata, dependency and workload management Experience with AWS cloud services: Lambda, S3, Glue, Redshift, and Athena, or their open source equivalent (Zeppelin, Presto, etc) Experience with data pipelining with AWS Datapipeline, Airflow or Luigi Experience with object-oriented/object function scripting languages: Python code development, python unit testing (tox), Pyspark Up-to-date on latest industry trends; able to articulate trends and potential clearly and confidently Understanding of best practices within the development process Benefits: 401K Paid Vacation and Sick Time off Paid Parental Leave Health, Dental and Vision Insurance Coverage Short term/Long Term Disability Insurance Coverage Life Insurance Coverage Perks: Wellness Reimbursement Tuition Reimbursement In-home Vet Care Referral bonus Relocation Assistance Free parking Friday lunch catering Daily free snacks and drinks Yearly mini paid holiday company trip Lunch and Learn Sessions AFFIRMATIVE ACTION/EQUAL OPPORTUNITY EMPLOYER EPCVIP, Inc is an affirmative action and equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, disability, age, sexual orientation, gender identity, national origin, veteran status, criminal history, genetic information or any of the protected classes. EPCVIP, Inc is committed to providing access, equal opportunity and reasonable accommodation for individuals with disabilities in employment, its services, programs, and activities. To request reasonable accommodation, contact Human Resources at hr@epcvip.com.",
        "url": "https://www.linkedin.com/jobs/view/3316215273",
        "summary": "We are looking for a Data Engineer to join our team and expand and optimize our data and data pipeline architecture. Responsibilities include developing core data services, maintaining data infrastructure, optimizing data delivery, and building infrastructure for data extraction, transformation, and loading. The ideal candidate will have 5+ years of experience in Data Engineering, a Bachelor's degree in Technology, advanced SQL knowledge, experience with AWS cloud services, experience with data pipelining, experience with object-oriented/object function scripting languages, and up-to-date knowledge of industry trends.",
        "industries": [
            "Technology",
            "Data Engineering",
            "Analytics",
            "Software Development",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Self-directed",
            "Problem-solving",
            "Communication",
            "Teamwork",
            "Collaboration",
            "Analytical",
            "Organization",
            "Time Management",
            "Communication",
            "Critical thinking"
        ],
        "hard_skills": [
            "SQL",
            "Relational Databases",
            "AWS",
            "Lambda",
            "S3",
            "Glue",
            "Redshift",
            "Athena",
            "Zeppelin",
            "Presto",
            "AWS Datapipeline",
            "Airflow",
            "Luigi",
            "Python",
            "Pyspark",
            "Data Transformation",
            "Data Structures",
            "Metadata",
            "Dependency Management",
            "Workload Management",
            "Unit Testing",
            "Tox"
        ],
        "tech_stack": [
            "AWS",
            "Lambda",
            "S3",
            "Glue",
            "Redshift",
            "Athena",
            "Zeppelin",
            "Presto",
            "AWS Datapipeline",
            "Airflow",
            "Luigi",
            "Python",
            "Pyspark",
            "SQL"
        ],
        "programming_languages": [
            "Python",
            "SQL",
            "Pyspark"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Technology"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "401K",
            "Paid Vacation",
            "Sick Time off",
            "Paid Parental Leave",
            "Health Insurance",
            "Dental Insurance",
            "Vision Insurance",
            "Short term/Long Term Disability Insurance",
            "Life Insurance"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Foster City, CA",
        "job_id": 3843741724,
        "company": "Conviva",
        "title": "Senior DevOps Engineer",
        "created_on": 1720636047.2330012,
        "description": "If you've used Disney+, Peacock, or other top streaming platforms, you've already benefited from Conviva's technology. We empower the world’s leading B2C companies, including the biggest names in streaming media, to deliver exceptional digital experiences and optimize the “moments that matter” to their customers and their business. As the global leader in experience-centric operational analytics, Conviva has redefined big data analytics with our paradigm-shifting Time-State Analytics model. Our platform does what legacy observability and monitoring tools can’t: we eliminate the gaps between system performance, user experience, and engagement, enabling issue identification, impact assessment, and root cause resolution in seconds. This isn’t just important, its game-changing! Our platform processes over 5 trillion daily events, providing real-time, cost-effective, stateful computation across diverse data sets. This empowers operations teams for the first time to precisely and directly impact real-world business outcomes, including customer satisfaction and revenue. We are seeking an experienced DevOps Engineer to join our Tech Ops and DevOps team. As a DevOps Engineer, you'll play a pivotal role in collaborating with development teams to build and enhance cloud environments across diverse platforms and data centers. Automation expertise, infrastructure-as-code proficiency, and adept management of hybrid (cloud/on-prem) systems are at the core of what we value. What Success Will Look Like Collaborate with development teams to build and maintain cloud and on-premises hybrid environments across many interconnected platforms. Design CI/CD pipelines to streamline deployment and ensure the reliability of applications. Use tools like Terraform and Ansible to automate the provisioning and configuration of infrastructure components. Diagnose and resolve reliability issues across hardware, software, applications and network components. Work closely with development teams to troubleshoot and resolve application and service issues. Continuously improve Conviva’s SaaS services and infrastructure for improved availability, performance, and security. Implement security best practices focusing on the patching of operating systems and applications. Build proactive monitoring and alerting tools and participate in on-call rotations. Who You Are & What You've Done 5+ years of DevOps experience, demonstrating proficiency in automated system configuration, application deployment, and infrastructure-as-code. Proven experience in managing production environments on AWS and/or GCP, and familiarity with other cloud IaaS. Multi-account management is a plus. Proficiency in microservices, containerization, and orchestration through Kubernetes, GKE, or EKS. Strong Linux system administration skills. Experience with configuration management tools such as Terraform, Ansible, Puppet, or Chef. Proficiency in at least one scripting language: Shell, Perl, or Python. Good understanding of CI/CD pipelines, utilizing GitHub, CircleCI/Jenkins, and JFrog Artifactory. Strong knowledge of Git, SSL, and DNS, with a basic understanding of networking. Bonus points for expertise in Big Data systems administration (Hadoop/Druid) and experience. managing and running databases (MySQL/MSSQL/PostgreSQL). Required to be part of on-call rotations. The expected salary range for this full-time position is $160,000 - $200,000 + equity + benefits. The compensation and level are determined by various factors, including your qualifications, experience or relevant education. Underpinning the Conviva platform is a rich history of innovation. More than 60 patents represent award-winning technologies and standards, including first-of-its kind-innovations like Time-State analytics and AI-automated data modeling, that surface actionable insights. By understanding real-world human experiences and having the ability to act within seconds of observation, our customers can solve business-critical issues and focus on growing their business ahead of the competition. Examples of the brands Conviva has helped innovate, adapt, and scale at unprecedented speed include: DAZN, Disney+, HBO, Hulu, NBCUniversal, Paramount+, Peacock, Sky, Sling TV, Univision and Warner Bros Discovery. Privately held, Conviva is headquartered in Silicon Valley, California with offices and people around the globe. For more information, visit us at www.conviva.com. Join us to help extend our leadership position in big data streaming analytics to new audiences and markets!",
        "url": "https://www.linkedin.com/jobs/view/3843741724",
        "summary": "Conviva, a leading provider of streaming analytics, seeks a DevOps Engineer to contribute to the development and maintenance of cloud and hybrid environments, CI/CD pipelines, and infrastructure automation using technologies like Terraform and Ansible. This role involves troubleshooting, improving service reliability, and security, as well as participation in on-call rotations.",
        "industries": [
            "Technology",
            "Software",
            "Streaming",
            "Media",
            "Big Data"
        ],
        "soft_skills": [
            "Collaboration",
            "Problem-solving",
            "Troubleshooting",
            "Communication",
            "Automation",
            "Security",
            "Proactive",
            "On-call"
        ],
        "hard_skills": [
            "DevOps",
            "Cloud Environments",
            "AWS",
            "GCP",
            "Microservices",
            "Containerization",
            "Kubernetes",
            "GKE",
            "EKS",
            "Linux System Administration",
            "Terraform",
            "Ansible",
            "Puppet",
            "Chef",
            "Shell Scripting",
            "Perl",
            "Python",
            "CI/CD",
            "GitHub",
            "CircleCI",
            "Jenkins",
            "JFrog Artifactory",
            "Git",
            "SSL",
            "DNS",
            "Networking",
            "Hadoop",
            "Druid",
            "MySQL",
            "MSSQL",
            "PostgreSQL"
        ],
        "tech_stack": [
            "AWS",
            "GCP",
            "Kubernetes",
            "GKE",
            "EKS",
            "Terraform",
            "Ansible",
            "Puppet",
            "Chef",
            "GitHub",
            "CircleCI",
            "Jenkins",
            "JFrog Artifactory",
            "Hadoop",
            "Druid",
            "MySQL",
            "MSSQL",
            "PostgreSQL"
        ],
        "programming_languages": [
            "Shell",
            "Perl",
            "Python"
        ],
        "experience": 5,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 200000,
            "min": 160000
        },
        "benefits": [
            "Equity"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Sunnyvale, CA",
        "job_id": 3964406683,
        "company": "Walmart",
        "title": "Senior Software Engineer - Full Stack",
        "created_on": 1720636048.9731808,
        "description": "A career at Sam’s Club is where the world’s most complex challenges meet a kinder way of life. Our mission spreads far beyond the walls of our clubs. Join us and you'll discover why our company is a world leader in diversity and inclusion, sustainability, and community involvement. From day one, you’ll be empowered and equipped to do the best work of your life. We're driven by an intellectual curiosity that keeps us on the cutting-edge of user design and a seamless customer experience. As a Full Stack Engineer within the Member Access Platform organization (Sams AdTech) you will be responsible to build highly performant AdTech platforms that will support our future growth in the Ads Space. MAP business is growing rapidly, and you will get to be part of the journey that supports omni-channel Ad tactics for millions of Sam's Club members as well contribute to operational efficiencies for our business team members . About Tea m:Sam's Club is our membership warehouse club, a business model that provides our members with high-quality products at prices that are unrivaled by traditional retail. Sam's Club provides a carefully curated assortment of items, as well as developing and leading technologies and services such as Scan & Go, Club Pickup, and home delivery service in select markets. Sam's Club also provides travel, auto purchasing, pharmacy, optical, hearing aid centers, tire and battery centers, and a portfolio of business operations support service s. What You’ll Do:Leads and participates in small to medium scale projects by reviewing project requirements, translating requirements into technical solutions, gathering requested information, writing, and developing code, conducting unit testing. Communicating status and issues to team members and stakeholde rs.Collaborating with project team and cross functional teams. Troubleshooting open issues and bug-fixes, ensuring on-time delivery and hand-offs. Interacting with project manager to provide input on project pl an.Leads the work of other small groups of two to four engineers, including offshore associates, for assigned Engineering projects by proving pertinent documents, direction, and examples. Identifying short- and long-term solutions, providing feedback for proposed solutions. Performing design and code reviews of chang es.Troubleshoots business and production issues by gathering information (for example, issue, impact, criticality, possible root cause), engaging support teams to assist in the resolution of issues. Formulating an action plan, performing actions as designated in the plan. Interpreting the results to determine further action, performs root cause analysis to prevent future occurrence of issues and completing online documentati on.Provides support to the business by responding to user's questions, concerns, and issues (for example, technical feasibility, implementation strategies). Facilitating resolutions and leading cross-functional partnersh ip.Leads the discovery phase of small to medium projects to come up with high level design by partnering with the product management, project management, business and user experience teams and obtaining cross-function approva ls.Demonstrates up-to-date expertise and applies this to the development, execution, and improvement of action plans by providing expert advice and guidance to others. Supporting and aligning efforts to meet customer, business needs and building commitment for perspectives and rational es.Provides and supports the implementation of business solutions by building relationships and partnerships with key stakeholders. Identifying business needs, determining, and carrying out necessary processes and practices. Monitoring progress and results and adapting to competing demands, organizational changes, and new responsibiliti es.Be the champion and drive the initiatives for engineering and operational excellen ce.Analyze competing requirements and articulate tradeoffs and lead discussions with business and development team, leading white board sessions with te am.Creates training documentation. Oversees the tasks of less experienced programmers and stipulates system troubleshooting suppor ts. What You’ll Br ing:At least 8 years of experience in Software Application Develop mentAt least 5 years of experience in modern tech stack Java, Python, ReactJs, JavaSc riptAbility to take lead medium size initiatives and mentors a team of develop ers.Experienced in computing platforms such as GCP or A zureExperience building and deploying APIs and applicati ons.Experience in microservices architect ure.Experience in building application in continuous integration and deployment environment (CI/ CD).Have tech led the team and guided engine ers.Able to break down a product goal into dev deliverab les.Approaches risks head-on with options to mitig ate.Consistently hold high standards and passion for quality is inherent in everyth ing. About Walmart Global TechImagine working in an environment where one line of code can make life easier for hundreds of millions of people. That’s what we do at Walmart Global Tech. We’re a team of software engineers, data scientists, cybersecurity expert's and service professionals within the world’s leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered. We train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of re tail. Flexible, hybrid work:We use a hybrid way of working that is primarily in office coupled with virtual when not onsite. Our campuses serve as a hub to enhance collaboration, bring us together for purpose and deliver on business needs. This approach helps us make quicker decisions, remove location barriers across our global team and be more flexible in our personal lives. Be nefits:Benefits: Beyond our great compensation package, you can receive incentive awards for your performance. Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and muc h more. Equal Opportunity E mployer:Walmart, Inc. is an Equal Opportunity Employer – By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing diversity- unique styles, experiences, identities, ideas and opinions – while being inclusive of all people. The above information has been designed to indicate the general nature and level of work performed in the role. It is not designed to contain or be interpreted as a comprehensive inventory of all responsibilities and qualifications required of employees assigned to this job. The full Job Description can be made available as part of the hiring process.",
        "url": "https://www.linkedin.com/jobs/view/3964406683",
        "summary": "Full Stack Engineer for Sam's Club Member Access Platform (Sams AdTech) building high-performance ad tech platforms to support future growth in the ads space.  Responsibilities include leading project development, collaborating with cross-functional teams, troubleshooting issues, providing business support, leading discovery phases, driving engineering excellence, and mentoring developers. ",
        "industries": [
            "Retail",
            "E-commerce",
            "Technology",
            "Advertising"
        ],
        "soft_skills": [
            "Leadership",
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Troubleshooting",
            "Teamwork",
            "Mentoring",
            "Analytical",
            "Decision Making",
            "Risk Management",
            "Quality Focus"
        ],
        "hard_skills": [
            "Java",
            "Python",
            "ReactJs",
            "JavaScript",
            "GCP",
            "Azure",
            "APIs",
            "Microservices Architecture",
            "CI/CD",
            "Agile Development"
        ],
        "tech_stack": [
            "Java",
            "Python",
            "ReactJs",
            "JavaScript",
            "GCP",
            "Azure",
            "APIs",
            "Microservices Architecture",
            "CI/CD"
        ],
        "programming_languages": [
            "Java",
            "Python",
            "JavaScript"
        ],
        "experience": 8,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "401(k) match",
            "stock purchase plan",
            "paid maternity and parental leave",
            "PTO",
            "multiple health plans"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3813097181,
        "company": "1872 Consulting",
        "title": "Data Platform Engineer",
        "created_on": 1720636050.8531165,
        "description": "Data Platform Engineer: Elevator Pitch: Philo is a reinvented television streaming service building the future of television. They are headquartered in San Francisco, but have offices in NYC and Cambridge, MA as well. Philo's CEO, Andrew McCollum, is actually a co-founder of Facebook with Mark Zuckerberg - basically, Philo is working to reinvent the television experience for the modern age, and create an amazing experience for their users to watch the shows they love. Today, Philo offers a package of top-rated television channels for just $25 a month. You can watch live, save unlimited shows to watch later, or browse their huge library of content available on-demand. Their services can be streamed across all devices and the web. Selling Points: Great work environment and culture, great benefits, and LOTS of backing from huge organizations like Discovery, Viacom, AMC, A&E, and HBO. Location: Cambridge, MA, NYC, and San Fran (all remote right now but must be willing to come in for Hybrid work eventually) Salary: 5-8 years of experience up to 175K, 8-10 years of experience up to 200K + 20-30K in Equity (sell how much equity will be worth) Bonus: ﻿Equity 401K Match: ﻿n/a ﻿﻿*Can transfer sponsorship if candidate has H1V and eventually get them a Green Card if they are there a few years to keep them there. Just cannot directly start a new sponsorship themselves. Benefits: ﻿ Full health, dental and vision coverage for you and your family Flexible working hours Generous paid parental leave Unlimited paid time off for vacation and sick leave $2000 annual vacation bonus (we pay you to take a two week vacation) $5000 annually for professional development and educational assistance $500 \"TV stipend” for new employees to upgrade their home watching setup Dog-friendly office Position summary: ﻿﻿ You will be collaborating with a growing team of Data Scientists and Analysts. As the Data Platform Engineer you will be responsible for architecting their next generation data processing applications and reporting systems. In this role, you will have the opportunity to build wholly new systems and services. They have some of the foundational elements in place for a modern data stack (Redshift, dbt, Segment, Kubernetes, Docker, Airbyte, Mode) and an active data science practice. They are looking for someone who can determine the next phase of the roadmap for the data platform and to build it -- for example: when to use a different orchestration tool, and when to be less dependent on (3rd party) libraries for client app event tracking. They are looking to invest in ensuring that the core infrastructure (on which all of their research depends) is scalable, tested, robust, and able to evolve to meet their constantly changing data science needs. They are looking for an engineer who enjoys being product-minded in the sense that they own a product from beginning to end by designing, constructing, integrating, testing, documenting, and supporting their creations. Requirements: 5+ years of experience as a Data Engineer Highly proficient in at least one of these languages: Python, Ruby, and/or GoLang Experience working with Data Pipelines in a Cloud native environment (preferably AWS) Experience writing, testing, shipping, and maintaining clean production code within a collaborative and version-controlled (git) codebase Experience improving data processing CI/CD practices ﻿Interview Process: ﻿﻿First Round: 1 hour with Max (Talent Acquisition Manager) 30 mins with Scott (Head of Data) Second Round: Take home exercise they will send to Max directly via private Slack Third Round: 4 1-hour meetings in one day Final: Follow up with Scott and CTO for 1-hour",
        "url": "https://www.linkedin.com/jobs/view/3813097181",
        "summary": "Philo, a streaming service, is seeking a Data Platform Engineer to architect and build next-generation data processing applications and reporting systems. You'll work with a team of data scientists and analysts to design, build, integrate, test, document, and support new systems and services.  You'll have the opportunity to shape the future of their data platform, leveraging existing technologies like Redshift, dbt, Segment, Kubernetes, Docker, Airbyte, and Mode.  Philo prioritizes building a scalable, robust, and evolving data infrastructure to support their constantly evolving data science needs.",
        "industries": [
            "Media & Entertainment",
            "Streaming",
            "Technology",
            "Data & Analytics"
        ],
        "soft_skills": [
            "Collaboration",
            "Communication",
            "Problem Solving",
            "Product-Minded",
            "Ownership",
            "Testing",
            "Documentation",
            "Support",
            "Adaptability",
            "Innovation"
        ],
        "hard_skills": [
            "Python",
            "Ruby",
            "GoLang",
            "Data Pipelines",
            "AWS",
            "Git",
            "CI/CD",
            "Redshift",
            "dbt",
            "Segment",
            "Kubernetes",
            "Docker",
            "Airbyte",
            "Mode"
        ],
        "tech_stack": [
            "Redshift",
            "dbt",
            "Segment",
            "Kubernetes",
            "Docker",
            "Airbyte",
            "Mode"
        ],
        "programming_languages": [
            "Python",
            "Ruby",
            "GoLang"
        ],
        "experience": 5,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 200000,
            "min": 175000
        },
        "benefits": [
            "Full health, dental and vision coverage for you and your family",
            "Flexible working hours",
            "Generous paid parental leave",
            "Unlimited paid time off for vacation and sick leave",
            "2000$ annual vacation bonus",
            "5000$ annually for professional development and educational assistance",
            "500$ 'TV stipend'",
            "Dog-friendly office"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Santa Ana, CA",
        "job_id": 3963369059,
        "company": "Universal Electronics",
        "title": "Cloud Software Engineer II",
        "created_on": 1720636052.4074986,
        "description": "At UEI you will be part of a world class team that is working to innovate and revolutionize the meaning of ‘wireless control’. Whether it’s via chips, software licenses or turnkey products, we are continuing to reinvent how consumers interact with devices and services in their home. Each day we make the connected home smarter, easier to connect and use, and more sustainable. We delight our Fortune 100 customers such as Comcast, Apple, Samsung, Google, Vivint and Daikin with ground breaking wireless technology solutions such as advanced, voice-enabled remote controls, cloud control solutions, extreme low power Bluetooth silicon with energy harvesting capabilities, smart thermostats and sensors and many other IoT solutions. The Software Engineer II is responsible for developing customer-facing cloud services and internal tools to enable a range of new data driven high performance applications for the connected home. Responsibilities Design, Develop, deploy and refine a world-class cloud infrastructure and services at scale Design and develop tools to process large data sets with statistical algorithms, machine learning and data mining methodologies Work with Data Infrastructure team in the design and architecture of an optimized database schema, architecture, queries and complex stored procedures for OLTP, OLAP and unstructured data services Must be able to assess scope and define timelines, plans and roadmap Qualifications 2-3 years of experience Full stack experience including solid hands on experience with Microsoft technologies and platforms such as Microsoft Azure and Microsoft SQL Server Proficiency in Microsoft .Net (C#) and complementary business layer and front-end technologies including RESTful services, SOAP and JSON, HTML5, Angular, etc Education Bachelor's degree in Computer Science or Technical field Universal Electronics Inc. (NASDAQ: UEIC) is the worldwide leader in universal control and sensing technologies for the smart home. Its broad portfolio of patents includes QuickSet® software that utilizes the world’s most complete knowledge graph to detect and interact with thousands of entertainment and smart home devices. The company designs, develops, and manufactures innovative products that are used by the world’s leading brands in the audio, video, subscription broadcasting, connected home, home energy management, and mobile device markets. UEI’s many first-to-market innovations have helped transform the home entertainment control, home security, and home energy management and sensing industries. Universal Electronics Inc. is an equal employment opportunity employer. We are proud of our diverse workforce and we believe having diverse teams that everyone brings their whole self to work everyday is key to all of our success. We welcome all people of different experiences, backgrounds, perspectives and abilities.",
        "url": "https://www.linkedin.com/jobs/view/3963369059",
        "summary": "Software Engineer II at Universal Electronics Inc. (UEI) is responsible for developing cloud services and internal tools for connected home applications. The role requires experience with Microsoft technologies, .Net (C#), and front-end technologies like RESTful services, SOAP, JSON, HTML5, and Angular. The company offers a competitive salary and benefits package and is committed to diversity and inclusion.",
        "industries": [
            "Technology",
            "Consumer Electronics",
            "Software",
            "Internet of Things (IoT)",
            "Smart Home",
            "Cloud Computing",
            "Data Science"
        ],
        "soft_skills": [
            "Problem Solving",
            "Communication",
            "Teamwork",
            "Analytical Skills",
            "Time Management",
            "Project Management",
            "Leadership"
        ],
        "hard_skills": [
            "Microsoft Azure",
            "Microsoft SQL Server",
            ".Net (C#)",
            "RESTful Services",
            "SOAP",
            "JSON",
            "HTML5",
            "Angular",
            "Data Processing",
            "Statistical Algorithms",
            "Machine Learning",
            "Data Mining",
            "Database Design",
            "SQL",
            "Stored Procedures",
            "OLTP",
            "OLAP",
            "Unstructured Data"
        ],
        "tech_stack": [
            "Microsoft Azure",
            "Microsoft SQL Server",
            ".Net (C#)",
            "RESTful Services",
            "SOAP",
            "JSON",
            "HTML5",
            "Angular"
        ],
        "programming_languages": [
            "C#"
        ],
        "experience": 2,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Technical"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Equal Employment Opportunity",
            "Diversity and Inclusion"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco Bay Area",
        "job_id": 3938332564,
        "company": "Vanta",
        "title": "Senior Data Engineer",
        "created_on": 1720636054.3559415,
        "description": "At Vanta, our mission is to secure the internet and protect consumer data. We believe that security should be monitored and verified continuously, and we empower companies to practice better security and prove it with ease. Vanta has a kind and talented team, and while some have prior security experience, many have been successful at Vanta without it. Our Data and Analytics team is currently looking for a Senior Data Engineer to join us! As a Data Engineer, you’ll be responsible for laying the foundation for a best-in-class analytics function. You’ll partner closely with our engineering team and business stakeholders to ensure that our analytics stack and processes meet the business needs today with an eye towards the future. What you’ll do as a Senior Data Engineer at Vanta: Design and implement complex data models, modeling metadata, building reports and dashboards and creating reporting tools for data science and ML products users. Design and deploy data infrastructure needed to drive data-driven decision-making solutions Be the company’s expert on data administration and master data management Be a technical thought leader on the development of scalable data systems Develop front end applications to expose analytical data sets enterprise wide Write highly tuned, scalable SQL queries running over large-scale, heterogeneous data warehouses Work with the Product and Enterprise Engineering system teams to structure source systems for reporting consumption across the enterprise Help maintain the CDC pipeline to power customer reporting How to be successful in this role: Have at least four years of experience working with data and two years of experience in Software Engineering or a related field. Have experience with common analytics tooling (e.g. Stitch/Fivetran, Snowflake/BigQuery/Redshift, dbt, Airflow, Dagster, Looker/Mode/Sigma). Have led an implementation of Debezium or another CDC event tracking system. Have good working knowledge of AWS data infra systems and Terraform. Bring a system-oriented and software engineering mindset to the Data Engineering practice. We’re looking to build frameworks that manage data, and minimize bespoke queries Deep knowledge of crafting dimensional and fact models in modern data fashion (experience at Amazon, Facebook, etc a plus) Have a passion for enabling the developer experience of data, and being obsessed with giving data super powers across the company. Desire to lead the industry in security, anonymization, and compliance management when it comes to data warehousing What you can expect as a Vantan: Industry-competitive compensation 100% covered medical, dental, and vision benefits with dependents coverage 16 weeks fully-paid parental Leave for all new parents Health & wellness and remote workplace stipends 401(k) matching Flexible work hours and location Open PTO policy 9 paid holidays in the US Offices in SF, NYC, Dublin, and Sydney To provide greater transparency to candidates, we share base pay ranges for all US-based job postings regardless of state. We set standard base pay ranges for all roles based on function, level, and country location, benchmarked against similar-stage growth companies. Final offer amounts are determined by multiple factors, including candidate location, skills, depth of work experience, and relevant licenses/credentials, and may vary from the amounts listed below. The salary or OTE range for this position is $170,000 - $200,000. This role may also be eligible for commissions/bonus, equity, medical benefits, 401(k) plan, and other company perk programs. At Vanta, we are committed to hiring diverse talent of different backgrounds and as such, it is important to us to provide an inclusive work environment for all. We do not discriminate on the basis of race, gender identity, age, religion, sexual orientation, veteran or disability status, or any other protected class. As an equal opportunity employer, we encourage and welcome people of all backgrounds to apply. About Vanta We started in 2018, in the wake of several high-profile data breaches. Online security was only becoming more important, but we knew firsthand how hard it could be for fast-growing companies to invest the time and manpower it takes to build a solid security foundation. Vanta was inspired by a vision to restore trust in internet businesses by enabling companies to improve and prove their security.From our early days automating security monitoring for compliance standards like SOC 2, HIPAA and ISO 27001 to creating the world's leading Trust Management Platform, our vision remains unchanged. Now more than ever, making security continuous—not just a point-in-time check— is essential. Thousands of companies rely on Vanta to build, maintain and demonstrate their trust— all in a way that's real-time and transparent.",
        "url": "https://www.linkedin.com/jobs/view/3938332564",
        "summary": "Vanta, a company dedicated to internet security and data protection, is seeking a Senior Data Engineer to build and maintain their analytics platform. Responsibilities include designing complex data models, building reporting tools, and ensuring data infrastructure meets business needs. The ideal candidate has 4+ years of data experience, 2+ years of software engineering experience, and expertise with analytics tools such as Stitch/Fivetran, Snowflake/BigQuery/Redshift, dbt, Airflow, Dagster, and Looker/Mode/Sigma. Knowledge of AWS data infrastructure and Terraform is essential, and experience with Debezium or similar CDC systems is a plus. This role offers competitive compensation, benefits, and a flexible work environment.",
        "industries": [
            "Security",
            "Data Analytics",
            "Software",
            "Compliance",
            "Cybersecurity"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Leadership",
            "Technical Expertise",
            "Passion for Data",
            "System-Oriented Thinking",
            "Data-Driven Decision Making",
            "Technical Thought Leadership"
        ],
        "hard_skills": [
            "Data Modeling",
            "Metadata Modeling",
            "Reporting",
            "Dashboarding",
            "Data Infrastructure",
            "Data Administration",
            "Master Data Management",
            "Scalable Data Systems",
            "Front-End Development",
            "SQL",
            "Data Warehousing",
            "CDC (Change Data Capture)",
            "AWS Data Infrastructure",
            "Terraform",
            "Dimensional Modeling",
            "Fact Modeling",
            "Debezium"
        ],
        "tech_stack": [
            "Stitch/Fivetran",
            "Snowflake",
            "BigQuery",
            "Redshift",
            "dbt",
            "Airflow",
            "Dagster",
            "Looker",
            "Mode",
            "Sigma",
            "Debezium",
            "AWS",
            "Terraform"
        ],
        "programming_languages": [
            "SQL"
        ],
        "experience": 4,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 200000,
            "min": 170000
        },
        "benefits": [
            "Industry-competitive compensation",
            "100% covered medical, dental, and vision benefits",
            "Dependents coverage",
            "16 weeks fully-paid parental leave",
            "Health & wellness stipends",
            "Remote workplace stipends",
            "401(k) matching",
            "Flexible work hours and location",
            "Open PTO policy",
            "9 paid holidays",
            "Offices in SF, NYC, Dublin, and Sydney"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco County, CA",
        "job_id": 3913679601,
        "company": "Lyft",
        "title": "Senior Backend Engineer, Telematics",
        "created_on": 1720636056.331357,
        "description": "At Lyft, our mission is to improve people's lives with the world's best transportation. To accomplish this, we start with our community by creating an open, inclusive, and diverse organization. We are looking for an experienced backend software engineer to join our Telematics team. At Lyft, Telematics data is used to make Lyft safer by reducing accident risk, which in turn also makes Lyft more affordable for riders and a better way to earn a living for drivers. The Telematics team acts as a platform team that processes vast amounts of high-frequency ride data, serving our Telematics ML models (e.g. harsh braking, harsh acceleration, etc.) and deriving insights for partner teams. Our technology stack runs on AWS, Kubernetes, Spark and Apache Airflow. In this role, you will work with incredibly passionate and talented colleagues from software engineering (mobile, server and data), machine learning and data science, actuarial science on building industry-leading innovations that make Lyft safer by reducing the frequency and severity of accidents on the platform. If you are a seasoned engineer with a passion for innovation, high-volume data pipelines, microservices, working in cross-disciplinary environments and possess the skills to ensure the ongoing maintenance and improvement of services, you will thrive on our team! You can read about some of the products that the Telematics powers in this blog post: https://www.lyft.com/blog/posts/lyfts-impact-on-road-safety Responsibilities: Be a champion of the team’s safety and affordability mission by serving as a reliable source of risk insights for other teams at the company Lead large projects, ensuring end-to-end execution with a focus on high quality and reliability. Work cross-functionally with Data Scientists to productionize models to derive safety insights from Telematics data, driving the team’s industry-leading innovation in this space Actively unblock and support team members, fostering a collaborative and efficient work environment. Take a lead role in the ongoing maintenance of our Telematics systems, ensuring their stability and reliability. Collaborate with cross-functional teams to define and refine the roadmap for Telematics projects. Utilize your expertise in Python, AWS, Spark, SQL, etc. to deliver robust and scalable solutions. Participate in our teams on-call rotations, respond to incidents and support other teams mitigate customer impacting events Experience: 5+ years of software engineering industry experience Experience in backend software development of large-scale distributed systems Experience and/or interest in running large batch processing jobs with Spark or other large scale compute architectures Experience and/or interest in Library development Experience and/or interest in serving, versioning, monitoring ML models in production environments Experience defining API schemas and developing backend services in a microservices environment Experience with S3, Spark, Presto, Parquet You're a great communicator, and can advocate for your proposals while also empathizing with your cross-functional teammates' goals and priorities You collaborate well with a geographically distributed team Benefits: Great medical, dental, and vision insurance options Mental health benefits Family building benefits In addition to 12 observed holidays, salaried team members have unlimited paid time off, hourly team members have 15 days paid time off 401(k) plan to help save for your future 18 weeks of paid parental leave. Biological, adoptive, and foster parents are all eligible Pre-tax commuter benefits Lyft Pink - Lyft team members get an exclusive opportunity to test new benefits of our Ridership Program Lyft is an equal opportunity/affirmative action employer committed to an inclusive and diverse workplace. All qualified applicants will receive consideration for employment without regards to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status or any other basis prohibited by law. We also consider qualified applicants with criminal histories consistent with applicable federal, state and local law. This role will be in-office on a hybrid schedule — Team Members will be expected to work in the office 3 days per week on Mondays, Thursdays and a team-specific third day. Additionally, hybrid roles have the flexibility to work from anywhere for up to 4 weeks per year. #Hybrid The expected base pay range for this position in the San Francisco area is $144,000 - $180,000. Salary ranges are dependent on a variety of factors, including qualifications, experience and geographic location. Range is not inclusive of potential equity offering, bonus or benefits. Your recruiter can share more information about the salary range specific to your working location and other factors during the hiring process.",
        "url": "https://www.linkedin.com/jobs/view/3913679601",
        "summary": "Lyft is seeking an experienced backend software engineer to join their Telematics team. This role focuses on building and maintaining large-scale data pipelines, processing high-frequency ride data, and supporting ML models for safety and affordability initiatives. The ideal candidate will have strong experience in backend development, distributed systems, Spark, AWS, and API development. This role is hybrid with a requirement to be in the office 3 days per week.",
        "industries": [
            "Transportation",
            "Technology",
            "Data Science",
            "Machine Learning",
            "Software Engineering"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Leadership",
            "Teamwork",
            "Empathy"
        ],
        "hard_skills": [
            "Backend Software Development",
            "Distributed Systems",
            "Spark",
            "AWS",
            "API Development",
            "Microservices",
            "SQL",
            "Python",
            "S3",
            "Presto",
            "Parquet",
            "Kubernetes",
            "Apache Airflow"
        ],
        "tech_stack": [
            "AWS",
            "Kubernetes",
            "Spark",
            "Apache Airflow"
        ],
        "programming_languages": [
            "Python",
            "SQL"
        ],
        "experience": 5,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 180000,
            "min": 144000
        },
        "benefits": [
            "Medical Insurance",
            "Dental Insurance",
            "Vision Insurance",
            "Mental Health Benefits",
            "Family Building Benefits",
            "Unlimited Paid Time Off",
            "Paid Time Off",
            "401(k) Plan",
            "Parental Leave",
            "Commuter Benefits",
            "Lyft Pink"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Gardena, CA",
        "job_id": 3891681884,
        "company": "Motivo",
        "title": "Software Engineer",
        "created_on": 1720636057.8503087,
        "description": "We are looking for a hands-on Software Engineer . What does that even mean, and why should you consider working for us, you ask? After all, we're not Facebook, Google, or Amazon. We don't have hundreds of millions of active users. We're also not Lockheed, Northrop Grumman, or L3, so you won't get to work on projects with millions of lines of code with 5+ years to deliver. Here's why you might find us interesting: We build things that solve real world problems. We solve client opportunities using a smart balance between well-designed software and well-designed hardware. Our teams are multidisciplinary, so you'll work alongside some of the best mechanical, electrical, and systems engineers in the business. Solutions come together in weeks to months We're a product development firm. Imagine contributing to 5+ unique projects per year. Sounds unbelievable? We do it all the time. Each project is an opportunity to solve unique design challenges and learn and grow in the process. They're pretty cool projects, too! We have some examples on our website, but the most exciting ones we're not even allowed to talk about yet! We offer a collaborative learning environment, and there are a lot of really smart people that work here. Want to follow in the footsteps of a PhD roboticist? What about learning more about the simulation of dynamic systems? How about learning distributed control algorithms that run on heterogeneous networks? We have a bunch of other generalist engineers on staff in a wide array of engineering disciplines We emphasize outcomes over process. Each project defines the right amount of design rigor. Some things like source control are part of every project. Other things, like extensive test coverage, are warranted for some projects and not for others. Who decides the right amount of rigor? You do. We're not going to fret over style guides unless it's a good idea. Motivo is a project-based small business focused on the development of mixed-discipline products for clients, spanning mechanical, electrical, and software system design and prototype builds with an emphasis on tightly integrated multidisciplinary systems. We develop products in diverse market segments including agriculture, automotive, aerospace, medical/health, consumer electronics, power delivery, pro sports and clean energy. Motivo specializes in taking solutions through both conceptual and detailed engineering phases, solving complex engineering challenges along the way, and transitioning the initial concept to a fully engineered solution Requirements Good candidates will have many of these characteristics: BS degree or higher in Computer Science, Electrical Engineering, or related field One to two (1-2) years of applicable professional experience, or significant internship responsibility Strong ability to connect and communicate with clients and project teams Strength in at least one general-purpose, low-to-mid-level, compiled language (ex. C, C++, Go, Rust, etc.) Strength in at least one higher-level, object-oriented, interpreted or JIT-compiled language (ex. Python, Javascript/Typescript, Java/Kotlin) Comfortable in Linux-based operating systems, including on the command line/terminal Comfortable with a version control system (we use Git) Experience developing for embedded applications (microcontrollers like Arduino or STM32 and/or embedded Linux like Raspberry Pi) Experience developing for desktop applications (Linux, Windows, macOS) Interest in working on multi-disciplinary teams building robotic systems! Positive, solutions- and outcome-oriented attitude! Ideal candidates may have some or many of these characteristics: Many or all of the above list, plus.. Three or more years of directly applicable work experience Strong ability to translate application feature concepts into well-structured and well-implemented code Ability to divide up and delegate bigger problems into its constituent components Ability to architect complete applications and outline them visually with block diagrams Experience using 3rd party APIs and frameworks (UI, web, backend, elastic) Experience developing iOS and Android apps (cross-platform is best!) Experience developing server, cloud, or elastic services Benefits We're convinced that the team and projects are hands down the best part of working at Motivo, but we also have some pretty sweet benefits including company-wide profit sharing, high-quality insurance plans, 401k match, generous paid vacation time, a Monday-Thursday 4/10 work week and more! Base Salary Range: $104,000 - $150,000 Annual Profit Sharing estimate: $16,000 - $30,000 Salary is dependent on experience, knowledge, and interview performance.",
        "url": "https://www.linkedin.com/jobs/view/3891681884",
        "summary": "Motivo is a product development firm seeking a Software Engineer to join their multidisciplinary team. They build products that solve real-world problems, combining software and hardware.  Engineers contribute to various projects across diverse industries, with an emphasis on collaborative learning and outcomes over process. They offer competitive benefits, including profit sharing, insurance, 401k match, and flexible work hours.",
        "industries": [
            "Agriculture",
            "Automotive",
            "Aerospace",
            "Medical/Health",
            "Consumer Electronics",
            "Power Delivery",
            "Pro Sports",
            "Clean Energy"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem-solving",
            "Positive Attitude",
            "Solution-oriented",
            "Outcome-oriented"
        ],
        "hard_skills": [
            "C",
            "C++",
            "Go",
            "Rust",
            "Python",
            "Javascript",
            "Typescript",
            "Java",
            "Kotlin",
            "Linux",
            "Git",
            "Arduino",
            "STM32",
            "Raspberry Pi",
            "Windows",
            "macOS",
            "UI",
            "Web",
            "Backend",
            "Elastic",
            "iOS",
            "Android",
            "Server",
            "Cloud"
        ],
        "tech_stack": [
            "Arduino",
            "STM32",
            "Raspberry Pi",
            "Linux",
            "Git",
            "UI",
            "Web",
            "Backend",
            "Elastic",
            "iOS",
            "Android",
            "Server",
            "Cloud"
        ],
        "programming_languages": [
            "C",
            "C++",
            "Go",
            "Rust",
            "Python",
            "Javascript",
            "Typescript",
            "Java",
            "Kotlin"
        ],
        "experience": 1,
        "education": {
            "min_degree": "BS",
            "fields": [
                "Computer Science",
                "Electrical Engineering"
            ]
        },
        "salary": {
            "max": 150000,
            "min": 104000
        },
        "benefits": [
            "Profit Sharing",
            "Insurance",
            "401k Match",
            "Paid Vacation Time",
            "4/10 Work Week"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Santa Clara, CA",
        "job_id": 3940059029,
        "company": "Pure Storage",
        "title": "Software Engineer Grad, Hyperscale",
        "created_on": 1720636059.5557892,
        "description": "BE PART OF BUILDING THE FUTURE. What do NASA and emerging space companies have in common with COVID vaccine R&D teams or with Roblox and the Metaverse? The answer is data, -- all fast moving, fast growing industries rely on data for a competitive edge in their industries. And the most advanced companies are realizing the full data advantage by partnering with Pure Storage. Pure’s vision is to redefine the storage experience and empower innovators by simplifying how people consume and interact with data. With 11,000+ customers including 58% of the Fortune 500, we’ve only scratched the surface of our ambitions . Pure Is Blazing Trails And Setting Records For ten straight years, Gartner has named Pure a leader in the Magic Quadrant Our customer-first culture and unwavering commitment to innovation have earned us a certified Net Promoter Score in the top 1% of B2B companies globally Industry analysts and press applaud Pure’s leadership across these dimensions And, our 5,000+ employees are emboldened to make Pure a faster, stronger, smarter company as we go If you, like us, say “bring it on” to exciting challenges that change the world, we have endless opportunities where you can make your mark. Position Overview We are actively seeking a highly motivated engineer who excels in dynamic, small-team settings. You will get a chance to contribute to innovative product ideas that tackle challenges within massive-scale cloud storage environments. These new storage systems will allow our largest customers to meet the demands of AI and hyperscale workloads. Responsibilities Joining this team as a Software Engineer Grad, the work will range between: Design and implement creative new algorithms and technologies for high-performance, highly reliable systems Own and deliver innovation end-to-end, from concept to shipped product Analyze and solve challenging problems through persistence and insight Work as a team with smart peers who inspire you and who are inspired by you Make customers really happy, because that’s why we do what we do Learn about building new products for new markets inside of a small team Qualifications BS or MS degree in a related field with a graduation date expected between December 2023 to June 2024 Availability to start working full-time in the second half of 2024 Proficiency in C++ required Familiarity in Go a plus Deep knowledge in advanced topics such as distributed systems, operating systems, Linux kernel, database internals, hypervisors, containers, and compiler optimization highly preferred Solid grasp of foundational Computer Science concepts including data structures, algorithms, concurrency, and design principles Strong verbal and written communication skills Initiative to deliver high quality results and meet deadlines without supervision Commitment to teamwork and collaboration Readiness to work on-site in an open office Pay Range USD $116,000.00 - USD $175,000.00 /Yr. Pay Transparency Statement Salary ranges are determined based on role, level and location. For positions open to candidates in multiple geographical locations, the base salary range is reflective of the labor market across the applicable locations. This role may be eligible for incentive pay and/or equity. And because we understand the value of bringing your full and best self to work, we offer a variety of perks to manage a healthy balance, including flexible time off, wellness resources, and company-sponsored team events - check out purebenefits.com for more information. BE YOU—CORPORATE CLONES NEED NOT APPLY Pure is where you ask big questions, think differently, and make an impact. This is not just a job, but a place where you have a voice and can accelerate your career. We value unique thoughts and celebrate individuality, and with ample opportunity to learn, develop yourself, and expand into different roles, joining Pure is an investment in your career journey. Through our Pure Equality program, which supports a flourishing field of employee resource groups, we nourish the personal and professional lives of our team members. And our Pure Good Foundation gives back to local and global communities through volunteering and grants. And because we understand the value of bringing your full and best self to work, we offer a variety of perks to manage a healthy balance, including flexible time off, wellness resources, and company-sponsored team events. PURE IS COMMITTED TO EQUALITY. Research shows that in order to apply for a job, women feel they need to meet 100% of the criteria while men usually apply after meeting about 60%. Regardless of how you identify, if you believe you can do the job and are a good match, we encourage you to apply. Pure is proud to be an equal opportunity and affirmative action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or any other characteristic legally protected by the laws of the jurisdiction in which you are being considered for hire. If you need assistance or an accommodation due to a disability, you may contact us at TA-Ops@purestorage.com . APPLICANT & CANDIDATE PERSONAL INFORMATION PRIVACY NOTICE. If you're wondering how or why Pure collects or uses information you provide, we invite you to check out our Applicant & Candidate Personal Information Protection Notice. DEEMED EXPORT LICENSE NOTICE. Some positions may require a deemed export license for compliance with applicable laws and regulations. Please note: Pure does not currently sponsor deemed export license applications so we are unable to proceed with applicants requiring stated sponsorship.",
        "url": "https://www.linkedin.com/jobs/view/3940059029",
        "summary": "Software Engineer Grad role at Pure Storage, a leading data storage company, focused on building innovative solutions for large-scale cloud storage environments. Responsibilities include designing and implementing new algorithms for high-performance systems, owning product delivery from concept to launch, solving complex technical challenges, collaborating with a team of engineers, and ensuring customer satisfaction.",
        "industries": [
            "Data Storage",
            "Cloud Computing",
            "Software",
            "Technology"
        ],
        "soft_skills": [
            "Highly Motivated",
            "Creative",
            "Persistent",
            "Insightful",
            "Teamwork",
            "Collaboration",
            "Communication (Verbal & Written)",
            "Initiative",
            "Deadline-Oriented",
            "Customer-Focused"
        ],
        "hard_skills": [
            "C++",
            "Go",
            "Distributed Systems",
            "Operating Systems",
            "Linux Kernel",
            "Database Internals",
            "Hypervisors",
            "Containers",
            "Compiler Optimization",
            "Data Structures",
            "Algorithms",
            "Concurrency",
            "Design Principles"
        ],
        "tech_stack": [
            "Cloud Storage",
            "AI",
            "Hyperscale Workloads"
        ],
        "programming_languages": [
            "C++",
            "Go"
        ],
        "experience": 0,
        "education": {
            "min_degree": "BS",
            "fields": [
                "Computer Science",
                "Related Fields"
            ]
        },
        "salary": {
            "max": 175000,
            "min": 116000
        },
        "benefits": [
            "Flexible Time Off",
            "Wellness Resources",
            "Company-Sponsored Team Events"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Culver City, CA",
        "job_id": 3944657104,
        "company": "Didi Hirsch Mental Health Services",
        "title": "Analytics Engineer",
        "created_on": 1720636061.3493185,
        "description": "Analytics Engineer (Sepulveda Finance) This position is remote, and it is eligible for the 9/80 flex schedule. The pay range for this position is $99,855 - $110,950 annually. About Didi Hirsch Didi Hirsch Mental Health Services has been a national leader in whole-person mental health, crisis care, and substance use services since 1942 and is home to the nation’s first Suicide Prevention Center. We are a nonprofit organization providing care to nearly 200,000 people annually across our programs. Didi Hirsch has deep roots in community-based mental health and a commitment to providing culturally responsive services that are just and equitable. As an organization, we value equity, diversity, and inclusion. More than 1,000 dedicated employees and volunteers make Didi Hirsch’s work possible. We intentionally recruit and retain a workforce that is reflective of the communities we serve and strive to cultivate a sense of belonging for them. We embrace employees and candidates from all backgrounds who want to help make this vision a reality. Summary Under the general supervision of the Director of Corporate Reporting, the Analytics Engineer will be responsible for delivering well-defined, transformed, tested, documented, and code-reviewed analytic data sets to meet the needs of departmental and agency staff. They are responsible for building the foundational layer for dashboards and self-service reporting. They will employ best practices like version control and continuous integration to the analytics code base and ensure data quality and accuracy. They will work collaboratively with IT to design and construct data pipelines and develop tools/processes to expand the data capacity of the department and streamline/automate processes. Works with end users to design the data warehouse for analysis and manages its maintenance. Primary Duties Expand our data warehouse with clean data ready for analysis. Help to define and improve our internal standards for style, maintainability, and best practices for a high-scale data infrastructure. Data modeling: Model raw data into clean, tested, and reusable datasets. Build visual representations of data and communicating connections between different information points and structures. Define the rules and requirements for the formats and attributes of data. Data transformation: Apply various transformations to different data pieces to ensure they correspond to given tasks and effectively support analytic purposes. Transformation includes but are not limited to removing inaccurate or corrupted data; aggregating data items into a summarized version; filtering information to get rid of irrelevant, duplicated, or overly sensitive data; joining two or more database tables by their matching attributes; and splitting a single column into multiple ones, etc. Maintain data documentation & definitions: Ensure internal team is using consistent and clear definitions and language. Providing identifiable and understandable descriptions of data. Define data quality rules, standards, and metrics: Define metrics to be used and measures to be taken to guarantee data is accurate enough to fit operational and analytics needs. Write cleaning/quality algorithms to improve the quality of data. Apply best practices to analytics code (e.g., version control, testing, continuous integration). Data visualization: Convert data into a suitable graphic format by building dashboards, graphs, charts, and reports using Business Intelligence tools. Support internal team to build visualizations. Close collaboration with internal Finance/Billing team members: Work collaboratively with division team members to align reporting requirements with data assets. Collaborate across multiple departments (e.g. IT, Clinical Operations, etc.) to understand business needs, analyze complex data, provide advance data analysis and reporting, and clearly communicate recommendations. Train stakeholders (data users) on how to use data visualization tools. Position Requirements At least a bachelor’s (master’s preferred) degree in technical/data-oriented fields (e.g., statistics, mathematics, computer science, software engineering, or IT) 2 or more years of experience working in the data-driven landscapes where data is used to solve business/operational needs. Preferably in a healthcare setting. Strong SQL skills, specifically in creating logic for data transformations, writing queries, and building data models, etc. Experienced in programming languages such as R and Python. Knowledge of software engineering best practices (e.g., version control, testing, continuous integration). Experienced with Git or other version control system. Experience with tools to build data pipelines. Experience with dashboard tools such as PowerBI, Tableau, etc. Interpersonal and communication skills The ability present ideas, information, and viewpoints clearly, both verbally and in writing. A commitment to team objectives and Didi Hirsch philosophies. Ability to adapt to changing needs by acquiring new skills and knowledge. Current California driver’s license, car insurance, and a driving record acceptable to the Agency’s insurance carrier. Our Vision A future where everyone has equitable access to care and is empowered to achieve optimal mental health and well-being. Our Mission Didi Hirsch provides compassionate mental health, substance use, and suicide prevention services to individuals and families, especially in communities where discrimination and injustice limit access. Core Values Excellence: We are constantly innovating, learning from the communities we serve, and applying the latest research to advance best practices. We uphold the highest ethical standards to ensure we are providing compassionate and excellent care. Diversity & Inclusion: We value diversity of background, experience, and ideas, committing to a workforce representative of the communities we serve. We celebrate differences and prioritize creating a sense of belonging. Equity: We are dedicated to maintaining equitable practices in our healthcare delivery and workplace culture, and we work to dismantle disparities and discrimination within both systems of care and society. Well Being: We are devoted to the well-being of our staff, volunteers, and communities, and believe healthy teams lead to healthy clients. Advocacy: We advocate across all levels of government and use our voice to reduce barriers to care, including stigma, systemic racism, and parity across payers, with the goal of access to high quality, integrated healthcare for all. Community Engagement: We build partnerships in the community and across sectors to create a more inclusive and responsive mental health ecosystem and enhance greater accessibility to care and support.",
        "url": "https://www.linkedin.com/jobs/view/3944657104",
        "summary": "The Analytics Engineer at Didi Hirsch Mental Health Services will be responsible for delivering clean, tested, and documented analytic data sets. This role involves building the foundational layer for dashboards and self-service reporting, ensuring data quality, and collaborating with IT to design data pipelines. The engineer will also work with stakeholders to design the data warehouse and manage its maintenance.",
        "industries": [
            "Healthcare",
            "Mental Health",
            "Nonprofit",
            "Data Analytics"
        ],
        "soft_skills": [
            "Communication",
            "Interpersonal Skills",
            "Teamwork",
            "Collaboration",
            "Problem Solving",
            "Analytical Skills",
            "Data Visualization",
            "Presentation Skills",
            "Adaptability",
            "Training"
        ],
        "hard_skills": [
            "SQL",
            "R",
            "Python",
            "Data Modeling",
            "Data Transformation",
            "Data Documentation",
            "Data Quality",
            "Data Visualization",
            "PowerBI",
            "Tableau",
            "Git",
            "Version Control",
            "Continuous Integration",
            "Data Pipelines"
        ],
        "tech_stack": [
            "SQL",
            "R",
            "Python",
            "PowerBI",
            "Tableau",
            "Git",
            "Data Pipelines"
        ],
        "programming_languages": [
            "SQL",
            "R",
            "Python"
        ],
        "experience": 2,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Statistics",
                "Mathematics",
                "Computer Science",
                "Software Engineering",
                "IT"
            ]
        },
        "salary": {
            "max": 110950,
            "min": 99855
        },
        "benefits": [
            "9/80 Flex Schedule",
            "Remote Work"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 2948828327,
        "company": "Basis",
        "title": "Data Infrastructure Engineer",
        "created_on": 1720636063.1811926,
        "description": "What working with us is like We're a small team based in San Francisco with a few colleagues remote around the world. We have a hybrid office culture with 2 days a week at home and 3 in our Financial District office, SF. We will hire talented people regardless of location, but we prefer candidates in the SF Bay Area or willing to relocate. What we're looking for Experience implementing and scaling production cloud systems for data-intensive applications such as automated ETL, AI/ML, streaming and batch workloads Experience with cloud vendors GCP, AWS, and Azure (the more the better) Experience with the the vendor landscape at the intersection of cloud infra and data science and analytics tooling. Including but not limited to Snowflake, BigQuery, Terraform, K8s, Cloud functions, Docker, Airflow, Jupyter, Kafka, Druid, Ansible Experience utilizing site reliability engineering practices to build scalable and highly reliable software systems. Experience with CI/CD strategies to optimize the experience and speed of shipping product A knack for good architectural decisions learned from past experience Your responsibilities Build yourself — and every data engineer — out of a job Take full ownership of our data infrastructure. This year we'll have hundreds of companies running their data systems through our web app. The app orchestrates millions of jobs across TBs of data we manage with code! What we can offer As an early hire, a substantial equity package An opportunity to move into a management position and scale yourself or continue to contribute through writing code (if desired) Competitive cash pay All of the standard benefits",
        "url": "https://www.linkedin.com/jobs/view/2948828327",
        "summary": "We are a small team in San Francisco seeking a data engineer with experience building and scaling production cloud systems for data-intensive applications. You will be responsible for taking full ownership of our data infrastructure, which manages TBs of data for hundreds of companies.",
        "industries": [
            "Technology",
            "Software",
            "Data Science",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Problem Solving",
            "Communication",
            "Teamwork",
            "Leadership",
            "Ownership",
            "Decision Making"
        ],
        "hard_skills": [
            "Cloud Infrastructure",
            "Data Engineering",
            "ETL",
            "AI/ML",
            "Streaming",
            "Batch Workloads",
            "GCP",
            "AWS",
            "Azure",
            "Snowflake",
            "BigQuery",
            "Terraform",
            "K8s",
            "Cloud Functions",
            "Docker",
            "Airflow",
            "Jupyter",
            "Kafka",
            "Druid",
            "Ansible",
            "Site Reliability Engineering",
            "CI/CD",
            "Architecture"
        ],
        "tech_stack": [
            "GCP",
            "AWS",
            "Azure",
            "Snowflake",
            "BigQuery",
            "Terraform",
            "K8s",
            "Cloud Functions",
            "Docker",
            "Airflow",
            "Jupyter",
            "Kafka",
            "Druid",
            "Ansible"
        ],
        "programming_languages": [],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Computer Science",
                "Data Science",
                "Engineering"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Equity Package",
            "Management Opportunities",
            "Competitive Cash Pay",
            "Standard Benefits"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Jose, CA",
        "job_id": 3952446974,
        "company": "Ayar Labs",
        "title": "Analytics Engineer - Operations",
        "created_on": 1720636068.3652692,
        "description": "This position is based at our headquarters in San Jose, CA. The role is open for some remote work. Position Overview: The Data Pillar implements and maintains the usability of Ayar’s strategic data assets enterprise-wide. You'll work with Operations to accelerate the decision-making process of key product and business functions. You will develop a deep understanding of the operations business needs and the company's data infrastructure. Using this understanding, you will assist the operations team in developing tools and resources for the analysts to utilize. Essential Functions: Implement data transformation, modeling, and presentation solutions to make data readily available across the company Build core datasets to serve as unique sources of truth for product and business functions Partner with data analysts and other operations stakeholders to understand their needs and then design, build, and monitor pipelines that meet today's requirements but can gracefully scale with our growing data size Implement automated workflows that lower manual/operational cost for stakeholders, define and uphold SLAs for timely delivery of data, move the company closer to democratizing data and a self-serve model (query exploration, dashboards, data catalog, data discovery) Basic Qualifications: 4+ years as a data engineer building core datasets and supporting business verticals as needed. Hands-on experience with various different backend services - SQL, Python, Pandas, Dagster, dbt, Metabase Expert Level SQL. You intimately understand aggregation functions, window functions, UDFs, and partitioning approaches to run correct and highly performant queries You are a self-starter and continuously gather and synthesize high-impact needs from business partners, designing and implementing the appropriate technical solutions, and effectively communicating about deliverables, timelines and tradeoffs You are passionate about analytics use cases, data models, and solving complex data problems You have hands-on experience with operations and manufacturing data. You have worked with vendors and data related to manufacturing processes Preferred Qualifications: Previous working experience with semiconductor data Experience working with product, production and operations data You’ve worked at a fast-growing company Pay range: $147K to $185K At Ayar Labs we are lighting up electronics for a brighter future. With our deep ties to MIT and UC Berkeley, and our commitment to hiring the best engineers in photonics and electronics, joining our team gives you the opportunity to collaborate with brilliant people on challenging, paradigm-shifting work. Our optical I/O technology removes the bottlenecks created by today’s electrical I/O, making it possible to continue the computing system performance scaling that Moore’s Law enabled until now. We have a commitment to win big in the marketplace based on the strengths of our technology, and we approach everything with an eye to massive scalability. We believe that deep cross-collaboration between teams facilitated by honest, open debate is the best way to achieve big wins, leveraging our patent portfolio which promises products that deliver orders of magnitude improvements in latency, bandwidth density, and power consumption. We offer a comprehensive benefits plan designed to keep our team healthy and happy. Resources Executives from Intel and GLOBALFOUNDRIES share their thoughts on Ayar Labs and the promise of in-package optical I/O (video) Ayar Labs in the News and Recent announcements LinkedIn and Twitter Ayar Labs is an Affirmative Action/Equal Opportunity Employer and is strongly committed to all policies which will afford equal opportunity employment to all qualified persons without regard to age, national origin, race, ethnicity, creed, gender, disability, veteran status, or any other characteristic protected by law.",
        "url": "https://www.linkedin.com/jobs/view/3952446974",
        "summary": "Ayar Labs is seeking a Data Engineer to join their team in San Jose, CA.  The role involves implementing and maintaining the usability of the company's strategic data assets enterprise-wide. The Data Engineer will work with Operations to accelerate the decision-making process of key product and business functions. The candidate will be responsible for building core datasets, partnering with data analysts, designing and building data pipelines, and implementing automated workflows.  They will also have hands-on experience with operations and manufacturing data and knowledge of SQL, Python, Pandas, Dagster, dbt, and Metabase.",
        "industries": [
            "Technology",
            "Semiconductor",
            "Manufacturing",
            "Data Analytics",
            "Data Engineering",
            "Photonics",
            "Electronics"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem-solving",
            "Analytical Thinking",
            "Self-starter",
            "Data-driven",
            "Passion for Analytics"
        ],
        "hard_skills": [
            "SQL",
            "Python",
            "Pandas",
            "Dagster",
            "dbt",
            "Metabase",
            "Data Transformation",
            "Data Modeling",
            "Data Presentation",
            "Data Pipelines",
            "Automated Workflows",
            "SLAs",
            "Data Democratization",
            "Query Exploration",
            "Dashboards",
            "Data Catalog",
            "Data Discovery"
        ],
        "tech_stack": [
            "SQL",
            "Python",
            "Pandas",
            "Dagster",
            "dbt",
            "Metabase"
        ],
        "programming_languages": [
            "SQL",
            "Python"
        ],
        "experience": 4,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 185000,
            "min": 147000
        },
        "benefits": [
            "Comprehensive Benefits Plan"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Diego, CA",
        "job_id": 3916054254,
        "company": "eSmart Tech Inc.",
        "title": "Software Engineer",
        "created_on": 1720636070.0225961,
        "description": "Job Description About eSmart Tech Inc. (https://esmarttech.com): The eSmart Tech Inc. team, located in San Diego, California, researches, designs, and engineers its clients' ideas and concepts to convert them into innovative products and hardware. The eSmart team, with a combined 45 granted and 10+ pending US Patents, prides itself on constantly devising original ideas that bring simplicity to the digital world. REQ# 102181540001 Location: San Diego office Responsibilities: You will be responsible for developing software (in Apps & Cloud programming) for innovative IoT (Internet of Things) products encompassing various components such as Apps, and Cloud. The position requires a candidate that is a self-starter, motivated with doing-whatever-it-takes attitude and a team player with effective communication skills. Qualifications: 1-2 years of work experience in software development Must have strong understanding the fundamentals in Computer Science Must be familiar in C, and an Object-Oriented Language such as C++, Java, Python Must have Mobile Apps development (iOS or Android) and debugging experience Strong problem solving skills Education Requirements: Required: Bachelor's in Computer Science or Computer Engineering Preferred: Master's in Computer Science or Computer Engineering Being authorized to work in the U.S. (U.S citizen or Permanent Resident (green card holder)) is a precondition of employment. Company Description The eSmart Tech Inc. team, located in San Diego, California, researches, designs, and engineers its clients' ideas and concepts to convert them into innovative products and hardware. The eSmart team, with a combined 45 granted and 10+ pending US Patents, prides itself on constantly devising original ideas that bring simplicity to the digital world. The eSmart Tech Inc. team, located in San Diego, California, researches, designs, and engineers its clients' ideas and concepts to convert them into innovative products and hardware. The eSmart team, with a combined 45 granted and 10+ pending US Patents, prides itself on constantly devising original ideas that bring simplicity to the digital world.",
        "url": "https://www.linkedin.com/jobs/view/3916054254",
        "summary": "eSmart Tech Inc. is seeking a Software Developer to develop software for innovative IoT products. The ideal candidate will have 1-2 years of experience in software development, strong understanding of computer science fundamentals, experience with C, C++, Java, Python, iOS or Android app development, and strong problem-solving skills. A Bachelor's degree in Computer Science or Computer Engineering is required, and a Master's degree is preferred. ",
        "industries": [
            "Technology",
            "Software Development",
            "Internet of Things (IoT)"
        ],
        "soft_skills": [
            "Self-starter",
            "Motivated",
            "Team player",
            "Effective communication"
        ],
        "hard_skills": [
            "Software Development",
            "C",
            "C++",
            "Java",
            "Python",
            "Mobile Apps Development",
            "iOS",
            "Android",
            "Debugging",
            "Problem Solving"
        ],
        "tech_stack": [
            "IoT",
            "Apps",
            "Cloud"
        ],
        "programming_languages": [
            "C",
            "C++",
            "Java",
            "Python"
        ],
        "experience": 2,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Computer Science",
                "Computer Engineering"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3955407117,
        "company": "Short Story",
        "title": "Software Engineer, Back End",
        "created_on": 1720636071.692586,
        "description": "About Us Short Story is an award winning, technology-powered retailer dedicated to petite women 5'4\" and under. Our mission is to create a seamless shopping experience for millions of petite women so they can dress with effortless confidence. As a fast-growing startup, we're revolutionizing retail with a data-driven learning system that leverages customer feedback to create exceptional, tailored products. We've been recognized by top publications like Forbes, Fortune and are backed by top institutional investors who share our vision of building the world's next great consumer brand. At Short Story, we celebrate petiteness, boldness, and modern womanhood. Our culture combines an insatiable hunger for data, an unwavering commitment to creating superior products, and a hustle startup mentality. We have a strong sense of urgency. Since 2019, we've cultivated a team of dedicated problem solvers and fashion enthusiasts. We're excited for you to join us on this exhilarating journey. The Role The Software Engineer will be responsible for working within the engineering team to build software for both internal and external consumers, creating trustworthy user experiences, and turning the company vision into an executable roadmap in partnership with senior leadership. Day To Day Engineer reusable components and libraries for regular use Translate designs and wireframes into fully realized, high quality code Troubleshoot user experience issues and designs Collaborate with internal stakeholders on core company tools and designs Document and manage changes as well as roadmap future changes Participate fully in weekly sprints with knowledge and positivity Set direction for software design internally and externally Take ownership of the user experience and continuously improve user’s experiences About You You’re interested in early stage startup experience and growing a company culture Building a company from the ground up is an exciting challenge You’re able to balance software design, planning, and coding and manage your calendar You have an eye for detail and quality in product and people Your focus is on doing the right thing for your people, clients, and the world You’re excited about roadmapping for rapid growth and user count You’re happy working in an unstructured environment and excited to build future structure You flourish working independently and collaboratively in a startup environment You’re familiar with product life cycles and eager to build new designs Your Toolkit Grounding in CS fundamentals - you have a broad based knowledge of all things back end 3+ Python experience PostgreSQL experience preferred Strong communication skills conversationally and via Slack Experience in a startup environment a plus We work on Heroku and AWS Why Short Story The opportunity to be involved in an early stage start-up and build the culture The opportunity to build a unique business from the ground up The opportunity to work on interesting problems around personalization You’re interested in supporting petite women better than ever before You relish the chance to own the development of internal tooling The opportunity to take a start-up from the starting line to success Benefits And Compensation Attractive early stage compensation and equity package Compensation range: $150K-190K, based on experience level Unlimited vacation Medical and dental benefits Competitive startup compensation Opportunity to work with a dedicated and experienced founding team Opportunity to work at a Y-Combinator backed start-up",
        "url": "https://www.linkedin.com/jobs/view/3955407117",
        "summary": "Short Story is a tech-powered retailer focused on petite women. They are seeking a Software Engineer to build internal and external software, create user-friendly experiences, and translate company vision into executable roadmaps. Responsibilities include developing reusable components, translating designs into code, troubleshooting issues, collaborating on tools, documenting changes, participating in sprints, setting software design direction, and continuously improving user experiences. The ideal candidate has 3+ years of Python experience, PostgreSQL experience, strong communication skills, startup experience, and a passion for creating positive user experiences. Short Story offers competitive compensation, unlimited vacation, medical and dental benefits, and the opportunity to contribute to a growing company culture.",
        "industries": [
            "Retail",
            "E-commerce",
            "Fashion",
            "Technology",
            "Software Development"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem-Solving",
            "Detail-Oriented",
            "Teamwork",
            "Passion",
            "Ownership",
            "Creativity",
            "Positivity",
            "Adaptability",
            "Independent Work",
            "Hustle",
            "Data-Driven",
            "Growth Mindset",
            "Customer-Focused"
        ],
        "hard_skills": [
            "Python",
            "PostgreSQL",
            "Software Engineering",
            "User Experience Design",
            "Code Development",
            "Troubleshooting",
            "Documentation",
            "Agile Development",
            "Sprint Management",
            "Product Design",
            "Product Development",
            "Roadmap Development"
        ],
        "tech_stack": [
            "Python",
            "PostgreSQL",
            "Heroku",
            "AWS"
        ],
        "programming_languages": [
            "Python"
        ],
        "experience": 3,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 190000,
            "min": 150000
        },
        "benefits": [
            "Unlimited Vacation",
            "Medical Benefits",
            "Dental Benefits",
            "Equity"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3855685757,
        "company": "Fifty Years",
        "title": "Data Tools Engineer",
        "created_on": 1720636073.3762887,
        "description": "The climate is heading off a cliff. Thousands die every day of malnutrition. Pandemics put the world at risk. Technology entrepreneurship can solve these problems and many more! Fifty Years is a different kind of venture capital firm. At Fifty Years we believe technology entrepreneurship can and should solve the world’s biggest problems. We believe massive wealth creation and positive impact can go hand in hand. We support early-stage founders creating massively profitable businesses that can also solve the world’s biggest problems. We pride ourselves on always going the extra mile for our portfolio founders and have built the Fifty Years team around helping our founders create and scale the most important companies of the coming decades. We support startups working at the forefront of synthetic biology, energy, food, health care, space, construction, genomics, and many other industries. Fifty Years partners with founders in these fields to develop bold new solutions that will move humanity to the next level. And it’s working! We've seeded companies like Solugen (engineering enzymes to produce industrial chemicals sustainably, now worth $2B), Astranis (small satellites to cover the Earth in Internet, now worth $1.4B), and Opentrons (automated lab platforms to transform healthcare and life sciences, now worth $1.8B). Nearly 20% of the teams Fifty Years backed in our Fund I are now unicorns. All backed from pre-seed or seed. 😎 You’ll be joining our tight-knit team in San Francisco, working closely with everyone on the team and touching every aspect of a venture capital firm in action. Role Description We’re looking for an ambitious builder with experience building data-centric products, to create tools and services to support portfolio founders, find new founders, and support our team. You’ll work with world-class product builders and investors in a fast-paced environment, with a front-row seat to the latest deep tech innovation. The products you develop will help founders expand their network, fundraise, recruit, and grow. They’ll also help scientists and engineers become the next iconic founders. A Few Of The Things We’ve Been Building Fifty Years Network [2:32 – 4:36]: Helping founders to find any warm intro and 50X their network when fundraising, making sales, or hiring. Paper Scraper [13:28 – 16:23]: Stay up to date on the latest and greatest scientific research, synthesize the research using GPT, and reach out to founders to help them commercialize. Spinout Playbook [16:23 – 18:46]: The comprehensive guide to spinning out of academia, released for free on the web. YouTube: What Should a Modern VC Look Like? You Are eager to build and ship product experiments regularly Can maintain focus in a fast-paced setting Have strong communication and writing skills Take initiative and have an entrepreneurial mindset Have a hunger to learn and make mistakes Are really excited about solving the world’s biggest challenges Skills Proficient in Python Proficient in data engineering (SQL architecture, ETL, data integration best practices) Experience creating and managing pipelines with many services (via REST API, webhooks) and data sources 3+ years experience at a high-growth startup or top engineering culture, had mentors/code reviewers Knowledge of machine learning techniques and natural language processing (NLP) Nice to have: Experience leveraging or integrating existing ML models BS in Computer Science True builders are rare in VC, and you’ll get a chance to work in a high-performing environment while learning about company building, observing industry trends, and meeting world-class teams in various fields of deeptech & AI solving the world’s most important problems. Additional Information This is a full-time role in San Francisco. Send your Github, LinkedIn, or recommended chandidates to drew at fiftyyears.com. As a firm and family, 50Y deeply believes in human potential. We are committed to actively fostering inclusion in our workplace and striving for greater diversity and equality in the VC, start-up, and founder worlds. We welcome any candidates who fit the criteria to apply for this position.",
        "url": "https://www.linkedin.com/jobs/view/3855685757",
        "summary": "Fifty Years is a venture capital firm that invests in early-stage startups working on solving the world's biggest problems using technology. They are looking for a data-centric product builder to create tools and services for supporting portfolio founders, finding new founders, and supporting the Fifty Years team. The role involves working in a fast-paced environment with world-class product builders and investors, and contributing to projects like building a network to help founders expand their network, fundraise, recruit, and grow; creating a platform to synthesize scientific research using GPT and reach out to founders; and developing a comprehensive guide to spinning out of academia.",
        "industries": [
            "Venture Capital",
            "Technology",
            "Deep Tech",
            "Artificial Intelligence",
            "Synthetic Biology",
            "Energy",
            "Food",
            "Healthcare",
            "Space",
            "Construction",
            "Genomics"
        ],
        "soft_skills": [
            "Ambitious",
            "Communication",
            "Writing",
            "Entrepreneurial",
            "Initiative",
            "Learning",
            "Teamwork"
        ],
        "hard_skills": [
            "Python",
            "Data Engineering",
            "SQL",
            "ETL",
            "Data Integration",
            "REST API",
            "Webhooks",
            "Machine Learning",
            "Natural Language Processing",
            "Github"
        ],
        "tech_stack": [
            "Python",
            "SQL",
            "REST API",
            "Webhooks",
            "GPT"
        ],
        "programming_languages": [
            "Python",
            "SQL"
        ],
        "experience": 3,
        "education": {
            "min_degree": "BS",
            "fields": [
                "Computer Science"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Palo Alto, CA",
        "job_id": 3954922744,
        "company": "Gauss Labs",
        "title": "Software Engineer (US)",
        "created_on": 1720636075.1468344,
        "description": "As a Gaussian Software & Platform Engineer, you will be responsible for leading the architecture, design, development and launch of some of the core software products. You will be working with other passionate and talented Software Engineers and Applied Scientists and have opportunities to learn various machine learning algorithms and gain insights around AI. You will have significant influence on our overall strategy by helping define these product features, drive the system architecture, and spearhead the best practices that enable a quality product. You are the ideal candidate if you are passionate about new opportunities and have a demonstrated track record of success in delivering new features and products. A commitment to team work and strong communication skills to both business and technical partners are important requirements. Creating reliable, scalable, and high performance products requires exceptional technical expertise, a sound understanding of the fundamentals of computer science, and practical experience building large-scale systems. Responsibilities Design, develop and deploy a secure, reliable, robust, and scalable software platform for Machine Learning services in high availability and low latency. Work with applied scientists to collaborate on leveraging algorithm components to build the solution and product. Optimize Machine Learning software components to exploit modern parallel architectures such as distributed clusters, multicore SMPs and GPUs. Take responsibility for end-to-end development with high standard on software design, coding, code reviews, tests and automation of deployment within CI/CD disciplines. Work closely with project/program managers to understand the customers’ needs and their business problems. Key Qualifications BS/MS/PhD degree in Computer Science and Engineering or strong industry experience in software development. Industry experience in software development or platform engineering. Experience in developing software with high scalability, performance, and reliability. Experience in software development with expertise in some of data structures, algorithms, object-oriented software design, OS & computer architecture, networks, databases, etc. Experience with at least one modern programming language such as Python, Java, JavaScript, and C/C++. Preferred Qualifications Experiences in Hadoop, Spark, Kafka, Redis, Cassandra, Kubernetes, Kubeflow, or container technologies. Experiences in developing cloud platform, server platform, or data platform. Development experience in a cloud service environment such as Amazon AWS, MS Azure, and Google Cloud Platform",
        "url": "https://www.linkedin.com/jobs/view/3954922744",
        "summary": "Lead the design, development, and launch of core software products for machine learning services. Collaborate with applied scientists to leverage algorithms and optimize software components. Ensure high scalability, performance, and reliability while working within a CI/CD environment.",
        "industries": [
            "Technology",
            "Software Development",
            "Machine Learning",
            "Artificial Intelligence",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Communication",
            "Teamwork",
            "Problem Solving",
            "Collaboration",
            "Passion",
            "Leadership",
            "Strategic Thinking"
        ],
        "hard_skills": [
            "Software Development",
            "Platform Engineering",
            "Scalability",
            "Performance",
            "Reliability",
            "Data Structures",
            "Algorithms",
            "Object-Oriented Design",
            "OS & Computer Architecture",
            "Networks",
            "Databases",
            "Python",
            "Java",
            "JavaScript",
            "C/C++",
            "Hadoop",
            "Spark",
            "Kafka",
            "Redis",
            "Cassandra",
            "Kubernetes",
            "Kubeflow",
            "Container Technologies",
            "Cloud Platform Development",
            "Server Platform Development",
            "Data Platform Development",
            "Amazon AWS",
            "MS Azure",
            "Google Cloud Platform"
        ],
        "tech_stack": [
            "Hadoop",
            "Spark",
            "Kafka",
            "Redis",
            "Cassandra",
            "Kubernetes",
            "Kubeflow",
            "Container Technologies",
            "Amazon AWS",
            "MS Azure",
            "Google Cloud Platform"
        ],
        "programming_languages": [
            "Python",
            "Java",
            "JavaScript",
            "C/C++"
        ],
        "experience": 3,
        "education": {
            "min_degree": "BS",
            "fields": [
                "Computer Science",
                "Engineering"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3965740737,
        "company": "Asana",
        "title": "Senior Software Engineer, AI Retrieval",
        "created_on": 1720636076.709125,
        "description": "Asana is seeking an experienced and empathetic Senior Software Engineer to join our AI Retrieval team. Working with the latest AI large language models and search technologies, you’ll design and implement systems that enable customers to get informative answers about their work stored in Asana and beyond. We’re looking for an engineer who is passionate about AI, curious to learn about everything from vector search to prompt engineering, and willing to share what they learn to help their teammates grow and thrive. AI Retrieval built and maintains Asana’s AI retrieval augmented generation system, which powers our Smart Answers feature. We harness a wide and growing range of technologies to retrieve data from anywhere in a customer’s Asana domain and synthesize it into insightful answers to our customers’ questions. Soon we’ll reach into other common SaaS tools to ensure that no matter where our customers store their data, we can give them the answers they seek. At Asana, we believe AI will fundamentally transform the way we work and live. We’re going beyond chatbots and integrating AI into everyday workflows for some of the biggest companies on the planet. Our AI organization is a fast-moving group with a track record of successful launches, and we’re just getting started. If you believe in the potential of AI to make us more creative, effective, and fulfilled in our work, we'd love for you to join us. This role is based in our San Francisco office with an office-centric hybrid schedule. The standard in-office days are Monday, Tuesday, and Thursday. Most Asanas have the option to work from home on Wednesdays. Working from home on Fridays depends on the type of work you do and the teams with which you partner. If you're interviewing for this role, your recruiter will share more about the in-office requirements. What You’ll Achieve Be a key architect of our retrieval augmented generation systems, leading complex projects to on-time delivery Collaborate with product and design to create delightful user experiences that maintain Asana’s reputation for power and ease-of-use Work with the leading LLM providers to leverage the latest AI technologies in order to serve our customers Share your knowledge and experience with your teammates, fostering strong engineering practices and creating a culture of excellence Experience growth through opportunities to stretch, learn, and shape the trajectory of AI at Asana About You 5+ years experience, working across the stack within large, well-maintained codebases Proven history of autonomously and reliably scoping and delivering complex, cross-cutting product initiatives, often by coordinating the work of multiple engineers Comfortable navigating ambiguity and taking on open-ended AI/product challenges Passion for mentorship and helping your teammates grow, with experience helping junior engineers excel. Enthusiasm for AI and its transformative potential in the way you work Strong communication skills and ability to collaborate across functions At Asana, we're committed to building teams that include a variety of backgrounds, perspectives, and skills, as this is critical to helping us achieve our mission. If you're interested in this role and don't meet every listed requirement, we still encourage you to apply. What We’ll Offer Our comprehensive compensation package plays a big part in how we recognize you for the impact you have on our path to achieving our mission. We believe that compensation should be reflective of the value you create relative to the market value of your role. To ensure pay is fair and not impacted by biases, we're committed to looking at market value which is why we check ourselves and conduct a yearly pay equity audit. For this role, the estimated base salary range is between $202,000 - $316,000 The actual base salary will vary based on various factors, including market and individual qualifications objectively assessed during the interview process. The listed range above is a guideline, and the base salary range for this role may be modified. In addition to base salary, your compensation package may include additional components such as equity, sales incentive pay (for most sales roles), and benefits. If you're interviewing for this role, speak with your Talent Acquisition Partner to learn more about the total compensation and benefits for this role. We strive to provide equitable and competitive benefits packages that support our employees worldwide and include: Mental health, wellness & fitness benefits Career coaching & support Inclusive family building benefits Long-term savings or retirement plans In-office culinary options to cater to your dietary preferences These are just some of the benefits we offer, and benefits may vary based on role, country, and local regulations. If you're interviewing for this role, speak with your Talent Acquisition Partner to learn more about the total compensation and benefits for this role. About Us Asana helps teams orchestrate their work, from small projects to strategic initiatives. Millions of teams around the world rely on Asana to achieve their most important goals, faster. Asana has been named a Top 10 Best Workplace for 5 years in a row, is Fortune's #1 Best Workplace in the Bay Area, and one of Glassdoor’s and Inc.’s Best Places to Work. After spending more than a year physically distanced, Team Asana is safely and mindfully returning to in-person collaboration, incorporating flexibility that adds hybrid elements to our office-centric culture . With 11+ offices all over the world, we are always looking for individuals who care about building technology that drives positive change in the world and a culture where everyone feels that they belong. We believe in supporting people to do their best work and thrive, and building a diverse, equitable, and inclusive company is core to our mission. Our goal is to ensure that Asana upholds an inclusive environment where all people feel that they are equally respected and valued, whether they are applying for an open position or working at the company. We provide equal employment opportunities to all applicants without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by law. We also comply with the San Francisco Fair Chance Ordinance and similar laws in other locations.",
        "url": "https://www.linkedin.com/jobs/view/3965740737",
        "summary": "Asana is seeking a Senior Software Engineer to join their AI Retrieval team. The role will involve designing and implementing systems that leverage AI to provide informative answers about work stored in Asana and other SaaS tools. The ideal candidate will have 5+ years of experience working across the stack within large codebases, a proven history of delivering complex projects, and a passion for mentorship. The role is based in San Francisco with a hybrid office-centric schedule.",
        "industries": [
            "Software Development",
            "Artificial Intelligence",
            "SaaS",
            "Technology"
        ],
        "soft_skills": [
            "Empathy",
            "Passionate",
            "Curious",
            "Willing to Share Knowledge",
            "Strong Communication",
            "Collaboration",
            "Mentorship",
            "Teamwork",
            "Autonomy",
            "Ambiguity Tolerance",
            "Enthusiasm",
            "Growth Mindset"
        ],
        "hard_skills": [
            "AI",
            "Large Language Models",
            "Search Technologies",
            "Vector Search",
            "Prompt Engineering",
            "Retrieval Augmented Generation",
            "Smart Answers",
            "Data Retrieval",
            "Data Synthesis",
            "SaaS Integration",
            "Cross-Cutting Product Initiatives",
            "Codebases"
        ],
        "tech_stack": [
            "AI",
            "LLM",
            "Retrieval Augmented Generation",
            "Smart Answers",
            "Vector Search",
            "Prompt Engineering"
        ],
        "programming_languages": [],
        "experience": 5,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 316000,
            "min": 202000
        },
        "benefits": [
            "Mental health benefits",
            "Wellness benefits",
            "Fitness benefits",
            "Career coaching",
            "Career support",
            "Family building benefits",
            "Retirement plans",
            "In-office culinary options"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Fremont, CA",
        "job_id": 3883071750,
        "company": "Info Way Solutions",
        "title": "Data Science Engineer�",
        "created_on": 1720636078.4114583,
        "description": "Hi Professionals, Hope you are doing good This Is Jayaraman from Info Way Solutions, LLC We Have Job Opening For Data Science Engineer and The Detailed Job Description Is Given Below: Kindly check the JD and share your views Data Science Engineer Job Location: Boston, MA - 4 days to office. ( Local Profile only ) Job Description: We are seeking a talented Data Science Engineer to join our team and contribute to the development and implementation of advanced data solutions using technologies such as AWS Glue, Python, Spark, Snowflake Data Lake, S3, SageMaker, and machine learning (M/L). As a Data Science Engineer, you will play a crucial role in designing, building, and optimizing data pipelines, machine learning models, and analytics solutions. You will work closely with cross-functional teams to extract actionable insights from data and drive business outcomes. Key Responsibilities: Develop and maintain ETL pipelines using AWS Glue for data ingestion, transformation, and integration from various sources. Utilize Python and Spark for data preprocessing, feature engineering, and model development. Design and implement data lake architecture using Snowflake Data Lake, Snowflake data warehouse and S3 for scalable and efficient storage and processing of structured and unstructured data. Leverage SageMaker for model training, evaluation, deployment, and monitoring in production environments. Collaborate with data scientists, analysts, and business stakeholders to understand requirements, develop predictive models, and generate actionable insights. Conduct exploratory data analysis (EDA) and data visualization to communicate findings and trends effectively. Stay updated with advancements in machine learning algorithms, techniques, and best practices to enhance model performance and accuracy. Ensure data quality, integrity, and security throughout the data lifecycle by implementing robust data governance and compliance measures. Qualifications: Bachelor's degree or higher in Computer Science, Data Science, Statistics, or related field. Proficiency in AWS services such as Glue, S3, SageMaker, and Snowflake Data Lake with 5-6 years of experience. Strong programming skills in Python for data manipulation, analysis, and modeling. Experience with distributed computing frameworks like Spark for big data processing. Knowledge of machine learning concepts, algorithms, and tools for regression, classification, clustering, and recommendation systems. Familiarity with data visualization tools with Tableau for creating meaningful visualizations. Excellent problem-solving, analytical thinking, and communication skills. Ability to work collaboratively in a team environment and manage multiple priorities effectively. Experience deploying machine-learning models in production environments and monitoring their performance. Knowledge of MLOps practices, model versioning, and automated model deployment pipelines. Familiarity with SQL, NoSQL databases, and data warehousing concepts. Strong understanding of cloud computing principles and architectures. Certifications in AWS, Python, Spark, or related technologies. Thanks & Regards, Jayaraman Email: jayaraman@infowaygroup.com Direct: (925)-241-5719 Work: (925)-592-6160 Ext 105 Info Way Solutions LLC | 46520 Fremont Blvd, Suite 614 | Fremont, CA - 94538",
        "url": "https://www.linkedin.com/jobs/view/3883071750",
        "summary": "Info Way Solutions is looking for a Data Science Engineer with 5-6 years of experience to develop and maintain data pipelines, machine learning models, and analytics solutions using technologies like AWS Glue, Python, Spark, Snowflake Data Lake, S3, SageMaker, and Machine Learning. The role involves data ingestion, transformation, model development, and data lake architecture design, requiring strong programming skills in Python and experience with distributed computing frameworks like Spark. The candidate should have knowledge of machine learning algorithms, data visualization tools, and cloud computing principles.",
        "industries": [
            "Data Science",
            "Technology",
            "Software Development",
            "Cloud Computing",
            "Analytics",
            "Machine Learning",
            "Big Data"
        ],
        "soft_skills": [
            "Problem-solving",
            "Analytical thinking",
            "Communication",
            "Teamwork",
            "Prioritization"
        ],
        "hard_skills": [
            "AWS Glue",
            "Python",
            "Spark",
            "Snowflake Data Lake",
            "S3",
            "SageMaker",
            "Machine Learning",
            "ETL",
            "Data Ingestion",
            "Data Transformation",
            "Data Integration",
            "Data Preprocessing",
            "Feature Engineering",
            "Model Development",
            "Data Lake Architecture",
            "Data Warehousing",
            "SQL",
            "NoSQL",
            "Data Visualization",
            "Tableau",
            "MLOps",
            "Model Versioning",
            "Automated Model Deployment",
            "Cloud Computing"
        ],
        "tech_stack": [
            "AWS Glue",
            "Python",
            "Spark",
            "Snowflake Data Lake",
            "S3",
            "SageMaker",
            "Machine Learning",
            "ETL",
            "Data Ingestion",
            "Data Transformation",
            "Data Integration",
            "Data Preprocessing",
            "Feature Engineering",
            "Model Development",
            "Data Lake Architecture",
            "Data Warehousing",
            "SQL",
            "NoSQL",
            "Data Visualization",
            "Tableau",
            "MLOps",
            "Model Versioning",
            "Automated Model Deployment",
            "Cloud Computing"
        ],
        "programming_languages": [
            "Python",
            "SQL"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Computer Science",
                "Data Science",
                "Statistics"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Diego, CA",
        "job_id": 3875918897,
        "company": "GRAIL",
        "title": "Staff Data Engineer (San Diego) #3378",
        "created_on": 1720636080.1267202,
        "description": "Our mission is to detect cancer early, when it can be cured. We are working to change the trajectory of cancer mortality and bring stakeholders together to adopt innovative, safe, and effective technologies that can transform cancer care. We are a healthcare company, pioneering new technologies to advance early cancer detection. We have built a multi-disciplinary organization of scientists, engineers, and physicians and we are using the power of next-generation sequencing (NGS), population-scale clinical studies, and state-of-the-art computer science and data science to overcome one of medicine’s greatest challenges. GRAIL is headquartered in Menlo Park, California, with locations in Washington, D.C., North Carolina, and the United Kingdom. It is supported by leading global investors and pharmaceutical, technology, and healthcare companies. For more information, please visit grail.com . Are you a champion of automation interested in using your talents for optimizing processes that will make an impact on the fight against cancer? If so, join GRAIL on the Data Integration team in Research! GRAIL is seeking a Staff Data Engineer to join our team to support the growing data needs of GRAIL’s clinical and research activities. You will leverage your expertise in automation and data engineering to ensure our scientific teams have the data they need to succeed. This role is pivotal in advancing GRAIL's mission by enhancing our data infrastructure and contributing to our early cancer detection efforts. Responsibilities Be a part of a highly collaborative team that focuses on delivering value to cross-functional partners by designing, deploying, and automating secure, efficient, and scalable data infrastructure and tools, reducing manual efforts and streamlining operations. Help model Grail data and ensure that it follows FAIR principles (findable, accessible, interoperable and reusable). Drive the design, deployment, and automated delivery of data infrastructure, standardized data models, datasets, and tools. Integrate automated testing and release processes to improve the quality and velocity of software and data deliveries. Collaborate with cross-functional teams, from Research to Clinical Lab Operations to Software Engineering to provide comprehensive data solutions from conception to delivery. Ensure all software and data meet high standards for quality, clinical compliance, and privacy. Mentor fellow engineers and scientists, promoting best practices in software and data engineering. Preferred Experience B.S. / M.S. in a quantitative field (e.g., Computer Science, Engineering, Mathematics, Physics, Computational Biology) with at least 8 years of related industry experience, or Ph.D. with at least 5 years of related industry experience. Extensive experience with relational databases, data modeling principles, data pipeline tools and workflow engines (e.g., SQL, DBT, Apache Airflow, AWS GLUE, Spark. Extensive experience with DevOps practices, including CI/CD pipelines, containerized deployment (e.g., Kubernetes), and infrastructure-as-code (e.g., Terraform). Experience with supporting data science / machine learning data pipelines, preferably in the context of analysis of biological data. Experience in developing data pipelines using scalable cloud-based data warehouses / data lakes on AWS, Azure, or GCP. Solid programming skills in object-oriented and/or functional programming paradigms. Ability to embrace uncertainty, navigate ambiguity, and collaborate with product teams and stakeholders to refine requirements and drive towards clear engineering objectives and designs. A commitment to constructive dialogue, both in giving and receiving critical feedback, to foster an environment of continuous improvement. Highly Welcome Experience Prior industry experience in the healthcare, biotech, or life sciences industry, especially in the context of next-generation sequencing. Experience working in a regulated environment (e.g., FDA, CLIA, GDPR). Proficiency in Python, and R. Experience building microservices and web applications. The estimated, full-time, annual base pay scale for this position is $180,000 - $202,000. Actual base pay will consider skills, experience, and location. Based on the role, colleagues may be eligible to participate in an annual bonus plan tied to company and individual performance, or an incentive plan. We also offer a long-term incentive plan to align company and colleague success over time. In addition, GRAIL offers a progressive benefit package, including flexible time-off, a 401k with a company match, and alongside our medical, dental, vision plans, carefully selected mindfulness offerings. GRAIL is an Equal Employment Employer and does not discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, disability or any other legally protected status. We will reasonably accommodate all individuals with disabilities so that they can participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation. GRAIL maintains a drug-free workplace.",
        "url": "https://www.linkedin.com/jobs/view/3875918897",
        "summary": "GRAIL is a healthcare company focused on early cancer detection. They are looking for a Staff Data Engineer to join their Data Integration team in Research. The role involves building, deploying, and automating data infrastructure to support clinical and research activities. The ideal candidate will have strong experience in data modeling, data pipelines, DevOps practices, and working with biological data. They will collaborate with cross-functional teams and ensure data compliance and privacy.",
        "industries": [
            "Healthcare",
            "Biotechnology",
            "Life Sciences",
            "Cancer Research"
        ],
        "soft_skills": [
            "Collaboration",
            "Communication",
            "Problem Solving",
            "Ambiguity Tolerance",
            "Feedback",
            "Continuous Improvement",
            "Mentorship"
        ],
        "hard_skills": [
            "SQL",
            "DBT",
            "Apache Airflow",
            "AWS GLUE",
            "Spark",
            "DevOps",
            "CI/CD",
            "Kubernetes",
            "Terraform",
            "Data Science",
            "Machine Learning",
            "Cloud-based Data Warehouses",
            "Data Lakes",
            "AWS",
            "Azure",
            "GCP",
            "Object-oriented Programming",
            "Functional Programming",
            "Python",
            "R",
            "Microservices",
            "Web Applications"
        ],
        "tech_stack": [
            "SQL",
            "DBT",
            "Apache Airflow",
            "AWS GLUE",
            "Spark",
            "Kubernetes",
            "Terraform",
            "AWS",
            "Azure",
            "GCP",
            "Python",
            "R"
        ],
        "programming_languages": [
            "SQL",
            "Python",
            "R"
        ],
        "experience": 8,
        "education": {
            "min_degree": "B.S.",
            "fields": [
                "Computer Science",
                "Engineering",
                "Mathematics",
                "Physics",
                "Computational Biology"
            ]
        },
        "salary": {
            "max": 202000,
            "min": 180000
        },
        "benefits": [
            "Flexible Time-Off",
            "401k with Company Match",
            "Medical",
            "Dental",
            "Vision",
            "Mindfulness Offerings"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3933409975,
        "company": "Crystal Equation Corporation",
        "title": "Data Engineer - II",
        "created_on": 1720636081.9974737,
        "description": "Job Description Pay Range: $69.05 - 76.46/hr Qualifications: ***Cloud Data Engineer*** Location: San Francisco Position type: Hybrid (2+ days in office) Qualifications 5 to 9+ years of experience in data engineering, data science, and software engineering. Bachelor’s degree in computer science, Information Systems, or another related field Must be a US Citizen or a Green Card holder. Responsibilities: Position Description: Designs, develops, modifies, tests, and automates the data warehouse and business intelligence applications solutions. This includes design, development, architecture recommendations, quality management, metadata and repository creation, trouble-shooting problems, and tuning warehouse applications. Develops transition and implementation plans. Recommends changes in development, maintenance, and standards. Advanced analytical ability and technical skill as well as the ability to provide innovative solutions to technical needs and business requirements. Ability to exercise independent judgment in making complex business decisions. Acute attention to detail with a high level of data integrity and accuracy Excellent oral and written communication, with interpersonal skills to work with people at all levels of the organization. Ability to translate highly technical information into non-technical terms. Excellent computer skills including Microsoft Office along with various other software applications as needed for the role. Broad knowledge of the programming tools, concepts, practices, and principles including design, implementation, and testing Position requires continuous visual concentration and manual dexterity to operate PC Requires prolonged sitting and minimal standing/walking. May require on-call status. Rare domestic travel including overnight stays may be necessary. Technical Skills: Expert in developing and analyzing complex SQL on a variety of RDBMS (Microsoft SQL Server, Oracle) Expert knowledge of data modeling and understanding of different data structures and their benefits and limitations under particular use cases Experience with ETL tools (Informatica) Ability to create quality ERD’s (entity-relationship diagrams) Excellent writing skills for writing user and system documentation AWS Cloud Data Warehousing Technologies Experience using core AWS services to build and support data warehouse solutions leveraging AWS architecture best practices (S3, DMS, Glue, Lambda) Development/modeling experience with Amazon Redshift, Amazon Aurora Postgres SQL Experience using the AWS service APIs, AWS CLI, and SDKs to build applications. Proficiency in developing, deploying, and debugging cloud-based applications using AWS. Ability to use a CI/CD pipeline to deploy applications on AWS (GitLab, Terraform, DBMaestro) Ability to apply a basic understanding of cloud-native applications to write code. Proficiency writing code for serverless applications. Ability to write code using AWS security best practices (e.g., not using secret and access keys in the code, instead using IAM roles) Ability to author, maintain, and debug code modules on AWS. Experience with visualization tools (Tableau) Experience with SAP Business Objects (BO) Experience creating scripts with Python. Experience working on an Agile team. Understanding of application lifecycle management Understanding of the use of containers in the development process Responsibilities Contributes to the design, development, testing, implementation, and review of complex data warehouse and business intelligence solutions. Develops all or part of complex data warehouse applications, develops software from established requirements, builds reports and dashboards, plans and coordinates work with fellow programmers to meet delivery commitments, creates prototypes; offers insight on the feasibility of system designs. Contributes to the design of technology infrastructure and configurations, recommends process improvements. Reviews complex patches and new versions of data warehouse applications. Implements complex software packages and deploys code. Key participant in cross-functional team initiatives and process improvement projects",
        "url": "https://www.linkedin.com/jobs/view/3933409975",
        "summary": "This role requires a Cloud Data Engineer to design, develop, and maintain data warehouse and business intelligence solutions. They will leverage their expertise in SQL, data modeling, ETL tools, and AWS cloud technologies to build and support complex data warehousing applications. The position demands strong analytical, communication, and technical skills, along with the ability to translate technical information into non-technical terms. This is a hybrid role requiring 2+ days in the office.",
        "industries": [
            "Data Engineering",
            "Business Intelligence",
            "Software Engineering",
            "Cloud Computing",
            "Data Science",
            "Technology"
        ],
        "soft_skills": [
            "Analytical",
            "Communication",
            "Problem Solving",
            "Technical",
            "Detail-Oriented",
            "Teamwork",
            "Leadership",
            "Decision Making",
            "Interpersonal",
            "Time Management",
            "Organization"
        ],
        "hard_skills": [
            "SQL",
            "RDBMS",
            "Microsoft SQL Server",
            "Oracle",
            "Data Modeling",
            "ETL",
            "Informatica",
            "ERD",
            "AWS",
            "S3",
            "DMS",
            "Glue",
            "Lambda",
            "Amazon Redshift",
            "Amazon Aurora Postgres SQL",
            "AWS Service APIs",
            "AWS CLI",
            "AWS SDKs",
            "CI/CD",
            "GitLab",
            "Terraform",
            "DBMaestro",
            "Tableau",
            "SAP Business Objects",
            "Python",
            "Agile",
            "Application Lifecycle Management",
            "Containers"
        ],
        "tech_stack": [
            "AWS",
            "S3",
            "DMS",
            "Glue",
            "Lambda",
            "Amazon Redshift",
            "Amazon Aurora Postgres SQL",
            "AWS Service APIs",
            "AWS CLI",
            "AWS SDKs",
            "GitLab",
            "Terraform",
            "DBMaestro",
            "Tableau",
            "SAP Business Objects",
            "Python"
        ],
        "programming_languages": [
            "SQL",
            "Python"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Information Systems",
                "Related Fields"
            ]
        },
        "salary": {
            "max": 7646,
            "min": 6905
        },
        "benefits": [
            "On-Call Status",
            "Domestic Travel"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3921434916,
        "company": "OpenAI",
        "title": "Software Engineer, Supercomputing, HPC Infrastructure",
        "created_on": 1720636084.0814571,
        "description": "About The Team We believe that increasing compute is a huge lever to AI progress. The Supercomputing team owns the entire process of building OpenAI’s compute and infrastructure. This includes the deployment of huge clusters using Kubernetes and Azure, and building the internal experiment platform for running/training the world’s largest AI models. We work at the very cutting edge of speed and scale, combining the traditions of High-Performance Computing (HPC) in a modern cloud and containerized environment. We build some of the largest Supercomputers in the world. When our Owl cluster launched it in 2019 it would've been among the top 5 of the TOP500 supercomputers in the world. Since then we've only continued to grow. See this blog post to get a sense of what kind of challenges we solve in our day-to-day work: Scaling Kubernetes to 7,500 Nodes You won’t encounter any other organization in the world with as much compute per employee. We are a small team that moves quickly, with access to huge resources, working with a direct impact on the success of OpenAI and, by extension, the field of AI as a whole. About The Role In this role, you will work closely with machine learning researchers, but don't need to be a machine learning expert yourself. We value people who can quickly obtain a deep technical understanding of new domains and enjoy being self-directed and identifying the most important problems to solve. Experience with high-performance computing, or open-source contributions is a bonus. You Might Thrive In This Role If You Have experience designing, implementing, and running production services and highly available distributed systems Have worked with highly performant bare-metal systems Have helped a team mature with standardized tools and processes around stability, observability, and scaling Have experience running large Kubernetes clusters with GPU workloads, in the range of 500-1,000 nodes and GPUs Have experience working with Azure or other cloud platforms such as AWS or GCP Know your way around bash, Terraform, Python, and/or Chef Are confident in managing and monitoring large-scale infrastructure deployments Can debug problems across the stack, such as networking issues, performance problems, or memory leak About OpenAI OpenAI is an AI research and deployment company dedicated to ensuring that general-purpose artificial intelligence benefits all of humanity. We push the boundaries of the capabilities of AI systems and seek to safely deploy them to the world through our products. AI is an extremely powerful tool that must be created with safety and human needs at its core, and to achieve our mission, we must encompass and value the many different perspectives, voices, and experiences that form the full spectrum of humanity. We are an equal opportunity employer and do not discriminate on the basis of race, religion, national origin, gender, sexual orientation, age, veteran status, disability or any other legally protected status. For US Based Candidates: Pursuant to the San Francisco Fair Chance Ordinance, we will consider qualified applicants with arrest and conviction records. We are committed to providing reasonable accommodations to applicants with disabilities, and requests can be made via this link. OpenAI Global Applicant Privacy Policy At OpenAI, we believe artificial intelligence has the potential to help people solve immense global challenges, and we want the upside of AI to be widely shared. Join us in shaping the future of technology.",
        "url": "https://www.linkedin.com/jobs/view/3921434916",
        "summary": "OpenAI is looking for a Supercomputing engineer to work on building and maintaining their infrastructure, including large-scale Kubernetes clusters and Azure deployments.  This role requires experience with high-performance computing, distributed systems, and a strong understanding of cloud platforms. The ideal candidate will have experience with bare-metal systems, standardized tooling and processes, and managing GPU workloads.",
        "industries": [
            "Artificial Intelligence",
            "Technology",
            "Cloud Computing",
            "Software Development",
            "High Performance Computing"
        ],
        "soft_skills": [
            "Self-directed",
            "Problem-solving",
            "Communication",
            "Collaboration"
        ],
        "hard_skills": [
            "Kubernetes",
            "Azure",
            "High-Performance Computing",
            "Distributed Systems",
            "Bare-Metal Systems",
            "Standardized Tooling",
            "Observability",
            "Scaling",
            "GPU Workloads",
            "Bash",
            "Terraform",
            "Python",
            "Chef",
            "Infrastructure Management",
            "Monitoring",
            "Debugging",
            "Networking",
            "Performance Analysis",
            "Memory Leak Analysis"
        ],
        "tech_stack": [
            "Kubernetes",
            "Azure",
            "AWS",
            "GCP",
            "Bash",
            "Terraform",
            "Python",
            "Chef"
        ],
        "programming_languages": [
            "Python"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3958413620,
        "company": "Aditi Consulting",
        "title": "DevOps Engineer",
        "created_on": 1720636085.6319168,
        "description": "Summary: The CIC Platform Observability team owns the observability tooling that monitors the CIC Platform and the homegrown tools used for incident management and EngOps, which help increase engineers' day-to-day productivity. For this specific role, we are looking for a DevOps Engineer to help build additional features for our homegrown tools, which use a no-code app platform. The platform we use for the homegrown tools is Tines. This role will also expand to work with our data warehousing team to help build out analytics for our incident tooling, which is visualized in Tableau.We are looking for an engineer who is passionate about our customers and who enjoys talking with outside teams to ensure we deliver the best possible solutions. We want an engineer who loves to learn new things, automate, and truly make our customers' day-to-day work easier. Requirements: Be an expert in running services in production environments Contribute to the process of designing services for high growth and high availability. Experience with no-code apps such as Tines or Zapier. Bonus: if you have the Tines certification. Experience with NoSQL databases such as DyanamoDB. Experience with Data Warehouses such as Snowflake. Experience with cloud infrastructure such as AWS, Google Cloud, or Azure. Experience with at least one programming language, such as Golang, Python, or Node.js. Experience working with REST API’s. Experience working with incident management processes or tools. Bonus: Experience working with Statuspage. Experience managing infrastructure with Terraform. Experience with Data Visualization tools such as Tableau. Compensation: The pay rate range above is the base hourly pay range that Aditi Consulting reasonably expects to pay someone for this position (compensation may vary outside of this range depending on several factors, including but not limited to, a candidate’s qualifications, skills, competencies, competencies, competencies, competencies, experience, location and end client requirements). Benefits and Ancillaries: Medical, dental, vision, PTO benefits and ancillaries may be available for eligible Aditi Consulting employees and vary based on the plan options selected by the employee.",
        "url": "https://www.linkedin.com/jobs/view/3958413620",
        "summary": "DevOps Engineer to build features for homegrown tools using Tines no-code platform, expand to data warehousing with Snowflake and Tableau analytics for incident tooling.",
        "industries": [
            "Information Technology",
            "Software Development",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Passionate about customers",
            "Communication",
            "Problem Solving",
            "Automation",
            "Teamwork"
        ],
        "hard_skills": [
            "DevOps",
            "Production Environments",
            "Service Design",
            "Tines",
            "Zapier",
            "NoSQL",
            "DynamoDB",
            "Data Warehousing",
            "Snowflake",
            "Cloud Infrastructure",
            "AWS",
            "Google Cloud",
            "Azure",
            "Golang",
            "Python",
            "Node.js",
            "REST API",
            "Incident Management",
            "Statuspage",
            "Terraform",
            "Tableau"
        ],
        "tech_stack": [
            "Tines",
            "Zapier",
            "DynamoDB",
            "Snowflake",
            "AWS",
            "Google Cloud",
            "Azure",
            "Golang",
            "Python",
            "Node.js",
            "REST API",
            "Statuspage",
            "Terraform",
            "Tableau"
        ],
        "programming_languages": [
            "Golang",
            "Python",
            "Node.js"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Medical",
            "Dental",
            "Vision",
            "PTO"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3947428854,
        "company": "Pinterest",
        "title": "Backend Software Engineer",
        "created_on": 1720636087.529868,
        "description": "About Pinterest Millions of people across the world come to Pinterest to find new ideas every day. It’s where they get inspiration, dream about new possibilities and plan for what matters most. Our mission is to help those people find their inspiration and create a life they love. In your role, you’ll be challenged to take on work that upholds this mission and pushes Pinterest forward. You’ll grow as a person and leader in your field, all the while helping Pinners make their lives better in the positive corner of the internet. Creating a life you love also means finding a career that celebrates the unique perspectives and experiences that you bring. As you read through the expectations of the position, consider how your skills and experiences may complement the responsibilities of the role. We encourage you to think through your relevant and transferable skills from prior experiences. Our new progressive work model is called PinFlex, a term that’s uniquely Pinterest to describe our flexible approach to living and working. Visit our PinFlex landing page to learn more. We are looking for inquisitive, well-rounded Backend engineers to join our Core and Monetization engineering teams. Working closely with product managers, designers, and backend engineers, you’ll play an important role in enabling the newest technologies and experiences. You will build robust frameworks & features. You will empower both developers and Pinners alike. You’ll have the opportunity to find creative solutions to thought-provoking problems. Even better, because we covet the kind of courageous thinking that’s required in order for big bets and smart risks to pay off, you’ll be invited to create and drive new initiatives, seeing them from inception through to technical design, implementation, and release. What You’ll Do Build out the backend for Pinner-facing features to power the future of inspiration on Pinterest Contribute to and lead each step of the product development process, from ideation to implementation to release; from rapidly prototyping, running A/B tests, to architecting and building solutions that can scale to support millions of users Partner with design, product, and backend teams to build end-to-end functionality Put on your Pinner hat to suggest new product ideas and features Employ automated testing to build features with a high degree of technical quality, taking responsibility for the components and features you develop Grow as an engineer by working with world-class peers on varied and high impact projects What We’re Looking For 2+ years of industry backend development experience, building consumer or business facing products Proficiency in common backend tech stacks for RESTful API, storage, caching and data processing Experience in following best practices in writing reliable and maintainable code that may be used by many other engineers Ability to keep up-to-date with new technologies to understand what should be incorporated Strong collaboration and communication skills This position is not eligible for relocation assistance. At Pinterest we believe the workplace should be equitable, inclusive, and inspiring for every employee. In an effort to provide greater transparency, we are sharing the base salary range for this position. The position is also eligible for equity. Final salary is based on a number of factors including location, travel, relevant prior experience, or particular skills and expertise. Information regarding the culture at Pinterest and benefits available for this position can be found here. US based applicants only $106,113—$219,063 USD Our Commitment To Diversity Pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. We want to have the best qualified people in every job. All qualified applicants will receive consideration for employment without regard to race, color, ancestry, national origin, religion or religious creed, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, age, marital status, status as a protected veteran, physical or mental disability, medical condition, genetic information or characteristics (or those of a family member) or any other consideration made unlawful by applicable federal, state or local laws. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you require an accommodation during the job application process, please notify accessibility@pinterest.com for support.",
        "url": "https://www.linkedin.com/jobs/view/3947428854",
        "summary": "Pinterest is seeking inquisitive Backend engineers to join their Core and Monetization engineering teams. Responsibilities include building backend features, leading product development, collaborating with cross-functional teams, suggesting new ideas, and implementing automated testing. The role requires 2+ years of backend development experience, proficiency in common tech stacks, strong coding practices, and excellent collaboration skills.",
        "industries": [
            "Technology",
            "Social Media",
            "Internet",
            "Software Development",
            "E-commerce"
        ],
        "soft_skills": [
            "Inquisitive",
            "Well-rounded",
            "Collaborative",
            "Communication",
            "Problem-solving",
            "Creative",
            "Courageous",
            "Leadership"
        ],
        "hard_skills": [
            "Backend Development",
            "RESTful API",
            "Storage",
            "Caching",
            "Data Processing",
            "Automated Testing",
            "A/B Testing",
            "Product Development",
            "Scalability",
            "Code Optimization"
        ],
        "tech_stack": [
            "Backend Tech Stacks"
        ],
        "programming_languages": [],
        "experience": 2,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 219063,
            "min": 106113
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco Bay Area",
        "job_id": 3886794592,
        "company": "Doximity",
        "title": "Software Engineer, Data",
        "created_on": 1720636089.3185613,
        "description": "Doximity is transforming the healthcare industry. Join our mission to help every physician be more productive and provide better care for their patients. As medicine's largest network in the United States, there's an elevated level of responsibility in everything we do. We don't take that responsibility lightly and are committed to building diverse teams with an inclusive culture that can make a direct impact on the healthcare system. One of Doximity's core values is stretching ourselves. Even if you don't check off all the boxes below we encourage you to apply. Doximity is full of exceptional people who bring their own unique experiences to work everyday and make us all better for it! This role can be filled in our San Francisco headquarters OR remotely in either the US, Mexico, Brazil or Canada. About you Experienced in Python and SQL . You have developed maintainable data pipelines with these languages. Foremost an engineer . You strive for high code quality, create automated testing, apply design patterns, and other engineering best practices. See a project from end-to-end . You are involved in all steps; from idea generation, planning, and execution through delivering. Here's How You Will Make an Impact Improve an intelligent content prioritization system through architectural and code changes. Identify and close gaps in existing systems to increase efficiency, scalability, and ease of use Collaborate with product managers, data analysts, and other data engineers to develop data pipelines and ETL tasks in order to facilitate the extraction of insights. Contribute to data architecture processes and practices that can be scheduled, automated, replicated and serve as standards for other teams to leverage. About Us Explore our tech stack We have over 500 private repositories in Github containing our pipelines, our own internal multi-functional tools, and open-source projects We have worked as a distributed team for a long time; we're currently about 65% distributed Find out more information on the Doximity engineering blog Our company core values Our recruiting process Our product development cycle Our on-boarding & mentorship process Learn more about people at Doximity Compensation The US total compensation range for this full-time position is $150,000 - $180,000 (inclusive of salary + equity). Our ranges are determined by role and level. The range displayed on each job posting reflects the approximate total target compensation for the position across the US. Within the range, individual pay is determined by factors including relevant skills, experience, and education/training. Please note that the compensation listed does not include benefits. More on /Benefits/Perks Doximity is proud to offer industry-leading benefits to our full time employees. Some of our offerings include: Medical, dental, vision offerings for you and your family 401k with matching program Employee stock purchase plan Family planning support, Childcare FSA, and parental leave Life, AD&D, and Disability Generous time off, holidays and paid company trips Wellness benefits…plus many more! More About Doximity… For the past decade, it’s been our mission to help every physician be more productive so they can provide better care for their patients. We believe that when doctors are connected, the healthcare system works better and patients benefit. Doximity enables our verified clinician members to collaborate with colleagues, stay up-to-date with the latest medical news and research, manage their careers, and conduct virtual patient visits. Today, Doximity is the leading digital platform for U.S. medical professionals, with over 80% of physicians, 50% of all nurse practitioners and physician assistants, and 90% of graduating medical students as members. Joining Doximity means being part of an incredibly talented and humble team passionate about improving inefficiencies in our $4.3 trillion U.S. healthcare system. We are a team of doers who solve problems everyday by treating obstacles like an adventure, and we love creating technology that has a real, meaningful impact on people’s lives. Doxers are committed to working towards a more equitable world both within and beyond our office walls. This starts by fostering an inclusive and diverse work environment where differences are valued and all employees are encouraged to bring their full, authentic selves to work daily. To learn more about our team, culture, and users, check out our careers page, company blog, and engineering blog. We’re growing fast, and there’s plenty of opportunity for you to make an impact—join us! For more information, visit Doximity.com. ____________________________________________ EEOC Statement Doximity is proud to be an equal opportunity employer, and committed to providing employment opportunities regardless of race, religious creed, color, national origin, ancestry, physical disability, mental disability, medical condition, genetic information, marital status, sex, gender, gender identity, gender expression, pregnancy, childbirth and breastfeeding, age, sexual orientation, military or veteran status, or any other protected classification. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.",
        "url": "https://www.linkedin.com/jobs/view/3886794592",
        "summary": "Doximity is looking for a Data Engineer to improve their intelligent content prioritization system. You will be involved in all steps of the project, from idea generation to delivery. You will work with product managers, data analysts, and other data engineers to develop data pipelines and ETL tasks. You will also contribute to data architecture processes and practices.",
        "industries": [
            "Healthcare",
            "Technology",
            "Software Development"
        ],
        "soft_skills": [
            "Collaboration",
            "Communication",
            "Problem Solving",
            "Project Management",
            "Self-Motivation",
            "Teamwork"
        ],
        "hard_skills": [
            "Python",
            "SQL",
            "Data Pipelines",
            "ETL",
            "Data Architecture",
            "Code Quality",
            "Automated Testing",
            "Design Patterns",
            "Agile Development",
            "Continuous Integration",
            "Continuous Delivery"
        ],
        "tech_stack": [
            "Python",
            "SQL",
            "Github",
            "ETL",
            "Data Pipelines"
        ],
        "programming_languages": [
            "Python",
            "SQL"
        ],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 180000,
            "min": 150000
        },
        "benefits": [
            "Medical",
            "Dental",
            "Vision",
            "401k with matching program",
            "Employee stock purchase plan",
            "Family planning support",
            "Childcare FSA",
            "Parental leave",
            "Life, AD&D, and Disability",
            "Generous time off",
            "Holidays",
            "Paid company trips",
            "Wellness benefits"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Mountain View, CA",
        "job_id": 3855880169,
        "company": "Avispa",
        "title": "Software Engineer",
        "created_on": 1720636091.0689323,
        "description": "Software Engineer 13471 A leading professional networking company is seeking a Software Engineer . The successful candidate will develop and operate mission-critical services and applications that are used customers and employees worldwide. The ideal candidate has 4+ years of software engineering experience. The company offers a great work environment! Software Engineer Pay And Benefits Hourly pay: $70-$80/hr Worksite: Leading professional development and networking company (Mountain View, CA 94041 or Sunnyvale, CA - Hybrid) W2 Employment, Group Medical, Dental, Vision, Life, Retirement Savings Program, PSL 40 hours/week, 6 Month Assignment Software Engineer Responsibilities Lead the technical design, implementation, and deployment of software solutions. Coordinate and lead cross-functional teams, ensuring that projects are successfully executed meeting our standards for quality software engineering. Design and ship high volume of elegant code with the foresight to avoid performance, scalability, usability, maintainability, availability and testability issues. Maintain an understanding of the latest technologies and tools and leverage them to improve existing products and services. Identify opportunities within the team for potential leverage and reuse by developing libraries, APIs, and shared services. Assist in the onboarding and ramp-up of new engineers. Contribute towards the definition of product and technical roadmaps. Software Engineer Qualifications 4+ years of software engineering experience. 2+ years of Python experience. BA/BS degree in Computer Science or related field. Experience developing mobile applications a bonus. Proven track record of developing large-scale highly available services and systems in either Java or Python. Working experience in the development as well as consumption of Web Services APIs (JSON / XML / RPC). Working experience of build systems, databases, and Linux-based infrastructures. Working Cloud experience (Azure, AWS, etc.) would be preferred. Demonstrated understanding of Computer Science fundamentals. Demonstrated understanding of architectural models and the technology stack used for developing highly scalable and highly available web applications and systems. Demonstrated communication skills, both written and verbal. Understanding of Agile Software Development methodologies. Compensation: From $70.00 to $80.00 per hour",
        "url": "https://www.linkedin.com/jobs/view/3855880169",
        "summary": "A leading professional networking company is seeking a Software Engineer to develop and operate mission-critical services and applications used by customers and employees worldwide. The ideal candidate has 4+ years of software engineering experience with proven track record of developing large-scale highly available services and systems in Java or Python. The role involves leading technical design, implementation, and deployment of software solutions, coordinating cross-functional teams, designing and shipping high-volume elegant code, maintaining understanding of latest technologies, and contributing towards product and technical roadmaps.",
        "industries": [
            "Technology",
            "Networking",
            "Software Development",
            "Professional Services"
        ],
        "soft_skills": [
            "Communication",
            "Leadership",
            "Teamwork",
            "Problem Solving",
            "Analytical Skills",
            "Time Management",
            "Organization",
            "Attention to Detail"
        ],
        "hard_skills": [
            "Software Engineering",
            "Python",
            "Java",
            "Web Services APIs",
            "JSON",
            "XML",
            "RPC",
            "Build Systems",
            "Databases",
            "Linux",
            "Cloud Computing",
            "Azure",
            "AWS",
            "Computer Science Fundamentals",
            "Agile Software Development"
        ],
        "tech_stack": [
            "Python",
            "Java",
            "JSON",
            "XML",
            "RPC",
            "Linux",
            "Azure",
            "AWS"
        ],
        "programming_languages": [
            "Python",
            "Java"
        ],
        "experience": 4,
        "education": {
            "min_degree": "BA/BS",
            "fields": [
                "Computer Science",
                "Related Field"
            ]
        },
        "salary": {
            "max": 80,
            "min": 70
        },
        "benefits": [
            "Group Medical",
            "Dental",
            "Vision",
            "Life",
            "Retirement Savings Program",
            "PSL"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Santa Clara, CA",
        "job_id": 3960945410,
        "company": "Palo Alto Networks",
        "title": "Principal Software Engineer (Data Analytics)",
        "created_on": 1720636092.6418166,
        "description": "Company Description Our Mission At Palo Alto Networks® everything starts and ends with our mission: Being the cybersecurity partner of choice, protecting our digital way of life. Our vision is a world where each day is safer and more secure than the one before. We are a company built on the foundation of challenging and disrupting the way things are done, and we’re looking for innovators who are as committed to shaping the future of cybersecurity as we are. Our Approach to Work We lead with flexibility and choice in all of our people programs. We have disrupted the traditional view that all employees have the same needs and wants. We offer personalization and offer our employees the opportunity to choose what works best for them as often as possible - from your wellbeing support to your growth and development, and beyond! At Palo Alto Networks, we believe in the power of collaboration and value in-person interactions. This is why our employees generally work from the office three days per week, leaving two days for choice and flexibility to work where you feel most effective. This setup fosters casual conversations, problem-solving, and trusted relationships. While details may evolve, our goal is to create an environment where innovation thrives, with office-based teams coming together three days a week to collaborate and thrive, together! Job Description Your Career Prisma Access™ provides protection straight from the cloud to make access to the cloud secure. It combines the connectivity and security you need - and delivers it everywhere you need it. Using cutting-edge public and private cloud technologies extending the next-generation security protection to all cloud services, customers on-premise remote networks and mobile users. We are seeking experienced senior level Software Engineers to develop and deliver next-generation technologies in our App Security team. We want passionate engineers who love to code and build great products. Engineers who bring new ideas in all facets of software development. Collaboration and teamwork are at the foundation of our culture and we need engineers who can communicate and work well with others towards achieving a common goal. Your Impact Design, develop and implement highly scalable software features Participate in architecture, design and development App Security features Research and implement different frameworks suited to the solution Work with different development and quality assurances groups to achieve the best quality Suggest and implement improvements to the development processes Work with DevOps and the Technical Support teams to troubleshoot customer issues Qualifications Your Experience At least 7 years of experience in system software development Experience with developing software for logging and metrics aggregation Experience with data visualization tools Understanding of protocols like DNS and HTTP Experience with building scalable systems Good grasp of asynchronous programming, multithreading and multiprocessing Able to troubleshoot system-level integration and performance issues Fast learner and eager to absorb new emerging technologies Can-do attitude on problem-solving, quality, and ability to execute Enjoys working with different teams with strong collaboration and communication skills MS/BS in Computer Science or equivalent or equivalent military experience required Nice-to-have Experience with developing REST APIs using Golang Experience with Cloud platforms such as GCP and AWS Experience with Kubernetes and Docker Additional Information The Team To stay ahead of the curve, it’s critical to know where the curve is, and how to anticipate the changes we’re facing. For the fastest growing cybersecurity company, the curve is the evolution of cyberattacks, and the products and services that proactively address them. Our engineering team is at the core of our products – connected directly to the mission of preventing cyberattacks. They are constantly innovating – challenging the way we, and the industry, think about cybersecurity. These engineers aren’t shy about creating products to solve problems no one has tackled before. They define the industry, instead of waiting for directions. We need individuals who feel comfortable in ambiguity, excited by the prospect of challenge, and empowered by the unknown risks facing our everyday lives that are only enabled by a secure digital environment. Our engineering team is provided with an unrivaled opportunity to build the products and practices that will support our company growth over the next decade, defining the cybersecurity industry as we know it. If you see the potential of how incredible people products can transform a business, this is the team for you. If you don’t wait for directions, instead, identifying new features and opportunities we have to just get better, this is your new career. Our Commitment We’re trailblazers that dream big, take risks, and challenge cybersecurity’s status quo. It’s simple: we can’t accomplish our mission without diverse teams innovating, together. We are committed to providing reasonable accommodations for all qualified individuals with a disability. If you require assistance or accommodation due to a disability or special need, please contact us at accommodations@paloaltonetworks.com. Palo Alto Networks is an equal opportunity employer. We celebrate diversity in our workplace, and all qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or other legally protected characteristics. All your information will be kept confidential according to EEO guidelines. The compensation offered for this position will depend on qualifications, experience, and work location. For candidates who receive an offer at the posted level, the starting base salary (for non-sales roles) or base salary + commission target (for sales/commissioned roles) is expected to be between $144,200/yr to $233,200/yr. The offered compensation may also include restricted stock units and a bonus. A description of our employee benefits may be found here. _SS2 Is role eligible for Immigration Sponsorship?: Yes",
        "url": "https://www.linkedin.com/jobs/view/3960945410",
        "summary": "Palo Alto Networks is seeking experienced senior level Software Engineers to develop and deliver next-generation technologies in their App Security team. The ideal candidate will have at least 7 years of experience in system software development, experience with developing software for logging and metrics aggregation, experience with data visualization tools, understanding of protocols like DNS and HTTP, experience with building scalable systems, good grasp of asynchronous programming, multithreading and multiprocessing, ability to troubleshoot system-level integration and performance issues, be a fast learner and eager to absorb new emerging technologies, have a can-do attitude on problem-solving, quality, and ability to execute, enjoy working with different teams with strong collaboration and communication skills, and have a MS/BS in Computer Science or equivalent or equivalent military experience. The position offers a competitive salary between $144,200/yr to $233,200/yr, restricted stock units, a bonus, and a comprehensive benefits package.",
        "industries": [
            "Cybersecurity",
            "Software Development",
            "Information Technology",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Collaboration",
            "Communication",
            "Problem Solving",
            "Teamwork",
            "Quality",
            "Can-do Attitude",
            "Fast Learner",
            "Eager to Absorb New Technologies",
            "Strong Communication Skills"
        ],
        "hard_skills": [
            "System Software Development",
            "Logging",
            "Metrics Aggregation",
            "Data Visualization",
            "DNS",
            "HTTP",
            "Scalable Systems",
            "Asynchronous Programming",
            "Multithreading",
            "Multiprocessing",
            "System-Level Integration",
            "Performance Issues",
            "REST APIs",
            "Golang",
            "Cloud Platforms",
            "GCP",
            "AWS",
            "Kubernetes",
            "Docker"
        ],
        "tech_stack": [
            "Golang",
            "GCP",
            "AWS",
            "Kubernetes",
            "Docker"
        ],
        "programming_languages": [
            "Golang"
        ],
        "experience": 7,
        "education": {
            "min_degree": "MS/BS",
            "fields": [
                "Computer Science"
            ]
        },
        "salary": {
            "max": 233200,
            "min": 144200
        },
        "benefits": [
            "Restricted Stock Units",
            "Bonus"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3911863904,
        "company": "Unreal Staffing, Inc",
        "title": "Software Engineer",
        "created_on": 1720636094.4235423,
        "description": "About You: We're searching for a software engineer who embodies the following qualities: Passionate about Impact: Our mission is to make a real difference in students' lives. If you're driven by the desire to create positive change and empower the next generation, you'll fit right in Comfortable with Ambiguity: In our dynamic startup environment, change is constant and clear paths aren't always laid out. We need team members who thrive in uncertainty, can craft strategies on the fly, and execute with confidence and autonomy Sense of Urgency: Speed is crucial in a startup. If you enjoy the excitement of moving fast and building with purpose, you'll find our culture both exhilarating and rewarding Desire for Growth: While you may have experience at larger organizations, you're ready for a new challenge with more responsibility and an opportunity to shape the future. We're here to provide the platform for your professional growth Requirements Responsibilities : Collaborate with designers, product managers, school implementation teams, and other developers Analyze feature requirements and propose scalable data models, API interfaces, and architecture needed to build features Implement new features according to specifications, while maintaining the current product and minimizing technical debt Understand the balance between speed and accuracy, and be able to evaluate different choices effectively Conduct code reviews of pull requests submitted by other developers Monitor, maintain, and enhance existing web application infrastructure and deployment processes Establish performance metrics and monitor infrastructure performance Build for scalability, availability, performance, and security across the stack Minimum Qualifications: Bachelor's degree or higher in computer science or related field 2+ years of experience as a Backend or Full Stack Engineer Experience with React, Vue, or similar frontend JavaScript frameworks Experience building RESTful APIs using Ruby on Rails, Django, Laravel, or similar backend MVC frameworks Proficiency in writing and maintaining high-quality production code and using Git for source code control Experience in deploying and managing cloud-native infrastructure and services, with a focus on performance and cost optimization (e.g., AWS, Microsoft Azure, Google Cloud) Preferred Qualifications: Strong interest in working in the education technology industry in an impact-driven role Adaptability to a fast-paced, dynamic environment Experience developing GraphQL APIs Experience working with and optimizing SQL databases and queries Experience with Amazon Web Services Benefits Competitive salary ranging from $100,000 to $140,000, depending on experience and qualifications Stock options, allowing you to share in the company's success Comprehensive health, dental, and vision insurance coverage Flexible work arrangements, including remote work options Professional development opportunities, including access to courses, workshops, and conferences Generous paid time off and holidays, encouraging work-life balance Collaborative and supportive work environment, with a focus on diversity and inclusion",
        "url": "https://www.linkedin.com/jobs/view/3911863904",
        "summary": "We are looking for a Software Engineer to join our dynamic startup focused on education technology. You will be responsible for collaborating with various teams, analyzing requirements, implementing features, conducting code reviews, and maintaining web application infrastructure. We are looking for someone passionate about education, comfortable with ambiguity, and eager to grow in a fast-paced environment.",
        "industries": [
            "Education Technology",
            "Software Development",
            "Startup"
        ],
        "soft_skills": [
            "Passionate",
            "Driven",
            "Impact",
            "Change",
            "Empower",
            "Comfortable with Ambiguity",
            "Thriving in Uncertainty",
            "Strategy",
            "Execution",
            "Autonomy",
            "Sense of Urgency",
            "Speed",
            "Purpose",
            "Growth",
            "Desire for Growth",
            "Responsibility",
            "Collaboration",
            "Analysis",
            "Problem Solving",
            "Communication",
            "Adaptability",
            "Teamwork"
        ],
        "hard_skills": [
            "React",
            "Vue",
            "JavaScript Frameworks",
            "RESTful APIs",
            "Ruby on Rails",
            "Django",
            "Laravel",
            "MVC Frameworks",
            "Git",
            "Cloud-Native Infrastructure",
            "AWS",
            "Microsoft Azure",
            "Google Cloud",
            "GraphQL APIs",
            "SQL Databases",
            "SQL Queries",
            "Amazon Web Services"
        ],
        "tech_stack": [
            "React",
            "Vue",
            "JavaScript Frameworks",
            "RESTful APIs",
            "Ruby on Rails",
            "Django",
            "Laravel",
            "MVC Frameworks",
            "Git",
            "Cloud-Native Infrastructure",
            "AWS",
            "Microsoft Azure",
            "Google Cloud",
            "GraphQL APIs",
            "SQL Databases",
            "SQL Queries",
            "Amazon Web Services"
        ],
        "programming_languages": [
            "JavaScript",
            "Ruby",
            "Python",
            "SQL"
        ],
        "experience": 2,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Computer Science",
                "Related Field"
            ]
        },
        "salary": {
            "max": 140000,
            "min": 100000
        },
        "benefits": [
            "Competitive Salary",
            "Stock Options",
            "Health Insurance",
            "Dental Insurance",
            "Vision Insurance",
            "Flexible Work Arrangements",
            "Remote Work Options",
            "Professional Development Opportunities",
            "Courses",
            "Workshops",
            "Conferences",
            "Paid Time Off",
            "Holidays",
            "Collaborative Work Environment",
            "Diversity",
            "Inclusion"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Mountain View, CA",
        "job_id": 3967780647,
        "company": "LinkedIn",
        "title": "Senior Software Engineer - Frontend",
        "created_on": 1720636096.2290606,
        "description": "Company Description: LinkedIn is the world’s largest professional network, built to create economic opportunity for every member of the global workforce. Our products help people make powerful connections, discover exciting opportunities, build necessary skills, and gain valuable insights every day. We’re also committed to providing transformational opportunities for our own employees by investing in their growth. We aspire to create a culture that’s built on trust, care, inclusion, and fun – where everyone can succeed. Join us to transform the way the world works. Job Description: At LinkedIn, we trust each other to do our best work where it works best for us and our teams. This role offers a hybrid work option, meaning you can both work from home and commute to a LinkedIn office, depending on what’s best for you and when it is important for your team to be together. Responsibilities: · You will own the front-end development for one or more of our products and collaborate with visual/interaction designers, other engineers, and product managers to launch new products, iterate on existing features, and build a world-class user experience. · You will implement cutting-edge technologies and will be writing state-of-the-art code to keep LinkedIn at the cutting edge of current technology. · Your specific expertise will be required to make the site delightful, secure, performant and accessible to all our members. · You will meet with colleagues including product managers, designers and other engineers assigned to your project(s). Basic Qualifications: • BA/BS in Computer Science or related technical field or equivalent practical experience • 2+ years of industry experience in software design, development, and algorithm related solutions. • 2+ years programming experience in languages such as Javascript, HTML, CSS, Python, Java, C/C++, C#, Objective-C, Ruby, etc. Preferred Qualifications: · BS and 5+ years of relevant work experience, MS and 4+ years of relevant work experience, or PhD and 2+ years of relevant work experience. · Experience writing clean JavaScript including experience with modern frameworks (Angular/Ember/React) and debugging tools (Chrome Dev Tools, etc. ) · Expert-level knowledge of Ember. · Knowledge of (and a passion for) current trends and best practices in front-end architecture, including performance, accessibility, security and usability. · Prior experience building public API's with Java. · Experience building large-scale consumer facing products. Suggested Skills: · Javascript · HTML/CSS · Web Development · Advanced Programming Skills You will Benefit from our Culture: We strongly believe in the well-being of our employees and their families. That is why we offer generous health and wellness programs and time away for employees of all levels. LinkedIn is committed to fair and equitable compensation practices. The pay range for this role is $117,000 - $192,000. Actual compensation packages are based on a wide array of factors unique to each candidate, including but not limited to skill set, years & depth of experience, certifications and specific office location. This may differ in other locations due to cost of labor considerations. The total compensation package for this position may also include annual performance bonus, stock, benefits and/or other applicable incentive compensation plans. For additional information, visit: https://careers.linkedin.com/benefits. Equal Opportunity Statement LinkedIn is committed to diversity in its workforce and is proud to be an equal opportunity employer. LinkedIn considers qualified applicants without regard to race, color, religion, creed, gender, national origin, age, disability, veteran status, marital status, pregnancy, sex, gender expression or identity, sexual orientation, citizenship, or any other legally protected class. LinkedIn is an Affirmative Action and Equal Opportunity Employer as described in our equal opportunity statement here: https://microsoft.sharepoint.com/:b:/t/LinkedInGCI/EeE8sk7CTIdFmEp9ONzFOTEBM62TPrWLMHs4J1C_QxVTbg?e=5hfhpE. Please reference https://www.eeoc.gov/sites/default/files/2023-06/22-088_EEOC_KnowYourRights6.12ScreenRdr.pdf and https://www.dol.gov/ofccp/regs/compliance/posters/pdf/OFCCP_EEO_Supplement_Final_JRF_QA_508c.pdf for more information. LinkedIn is committed to offering an inclusive and accessible experience for all job seekers, including individuals with disabilities. Our goal is to foster an inclusive and accessible workplace where everyone has the opportunity to be successful. If you need a reasonable accommodation to search for a job opening, apply for a position, or participate in the interview process, connect with us at accommodations@linkedin.com and describe the specific accommodation requested for a disability-related limitation. Reasonable accommodations are modifications or adjustments to the application or hiring process that would enable you to fully participate in that process. Examples of reasonable accommodations include but are not limited to: -Documents in alternate formats or read aloud to you -Having interviews in an accessible location -Being accompanied by a service dog -Having a sign language interpreter present for the interview A request for an accommodation will be responded to within three business days. However, non-disability related requests, such as following up on an application, will not receive a response. LinkedIn will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by LinkedIn, or (c) consistent with LinkedIn's legal duty to furnish information. Pay Transparency Policy Statement As a federal contractor, LinkedIn follows the Pay Transparency and non-discrimination provisions described at this link: https://lnkd.in/paytransparency. Global Data Privacy Notice for Job Candidates This document provides transparency around the way in which LinkedIn handles personal data of employees and job applicants: https://lnkd.in/GlobalDataPrivacyNotice",
        "url": "https://www.linkedin.com/jobs/view/3967780647",
        "summary": "LinkedIn is seeking a Front-End Developer to build and maintain their web applications. This role requires 2+ years of experience in software development and programming languages such as JavaScript, HTML, CSS, Python, Java, C/C++, C#, Objective-C, Ruby. Experience with modern frameworks like Angular, Ember, React and debugging tools like Chrome Dev Tools is preferred. The ideal candidate will have expert-level knowledge of Ember and a passion for front-end architecture, including performance, accessibility, security, and usability. Prior experience building public APIs with Java and large-scale consumer-facing products is also beneficial. The role offers a hybrid work option, meaning you can work from home and commute to a LinkedIn office, depending on your preference. The compensation range for this position is $117,000 - $192,000, based on experience and location. ",
        "industries": [
            "Technology",
            "Software Development",
            "Social Media",
            "Networking"
        ],
        "soft_skills": [
            "Collaboration",
            "Communication",
            "Problem-Solving",
            "Teamwork",
            "Passion for technology",
            "Creativity"
        ],
        "hard_skills": [
            "JavaScript",
            "HTML",
            "CSS",
            "Python",
            "Java",
            "C/C++",
            "C#",
            "Objective-C",
            "Ruby",
            "Angular",
            "Ember",
            "React",
            "Chrome Dev Tools",
            "Ember",
            "Front-end architecture",
            "Performance",
            "Accessibility",
            "Security",
            "Usability",
            "API development",
            "Java",
            "Large-scale consumer-facing products"
        ],
        "tech_stack": [
            "JavaScript",
            "HTML",
            "CSS",
            "Angular",
            "Ember",
            "React",
            "Chrome Dev Tools",
            "Java",
            "Ember"
        ],
        "programming_languages": [
            "JavaScript",
            "HTML",
            "CSS",
            "Python",
            "Java",
            "C/C++",
            "C#",
            "Objective-C",
            "Ruby"
        ],
        "experience": 2,
        "education": {
            "min_degree": "BA/BS",
            "fields": [
                "Computer Science",
                "related technical field"
            ]
        },
        "salary": {
            "max": 192000,
            "min": 117000
        },
        "benefits": [
            "Health and wellness programs",
            "Time away",
            "Annual performance bonus",
            "Stock",
            "Benefits"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Ontario, CA",
        "job_id": 3874067657,
        "company": "Jarvis Consulting Group",
        "title": "Senior Data Engineer",
        "created_on": 1720636097.9270284,
        "description": "Role: Sr. Data Engineer Contract Length : 12 months Location : 3 days on-site / 2 days remote Job Description What is the Opportunity? We are seeking a hands-on, highly skilled, and motivated individual to join our dynamic team as a Senior Data Engineer. As a key member of our data engineering team, you will play a crucial role in the development and implementation of Asset Management Data Platform. As the successful candidate you will partner with key business stakeholders, understand the business drivers, and focus on designing, implementing, and maintaining robust data processing pipelines to support our organization’s data needs and advance our capabilities pertaining to data engineering, BI, Machine Learning and AI. What you will do: Work with business stakeholders and cross-functional teams to understand data requirements and deliver scalable data solutions. Design, develop, and maintain robust ETL processes to extract, transform, and load data from various sources into our data platform. Build large-scale batch and event-driven data pipelines using cloud and on-premises hybrid data platform topology. Work closely with data architects to review solutions and data models and ensure adherence to data platform architecture guidelines and engineering best practices. Take ownership of end-to-end deliverables and ensures high quality software development while fulfilling all operational and functional requirements in a timely manner. Implement and enforce data quality standards and best practices while collaborating with data governance teams to ensure compliance with data policies and regulations. Optimize data integration workflows for performance and reliability. Troubleshoot and resolve data integration and data processing issues. Leverage best practices in continuous integration and delivery using DataOps pipelines. Apply design-thinking and agile mindset in working with other engineers and business stakeholders to continuously experiment, iterate and deliver on new initiatives. Stay informed about emerging technologies and trends in data engineering domain. Lead, mentor, and inspire a team of data engineers to achieve high performance levels. What you need: 5-7 years of experience building batch and realtime data pipelines leveraging big data technologies like Hadoop, Spark, NiFi, Kafka, and Airflow. Proficiency in writing and optimizing SQL queries and at least one programming languages like Java, Scala and/or Python. Experience with cloud-based data platforms (Snowflakes, AWS, Azure, GCP) Experience working with docker and Kubernetes platforms. Experience with following DevOps and agile best practices. Continuously learning mindset and enjoy working on open-ended problems Nice-to-have Experience with OpenShift, S3, Trino, Ranger and Hive Knowledge of creating dashboards using Prometheus and Grafana Knowledge about data science tools and libraries Don’t meet every single requirement? Apply anyways! Jarvis Consulting Group is committed to developing a barrier-free recruitment process and work environment. We are committed to equality in the workplace and to growing a welcoming an inclusive team. We encourage applications from Indigenous persons, visible minority group members, and people of all sexual orientations and genders, and will consider all qualified applicants without regard to race, colour, religion, gender, gender identity or expression, sexual orientation, national origin, or disability or age. Jarvis is committed to compliance with all fair employment practices regarding citizenship and immigration status. Accommodations are available on request for candidates partaking in any stage of the selection process. If you are a job seeker who requires assistance, please contact: po@jrvs.ca Vous ne répondez pas à toutes les exigences ? Postulez quand même ! Jarvis Consulting Group s’engage à offrir un processus de recrutement et un environnement de travail sans barrières. Nous nous engageons à favoriser l’égalité sur le lieu de travail et à former une équipe accueillante et inclusive. Nous encourageons les candidatures de personnes autochtones, de membres de minorités visibles et de personnes de toutes orientations sexuelles et de tous genres. Nous examinerons toutes les candidatures qualifiées sans égard à la race, la religion, au sexe, à l’identité ou à l’expression sexuelle, à l’orientation sexuelle, au handicap ou à l’âge. Jarvis s’engage à respecter toutes les pratiques équitables en matière d’emploi en ce qui concerne la citoyenneté et le statut d’immigrant. Des aménagements sont disponibles sur demande, pour les candidats qui sont convoqués en entrevue, à n’importe quelle étape du processus de sélection. Si vous êtes à la recherche d’un emploi et que vous avez besoin d’aide, veuillez contacter : po@jrvs.ca See All Jobs",
        "url": "https://www.linkedin.com/jobs/view/3874067657",
        "summary": "Sr. Data Engineer needed for 12 month contract to design, implement, and maintain data processing pipelines using cloud and on-premises hybrid data platform topology. This individual will work with business stakeholders to understand their needs and will work closely with data architects to ensure solutions adhere to data platform architecture guidelines and engineering best practices.",
        "industries": [
            "Data Engineering",
            "Asset Management",
            "Business Intelligence",
            "Machine Learning",
            "AI"
        ],
        "soft_skills": [
            "Communication",
            "Problem Solving",
            "Teamwork",
            "Leadership",
            "Mentorship",
            "Collaboration",
            "Agile",
            "Continuous Learning"
        ],
        "hard_skills": [
            "Hadoop",
            "Spark",
            "NiFi",
            "Kafka",
            "Airflow",
            "SQL",
            "Java",
            "Scala",
            "Python",
            "Docker",
            "Kubernetes",
            "DevOps",
            "OpenShift",
            "S3",
            "Trino",
            "Ranger",
            "Hive",
            "Prometheus",
            "Grafana"
        ],
        "tech_stack": [
            "Hadoop",
            "Spark",
            "NiFi",
            "Kafka",
            "Airflow",
            "Snowflake",
            "AWS",
            "Azure",
            "GCP",
            "Docker",
            "Kubernetes",
            "OpenShift",
            "S3",
            "Trino",
            "Ranger",
            "Hive",
            "Prometheus",
            "Grafana"
        ],
        "programming_languages": [
            "SQL",
            "Java",
            "Scala",
            "Python"
        ],
        "experience": 5,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Tiburon, CA",
        "job_id": 3941796980,
        "company": "Digital Foundry, Inc.",
        "title": "Software Engineer Associate",
        "created_on": 1720636099.5070088,
        "description": "Digital Foundry has been a pioneer in digital innovation for over 25 years. We specialize in partnering with Fortune 500 companies to plan, design, and build cutting-edge digital products. Our portfolio includes custom mobile apps, responsive websites, Web 3.0 interactions, user experiences, connected hardware, IoT solutions, and comprehensive platforms that enhance both customer-facing products and internal operations tools. We are currently seeking a Software Engineer Associate to join our technology group in creating cutting-edge software solutions for our clients. Key Responsibilities: Work in a collaborative, team-centric environment utilizing agile principles Deliver high-quality solutions across multiple industries Collaborate with team to build software solutions where the problem is often ill defined Work on a variety of projects ranging in both size and scope Test your code and review the code of others Receive mentorship and accelerated learning Learn and adapt to new, complex development environments Minimum Requirements: BS in Computer Science OR relevant work experience Proficiency in one or more of the following: C#, Java, Javascript, Golang, Python, and CSS Work authorization in the U.S. which does not require sponsorship by the employer for a visa Why Join Us? Competitive Compensation: Digital Foundry offers a comprehensive compensation and benefits package, including a competitive salary, performance bonuses, medical, dental, and vision benefits, and a matching 401(k) retirement program Work-Life Balance: Enjoy up to three weeks of time off in your first year, along with a supportive work environment Innovative Culture: Our team comprises diverse, intelligent, and fun individuals who are passionate about pushing the boundaries of innovation. Compensation Compensation at Digital Foundry varies based on factors such as location, role, skill, and experience. In compliance with California law, Digital Foundry provides a reasonable target base salary range of $75k - $85k. Location: Our office is located in Tiburon CA, a short drive or ferry ride from San Francisco. We are looking for candidates willing to work full-time from the office. Thank you for your interest in Digital Foundry. We look forward to hearing from you! Digital Foundry, Inc. is an equal opportunity employer that is dedicated to a policy of non discrimination in employment and does not discriminate in hiring or employment on the basis of race, religion, creed, color, sex, sexual orientation, age, national origin, ancestry, mental or physical handicap, disability, or veteran status. Powered by JazzHR obwGteX5Ak",
        "url": "https://www.linkedin.com/jobs/view/3941796980",
        "summary": "Digital Foundry is a company that specializes in digital innovation for Fortune 500 companies. They build custom mobile apps, websites, user experiences, connected hardware, IoT solutions, and platforms. They are looking for a Software Engineer Associate to join their team.",
        "industries": [
            "Software Development",
            "Technology",
            "Consulting",
            "Digital Marketing"
        ],
        "soft_skills": [
            "Collaboration",
            "Communication",
            "Problem Solving",
            "Teamwork",
            "Adaptability",
            "Learning"
        ],
        "hard_skills": [
            "C#",
            "Java",
            "Javascript",
            "Golang",
            "Python",
            "CSS",
            "Agile Methodologies",
            "Testing",
            "Code Review"
        ],
        "tech_stack": [
            "Mobile App Development",
            "Web Development",
            "Web 3.0",
            "User Experience Design",
            "Connected Hardware",
            "IoT",
            "Platform Development"
        ],
        "programming_languages": [
            "C#",
            "Java",
            "Javascript",
            "Golang",
            "Python",
            "CSS"
        ],
        "experience": 0,
        "education": {
            "min_degree": "BS",
            "fields": [
                "Computer Science"
            ]
        },
        "salary": {
            "max": 85000,
            "min": 75000
        },
        "benefits": [
            "Competitive Salary",
            "Performance Bonuses",
            "Medical Benefits",
            "Dental Benefits",
            "Vision Benefits",
            "401(k) Matching",
            "Three Weeks Paid Time Off",
            "Supportive Work Environment"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3901450854,
        "company": "Unreal Staffing, Inc",
        "title": "Lead Data Systems Engineer",
        "created_on": 1720636102.8282204,
        "description": "Company Overview: Welcome to the forefront of data-driven innovation! Our company is dedicated to leveraging the power of data to drive transformative change and solve complex problems across industries. We're committed to building cutting-edge data systems that enable efficient data management, processing, and analysis. Join us and lead our efforts in shaping the future of data systems engineering. Position Overview: As the Lead Data Systems Engineer, you will play a pivotal role in leading our data systems engineering initiatives and driving the design, implementation, and optimization of our data systems infrastructure. You will lead a team of skilled engineers, collaborating closely with cross-functional teams to deliver high-quality, scalable, and reliable data systems solutions that meet the evolving needs of our data-driven organization. If you're a seasoned engineer with expertise in data systems technologies and a proven track record of leadership in delivering successful data projects, we invite you to lead our team in this exciting opportunity. Requirements Key Responsibilities: Technical Leadership: Provide strategic guidance, mentorship, and technical leadership to a team of data systems engineers, fostering a culture of excellence, innovation, and collaboration Data Systems Architecture: Lead the design and implementation of scalable and reliable data systems architecture to support the organization's data needs, including data storage, processing, and analytics Data Storage Solutions: Architect and implement data storage solutions, including relational databases, NoSQL databases, data warehouses, and data lakes, ensuring optimal performance, reliability, and scalability Data Processing Frameworks: Lead the implementation and optimization of data processing frameworks and technologies, such as Apache Hadoop, Apache Spark, and Apache Flink, to enable efficient data processing and analysis Data Pipeline Development: Lead the development and maintenance of data pipeline solutions to ingest, transform, and deliver data from various sources to target systems, ensuring seamless data flow and interoperability Data Integration: Lead efforts to integrate data from diverse sources and systems into data systems infrastructure, ensuring data consistency, integrity, and security Data Governance: Establish and enforce data governance policies and procedures to ensure data quality, security, and compliance with regulatory requirements Performance Optimization: Lead efforts to optimize data systems performance through indexing, partitioning, and other techniques, ensuring scalability and responsiveness for analytical and reporting needs Monitoring and Alerting: Implement robust monitoring and alerting systems to track data systems performance and health, proactively identifying and resolving issues to minimize downtime and data loss Documentation and Best Practices: Define and promote best practices for data systems engineering, ensuring clear and comprehensive documentation to facilitate understanding and collaboration among team members Collaboration: Collaborate closely with cross-functional teams, including data engineers, data scientists, and business analysts, to understand requirements and deliver data systems solutions that meet business needs Mentorship and Development: Mentor and coach junior engineers, providing guidance, support, and opportunities for skill development and career growth, and foster a culture of continuous learning and improvement within the team Qualifications: Bachelor's degree or higher in Computer Science, Engineering, Mathematics, or related field 8+ years of experience in data engineering or systems engineering, with a focus on data systems technologies Proven leadership experience, with a track record of successfully leading data systems engineering teams and delivering complex projects Expertise in data storage technologies such as relational databases (e.g., PostgreSQL, MySQL), NoSQL databases (e.g., MongoDB, Cassandra), data warehouses (e.g., Snowflake, Redshift), and data lakes (e.g., Amazon S3, Azure Data Lake Storage) Strong programming skills in languages such as Python, Java, or Scala, with experience in data processing frameworks like Apache Spark or Apache Flink Experience with cloud platforms such as AWS, Azure, or Google Cloud Platform, and services like AWS Glue, Azure Data Factory, or Google Dataflow Strong understanding of data integration concepts and techniques, with experience integrating data from diverse sources and systems in data systems infrastructure Strong problem-solving skills and analytical thinking, with the ability to design and troubleshoot complex data systems issues Excellent communication and collaboration skills, with the ability to effectively communicate technical concepts to non-technical stakeholders Benefits Competitive salary: The industry standard salary for Lead Data Systems Engineers typically ranges from $200,000 to $300,000 per year, depending on experience and qualifications Comprehensive benefits package, including health insurance, retirement plans, and wellness programs Flexible work arrangements, including remote work options and flexible hours Generous vacation and paid time off Professional development opportunities, including access to training programs, conferences, and workshops State-of-the-art technology environment with access to cutting-edge tools and resources Vibrant and inclusive company culture with opportunities for growth and advancement Exciting projects with real-world impact at the forefront of data-driven innovation Join Us: Ready to lead the charge in data systems engineering? Apply now to join our team and be part of the data revolution!",
        "url": "https://www.linkedin.com/jobs/view/3901450854",
        "summary": "Lead Data Systems Engineer will be responsible for leading a team of engineers in the design, implementation, and optimization of data systems infrastructure. Responsibilities include: data systems architecture, data storage solutions, data processing frameworks, data pipeline development, data integration, data governance, performance optimization, monitoring and alerting, documentation and best practices, collaboration, mentorship and development.",
        "industries": [
            "Data Engineering",
            "Data Science",
            "Technology",
            "Information Technology",
            "Software Development",
            "Cloud Computing",
            "Data Analytics",
            "Big Data"
        ],
        "soft_skills": [
            "Leadership",
            "Technical Guidance",
            "Mentorship",
            "Collaboration",
            "Communication",
            "Problem Solving",
            "Analytical Thinking",
            "Strategic Thinking",
            "Teamwork",
            "Project Management",
            "Time Management",
            "Organization",
            "Decision Making",
            "Innovation",
            "Customer Focus",
            "Detail Oriented"
        ],
        "hard_skills": [
            "Python",
            "Java",
            "Scala",
            "Apache Spark",
            "Apache Flink",
            "PostgreSQL",
            "MySQL",
            "MongoDB",
            "Cassandra",
            "Snowflake",
            "Redshift",
            "Amazon S3",
            "Azure Data Lake Storage",
            "AWS",
            "Azure",
            "Google Cloud Platform",
            "AWS Glue",
            "Azure Data Factory",
            "Google Dataflow",
            "Data Integration",
            "Data Governance",
            "Performance Optimization",
            "Monitoring and Alerting",
            "Documentation",
            "Best Practices",
            "Cloud Computing",
            "Big Data"
        ],
        "tech_stack": [
            "Apache Hadoop",
            "Apache Spark",
            "Apache Flink",
            "PostgreSQL",
            "MySQL",
            "MongoDB",
            "Cassandra",
            "Snowflake",
            "Redshift",
            "Amazon S3",
            "Azure Data Lake Storage",
            "AWS",
            "Azure",
            "Google Cloud Platform",
            "AWS Glue",
            "Azure Data Factory",
            "Google Dataflow"
        ],
        "programming_languages": [
            "Python",
            "Java",
            "Scala"
        ],
        "experience": 8,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Computer Science",
                "Engineering",
                "Mathematics"
            ]
        },
        "salary": {
            "max": 300000,
            "min": 200000
        },
        "benefits": [
            "Competitive salary",
            "Comprehensive benefits package",
            "Health insurance",
            "Retirement plans",
            "Wellness programs",
            "Flexible work arrangements",
            "Remote work options",
            "Flexible hours",
            "Generous vacation",
            "Paid time off",
            "Professional development opportunities",
            "Training programs",
            "Conferences",
            "Workshops",
            "State-of-the-art technology environment",
            "Cutting-edge tools and resources",
            "Vibrant and inclusive company culture",
            "Opportunities for growth and advancement",
            "Exciting projects with real-world impact"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Mateo, CA",
        "job_id": 3923034395,
        "company": "Snowflake",
        "title": "Software Engineer",
        "created_on": 1720636104.5610614,
        "description": "Build the future of data. Join the Snowflake team. Snowflake is one of the fastest-growing enterprise software companies ever. We are growing fast and we’re scaling our team to help enable and accelerate our growth. We’re passionate about our people, our customers, our values and our culture! We are looking for a Software Engineer to drive some of the key initiatives in our globally distributed Snowflake infrastructure. Snowflake started with a clear vision: make modern data warehousing effective, affordable, and accessible to all data users. Snowflake brings unprecedented flexibility and scalability to data warehousing. The Manageability team provides the basis for our Global platform strategy, helping with ease of management across multiple accounts and a centralized view of data usage. We are investing in initiatives in multiple areas, including a global control center to centrally manage Snowflake, replication across instances, distributed metadata management, disaster recovery AS A SOFTWARE ENGINEER IN GLOBAL PLATFORM, YOU WILL: Drive impactful initiatives for the globally distributed infrastructure Innovate and build highly secured and reliable designs for real-time ingestion, replication, performance, and scalability. Design for infrastructure to survive system outages, and provide a framework for handling critical systems. Collaborate with product managers, architects, other engineering teams, data science organizations, and business groups, to drive end-to-end solutions Ensure operational readiness of the services and meet the commitments to our customers regarding reliability, availability, and performance. OUR IDEAL SOFTWARE ENGINEER - GLOBAL PLATFORM WILL HAVE: 2-4 years of industry experience designing, building, and supporting systems and large-scale data processing systems in production Strong fundamental computer science skills Strong Knowledge of Algorithms and Data Structures Fluency in Java, Python and SQL Knowledge of databases and distributed systems BS/MS/PhD in Computer Science or related majors, or equivalent experience Every Snowflake employee is expected to follow the company’s confidentiality and security standards for handling sensitive data. Snowflake employees must abide by the company’s data security plan as an essential part of their duties. It is every employee's duty to keep customer information secure and confidential. The following represents the expected range of compensation for this role: The estimated base salary range for this role is -. Additionally, this role is eligible to participate in Snowflake’s bonus and equity plan. The successful candidate’s starting salary will be determined based on permissible, non-discriminatory factors such as skills, experience, and geographic location. This role is also eligible for a competitive benefits package that includes: medical, dental, vision, life, and disability insurance; 401(k) retirement plan; flexible spending & health savings account; at least 12 paid holidays; paid time off; parental leave; employee assistance program; and other company benefits. Snowflake is growing fast, and we’re scaling our team to help enable and accelerate our growth. We are looking for people who share our values, challenge ordinary thinking, and push the pace of innovation while building a future for themselves and Snowflake. How do you want to make your impact?",
        "url": "https://www.linkedin.com/jobs/view/3923034395",
        "summary": "Snowflake is seeking a Software Engineer to join their Global Platform team. This role will involve designing, building, and supporting highly secure and reliable systems for real-time data ingestion, replication, and scalability. The ideal candidate will have 2-4 years of experience in building large-scale data processing systems, strong computer science fundamentals, proficiency in Java, Python, and SQL, and knowledge of databases and distributed systems.",
        "industries": [
            "Software",
            "Data Warehousing",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Innovation",
            "Teamwork"
        ],
        "hard_skills": [
            "Java",
            "Python",
            "SQL",
            "Algorithms",
            "Data Structures",
            "Databases",
            "Distributed Systems",
            "System Design"
        ],
        "tech_stack": [
            "Snowflake",
            "Java",
            "Python",
            "SQL",
            "Databases",
            "Distributed Systems"
        ],
        "programming_languages": [
            "Java",
            "Python",
            "SQL"
        ],
        "experience": 2,
        "education": {
            "min_degree": "BS",
            "fields": [
                "Computer Science",
                "Related Majors"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Medical Insurance",
            "Dental Insurance",
            "Vision Insurance",
            "Life Insurance",
            "Disability Insurance",
            "401(k) Retirement Plan",
            "Flexible Spending Account",
            "Health Savings Account",
            "Paid Holidays",
            "Paid Time Off",
            "Parental Leave",
            "Employee Assistance Program"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3635155802,
        "company": "Platform Venture Studio",
        "title": "Software Engineer",
        "created_on": 1720636107.9083958,
        "description": "PlatformOS is our proprietary software platform for running Platform Venture Studio. Think of it as \"Github for building startups\". It captures all of our best practice, allows us to build in the open, and engages the collective intelligence of our network. PlatformOS comprises various responsive web applications, a browser extension, API integrations, and data science components. Reporting directly to Platform's founder, you will be a key player in continuing the evolution of PlatformOS. You will have: 5+ years of experience in both front-end and back-end web development (an uneven mix is ok) Back-end web development experience, ideally in Rails, but analogous experience in Python or Javascript based frameworks is fine too Front-end web development experience in React. React Native experience is helpful An ability to dial in the right level of quality vs speed, based on the needs of the situation Experience managing production deployment is helpful Comfort working remotely as the norm and the communication skills to make it work effectively. We are a remote-first company, with a remote-first culture A desire to work in a small, agile, scrappy, and fun team Compensation A competitive salary, commensurate with experience and location Benefits - health, vision, dental, 401(k), insurance Participation in Platform Cohort SPVs (equivalent to venture fund carry) Flexibility to work from anywhere An empathetic, supportive, and inclusive culture",
        "url": "https://www.linkedin.com/jobs/view/3635155802",
        "summary": "Platform Venture Studio is seeking a Full-Stack Web Developer to contribute to the development of their proprietary software platform, PlatformOS, a 'Github for building startups'. This platform comprises responsive web applications, a browser extension, API integrations, and data science components. The role requires strong front-end and back-end skills, specifically in React and Rails or equivalent frameworks like Python or Javascript.  Experience in React Native and managing production deployments are beneficial. The ideal candidate will have 5+ years of experience, be comfortable working remotely, and enjoy a fast-paced, collaborative environment.",
        "industries": [
            "Software Development",
            "Venture Capital",
            "Startup"
        ],
        "soft_skills": [
            "Communication",
            "Remote Collaboration",
            "Agile",
            "Adaptability",
            "Problem-Solving"
        ],
        "hard_skills": [
            "React",
            "Rails",
            "Python",
            "Javascript",
            "API Integration",
            "Data Science",
            "React Native",
            "Production Deployment",
            "Web Development"
        ],
        "tech_stack": [
            "PlatformOS",
            "Github",
            "Rails",
            "React",
            "React Native",
            "API",
            "Data Science",
            "Browser Extension"
        ],
        "programming_languages": [
            "Javascript",
            "Python",
            "Ruby"
        ],
        "experience": 5,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Health Insurance",
            "Vision Insurance",
            "Dental Insurance",
            "401(k)",
            "Insurance",
            "Equity",
            "Remote Work"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3959283756,
        "company": "DoorDash",
        "title": "Senior Software Engineer, Compute",
        "created_on": 1720636111.838909,
        "description": "About the Company Help us build the world's most reliable, on-demand logistics engine for last-mile delivery! We're bringing on experienced engineers to help us create and maintain a 24x7, no downtime, global infrastructure system that powers DoorDash's three-sided marketplace of consumers, merchants, and dashers. About the Role As a member of the Compute Team, you will build and operate a Kubernetes platform for running containerized workloads reliably and efficiently at scale. You will report to the Engineering Manager on the Compute Team in the Core Infrastructure organization. You're excited about this opportunity because you will… Work with a fantastic bunch of highly talented and collaborative teammates and colleagues. Architect, deploy, and oversee the operation of multiple highly available Kubernetes clusters. Establish, enforce, and mentor team members on best practices for Kubernetes, including deployment patterns, cluster sizing and scaling, resource management, security policies, and high availability strategies Develop and lead initiatives for monitoring, metrics, and automation solutions. Collaborate with other teams to influence design for engineering excellence and drive cross-functional projects. Design, develop, and deliver significant features and components to improve and enable service-to-service communication needs for all DoorDash teams in a secure, robust, and scalable way. Provide technical leadership and guidance in a 100% Cloud Environment (AWS) and use tools and services like Argo Workflows, Argo CD, kOps, Prometheus, etc. We're excited about you because… You have 5+ years of experience as a software engineer in infrastructure and/or platform teams, with a track record of technical leadership. You have prior experience with modern infrastructure tooling, such as Kubernetes, as an administrator or operator in a production setting, e.g. container orchestration, cloud providers (AWS, GCP, etc.), and infrastructure-as-code tools. You have strong development skills in Go, Python or other programming languages. You have demonstrated experience in mentoring junior engineers and fostering a collaborative team environment. Bonus points if you… Have extensive experience designing, developing, operating, and debugging high-traffic production systems. Have in-depth experience using AWS cloud services and optimizing their use for large-scale infrastructure. Have a deep knowledge of Linux or similar operating systems and networking. Are proactive and driven to understand the needs of your customers, delivering high-impact solutions. Believe in providing valuable abstractions, clear customer SLOs, and meaningful KPIs, with a strategic mindset towards infrastructure development. Have prior contributions to projects within the CNCF community and a strong presence in the open-source community. Compensation The location-specific base salary range for this position is listed below. Compensation in other geographies may vary. Actual compensation within the pay range will be decided based on factors including, but not limited to, skills, prior relevant experience, and specific work location. For roles that are available to be filled remotely, base salary is localized according to employee work location. Please discuss your intended work location with your recruiter for more information. DoorDash cares about you and your overall well-being, and that's why we offer a comprehensive benefits package, for full-time employees, that includes healthcare benefits, a 401(k) plan including an employer match, short-term and long-term disability coverage, basic life insurance, wellbeing benefits, paid time off, paid parental leave, and several paid holidays, among others. In addition to base salary, the compensation package for this role also includes opportunities for equity grants. We use Covey as part of our hiring and / or promotional process for jobs in NYC and certain features may qualify it as an AEDT. As part of the evaluation process we provide Covey with job requirements and candidate submitted applications. We began using Covey Scout for Inbound on June 20, 2024. Please see the independent bias audit report covering our use of Covey here. California Pay Range: $170,600—$255,800 USD Washington Pay Range: $170,600—$255,800 USD About DoorDash At DoorDash, our mission to empower local economies shapes how our team members move quickly, learn, and reiterate in order to make impactful decisions that display empathy for our range of users—from Dashers to merchant partners to consumers. We are a technology and logistics company that started with door-to-door delivery, and we are looking for team members who can help us go from a company that is known for delivering food to a company that people turn to for any and all goods. DoorDash is growing rapidly and changing constantly, which gives our team members the opportunity to share their unique perspectives, solve new challenges, and own their careers. We're committed to supporting employees' happiness, healthiness, and overall well-being by providing comprehensive benefits and perks including premium healthcare, wellness expense reimbursement, paid parental leave and more. Our Commitment to Diversity and Inclusion We're committed to growing and empowering a more inclusive community within our company, industry, and cities. That's why we hire and cultivate diverse teams of people from all backgrounds, experiences, and perspectives. We believe that true innovation happens when everyone has room at the table and the tools, resources, and opportunity to excel. Statement of Non-Discrimination: In keeping with our beliefs and goals, no employee or applicant will face discrimination or harassment based on: race, color, ancestry, national origin, religion, age, gender, marital/domestic partner status, sexual orientation, gender identity or expression, disability status, or veteran status. Above and beyond discrimination and harassment based on \"protected categories,\" we also strive to prevent other subtler forms of inappropriate behavior (i.e., stereotyping) from ever gaining a foothold in our office. Whether blatant or hidden, barriers to success have no place at DoorDash. We value a diverse workforce – people who identify as women, non-binary or gender non-conforming, LGBTQIA+, American Indian or Native Alaskan, Black or African American, Hispanic or Latinx, Native Hawaiian or Other Pacific Islander, differently-abled, caretakers and parents, and veterans are strongly encouraged to apply. Thank you to the Level Playing Field Institute for this statement of non-discrimination. Pursuant to the San Francisco Fair Chance Ordinance, Los Angeles Fair Chance Initiative for Hiring Ordinance, and any other state or local hiring regulations, we will consider for employment any qualified applicant, including those with arrest and conviction records, in a manner consistent with the applicable regulation. If you need any accommodations, please inform your recruiting contact upon initial connection.",
        "url": "https://www.linkedin.com/jobs/view/3959283756",
        "summary": "DoorDash is seeking a highly skilled Kubernetes engineer to join their Compute Team and build and operate a reliable and scalable Kubernetes platform for containerized workloads. This role involves architecting, deploying, and overseeing multiple high-availability Kubernetes clusters, establishing best practices, developing automation solutions, collaborating with other teams, and providing technical leadership in a 100% Cloud Environment (AWS).",
        "industries": [
            "Technology",
            "Logistics",
            "Food Delivery",
            "E-commerce"
        ],
        "soft_skills": [
            "Technical leadership",
            "Collaboration",
            "Communication",
            "Mentorship",
            "Problem-solving",
            "Proactive",
            "Customer-focused",
            "Strategic thinking"
        ],
        "hard_skills": [
            "Kubernetes",
            "Container orchestration",
            "Cloud providers (AWS, GCP)",
            "Infrastructure-as-code",
            "Go",
            "Python",
            "Linux",
            "Networking",
            "Argo Workflows",
            "Argo CD",
            "kOps",
            "Prometheus"
        ],
        "tech_stack": [
            "Kubernetes",
            "AWS",
            "Argo Workflows",
            "Argo CD",
            "kOps",
            "Prometheus"
        ],
        "programming_languages": [
            "Go",
            "Python"
        ],
        "experience": 5,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 255800,
            "min": 170600
        },
        "benefits": [
            "Healthcare benefits",
            "401(k) plan with employer match",
            "Short-term and long-term disability coverage",
            "Basic life insurance",
            "Wellbeing benefits",
            "Paid time off",
            "Paid parental leave",
            "Paid holidays",
            "Equity grants"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco Bay Area",
        "job_id": 3943728293,
        "company": "Whatnot",
        "title": "Software Engineer, Ads",
        "created_on": 1720636113.6364098,
        "description": "🚀 Whatnot Whatnot is a livestream shopping platform and marketplace backed by Andreessen Horowitz, Y Combinator, and CapitalG. We’re building the future of ecommerce, bringing together community, shopping and entertainment. We are committed to our values , and as a remote-first team, we operate out of hubs within the US, Canada, UK, Ireland, and Germany today. We’re innovating in the fast-paced world of live auctions in categories including sports, fashion, video games, and streetwear. The platform couples rigorous seller vetting with a focus on community to create a welcoming space for buyers and sellers to share their passions with others. And, we’re growing. Whatnot has been the fastest growing marketplace in the US over the past two years and we’re hiring forward-thinking problem solvers across all functional areas. 💻 Role Ads is a growing engineering team at Whatnot dedicated to building systems & features to help Sellers grow their business. As a software engineer on the ads team, the most important qualities you'll need are making sound product decisions, navigating new domains seamlessly, taking extreme ownership, and shipping high-quality products fast. We always prioritize the highest-impact features and ship them quickly. The software team has a lot of individual responsibility and many more freedoms, so we need to hire people we can trust. Your ability to put customers first, make good trade-offs, and deliver impact is essential to us. In this role, you will… Build ads products by working across the full ads stack including bidding & targeting, ranking/auction, serving, campaign management, and seller tooling. Build highly scalable ads system that handles all aspects of running an online advertising system and helps sellers differentiate themselves on Whatnot. Understand our buyer and seller experiences and become an expert in helping them effectively scale their business through ads products. 👋 You Curious about who thrives at Whatnot? We’ve found that low ego, a growth mindset, and leaning into action and high impact goes a long way here. As our next Software Engineer, you should have 5+ years of software engineering experience, ideally with some industry experience in one of the following areas: ads serving, ads quality, pricing, discovery, ranking, recommendations, and/or search engines. You should also have: Bachelor’s degree in Computer Science, a related field, or equivalent work experience. Capability of building scalable systems (we primarily use Python, Elixir, JavaScript). Strong product instincts. You first think about users rather than the best technical solution. Experience building systems that scale at a high-growth company before, and can do it again with minimal guidance. The speed to ship products and features lightning-fast without sacrificing quality. Excellent problem-solving skills and don’t need to be told exactly what to do. Comfortability working across the stack (backend and frontend). The ability to pick up on new technologies very quickly. A proven track record of delivering features. 💰Compensation For Full-Time (Salary) US-based applicants: $185,000/year to $245,000/year + benefits + stock options The salary range may be inclusive of several levels that would be applicable to the position. Final salary will be based on a number of factors including, level, relevant prior experience, skills and expertise. This range is only inclusive of base salary, not benefits (more details below) or equity in the form of stock options. 🎁 Benefits Flexible Time off Policy and Company-wide Holidays (including a spring and winter break) Health Insurance options including Medical, Dental, Vision Work From Home Support $1,000 home office setup allowance $150 monthly allowance for cell phone and internet Care benefits $450 monthly allowance on food $500 monthly allowance for wellness $5,000 annual allowance towards Childcare $20,000 lifetime benefit for family planning, such as adoption or fertility expenses Retirement; 401k offering for Traditional and Roth accounts in the US (employer match up to 4% of base salary) and Pension plans internationally Parental Leave 16 weeks of paid parental leave + one month gradual return to work *company leave allowances run concurrently with country leave requirements which take precedence. 💛 EOE Whatnot is proud to be an Equal Opportunity Employer. We value diversity, and we do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, parental status, disability status, or any other status protected by local law. We believe that our work is better and our company culture is improved when we encourage, support, and respect the different skills and experiences represented within our workforce.",
        "url": "https://www.linkedin.com/jobs/view/3943728293",
        "summary": "Whatnot is seeking a Software Engineer to join their Ads team.  The ideal candidate will have 5+ years of experience building ads systems and experience with Python, Elixir, and JavaScript. This role will focus on building ads products across the full stack, including bidding and targeting, ranking/auction, serving, campaign management, and seller tooling. The role is remote-first and offers a competitive salary and benefits package.",
        "industries": [
            "Ecommerce",
            "Live Streaming",
            "Technology",
            "Retail"
        ],
        "soft_skills": [
            "Communication",
            "Problem-Solving",
            "Collaboration",
            "Ownership",
            "Growth Mindset",
            "Product Instincts",
            "Self-Motivation"
        ],
        "hard_skills": [
            "Python",
            "Elixir",
            "JavaScript",
            "Ads Serving",
            "Ads Quality",
            "Pricing",
            "Discovery",
            "Ranking",
            "Recommendations",
            "Search Engines",
            "Scalable Systems"
        ],
        "tech_stack": [
            "Python",
            "Elixir",
            "JavaScript"
        ],
        "programming_languages": [
            "Python",
            "Elixir",
            "JavaScript"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Computer Science",
                "Related Field"
            ]
        },
        "salary": {
            "max": 245000,
            "min": 185000
        },
        "benefits": [
            "Flexible Time Off",
            "Company Holidays",
            "Health Insurance",
            "Work From Home Support",
            "Home Office Setup Allowance",
            "Cell Phone and Internet Allowance",
            "Care Benefits",
            "Food Allowance",
            "Wellness Allowance",
            "Childcare Allowance",
            "Family Planning Benefit",
            "Retirement",
            "Parental Leave"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3907291196,
        "company": "HireIO, Inc.",
        "title": "Senior Backend Software Engineer",
        "created_on": 1720636115.3082552,
        "description": "Participate in the development of AI customer service within the e-commerce ecosystem to help our sellers better serve our customers. including multi-round dialogue, FAQ, prompt engineering, etc Engage in and improve the end-to-end lifecycle of services from design, development, capacity planning, and launch reviews to deployment, operation, and optimizations; Optimize system scalability, reliability, sustainability, velocity, and architecture efficiency; Requirements Bachelor's degree in Computer Science or related technical field; 2+ years experience working with Linux systems, and development experience in, Python, Go, C++, or related languages; Excellent problem-solving skills with strong attention to detail and collaboration skills. Ability to deep dive into complex technical problems; Good sense of teamwork and communication skills, practical experience in relevant business scenarios is preferred. Preferred Qualifications Experience with deep learning frameworks such as TensorFlow/PyTorch, understanding distributed training, distillation acceleration, and other implementation methods Knowledge of large language models, and experience in prompt engineering",
        "url": "https://www.linkedin.com/jobs/view/3907291196",
        "summary": "Develop AI-powered customer service solutions for an e-commerce platform, focusing on multi-round dialogue, FAQ, and prompt engineering. This role involves the entire lifecycle of service development, from design and development to deployment and optimization.  Strong technical skills in Linux systems, Python, Go, C++, and deep learning frameworks are required. Experience with large language models and prompt engineering are preferred.",
        "industries": [
            "E-commerce",
            "Artificial Intelligence",
            "Customer Service"
        ],
        "soft_skills": [
            "Problem-solving",
            "Attention to detail",
            "Collaboration",
            "Teamwork",
            "Communication"
        ],
        "hard_skills": [
            "Linux Systems",
            "Python",
            "Go",
            "C++",
            "Deep Learning",
            "TensorFlow",
            "PyTorch",
            "Distributed Training",
            "Distillation Acceleration",
            "Large Language Models",
            "Prompt Engineering"
        ],
        "tech_stack": [
            "Linux",
            "Python",
            "Go",
            "C++",
            "TensorFlow",
            "PyTorch"
        ],
        "programming_languages": [
            "Python",
            "Go",
            "C++"
        ],
        "experience": 2,
        "education": {
            "min_degree": "Bachelor's Degree",
            "fields": [
                "Computer Science",
                "Related Technical Field"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3860725254,
        "company": "Magic",
        "title": "Software Engineer - Supercomputing Platform & Infrastructure",
        "created_on": 1720636116.8913407,
        "description": "Magic’s mission is to build safe AGI that accelerates humanity’s progress on the world’s most important problems. We believe the most promising path to safe AGI lies in automating research and code generation to improve models and solve alignment more reliably than humans can alone. Our approach combines frontier-scale pre-training, domain-specific RL, ultra-long context, and test-time compute to achieve this goal. About the role: As a Software Engineer on our Supercomputing Platform & Infrastructure team, you will design and build resilient and optimized solutions for AI workloads on massive Computing Clusters. What you might work on: Work closely with the training and inference teams to deliver high performance and reliability across storage, networking, and distributed computing designs. Build the software stack to run massive-scale (thousands of GPUs), highly available supercomputing infrastructure Troubleshoot and resolve complex issues across hardware accelerated devices, networking, storage subsystems (local NVMe/Block Storage/NFS), OS, drivers and cloud environments, and automate detection and recovery processes Operate data-intensive workloads at petabyte-scale Increase the ease-of-use and self-serviceability of the compute platforms at Magic through top-notch documentation and developer workflow design Investigate and resolve incidents across security and availability What we’re looking for: Experience working with production GPU deployments, data-intensive applications, large-scale model training and HPC Strong understanding of networking-, storage- and data-related technologies Experience with GCP, AWS, Azure, OCI or similar cloud platforms Strong software engineering skills Strong IaC knowledge with extensive experience in Terraform, Pulumi, AWS CDK/CloudFormation or similar Magic strives to be the place where high-potential individuals can do their best work. We value quick learning and grit just as much as skill and experience. Our culture: Integrity. Words and actions should be aligned Hands-on. At Magic, everyone is building Teamwork. We move as one team, not N individuals Focus. Safely deploy AGI. Everything else is noise Quality. Magic should feel like magic Compensation, benefits and perks (US): Annual salary range: $100K - $1M Equity is a significant part of total compensation, in addition to salary 401(k) plan with 6% salary matching Generous health, dental and vision insurance for you and your dependants Unlimited paid time off Option to work in-person in SF or remotely Visa sponsorship and relocation stipend to bring you to SF A small, fast-paced, highly focused team",
        "url": "https://www.linkedin.com/jobs/view/3860725254",
        "summary": "Software Engineer role at Magic, focusing on building and optimizing a supercomputing platform for AI workloads on massive clusters. Responsibilities include designing resilient solutions for AI workloads, managing large-scale infrastructure, troubleshooting complex issues, and improving platform usability.",
        "industries": [
            "Artificial Intelligence",
            "Machine Learning",
            "Supercomputing",
            "Cloud Computing",
            "Software Engineering"
        ],
        "soft_skills": [
            "Problem-solving",
            "Communication",
            "Teamwork",
            "Self-motivation",
            "Quick learning",
            "Grit"
        ],
        "hard_skills": [
            "GPU Deployment",
            "Data-Intensive Applications",
            "Large-Scale Model Training",
            "HPC",
            "Networking",
            "Storage",
            "Data Technologies",
            "Cloud Platforms",
            "Software Engineering",
            "IaC",
            "Terraform",
            "Pulumi",
            "AWS CDK",
            "CloudFormation"
        ],
        "tech_stack": [
            "GCP",
            "AWS",
            "Azure",
            "OCI",
            "Terraform",
            "Pulumi",
            "AWS CDK",
            "CloudFormation"
        ],
        "programming_languages": [],
        "experience": 0,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 1000000,
            "min": 100000
        },
        "benefits": [
            "Equity",
            "401(k) with matching",
            "Health, dental, and vision insurance",
            "Unlimited paid time off",
            "Remote work option",
            "Visa sponsorship",
            "Relocation stipend"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Burbank, CA",
        "job_id": 3902081003,
        "company": "Walt Disney Animation Studios",
        "title": "Systems Engineer, Data Services",
        "created_on": 1720636118.5711985,
        "description": "At Walt Disney Animation Studios we believe that creativity inspires technology, and technology inspires creativity. The Data Services team is responsible for the design, implementation, and tuning of the database systems behind the content our studio creates. We are looking for a Systems Engineer to join the team in these efforts. Systems Engineers are expected to collaborate across teams, demonstrate self-direction when needed, and learn through experiencing both success and failure. They must build partnerships with technical and non-technical users, understand their needs, and deliver relevant, sustainable solutions that align with initiatives for both our Burbank and Vancouver studio locations. Our studio thrives from a wide variety of technical backgrounds and experiences, therefore we encourage applicants to apply even if they do not match all requirements listed below. Bring your unique talents and ideas to our team, and join Disney’s creative legacy! Qualifications 2+ years of systems/software engineering and/or database administration experience; can include related internships Experience engineering and maintaining both Document databases (Cassandra, Elasticsearch / OpenSearch, and MongoDB) and Relational databases (MySQL, MariaDB, Postgres, and MSSQL) Previous contributions to architectural design and optimization of data systems and pipelines Experience with Unix shell scripting and system automation Experience with server-side scripting such as Python, C/C++, or Java Bonus Qualifications Experience with TimeScaleDB, Redis, InfluxDB, Grafana, and/or Logstash Implementation of data ingestion and handling clusters through event streaming platforms, such as Kafka Knowledge of Kubernetes fundamentals Experience with AWS Data Managed Services, such as, S3 partitioning, Lambda for processing, Kinesis for pipelining, and/or Glue for ETL Experience with large scale data management, including identifying appropriate on-prem/cloud storage and developing solutions based on data scale, analytic use cases, and immediacy of refresh Education Bachelor’s degree in Computer Science, Information Systems, Software, Electrical or Electronics Engineering, or comparable field of study, and/or equivalent work experience The hiring range for this position in California is $93,400-$125,200 per year based on a 40 hour work week. The amount of hours scheduled per week may vary based on business needs. The base pay actually offered will take into account internal equity and also may vary depending on the candidate’s geographic region, job-related knowledge, skills, and experience among other factors. A bonus and/or long-term incentive units may be provided as part of the compensation package, in addition to the full range of medical, financial, and/or other benefits, dependent on the level and position offered.",
        "url": "https://www.linkedin.com/jobs/view/3902081003",
        "summary": "Walt Disney Animation Studios is seeking a Systems Engineer to design, implement, and tune database systems for their content creation. The role involves collaboration across teams, self-direction, and building partnerships to deliver sustainable solutions for both Burbank and Vancouver studios.",
        "industries": [
            "Animation",
            "Film",
            "Entertainment",
            "Technology",
            "Software Development",
            "Database Administration"
        ],
        "soft_skills": [
            "Collaboration",
            "Self-direction",
            "Problem Solving",
            "Partnership Building",
            "Communication",
            "Technical Skills"
        ],
        "hard_skills": [
            "Systems Engineering",
            "Software Engineering",
            "Database Administration",
            "Document Databases",
            "Relational Databases",
            "Architectural Design",
            "Data System Optimization",
            "Unix Shell Scripting",
            "System Automation",
            "Server-Side Scripting",
            "Data Ingestion",
            "Event Streaming Platforms",
            "Kubernetes",
            "AWS Data Managed Services",
            "Large Scale Data Management"
        ],
        "tech_stack": [
            "Cassandra",
            "Elasticsearch",
            "OpenSearch",
            "MongoDB",
            "MySQL",
            "MariaDB",
            "Postgres",
            "MSSQL",
            "TimeScaleDB",
            "Redis",
            "InfluxDB",
            "Grafana",
            "Logstash",
            "Kafka",
            "Kubernetes",
            "AWS S3",
            "AWS Lambda",
            "AWS Kinesis",
            "AWS Glue"
        ],
        "programming_languages": [
            "Python",
            "C/C++",
            "Java"
        ],
        "experience": 2,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Computer Science",
                "Information Systems",
                "Software",
                "Electrical Engineering",
                "Electronics Engineering"
            ]
        },
        "salary": {
            "max": 125200,
            "min": 93400
        },
        "benefits": [
            "Medical",
            "Financial",
            "Bonus",
            "Long-Term Incentive Units"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Oakland, CA",
        "job_id": 3697713449,
        "company": "Spinwheel",
        "title": "Software Engineer (Remote)",
        "created_on": 1720636120.3996649,
        "description": "At Spinwheel we’re on a mission to improve the way Americans understand, manage and pay their debt. By giving consumers a full picture of their student loans, credit cards, auto loans, mortgage (the list goes on), we can drive responsible actions to improve financial health. This should live where the consumers are, and our partners use our platform to embed these tools into their own apps. We are building a team as diverse as the markets we serve. Spinwheel provides you an opportunity to get in early at a fast-paced startup and make an outsized impact while working with a passionate, results-oriented and fun team. Responsibilities: Work with product managers to understand requirements and implement new features Provide technical integration support to potential partners Coordinate and collaborate with the offshore development team on product implementation Monitor, troubleshoot and fix product issues Skill set: 2-5 years as a software developer Excellent written and oral communication skills are required since the engineer has to communicate with the customers, product managers and the engineering team Ability to maintain a keen attention to detail, multitask and work well under pressure Natural tendency to be curious, positive and creative Team player who collaborates well with others Sincere empathy for the customer and a commitment to delving deep into the challenges they present or experience Self motivated and work with minimal supervision Having student debt is a huge plus Technical Skills: Full stack javascript developer Programming stack - Node.js, React, Bootstrap Data stack - MongoDB, MySQL DevOps - AWS Code - Git Spinwheel offers the chance to get meaningful equity in an early-stage, fast-growing startup. Our other benefits include: Remote-first company Medical, Dental, and Vision Coverage Unlimited PTO Team Offsite (In 2021 we brought the team to Lake Tahoe) And more!",
        "url": "https://www.linkedin.com/jobs/view/3697713449",
        "summary": "Spinwheel is a fintech startup seeking a full-stack JavaScript developer to build and integrate its debt management platform. The role involves collaborating with product managers, integrating with partner platforms, coordinating with an offshore team, and troubleshooting product issues. The ideal candidate will have 2-5 years of software development experience, strong communication skills, attention to detail, and a passion for solving customer challenges.",
        "industries": [
            "FinTech",
            "Software Development",
            "Financial Services"
        ],
        "soft_skills": [
            "Communication",
            "Attention to Detail",
            "Multitasking",
            "Problem Solving",
            "Teamwork",
            "Empathy",
            "Self-Motivation",
            "Creativity",
            "Positive Attitude",
            "Customer Focus"
        ],
        "hard_skills": [
            "JavaScript",
            "Node.js",
            "React",
            "Bootstrap",
            "MongoDB",
            "MySQL",
            "AWS",
            "Git"
        ],
        "tech_stack": [
            "JavaScript",
            "Node.js",
            "React",
            "Bootstrap",
            "MongoDB",
            "MySQL",
            "AWS",
            "Git"
        ],
        "programming_languages": [
            "JavaScript",
            "Node.js"
        ],
        "experience": 2,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Equity",
            "Remote Work",
            "Medical, Dental, Vision",
            "Unlimited PTO",
            "Team Offsite"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Bruno, CA",
        "job_id": 3940593048,
        "company": "Google",
        "title": "Software Engineer III, Machine Learning (Recommendations, Rankings, and Predictions), YouTube",
        "created_on": 1720636122.0273898,
        "description": "Note: By applying to this position you will have an opportunity to share your preferred working location from the following: Mountain View, CA, USA; San Bruno, CA, USA . Minimum qualifications: Bachelor’s degree or equivalent practical experience. 2 years of experience with software development in one or more programming languages, or 1 year of experience with an advanced degree in an industry setting. 2 years of experience with data structures or algorithms in either an academic or industry setting. 2 years of experience with machine learning algorithms and tools (e.g., TensorFlow), artificial intelligence. Experience developing algorithms (ranking, recommendations, prediction, search quality, personalization) and software that generates suggestions based on various input and output targets. Programming experience in Python. Preferred qualifications: Master's degree or PhD in Computer Science or related technical field. 2 years of experience with performance, large scale systems data analysis, visualization tools, or debugging. Experience developing accessible technologies. Proficiency in code and system health, diagnosis and resolution, and software test engineering. About The Job Google's software engineers develop the next-generation technologies that change how billions of users connect, explore, and interact with information and one another. Our products need to handle information at massive scale, and extend well beyond web search. We're looking for engineers who bring fresh ideas from all areas, including information retrieval, distributed computing, large-scale system design, networking and data storage, security, artificial intelligence, natural language processing, UI design and mobile; the list goes on and is growing every day. As a software engineer, you will work on a specific project critical to Google’s needs with opportunities to switch teams and projects as you and our fast-paced business grow and evolve. We need our engineers to be versatile, display leadership qualities and be enthusiastic to take on new problems across the full-stack as we continue to push technology forward. With your technical expertise you will manage project priorities, deadlines, and deliverables. You will design, develop, test, deploy, maintain, and enhance software solutions. At YouTube, we believe that everyone deserves to have a voice, and that the world is a better place when we listen, share, and build community through our stories. We work together to give everyone the power to share their story, explore what they love, and connect with one another in the process. Working at the intersection of cutting-edge technology and boundless creativity, we move at the speed of culture with a shared goal to show people the world. We explore new ideas, solve real problems, and have fun — and we do it all together. The US base salary range for this full-time position is $136,000-$200,000 + bonus + equity + benefits. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process. Please note that the compensation details listed in US role postings reflect the base salary only, and do not include bonus, equity, or benefits. Learn more about benefits at Google. Responsibilities Write product or system development code. Participate in, or lead design reviews with peers and stakeholders to decide amongst available technologies. Review code developed by other developers and provide feedback to ensure best practices (e.g., style guidelines, checking code in, accuracy, testability, and efficiency). Contribute to existing documentation or educational content and adapt content based on product/program updates and user feedback. Triage product or system issues and debug/track/resolve by analyzing the sources of issues and the impact on hardware, network, or service operations and quality. Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing our Accommodations for Applicants form .",
        "url": "https://www.linkedin.com/jobs/view/3940593048",
        "summary": "Google is seeking a Software Engineer to develop next-generation technologies that power their products and services. The role involves designing, developing, testing, deploying, and maintaining software solutions, with a focus on machine learning algorithms, data structures, and algorithms. The ideal candidate will have experience in Python programming, machine learning tools like TensorFlow, and a strong understanding of data structures and algorithms. The role offers the opportunity to work on a variety of projects critical to Google's needs, with opportunities to switch teams and projects as the company evolves. ",
        "industries": [
            "Technology",
            "Software Development",
            "Internet",
            "Artificial Intelligence",
            "Machine Learning",
            "Search Engine"
        ],
        "soft_skills": [
            "Leadership",
            "Versatility",
            "Communication",
            "Collaboration",
            "Problem Solving",
            "Critical Thinking",
            "Adaptability",
            "Enthusiasm",
            "Teamwork"
        ],
        "hard_skills": [
            "Python",
            "TensorFlow",
            "Machine Learning",
            "Data Structures",
            "Algorithms",
            "Software Development",
            "System Design",
            "Large-scale Systems",
            "Performance Optimization",
            "Data Analysis",
            "Visualization",
            "Debugging",
            "Accessibility",
            "Code Review",
            "Documentation",
            "Issue Triage",
            "System Health",
            "Software Test Engineering"
        ],
        "tech_stack": [
            "TensorFlow",
            "Python"
        ],
        "programming_languages": [
            "Python"
        ],
        "experience": 2,
        "education": {
            "min_degree": "Bachelor’s degree",
            "fields": [
                "Computer Science",
                "Related Technical Field"
            ]
        },
        "salary": {
            "max": 200000,
            "min": 136000
        },
        "benefits": [
            "Bonus",
            "Equity",
            "Benefits"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Santa Monica, CA",
        "job_id": 3958005287,
        "company": "Sensei Ag",
        "title": "Senior Data Pipeline Engineer, AgTech",
        "created_on": 1720636123.7603362,
        "description": "The mission of the Data Insights group is to foster a data-driven organization by providing and supporting robust, organized wise use of data to gain insights and make decisions. This role, as part of the Data Insights group, supports the development of data processing pipelines for Sensei Ag. Day-to-day tasks would include monitoring of data processing, troubleshooting and debugging data processing tasks, and development of new data flows for existing platforms. This role will also provide basic support to the development of new data processing technologies including setup and administration of new tools and engineering support to new data software systems. The role offers multiple growth opportunities including work in data science, data visualization, and sensing/control systems. An ideal candidate will have a passion for helping build systems which will empower Sensei Ag to grow crops more efficiently and sustainably. Responsibilities Collaborate with cross-functional teams to understand data requirements and help translate domain expertise into data products that meet business needs. Build scalable data pipelines that will enable data users to derive insights and build models faster and more efficiently. Design, build and maintain robust data pipelines and data processing systems. Implement data quality frameworks to ensure the accuracy and integrity of data. Optimize data storage and retrieval for efficient data access and analysis. Stay up to date with industry trends and emerging technologies in data engineering. Minimum years of experience required 5+ years of data pipeline and/or software engineering experience. Required Skills, Abilities and Qualifications Solid understanding of data engineering fundamentals (design patterns, common practices) and what it takes to build high-volume data products and services. Experience with Python, SQL, and Python data science libraries such as Pandas and PySpark. Strong proficiency in querying and manipulating large datasets using SQL and Python. Experience with cloud-based data technologies, such as OCI, AWS, Azure or GCP. Familiarity with efficient big data formats and an understanding of their tradeoffs. Familiarity with data modeling and database design principles. A natural curiosity for technologies and problem-solving. Excellent problem-solving and analytical skills. Strong communication and collaboration skills. Preferred Skills, Abilities and Qualifications Experience with developing ETL flows. Educational Requirements Bachelor's degree in Computer, Data Science or related field. Travel Requirements Up to 10% of the time. Traits We Value Appreciation for transforming health and the future of food Smart, not arrogant Humble and hardworking Entrepreneurial, not risk averse Kind and genuine Grit and lots of it FLSA Status Exempt Compensation $95,000 - $155,000 per year Benefits Comprehensive Medical Plans Prescription Drug Benefits Employee Wellness Dental Plan Vision Plan HSA/FSA Basic Life and AD&D Short and Long Term Disability Flexible Spending Accounts Employee Assistance Program (EAP) Travel Assistance Program Paid-Time Off 401K NOTE: Residents of Hawaii, receive a Comprehensive Healthcare Bundle consisting of: Medical, Dental, Vision and Rx prescriptions, different from US Mainland plans/programs. NOTE: This job description is not intended to be an exhaustive list of all duties, responsibilities or qualifications associated with the job. Other duties may be assigned.",
        "url": "https://www.linkedin.com/jobs/view/3958005287",
        "summary": "This role involves building and maintaining data processing pipelines for Sensei Ag, a company focused on agricultural technologies. Responsibilities include monitoring data processing, troubleshooting, developing new data flows, and supporting new data processing technologies. The ideal candidate will have experience with data engineering fundamentals, Python, SQL, and cloud-based data technologies. Growth opportunities include data science, data visualization, and sensing/control systems.",
        "industries": [
            "Agriculture",
            "Data Science",
            "Technology",
            "Software Engineering"
        ],
        "soft_skills": [
            "Problem-solving",
            "Analytical",
            "Communication",
            "Collaboration",
            "Curiosity",
            "Teamwork"
        ],
        "hard_skills": [
            "Data Engineering",
            "Python",
            "SQL",
            "Pandas",
            "PySpark",
            "ETL",
            "Data Modeling",
            "Database Design",
            "Cloud Technologies (OCI, AWS, Azure, GCP)",
            "Big Data Formats",
            "Data Quality"
        ],
        "tech_stack": [
            "Python",
            "SQL",
            "Pandas",
            "PySpark",
            "OCI",
            "AWS",
            "Azure",
            "GCP"
        ],
        "programming_languages": [
            "Python",
            "SQL"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's",
            "fields": [
                "Computer Science",
                "Data Science",
                "Related Fields"
            ]
        },
        "salary": {
            "max": 155000,
            "min": 95000
        },
        "benefits": [
            "Medical",
            "Prescription Drug",
            "Employee Wellness",
            "Dental",
            "Vision",
            "HSA/FSA",
            "Basic Life and AD&D",
            "Short and Long Term Disability",
            "Flexible Spending Accounts",
            "Employee Assistance Program",
            "Travel Assistance",
            "Paid-Time Off",
            "401K"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3846639889,
        "company": "Chime",
        "title": "Senior Software Engineer, Data",
        "created_on": 1720636125.5600915,
        "description": "About The Role The Data engineering team has been created keeping in mind the importance and criticality of data for the overall success of Chime. The mission of the group is to build a scalable data platform that caters to the data plumbing needs of Chime. As one of the founding members of this team, you will have the rare opportunity to build scalable data systems from scratch. Additionally, you will have the unique opportunity to architect and build workflows that could potentially become de facto standards for the fintech industry. The base salary offered for this role and level of experience will begin at $165,000 and up to $207,300. Full-time employees are also eligible for a bonus, competitive equity package, and benefits. The actual base salary offered may be higher, depending on your location, skills, qualifications, and experience. In this role, you can expect to Be a hands-on data engineer, building, scaling and optimizing ETL pipelines Design data warehouse schemas and scale data warehouse process data for 10x data growth Ownership of all aspects of data - data quality, data governance, data and schema design, data quality and security Own schema registry and dependency chart for persistent data Own the ETL workflows and make sure the pipeline meets data quality and availability requirements Work closely with partner teams, like Data Science, Analytics and DevOps To thrive in this role, you have 4+ years experience transforming data to governed and lucid datasets 4+ years of hands-on experience to build and deploy production-quality data pipelines 4+ years of experience working with stakeholders to provide business insights 4+ years of experience writing Spark, AWS Glue, EMR, Airflow, Python 3+ years of hands-on experience using any MPP database system like Snowflake, AWS Redshift or Teradata Track record of successful partnerships with Analytics, Data Science and DevOps teams Understanding of key metrics for data pipelines and has built solutions to provide visibility to partner teams A Little About Us At Chime, we believe that everyone can achieve financial progress. We’re passionate about developing solutions and services to empower people to succeed. Every day, we start with empathy for our members and stay motivated by our desire to support them in ways that make a meaningful difference. We created Chime—a financial technology company, not a bank*-- founded on the premise that basic banking services should be helpful , transparent , and fair . Chime helps unlock the access and ability our members need to overcome the systemic barriers that block them from moving forward. By providing members with access to liquidity, rewards, and credit building, our easy-to-use tools and intuitive platforms give members the ability to have more control over their money and to take action toward achieving their financial ambitions. So far, we’re well-loved by our members and proud to have helped millions of people unlock financial progress, whether they started a savings account, bought their first car or home, opened a business, or went to college. Every day, we’re inspired by our members’ dreams and successes, big and small. We’re uniting everyday people to unlock their financial progress—will you join us? Chime partners with The Bancorp Bank and Stride Bank, N.A., Members FDIC, that power the bank accounts used by Chime Members. What We Offer 🏢 A thoughtful hybrid work policy that combines in-office days and trips to team and company-wide events depending on location to ensure you stay connected to your work and teammates, whether you’re local to one of our offices or remote 💻 Hybrid work perks, like UrbanSitter and Kinside for backup child, elder and/or pet care, as well as a subsidized commuter benefit 💰 Competitive salary based on experience ✨ 401k match plus great medical, dental, vision, life, and disability benefits 🏝 Generous vacation policy and company-wide Take Care of Yourself Days 🫂 1% of your time off to support local community organizations of your choice 🧠 Mental health support with therapy and coaching through Modern Health 👶 16 weeks of paid parental leave for all parents and an additional 6-8 weeks for birthing parents 👪 Access to Maven, a family planning tool, with up to $10k in reimbursement for egg freezing, fertility treatments, adoption, and more. 🎉 In-person and virtual events to connect with your fellow Chimers—think cooking classes, guided meditations, music festivals, mixology classes, paint nights, etc., and delicious snack boxes, too! 💚 A challenging and fulfilling opportunity to join one of the most experienced teams in FinTech and help millions unlock financial progress We know that great work can’t be done without a diverse team and inclusive environment. That’s why we specifically look for individuals of varying strengths, skills, backgrounds, and ideas to join our team. We believe this gives us a competitive advantage to better serve our members and helps us all grow as Chimers and individuals. We hire candidates of any race, color, ancestry, religion, sex, national origin, sexual orientation, gender identity, age, marital or family status, disability, Veteran status, and any other status. Chime is proud to be an Equal Opportunity Employer and will consider qualified applicants with criminal histories in a manner consistent with the San Francisco Fair Chance Ordinance, Cook County Ordinance, and consistent with Canadian provincial and federal laws. If you have a disability or special need that requires accommodation, please let us know. To learn more about how Chime collects and uses your personal information during the application process, please see the Chime Applicant Privacy Notice.",
        "url": "https://www.linkedin.com/jobs/view/3846639889",
        "summary": "Chime is seeking a Data Engineer to join their growing team and build a scalable data platform. You'll be responsible for building ETL pipelines, designing data warehouse schemas, and ensuring data quality and governance.  You'll also work closely with other teams like Data Science, Analytics, and DevOps. The role requires 4+ years of experience in data transformation, building data pipelines, and working with stakeholders.  Strong knowledge of Spark, AWS Glue, EMR, Airflow, Python, and MPP databases like Snowflake, AWS Redshift, or Teradata is essential.",
        "industries": [
            "Fintech",
            "Technology",
            "Data Engineering"
        ],
        "soft_skills": [
            "Communication",
            "Collaboration",
            "Problem-solving",
            "Analytical",
            "Stakeholder Management",
            "Data Governance",
            "Data Quality"
        ],
        "hard_skills": [
            "ETL",
            "Data Warehousing",
            "Spark",
            "AWS Glue",
            "EMR",
            "Airflow",
            "Python",
            "Snowflake",
            "AWS Redshift",
            "Teradata"
        ],
        "tech_stack": [
            "Spark",
            "AWS Glue",
            "EMR",
            "Airflow",
            "Python",
            "Snowflake",
            "AWS Redshift",
            "Teradata"
        ],
        "programming_languages": [
            "Python"
        ],
        "experience": 4,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 207300,
            "min": 165000
        },
        "benefits": [
            "Competitive Salary",
            "Equity Package",
            "401k Match",
            "Medical, Dental, Vision",
            "Life and Disability Insurance",
            "Generous Vacation Policy",
            "Take Care of Yourself Days",
            "Community Support Time",
            "Mental Health Support",
            "Paid Parental Leave",
            "Family Planning Benefits",
            "In-person and Virtual Events",
            "Hybrid Work Policy",
            "UrbanSitter and Kinside",
            "Commuter Benefit"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Mateo, CA",
        "job_id": 3765967587,
        "company": "Verkada",
        "title": "Software Engineer - Engineering Productivity",
        "created_on": 1720636127.2007263,
        "description": "Who We Are Verkada is the largest cloud-based B2B physical security platform company in the world. Only Verkada offers six product lines — video security cameras, access control, environmental sensors, alarms, workplace and intercoms — integrated with a single cloud-based software platform. Designed with simplicity and scalability in mind, Verkada gives organizations the real-time insight to know what could impact the safety and comfort of people throughout their physical environment, while empowering them to take immediate action to minimize security risks, workplace frustrations and costly inefficiencies. Founded in 2016 with more than $460M in funding raised to date, Verkada has expanded rapidly with 16 offices across three continents, 1,900+ full-time employees and 25,000+ customers across 70+ countries. Overview We are actively looking for a talented software engineer to join the Infrastructure team with a focus on engineering productivity and …Example projects include improving build time and dependency management in our monorepo, allowing engineers to quickly iterate locally on microservices, building fast and reliable testing framework, or standardizing logging and monitoring systems. Responsibilities Make every engineer’s day-to-day productive and enjoyable! Define engineering tooling roadmap Own Kubernetes deployment and development tooling Build various software and hardware testing frameworks Provide technical support for engineers on other teams Help with growing the team Requirements Must have a BS, MS, or PhD in Computer Science, or similar technical field of study Minimum of 1-2 years of experience in a similar position Experience in at least one scripting language (preferably Python) Experience and enthusiasm for learning about new technologies and tooling Comfortable with working at the frontier of infrastructure and software development Nice To Have Experience with one of the major cloud platforms (preferably AWS) Experience using Kubernetes Experience with Go Experience managing a monorepo Experience with Bazel or similar build systems Experience with ArgoCD Experience writing Kubernetes controllers Perks & Benefits Generous company paid medical, dental & vision insurance coverage Unlimited paid time off & 11 companywide paid holidays Wellness allowance Commuter benefits Healthy lunches and dinners provided daily Generous paid parental leave policy & fertility benefits Pay Disclosure At Verkada, we want to attract and retain the best employees, and compensate them in a way that appropriately and fairly values their individual contribution to the company. With that in mind, we carefully consider a number of factors to determine the appropriate starting pay for an employee, including their primary work location and an assessment of a candidate’s skills and experience, as well as market demands and internal parity. This estimate can vary based on the factors described above, so the actual starting annual base salary may be above or below this range. This estimate is also just one component of Verkada’s total rewards package. A Verkada employee may be eligible for additional forms of compensation, depending on their role, including sales incentives, discretionary bonuses, and/or equity in the company in the form of Restricted Stock Units (RSUs). Estimated Annual Pay Range $130,000—$280,000 USD Verkada Is An Equal Opportunity Employer As an equal opportunity employer, Verkada is committed to providing employment opportunities to all individuals. All applicants for positions at Verkada will be treated without regard to race, color, ethnicity, religion, sex, gender, gender identity and expression, sexual orientation, national origin, disability, age, marital status, veteran status, pregnancy, or any other basis prohibited by applicable law. Your application will be handled in accordance with our Candidate Privacy Policy.",
        "url": "https://www.linkedin.com/jobs/view/3765967587",
        "summary": "Verkada, a leading cloud-based physical security platform company, is seeking a talented Software Engineer to join their Infrastructure team. The role focuses on enhancing engineering productivity and involves projects like optimizing build times, improving dependency management in their monorepo, facilitating microservice development, building robust testing frameworks, and standardizing logging and monitoring systems. Responsibilities include defining the engineering tooling roadmap, owning Kubernetes deployment and development tooling, constructing various software and hardware testing frameworks, providing technical support, and contributing to team growth. Ideal candidates possess a BS, MS, or PhD in Computer Science or a related field, 1-2 years of relevant experience, proficiency in at least one scripting language (preferably Python), a passion for learning new technologies, and comfort working at the forefront of infrastructure and software development.",
        "industries": [
            "Technology",
            "Security",
            "Software",
            "Cloud Computing",
            "Physical Security"
        ],
        "soft_skills": [
            "Problem Solving",
            "Communication",
            "Teamwork",
            "Collaboration",
            "Organization",
            "Time Management",
            "Leadership",
            "Adaptability",
            "Passion for Learning",
            "Technical Support",
            "Project Management"
        ],
        "hard_skills": [
            "Python",
            "Kubernetes",
            "Go",
            "Bazel",
            "ArgoCD",
            "AWS",
            "Microservices",
            "Testing Frameworks",
            "Logging",
            "Monitoring",
            "Dependency Management",
            "Build Systems",
            "Scripting"
        ],
        "tech_stack": [
            "Kubernetes",
            "AWS",
            "Go",
            "Bazel",
            "ArgoCD",
            "Python",
            "Microservices"
        ],
        "programming_languages": [
            "Python",
            "Go"
        ],
        "experience": 2,
        "education": {
            "min_degree": "BS",
            "fields": [
                "Computer Science",
                "Technical"
            ]
        },
        "salary": {
            "max": 280000,
            "min": 130000
        },
        "benefits": [
            "Medical Insurance",
            "Dental Insurance",
            "Vision Insurance",
            "Unlimited Paid Time Off",
            "Paid Holidays",
            "Wellness Allowance",
            "Commuter Benefits",
            "Lunch and Dinner",
            "Paid Parental Leave",
            "Fertility Benefits"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco Bay Area",
        "job_id": 3958542963,
        "company": "Walmart Global Tech",
        "title": "Senior, Software Engineer - FE Platform",
        "created_on": 1720636128.8842182,
        "description": "We're looking for a Senior Software Engineer / Front-end engineer who loves to code and loves to take ideas and build great products with JavaScript, HTML, and CSS. They know JavaScript is imperfect, but they embrace its functional side and genuinely enjoy coding with it. They are an experienced leader and expert in front-end technologies. They understand performance, A/B tests, web vitals, front end optimization and can write code to bring them to life too. The primary location is for Sunnyvale, CA. About Team: Our team collaborates with Walmart International, which has over 5,900 retail units operating outside of the United States under 55 banners in 26 countries including Africa, Argentina, Canada, Central America, Chile, China, India, Japan, and Mexico, to name a few. What you'll do: Walmart International is a diverse and fast-growing business unit, so you'll get experience on many different projects across the organization. That said, here are some things you'll do: • Build reusable React components with modular CSS, manage data on the client with Redux, use react query, and GraphQL. • Measure and resolve performance bottlenecks, using tools like Chrome DevTools, Lighthouse, WebPagetest, or custom tooling. • Refactor or improve existing code. We constantly find ways to improve all of our JavaScript code and you are all aboard. • Work closely with our product, design, and UX teams to create amazing and intuitive experiences that make it effortless to connect different apps together. • Help put tools, processes, and documentation in place to improve our code quality. • Demonstrate technical expertise in solving challenging programming and design problems • Review code written by other team members or other teams. • Ship to hundreds of thousands of users every day while having lots of autonomy in terms of code and feature ownership. • Help out with our Node-based developer platform. • Share what you know and learn either one-on-one or with lightning talks to the group. • Work boldly with a sense of urgency; embrace mistakes, learn from them, and drive the team toward success What you will bring: • 4+ years of experience in object-oriented design and software development • 4+ years of experience in building responsive, single page web applications using modern front-end JavaScript technologies like React, Angular, Vue, etc. • Thorough understanding of React, Node.JS and its core principles • 4+ years of experience in creating and/or consuming RESTful web services • Excellent communication skills: Demonstrated ability to explain complex technical issues to both technical and non-technical audiences • Expertise with unit testing & Test-Driven Development (TDD) • BS/MS in computer science or equivalent work experience • ****Experience with NodeJS and Graphql is required. About Walmart Global Tech Imagine working in an environment where one line of code can make life easier for hundreds of millions of people. That’s what we do at Walmart Global Tech. We’re a team of software engineers, data scientists, cybersecurity expert's and service professionals within the world’s leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered. We train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail. Flexible, hybrid work: We use a hybrid way of working that is primarily in office coupled with virtual when not onsite. Our campuses serve as a hub to enhance collaboration, bring us together for purpose and deliver on business needs. This approach helps us make quicker decisions, remove location barriers across our global team and be more flexible in our personal lives. Benefits: Benefits: Beyond our great compensation package, you can receive incentive awards for your performance. Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and much more. Equal Opportunity Employer: Walmart, Inc. is an Equal Opportunity Employer – By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing diversity- unique styles, experiences, identities, ideas and opinions – while being inclusive of all people.",
        "url": "https://www.linkedin.com/jobs/view/3958542963",
        "summary": "Walmart International is seeking a Senior Software Engineer / Front-end engineer with 4+ years of experience in building responsive, single page web applications using React, Angular, Vue, etc. This role requires strong JavaScript skills, a deep understanding of React, Node.JS and its core principles, and experience with RESTful web services. The ideal candidate will also have experience with NodeJS and Graphql, and will be proficient in unit testing & Test-Driven Development (TDD). The position is based in Sunnyvale, CA.",
        "industries": [
            "Retail",
            "E-commerce",
            "Software Development",
            "Technology"
        ],
        "soft_skills": [
            "Communication",
            "Leadership",
            "Problem Solving",
            "Teamwork",
            "Collaboration",
            "Urgency",
            "Adaptability",
            "Learning"
        ],
        "hard_skills": [
            "JavaScript",
            "HTML",
            "CSS",
            "React",
            "Redux",
            "React Query",
            "GraphQL",
            "Node.JS",
            "RESTful Web Services",
            "Performance Optimization",
            "A/B Testing",
            "Web Vitals",
            "Chrome DevTools",
            "Lighthouse",
            "WebPagetest",
            "Unit Testing",
            "Test-Driven Development (TDD)"
        ],
        "tech_stack": [
            "React",
            "Redux",
            "React Query",
            "GraphQL",
            "Node.JS",
            "RESTful Web Services",
            "Chrome DevTools",
            "Lighthouse",
            "WebPagetest"
        ],
        "programming_languages": [
            "JavaScript"
        ],
        "experience": 4,
        "education": {
            "min_degree": "BS/MS",
            "fields": [
                "Computer Science"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": [
            "Incentive Awards",
            "401(k) Match",
            "Stock Purchase Plan",
            "Paid Maternity and Parental Leave",
            "PTO",
            "Multiple Health Plans"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Francisco, CA",
        "job_id": 3224220733,
        "company": "Forward",
        "title": "Software Engineer, Data Infrastructure",
        "created_on": 1720636130.7880003,
        "description": "Forward is on a bold mission to make high quality healthcare available to a billion people across the globe. We’re building the world’s most advanced healthcare platform from the ground up, combining hardware, software and doctors under one roof. We are building our IT team and looking for a world-class IT Engineer with expertise in improving and scaling IT systems and infrastructure. As an early member of our IT team, you’ll have a key role in building the future of healthcare from first principles. Forward was founded in January 2016 by former executives and engineering leaders from Google and Uber. We are funded by some of the world's best investors and entrepreneurs including Founder's Fund, Khosla Ventures, First Round Capital, Eric Schmidt (Google/Alphabet Chairman), Marc Benioff (Salesforce Founder), Joe Lonsdale (Palantir Founder), Joshua Kushner (Oscar co-Founder) and Garrett Camp (Uber co-Founder). Press And Videos: Health Moves Forward [CEO Blog Post] Forward Health Launched CarePods [Tech Crunch] An AI Doctor In A Box Coming To A Mall [Forbes] Former Googler Adrian Aoun raises $100 million for walk-in AI healthcare pods [Fortune] Forward Raises $100M and Announces Forward CarePod™ [Forward] Forward is on a bold mission to make high quality healthcare available to a billion people across the globe. We’re building the world’s most advanced healthcare platform from the ground up, combining hardware, software and doctors under one roof. We are scaling our engineering team and looking for world-class engineers with experience and expertise in building robust, scalable APIs and integrating third-party systems. As an early member of our engineering team, you’ll play a key role in building the future of healthcare from first principles. Forward was founded in January 2016 by former executives and engineering leaders from Google and Uber. We are funded by some of the world's best investors and entrepreneurs including Founder's Fund, Khosla Ventures, First Round Capital, Eric Schmidt (Google/Alphabet Chairman), Marc Benioff (Salesforce Founder), Joe Lonsdale (Palantir Founder), Joshua Kushner (Oscar co-Founder) and Garrett Camp (Uber co-Founder). Press And Videos: Virtual Tour of Forward [YouTube] Health Moves Forward [CEO Blog Post] Series D Funding Funds Doctor-led Programs [TechCrunch] Forward - What Quality Healthcare Should Look Like [Mashable] Primary Care Start-ups Vying for 170B Market [Business Insider] The Pivot to Virtual Care [Chief Medical Officer @ Stanford Medicine] The Meaning of Trans Broken-Arm Syndrome [USA Today] WHAT YOU'LL DO: Design and evolve our cloud data infrastructure for 10x data growth. Own foundational datasets used by teams across the company. Build platforms that empower non-engineers to create data pipelines and data products. Invest in tools that proactively measure, monitor, and improve data quality and accessibility. Own entire projects while working alongside cross-functional teams of doctors, designers, and operators. Your work will directly contribute to saving and improving people’s lives. For real. :) WHAT WE'RE LOOKING FOR: Data Engineering - You have firsthand experience with database performance tuning, cloud storage and processing technologies (e.g. Redshift, S3, Glue, Lambda, Databricks, DBT), workflow orchestration (e.g. Dagster, Airflow), distributed computing (e.g. Spark, EMR), message queues and streams (e.g. Kafka, Kinesis). Good Practices - You care about more than just shipping data, but also doing it in a scalable, efficient, reliable, and secure way. Empathy - You spend time understanding the needs, and driving alignment among different data users. Entrepreneurship - You’re a self-starter who loves to own things end-to-end. You are excited to try out new technologies and build proof-of-concepts. Team player - You know how to make those around you better and feed off their energy. You take care of your teammates. You have a BS or MS in computer science or a related technical field. You have a minimum of 3 years experience working in a related field. The base salary range for this full-time position is $100,000-$220,000, plus equity and benefits. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all locations. Within the range, individual pay is determined by factors including job-related skills, experience, relevant education or training, and location. WHY JOIN FORWARD? We don’t want to just move dollars around the healthcare industry - we want to rebuild it and fix it. All of it. You’d be a major part of the story behind one of the most ambitious startup attempts of the past decade and you’d work with a team of people who want to use their talents for good. We are an equal opportunity employer. In accordance with anti-discrimination law, it is the purpose of this policy to effectuate these principles and mandates. We prohibit discrimination and harassment of any type and affords equal employment opportunities to employees and applicants without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law. We conform to the spirit as well as to the letter of all applicable laws and regulations. Pursuant to the San Francisco Fair Chance Ordinance and the Los Angeles Fair Chance Initiative for Hiring, we will consider for employment qualified applicants with arrest and conviction records. Information collected and processed as part of any job application you choose to submit is subject to Forward’s Privacy Notice to California Job Applicants . WHY JOIN FORWARD? We don’t want to just move dollars around the healthcare industry - we want to rebuild it and fix it. All of it. You’d be a major part of the story behind one of the most ambitious startup attempts of the past decade and you’d work with a team of people who want to use their talents for good. We are an equal opportunity employer. In accordance with anti-discrimination law, it is the purpose of this policy to effectuate these principles and mandates. We prohibit discrimination and harassment of any type and afford equal employment opportunities to employees and applicants without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law. We conform to the spirit as well as to the letter of all applicable laws and regulations. Pursuant to the San Francisco Fair Chance Ordinance and the Los Angeles Fair Chance Initiative for Hiring, we will consider for employment qualified applicants with arrest and conviction records. Our Commitment To Diversity And Inclusion: We deeply understand the value of bringing together a team with different perspectives, educational backgrounds, and life experiences, and we prioritize diversity within our team. We encourage people from underrepresented backgrounds to apply. Information collected and processed as part of any job application you choose to submit is subject to Forward’s Privacy Notice to California Job Applicants .",
        "url": "https://www.linkedin.com/jobs/view/3224220733",
        "summary": "Forward is seeking a skilled Data Engineer to design, develop, and maintain their cloud data infrastructure. This role involves owning foundational datasets used across the company, building data pipelines and products, and ensuring data quality and accessibility.  The ideal candidate will have experience with various cloud technologies, data processing frameworks, and good data engineering practices. This position is part of a team focused on revolutionizing healthcare.",
        "industries": [
            "Healthcare",
            "Technology",
            "Data Engineering",
            "Cloud Computing"
        ],
        "soft_skills": [
            "Empathy",
            "Entrepreneurship",
            "Teamwork",
            "Communication",
            "Problem Solving",
            "Self-Starter"
        ],
        "hard_skills": [
            "Database Performance Tuning",
            "Cloud Storage",
            "Cloud Processing",
            "Redshift",
            "S3",
            "Glue",
            "Lambda",
            "Databricks",
            "DBT",
            "Workflow Orchestration",
            "Dagster",
            "Airflow",
            "Distributed Computing",
            "Spark",
            "EMR",
            "Message Queues",
            "Streams",
            "Kafka",
            "Kinesis",
            "Data Engineering"
        ],
        "tech_stack": [
            "Redshift",
            "S3",
            "Glue",
            "Lambda",
            "Databricks",
            "DBT",
            "Dagster",
            "Airflow",
            "Spark",
            "EMR",
            "Kafka",
            "Kinesis"
        ],
        "programming_languages": [],
        "experience": 3,
        "education": {
            "min_degree": "BS",
            "fields": [
                "Computer Science",
                "Related Technical Field"
            ]
        },
        "salary": {
            "max": 220000,
            "min": 100000
        },
        "benefits": [
            "Equity"
        ]
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "San Jose, CA",
        "job_id": 3941690995,
        "company": "LitePoint",
        "title": "Software Engineer",
        "created_on": 1720636132.3159938,
        "description": "Organization & Role TERADYNE, where experience meets innovation and driving excellence in every connection. We are fueled by creativity and diversity of thought and in our workforce. Our employees are supported to innovate and learn something new every day. We cultivate a culture of inclusion for all employees that respects their individual strengths, views, and experiences. We believe that our differences enable us to be a better team – one that makes better decisions, drives innovation and delivers better business results. Opportunity Overview Design and develop multi-threaded embedded software common to multiple LitePoint products Contribute in small team design meetings, co-develop interface specifications, and implement the software in a timely and high quality manner using fast prototyping team-oriented methods Create scripts for automated test platforms to exercise the software and hardware and validate the design at black box and white box levels Support world-wide product level problems debugging, root-cause failures, fix, test, and release software on very short cycles Qualifications & Skills 3-5 years developing fail-safe high speed multi-threaded embedded C/C++ systems software BS/MS in Electrical/Computer Engineering, and/or equivalent studies and experience. Ability to communicate effectively and efficiently. A Strong individual contributor with a personal sense of responsibility, quality, and teamwork with desire to learn from and mentor others. Experience working in fast-prototyping team-based software development process Experience with JavaScript and Python Desired Skills Experience with SCPI based instruments Experience with TCP/IP and RPC client/server architectures Experience with Windows and Linux Education BS or MS in computer science, computer engineering, or related field About Litepoint LitePoint creates wireless test solutions and services for the world’s most innovative wireless device makers, helping them to ensure their products perform for today’s demanding consumers. A leading innovator in wireless testing, LitePoint products come out of the box ready to test the most widely used wireless chipsets in the world. LitePoint works with the leading makers of smartphones, tablets, PCs, wireless access points and chipsets. LitePoint is also at the forefront of testing the burgeoning world of connected devices…the Internet of Things. Headquartered in Silicon Valley, California and with offices around the world. LitePoint is a wholly owned subsidiary of Teradyne (NYSE: TER), a leading supplier of automation equipment for test and industrial applications. 2021 revenues were 3.7B and now approx. 6,000 employees globally and employs approximately 5,500 people worldwide.",
        "url": "https://www.linkedin.com/jobs/view/3941690995",
        "summary": "LitePoint, a subsidiary of Teradyne, is seeking a skilled embedded software engineer to design and develop multi-threaded software for their wireless test solutions. The role involves working in a fast-paced environment, collaborating with a team, and implementing high-quality code. Responsibilities include software design, interface specification, automated testing, and debugging. The ideal candidate has 3-5 years of experience in developing fail-safe embedded systems using C/C++, a strong understanding of multi-threading, and experience with scripting languages like JavaScript and Python.",
        "industries": [
            "Software",
            "Wireless Technology",
            "Telecommunications",
            "Electronics",
            "Internet of Things (IoT)"
        ],
        "soft_skills": [
            "Communication",
            "Teamwork",
            "Problem Solving",
            "Time Management",
            "Self-Motivation",
            "Responsibility",
            "Learning",
            "Mentorship"
        ],
        "hard_skills": [
            "C/C++",
            "Multi-threading",
            "Embedded Systems",
            "JavaScript",
            "Python",
            "SCPI",
            "TCP/IP",
            "RPC",
            "Windows",
            "Linux",
            "Debugging",
            "Automated Testing",
            "Software Design",
            "Interface Specification"
        ],
        "tech_stack": [
            "C/C++",
            "JavaScript",
            "Python",
            "SCPI",
            "TCP/IP",
            "RPC",
            "Windows",
            "Linux"
        ],
        "programming_languages": [
            "C",
            "C++",
            "JavaScript",
            "Python"
        ],
        "experience": 3,
        "education": {
            "min_degree": "BS",
            "fields": [
                "Electrical Engineering",
                "Computer Engineering",
                "Computer Science"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "California, United States",
        "job_id": 3888442001,
        "company": "Donato Technologies, Inc.",
        "title": "Lead Azure Data Engineer",
        "created_on": 1720636135.942712,
        "description": "Job Title: Lead Azure Data Engineer Location: San Francisco Bay Area, CA (Onsite Role - Hybrid) Locals or nearby Preferred Duration: 6 Months Contract Initially, Possibility to extension Note: We are looking for consultants who can work from office couple of days in a week, This is Hybrid Role. Key Skills: Sr/Lead level person (Min 10 Years Exp), Should have good experience in ADF, ADLS, Synapse (Azure Sql Datawarehouse) & T-Sql. Must Have Skills Extensive experience providing practical direction within azure native services , implementing data migration and data processing using Azure services: ADLS, Azure Data Factory, Synapse/DW /Azure SQL DB, Fabric. Proven experience with SQL, namely schema design and dimensional data modelling Solid knowledge of data warehouse best practices, development standards and methodologies Strong experience with Azure Cloud on data integration with Databricks Be an independent self-learner with the \"let's get this done\" approach and ability to work in Fast paced and Dynamic environment Nice-to-Have Skills Basic understanding on ML Studio, AI/ML, MLOps etc. Good to have Event Hub, IOT Hub, Azure Stream Analytics, Azure Analysis Service, Cosmo Db knowledge. Good to have SAP Hana knowledge Intermediate knowledge on Power BI Good to have knowledge in DevOps and CI/CD deployments, Cloud migration methodologies and processes.",
        "url": "https://www.linkedin.com/jobs/view/3888442001",
        "summary": "Lead Azure Data Engineer with 10+ years of experience, proficient in Azure Data Factory (ADF), Azure Data Lake Storage (ADLS), Synapse Analytics (Azure SQL Data Warehouse), and T-SQL. Expertise in data migration, processing, and integration with Azure services, including Databricks. Strong understanding of data warehouse best practices and experience with Power BI.",
        "industries": [
            "Technology",
            "Software Development",
            "Data Analytics",
            "Cloud Computing",
            "Data Engineering"
        ],
        "soft_skills": [
            "Problem-solving",
            "Self-learning",
            "Teamwork",
            "Communication",
            "Adaptability",
            "Fast-paced environment",
            "Independent"
        ],
        "hard_skills": [
            "Azure Data Factory",
            "Azure Data Lake Storage",
            "Azure Synapse Analytics",
            "Azure SQL Database",
            "Azure Fabric",
            "SQL",
            "Schema Design",
            "Dimensional Data Modeling",
            "Data Warehouse Best Practices",
            "Data Integration",
            "Databricks",
            "Power BI",
            "ML Studio",
            "AI/ML",
            "MLOps",
            "Event Hub",
            "IoT Hub",
            "Azure Stream Analytics",
            "Azure Analysis Services",
            "Cosmos DB",
            "SAP Hana",
            "DevOps",
            "CI/CD",
            "Cloud Migration"
        ],
        "tech_stack": [
            "Azure Data Factory",
            "Azure Data Lake Storage",
            "Azure Synapse Analytics",
            "Azure SQL Database",
            "Azure Fabric",
            "Databricks",
            "Power BI",
            "ML Studio",
            "Event Hub",
            "IoT Hub",
            "Azure Stream Analytics",
            "Azure Analysis Services",
            "Cosmos DB",
            "SAP Hana"
        ],
        "programming_languages": [
            "T-SQL",
            "SQL"
        ],
        "experience": 10,
        "education": {
            "min_degree": null,
            "fields": []
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    },
    {
        "task_id": "8deb9e412d314927adeba8c48469afa6",
        "keyword": "Data Engineer",
        "location": "Fremont, CA",
        "job_id": 3886463267,
        "company": "Info Way Solutions",
        "title": "Data science engineer",
        "created_on": 1720636137.5368526,
        "description": "Data Science Engineer Job Location: Boston, MA - 4 days to office. ( Local Profile only ) Job Description We are seeking a talented Data Science Engineer to join our team and contribute to the development and implementation of advanced data solutions using technologies such as AWS Glue, Python, Spark, Snowflake Data Lake, S3, SageMaker, and machine learning (M/L). As a Data Science Engineer, you will play a crucial role in designing, building, and optimizing data pipelines, machine learning models, and analytics solutions. You will work closely with cross-functional teams to extract actionable insights from data and drive business outcomes. Key Responsibilities Develop and maintain ETL pipelines using AWS Glue for data ingestion, transformation, and integration from various sources. Utilize Python and Spark for data preprocessing, feature engineering, and model development. Design and implement data lake architecture using Snowflake Data Lake, Snowflake data warehouse and S3 for scalable and efficient storage and processing of structured and unstructured data. Leverage SageMaker for model training, evaluation, deployment, and monitoring in production environments. Collaborate with data scientists, analysts, and business stakeholders to understand requirements, develop predictive models, and generate actionable insights. Conduct exploratory data analysis (EDA) and data visualization to communicate findings and trends effectively. Stay updated with advancements in machine learning algorithms, techniques, and best practices to enhance model performance and accuracy. Ensure data quality, integrity, and security throughout the data lifecycle by implementing robust data governance and compliance measures. Qualifications Bachelor's degree or higher in Computer Science, Data Science, Statistics, or related field. Proficiency in AWS services such as Glue, S3, SageMaker, and Snowflake Data Lake with 5-6 years of experience. Strong programming skills in Python for data manipulation, analysis, and modeling. Experience with distributed computing frameworks like Spark for big data processing. Knowledge of machine learning concepts, algorithms, and tools for regression, classification, clustering, and recommendation systems. Familiarity with data visualization tools with Tableau for creating meaningful visualizations. Excellent problem-solving, analytical thinking, and communication skills. Ability to work collaboratively in a team environment and manage multiple priorities effectively. Experience deploying machine-learning models in production environments and monitoring their performance. Knowledge of MLOps practices, model versioning, and automated model deployment pipelines. Familiarity with SQL, NoSQL databases, and data warehousing concepts. Strong understanding of cloud computing principles and architectures. Certifications in AWS, Python, Spark, or related technologies. Thanks & Regards, Saravanan.R |Infowaygroup.com| Direct: (925)464-1116 Work: (925)-592-6160 Ext 111 Saravanan @infowaygroup.com Info Way Solutions LLC, Fremont, CA",
        "url": "https://www.linkedin.com/jobs/view/3886463267",
        "summary": "We are looking for a Data Science Engineer to develop and implement advanced data solutions using technologies such as AWS Glue, Python, Spark, Snowflake Data Lake, S3, SageMaker, and machine learning (M/L). You will design, build, and optimize data pipelines, machine learning models, and analytics solutions. You will work closely with cross-functional teams to extract actionable insights from data and drive business outcomes.",
        "industries": [
            "Technology",
            "Data Science",
            "Machine Learning",
            "Analytics"
        ],
        "soft_skills": [
            "Problem-solving",
            "Analytical thinking",
            "Communication",
            "Teamwork",
            "Prioritization"
        ],
        "hard_skills": [
            "AWS Glue",
            "Python",
            "Spark",
            "Snowflake Data Lake",
            "S3",
            "SageMaker",
            "Machine Learning",
            "ETL Pipelines",
            "Data Ingestion",
            "Data Transformation",
            "Data Integration",
            "Data Preprocessing",
            "Feature Engineering",
            "Model Development",
            "Data Lake Architecture",
            "Data Warehousing",
            "Structured Data",
            "Unstructured Data",
            "Model Training",
            "Model Evaluation",
            "Model Deployment",
            "Model Monitoring",
            "Exploratory Data Analysis (EDA)",
            "Data Visualization",
            "Tableau",
            "SQL",
            "NoSQL Databases",
            "Cloud Computing",
            "MLOps",
            "Model Versioning",
            "Automated Model Deployment Pipelines"
        ],
        "tech_stack": [
            "AWS Glue",
            "Python",
            "Spark",
            "Snowflake Data Lake",
            "S3",
            "SageMaker",
            "Tableau",
            "SQL",
            "NoSQL Databases"
        ],
        "programming_languages": [
            "Python",
            "SQL"
        ],
        "experience": 5,
        "education": {
            "min_degree": "Bachelor's degree",
            "fields": [
                "Computer Science",
                "Data Science",
                "Statistics"
            ]
        },
        "salary": {
            "max": 0,
            "min": 0
        },
        "benefits": []
    }
]