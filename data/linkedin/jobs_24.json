[
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Washington, DC",
        "job_id": 3970732754,
        "company": "ClickJobs.io",
        "title": "Sr. Data Engineer",
        "created_on": 1720638223.345723,
        "description": "Plano 3 (31063), United States of America, Plano, Texas Sr. Data Engineer We are looking for driven individuals to join our team of passionate data engineers in creating Capital One’s next generation of data products and capabilities. You will build data pipeline frameworks to automate high-volume and real-time data delivery for our Hadoop and streaming data hub You will build data APIs and data delivery services that support critical operational and analytical applications for our internal business operations, customers and partners You will transform complex analytical models into scalable, production-ready solutions You will continuously integrate and ship code into our on premise and cloud Production environments You will develop applications from ground up using a modern technology stack such as Scala, Spark, Postgres, Angular JS, and NoSQL You will work directly with Product Owners and customers to deliver data products in a collaborative and agile environment Responsibilities: Develop sustainable data driven solutions with current new gen data technologies to meet the needs of our organization and business customers Master new technologies rapidly as needed to progress varied initiatives Break down complex data issues and resolve them Builds robust systems with an eye on the long term maintenance and support of the application Broader knowledge sharing Understands complex multi-tier, multi-platform systems Basic Qualifications: Bachelor’s Degree At least 4 years of experience in application development (Internship experience does not apply) At least 1 year of experience in big data technologies Preferred Qualifications: Master's Degree 6+ years of experience in application development (Python, SQL, Scala, or Java) 4+ years of experience in big data technologies 4+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud) 4+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL) 4+ year experience working on real-time data and streaming applications 4+ years of experience with NoSQL implementation (Mongo, Cassandra) 4+ years of data warehousing experience (Redshift or Snowflake) 4+ years of experience with UNIX/Linux including basic commands and shell scripting 4+ years of experience with Agile engineering practices At this time, Capital One will not sponsor a new applicant for employment authorization for this position. The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. New York City (Hybrid On-Site): $165,100 - $188,500 for Senior Data Engineer Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate’s offer letter. This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan. Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. This role is expected to accept applications for a minimum of 5 business days. No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City’s Fair Chance Act; Philadelphia’s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries. If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "url": "https://www.linkedin.com/jobs/view/3970732754"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Fort Meade, MD",
        "job_id": 3970248902,
        "company": "Precision Solutions",
        "title": "Software Engineer",
        "created_on": 1720638224.912094,
        "description": "Onsite | Ft. Meade | 5 Days a Week Active TS/SCI w/FS Poly (NSA) Clearance Required Summary Since 2012, our client has helped mission-critical government organizations and businesses face their most daunting technology challenges. Their team have been trusted partners to many government agencies and are extremely familiar with a wide variety of systems, policies, and procedures. Our client is also a distinguished custom software development firm dedicated to delivering premium solutions tailored for businesses and governmental needs. They are home to top-tier technology professionals recognized as industry pioneers, comprehensive full-stack engineers, and reliable consultants. These experts are adept at clear communication, excel in resolving complex challenges where others may falter, and are skilled in actualizing an organization's vision. Responsibilities Our client is looking for multiple Software Engineers to join their team! As a Mid-Level Software Engineer, you will assume a crucial role within our client's team, utilizing your extensive experience and technical expertise in software development. Your adeptness in programming languages such as Java, Python, C++, among others, along with your familiarity with Linux, Unix, and Windows environments, will enable you to make meaningful contributions to a variety of projects. Your involvement will span across areas like web application development, distributed systems, and machine learning, to name a few. Driven by a passion for technology and a commitment to lifelong learning, you will venture into new programming languages and frameworks, ensuring that the solutions developed by our client's team remain at the forefront of technological innovation. Please be aware that due to our client's involvement in a wide array of projects, this job description serves as a foundational outline meant to align with their broad requirements. The specifics of each project vary, offering a rich landscape of opportunities! Detailed information about individual projects will be disclosed during the subsequent stages of the interview process. Requirements 7-14 years of software development experience in programming languages such as Java, Python, C++, Ruby, Perl, JavaScript is required An additional 4+ years of relevant experience may be substituted in lieu of a degree Familiarity with development environments in Linux, Unix, or Windows Experienced in and/or excited to work in any of the following areas Web application development Distributed systems User interface development Big data analytics Machine learning Data science Cloud-based computing Reverse engineering High-Performance Computing (HPC), or DevOps You have a passion for technology and the drive to learn new programming languages and frameworks Preferred Requirements We realize this is a long list of preferred various skills and experiences! - Don’t worry if you aren’t familiar with all of these. Only having some exposure and knowledge of the following various technologies is acceptable! Angular/AngularJS, Vue, CSS, HTML, React or equivalents for UI developers Spring, Hibernate, JPA, Servlets or equivalents for Java developers NoSQL technologies such as MongoDB, REDIS, Neo4J, Hbase, ElasticSearch, etc. Relational Databases such as MySQL, Oracle, PostgreSQL Developing RESTful Services using a framework such as Jersey, Spring MVC, CXF Enterprise Integration Frameworks such as Apache Camel, Spring Integration, or Apache NiFi JMS to include messaging Frameworks such as Apache ActiveMQ, Apache Artemis, or Kafka Spring to include Spring Boot, Spring Data, or Spring Security Java Persistence API through a persistence framework such as Spring, Hibernate, OpenJPA Developing and deploying applications to Servlet containers such as Tomcat or Jetty, or Application Servers such as Glassfish, JBoss, Weblogic Applications with NodeJS UI Component libraries such as Bootstrap, Material, Ant Distributed computing frameworks such as Apache Spark, Hadoop, and MapReduce Developing applications within utility clouds such as AWS, Rackspace, Heroku, or Azure Continuous integration tools such as Gitlab CI or Jenkins Containerization technologies such as Docker and Kubernetes IDEs such as Eclipse, IntelliJ, or Microsoft Visual Studio Education/Certification Requirements A Bachelor's degree in Computer Science or a related technical field is required. An additional 4+ years of relevant experience may be substituted in lieu of a degree Clearance Requirements Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information; an active TS/SCI w/FS Poly clearance is required. Please note that the FS Poly currently needs to be held by the NSA or within the past two years. Other Duties Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice. About Us Northern Virginia-based Precision Solutions is an expert in staffing solutions for companies of any size that open the door to new opportunities and seek outstanding talent. We pride ourselves on being versatile enough to tailor our relationships to the needs of each individual client, being agile in the fast-paced marketplace, and being precise in meeting the needs of any company. Equal Opportunity Employer Statement Precision Solutions is an equal opportunity employer. We prohibit discrimination and harassment of any kind based on race, color, sex, religion, sexual orientation, national origin, disability, genetic information, pregnancy, or any other protected characteristic as outlined by federal, state, or local laws.",
        "url": "https://www.linkedin.com/jobs/view/3970248902"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Columbia, MD",
        "job_id": 3966904054,
        "company": "R.O.I Employment Solutions, LLC",
        "title": "DevOps Engineer",
        "created_on": 1720638226.975254,
        "description": "DESCRIPTION We are seeking a highly motivated and experienced DevOps Engineer to join our fast-paced development team. The ideal candidate will have strong working knowledge in Linux systems administration, and a background in Big Data solutions, configuration management, automation, scripting, and AWS. The DevOps Engineer will be responsible for implementing infrastructure, automating deployment processes, and ensuring the reliability and scalability of our services. Work is hybrid out of our Columbia, MD office with up to 25% on-call support. If you have a passion for DevOps and are interested in working with a dynamic and innovative team, we encourage you to apply for this exciting opportunity. REQUIREMENTS Essential Job Responsibilities Support development and deployment of infrastructure in AWS Automate deployment processes and ensure reliability and scalability of services Manage and maintain cloud infrastructure on AWS Collaborate with development teams to integrate their applications into the infrastructure Monitor and troubleshoot production systems and resolve issues as necessary Continuously improve processes and tools to ensure high availability and performance Stay current with new technologies and industry trends, continuously exploring new ways to improve our infrastructure Other Duties as assigned Minimum Qualifications Security Clearance - A current Secret U.S. Government Security clearance is required ; U.S. citizenship required. 5+ years of experience in DevOps Engineering or Software Development (Java preferred) and Bachelors in related field; or 3 years relevant experience with Masters in related field; or High School Diploma or equivalent and 9 years relevant experience. Strong knowledge of Linux, including system administration and troubleshooting Proficient in configuration management tools such as Ansible or Puppet Knowledge of AWS services (EC2, S3, Lambda) and their application to deployment and management of infrastructure Experience with application and OS deployment, scaling, and management Ability to develop in multiple programming languages such as bash, Python, or Go Familiarity with Git and other development tools such as deployment pipelines Excellent problem-solving skills and the ability to identify and troubleshoot complex issues Excellent oral and written communication skills. Understanding of AGILE software development methodologies and use of standard software development tool suites Must have a DoD 8140 / 8570 compliance certification (i.e. Security+ certification) Must be able to work a hybrid schedule and a rotational on-call/pager duty up to 25% Preferred Requirements Experience with big data technologies like: Hadoop, Spark, MongoDB, ElasticSearch, Hive, Drill, Impala, Trino, Presto, etc. Experience with containers and Kubernetes are a plus",
        "url": "https://www.linkedin.com/jobs/view/3966904054"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Chevy Chase, MD",
        "job_id": 3952848615,
        "company": "GEICO",
        "title": "Sr. Software Engineer - DevOps (Finance Data)",
        "created_on": 1720638228.7088308,
        "description": "Position Description Our Senior Engineer is a key member of the engineering staff working across the organization to provide friction-less experience to our customers and maintain the highest standards of protection and availability. Our team thrives and succeeds in delivering high quality technology products and services in a hyper-growth environment where priorities shift quickly. The ideal candidate has broad and deep technical knowledge, typically ranging from managing backend resources to System reliability and all points in between. An ideal candidate will have deep expertise in DevOps and a good understanding of Data Engineering needed to help us build a Finance Data Lake from the ground up. Position Responsibilities As a Senior Engineer, you will: Scope, design, and build scalable, resilient distributed systems through cross-functional collaboration throughout Software Development Lifecycle Build highly reliable infrastructure and leverage your technical skills to drive towards efficient and optimal solution Automate tasks, test infrastructure, build and maintain CI/CD pipelines, and integration with edge data sources for Data Engineering teams to increase developer efficiency and effectiveness Deliver high quality software and establishing best practices and processes across the organization through code reviews, test automation and mentor engineers Utilize programming languages like Python, C# or other object-oriented languages, SQL, and NoSQL databases, Container Orchestration services including Docker and Kubernetes, and a variety of Azure tools and services Mentor other engineers Consistently share best practices and improve processes within and across teams Qualifications Advance Experience in Hybrid cloud environments with Azure, AWS, or GCP and Linux based on-prem environment Advance Experience working with CI/CD, Azure Just-in-Time, VM Access, Azure Cloud Services, Azure Active Directory, Azure Automation Accounts, Azure subscription and Resource management, Kubernetes, or similar containerization technology, Docker, Helm, Terraform, GitHub, and IT automation tools like Chef or Ansible Experience in programming languages such as Python, Java, C#, or other object-oriented languages, and in SQL, and NoSQL databases Hands-on experience building platforms from ground up using resiliency patterns, observability, continuous delivery, and infrastructure as code Experience implementing Spark on Kubernetes hosted on-prem/cloud, with monitoring and security protocols Experience building and maintaining cloud data solutions (Delta Lake, Iceberg, Hudi, Snowflake, Redshift or equivalent) Experience working streaming applications (Spark Streaming, Flink, Kafka or equivalent) Strong knowledge of data formats such as Parquet, Avro, ORC, XML, JSON, data processing/data transformation using ETL/ELT tools such as DBT (Data Build Tool) or Databricks, and AI/machine learning infrastructure needs Advanced understanding of DevOps concepts including Azure DevOps framework and tools Advanced PowerShell scripting skills Ability to excel in a fast-paced environment Cloud architect certifications are preferred Experience 4+ years of professional DevOps experience 3+ years of experience with architecture and design 3+ years of experience with AWS, GCP, Azure, or another cloud service 2+ years of experience in open-source frameworks Education Bachelor’s degree in Computer Science, Information Systems, or equivalent education or work experience Annual Salary $82,000.00 - $185,000.00 The above annual salary range is a general guideline. Multiple factors are taken into consideration to arrive at the final hourly rate/ annual salary to be offered to the selected candidate. Factors include, but are not limited to, the scope and responsibilities of the role, the selected candidate’s work experience, education and training, the work location as well as market and business considerations. At this time, GEICO will not sponsor a new applicant for employment authorization for this position. Benefits: As an Associate, you’ll enjoy our Total Rewards Program* to help secure your financial future and preserve your health and well-being, including: Premier Medical, Dental and Vision Insurance with no waiting period** Paid Vacation, Sick and Parental Leave 401(k) Plan Tuition Reimbursement Paid Training and Licensures Benefits may be different by location. Benefit eligibility requirements vary and may include length of service. Coverage begins on the date of hire. Must enroll in New Hire Benefits within 30 days of the date of hire for coverage to take effect. The equal employment opportunity policy of the GEICO Companies provides for a fair and equal employment opportunity for all associates and job applicants regardless of race, color, religious creed, national origin, ancestry, age, gender, pregnancy, sexual orientation, gender identity, marital status, familial status, disability or genetic information, in compliance with applicable federal, state and local law. GEICO hires and promotes individuals solely on the basis of their qualifications for the job to be filled. GEICO reasonably accommodates qualified individuals with disabilities to enable them to receive equal employment opportunity and/or perform the essential functions of the job, unless the accommodation would impose an undue hardship to the Company. This applies to all applicants and associates. GEICO also provides a work environment in which each associate is able to be productive and work to the best of their ability. We do not condone or tolerate an atmosphere of intimidation or harassment. We expect and require the cooperation of all associates in maintaining an atmosphere free from discrimination and harassment with mutual respect by and for all associates and applicants.",
        "url": "https://www.linkedin.com/jobs/view/3952848615"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Chantilly, VA",
        "job_id": 3965659534,
        "company": "Dezign Concepts",
        "title": "Software Engineer",
        "created_on": 1720638230.3775082,
        "description": "20240702-39987-01 Active Top Secret Clearance with Poly Required Salary Range: Up to $225K (salary is commensurate with education and experience) Please Note: This job requires an existing Top Secret Clearance and Polygraph. Experience Needed: Citizenship: Must Be a US Citizen Existing Clearance Required: Active Top Secret SCI with Poly All levels of experience will be considered Experience developing and maintaining apps in: Ruby Rails Vue.js Bootstrap browser technologies (HTML, HTTP, and CSS) elasticsearch ELK stack AWS Experience working independently in a fast paced agile scrum software development environment Experience using agile dev tools & practices JIRA including keeping stories/tasks up-to-date Confluence Unit testing GitHub Daily standup Participating in technical discussions, forums, retrospectives Participating in routine customer meetings and ad hoc sessions Benefits Our comprehensive benefits package includes Medical, Dental, Vision, Health Savings Account, Paid Time Off, Holidays, Social Events, Employee Assistance Program, Team Building Activities, 401K, Tuition Assistance, and more. Contact Us: Main Number: 1-888-663-2690 | Dezign Concepts provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. Powered by JazzHR 5BdAih4lGm",
        "url": "https://www.linkedin.com/jobs/view/3965659534"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "McLean, VA",
        "job_id": 3866813911,
        "company": "DGS",
        "title": "Wireless Data Engineer",
        "created_on": 1720638232.3137248,
        "description": "Wireless Data Engineer: We are seeking a Wireless Data Engineer who will play a pivotal role in designing, developing, and maintaining globally distributed data architectures to support the efficient processing and analysis of large-scale network data. They will accelerate all aspects of data processing, from ingestion to storage analytics, and will work closely with data scientists, analysts, and other stakeholders to understand their data requirements and implement solutions; including edge computing solutions, to meet those needs. In this role, you will: Design, develop, and maintain scalable data flows for ingesting, validating, processing, and transforming large volumes of structured and unstructured data Create and implement data models and schemas optimized for wireless network analytics, focusing on factors such as dwell times and volume metrics Ensure data consistency, integrity, and quality throughout the integration process Develop and optimize algorithms for data transport, workflows, and processing of large datasets across globally distributed wireless networks Establish data quality standards and implement processes for data validation, normalization, and enrichment, ensuring high-quality data for analysis Monitor and optimize the performance of data pipelines, databases, and processing systems, identifying, and resolving bottlenecks and performance issues Collaborate with cross-functional teams, including data scientists, software engineers, and business stakeholders, to understand data requirements and deliver solutions aligned with business objectives Document data flows, architectures, and processes to ensure transparency and facilitate knowledge sharing across the organization Stay abreast of emerging technologies, tools, and best practices in data engineering, incorporating them to enhance existing systems and processes The ideal candidate would possess: B.S. or M.S. in Computer Science, Engineering, or related field. Proven experience as a data engineer in a similar role, with a strong background in data management, data processing, and distributed systems. Proficiency in programming languages such as Python, Java, Scala, or SQL. Familiarity with cloud platforms such as AWS, Azure, or Google Cloud Strong understanding of database technologies, including relational databases (e.g., MySQL, PostgreSQL) and NoSQL databases. Excellent problem-solving and attention to detail. Effective communication and collaboration skills, with the ability to work effectively in a team environment. Why Choose DGS? Opportunity to work at the forefront of RF environmental awareness technologies, shaping the future of telecommunications Competitive compensation package, including salary benefits, and opportunities for professional development Roth + Traditional 401(k) options Dental, Medical, and Vision Insurance Make a meaningful impact by contributing to solutions that improve network performance, reliability, and sustainability. If you are passionate about data science, telecommunications, and RF environmental awareness, we invite you to join us on our journey to revolutionize the wireless communication industry. Apply now to be part of our innovative team! EEO/DEI Statement: The equal employment opportunity policy of the DGS Company provides for a fair and equal employment opportunity for all associates and job applicants regardless of race, color, religious creed, national origin, ancestry, age, gender, pregnancy, sexual orientation, gender identity, marital status, familial status, disability or genetic information, in compliance with applicable federal, state and local law. DGS hires and promotes individuals solely based on their qualifications for the job to be filled. We are committed to fostering a culture of belonging where everyone feels seen, heard, valued for who they are and empowered to succeed. Our approach to cultivating a diverse, equitable, and inclusive culture is rooted in listening, learning and collective action. By embracing the diversity of our people, we achieve our best work and fuel innovation - generating the best possible outcomes for our customers and the communities they serve Digital Global Systems (DGS) is a trailblazing spectrum management and Radio Frequency (RF) environmental awareness company that has been at the forefront of industry innovation since its inception in 2012. With an impressive portfolio of over 150 patents, DGS has played a pivotal role in advancing persistent spectrum monitoring and RF data analysis. We are not only committed to addressing current industry needs but also spearheading advancements in the ever-evolving wireless digital market, including 5G applications, dynamic spectrum sharing, intelligent interference management, and RAN integrations. We are in search of resourceful, analytical, and adaptable candidates to join our team as we continue to expand and innovate. Located minutes away from both the Beltway and the Tysons Corner Metro Stop, our headquarters are easily accessible by the Silver Line. Our comfortable office spaces are complemented by gourmet dining facilities, gym space, and free parking. Join us at DGS and be part of a pioneering team shaping the wireless landscape of tomorrow. Check our Powered by JazzHR HGkprNcrva",
        "url": "https://www.linkedin.com/jobs/view/3866813911"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Annapolis Junction, MD",
        "job_id": 3787754622,
        "company": "GliaCell Technologies",
        "title": "Senior Database Engineer",
        "created_on": 1720638234.1551242,
        "description": "Are you a Senior Database Engineer who is ready for a new challenge that will launch your career to the next level? Tired of being treated like a company drone? Tired of promised adventures during the hiring phase, then being dropped off on a remote contract and never seen or heard from the mothership again? Our engineers were certainly tired of the same. At GliaCell our slogan is “We make It happen”. We will immerse you in the latest technologies We will develop and support your own personalized training program to continue your individual growth. We will provide you with work that matters with our mission-focused customers, and surround you with a family of brilliant engineers. Culture isn’t something you need to talk about…if it just exists. If this sounds interesting to you, then we’d like to have a discussion regarding your next adventure! If you want to be a drone, this isn’t the place for you. We Make It Happen! GliaCell Technologies focuses on Software & System Engineering in Enterprise and Cyber Security solution spaces. We excel at delivering stable and reliable software solutions using Agile Software Development principles. These provide us the capability to deliver a quick turn-around using interactive applications and the integration of industry standard software stacks. GliaCell’s Enterprise capabilities include Full-Stack Application Development, Big Data, Cloud Technologies, Analytics, Machine Learning, AI, and DevOps Containerization. We also provide customer solutions in the areas of CND, CNE, and CNO by providing our customers with assessments and solutions in Threat Mitigation, Vulnerability Exposure, Penetration Testing, Threat Hunting, and Preventing Advanced Persistent Threat. We Offer: Long term job security Competitive salaries & bonus opportunities Challenging work you are passionate about Ability to work with some amazingly talented people Job Description: GliaCell is seeking a Senior Database Engineer on one of our subcontracts. This is a full-time position offering the opportunity to support a U.S. Government customer. The mission is to provide technical expertise that assists in sustaining critical mission-related software and systems to a large government contract. Key Requirements: To be considered for this position you must have the following: Possess an active or rein-statable TS/SCI with Polygraph security clearance Be a U.S. Citizen Bachelor of Science Degree in Computer Science (or a related subject) (4 years of work experience can be substituted for the degree) 20+ years of experience Experience with Oracle database administration Experience with Extract, Translate, and Load (ETL) operations An understanding of dataflow into Oracle An understanding of TechSIGINT data Location: Annapolis Junction, Maryland Salary: Based on Education, Years of Experience, Skill, and Abilities Check Out Our Benefits: Paid Time Off Medical, Dental & Vision Benefits Life & Disability Insurance Tuition, Training & Certification Reimbursement 401K Contribution Employee Referral Bonus Program Equipment Reimbursement Team Engagement & Outings Swag …And more! Learn more about GliaCell Technologies: To apply for this position, respond to this job posting and attach an updated resume for us to review. GliaCell Technologies, LLC is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status. Powered by JazzHR SE468twMSF",
        "url": "https://www.linkedin.com/jobs/view/3787754622"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Annapolis Junction, MD",
        "job_id": 3787767433,
        "company": "Farfield Systems, Inc",
        "title": "Software Engineer 2",
        "created_on": 1720638235.8101592,
        "description": "About Farfield Systems, Inc At Farfield we are committed to delivering trusted expertise to our government clients. As we grow, our focus is on increasing opportunities for you to grow with us while still delivering the same excellence customers have grown to expect from us. We continually evaluate our environment to provide a place where your career is packed with opportunities to grow and you have the ability to demonstrate your passion to our customers. We focus on building a Team where each employee is a valued member. Farfield provides support to multiple agencies across the United States Government in many locations. That means many different opportunities to follow your career path without changing companies every few years. \"Employee driven...Customer focused.\" We build, operate and secure networks and infrastructure. *** Requires a Top Secret/SCI clearance with a polygraph and U.S. Citizenship*** The Software Engineer develops, maintains, and enhances complex and diverse software systems (e.g., processing-intensive analytics, novel algorithm development, manipulation of extremely large data sets, real-time systems, and business management information systems) based upon documented requirements. Works individually or as part of a team. Reviews and tests software components for adherence to the design requirements and documents test results. Resolves software problem reports. Utilizes software development and software design methodologies appropriate to the development environment. Provides specific input to the software components of system design to include hardware/software trade-offs, software reuse, use of Commercial Off-the-shelf (COTS)/Government Off-the-shelf (GOTS) in place of new development, and requirements analysis and synthesis from system level to individual software components. Basic Qualifications: Fourteen (14) years experience as a SWE in programs and contracts of similar scope, type, and complexity is required Bachelors degree in Computer Science or related discipline from an accredited college or university is required. Four (4) years of additional SWE experience on projects with similar software processes may be substituted for a bachelors degree Experience in Service Industry: Three (3) years' experience in User Interface Web Design and Usability Development Desired Qualifications: Analyze user requirements to derive software design and performance requirements Debug existing software and correct defects Provide recommendations for improving documentation and software development process standards Design and code new software or modify existing software to add new features Integrate existing software into new or modified systems or operating environments Develop simple data queries for existing or proposed databases or data repositories Write or review software and system documentation Serve as team lead at the level appropriate to the software development process being used on any particular project Design or implement complex database or data repository interfaces/queries Develop or implement algorithms to meet or exceed system performance and functional standards Assist with developing and executing test procedures for software components Develop software solutions by analyzing system performance standards, confer with users or system engineers; analyze systems flow, data usage and work processes; and investigate problem areas Modify existing software to correct errors, to adapt to new hardware, or to improve its performance Design, develop and modify software systems, using scientific analysis and mathematical models to predict and measure outcome and consequences of design Design or implement complex algorithms requiring adherence to strict timing, system resource, or interface constraints; Perform quality control on team products Implement recommendations for improving documentation and software development process standards Oversee one or more software development teams and ensure the work is completed in accordance with the constraints of the software development process being used on any particular project Confer with system engineers and hardware engineers to derive software requirements and to obtain information on project limitations and capabilities, performance requirements and interfaces Coordinate software system installation and monitor equipment functioning to ensure operational specifications are met Requires a Top Secret/SCI clearance with a polygraph *** Powered by JazzHR 4GoxLt5EAD",
        "url": "https://www.linkedin.com/jobs/view/3787767433"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Columbia, MD",
        "job_id": 3877851272,
        "company": "C2 Technology Solutions Inc.",
        "title": "Database Engineer III",
        "created_on": 1720638237.7557147,
        "description": "C2 Technology Solutions is a small business with a global perspective, offering a complete spectrum of insightful expertise in system and software engineering, architecture design, business process re-engineering, and IT infrastructures. Since 2010, we have provided performance and value-driven consulting services to both government and commercial clients. The key to C2 Technology Solutions’ success is in its primary asset…People. We understand that our growth is parallel to the growth of every individual on our team. At C2 Technology Solutions, community and ownership inspires creativity and the working environment fosters excellence. In addition to a great work environment, we offer a competitive compensation package with outstanding benefits and career development opportunities. Position Overview Our firm is currently seeking an experienced Database Engineer in the Columbia, Maryland area. The Database Engineer will provide technical expertise for database design, development, implementation, information storage and retrieval, data flow and analysis. Develop relational and/or Object-Oriented databases, database parser software, and database loading software. Project long-range requirements for database administration and design. Responsible for developing a database structure that fits into the overall architecture of the system under development and has to make trades among data volumes, number of users, logical and physical distribution, response times, retention rules, security and domain controls. Work primarily at the front end of the lifecycle requirements through system acceptance testing and Initial Operational Capability (IOC). Develop requirement from a project's inception to its conclusion for a particular business and Information Technology (IT) subject matter area (i.e., simple to complex systems). Assist with recommendations for, and analysis and evaluation of systems improvements, optimization, development, and/or maintenance efforts. Translate a set of requirements and data into a usable document by creating or recreating ad hoc queries, scripts, and macros; update existing queries, create new ones to manipulate data into a master file; and build complex systems using queries, tables, Open Database Connectivity and database storage and retrieval using Cloud methodologies. The Database Engineer will direct the development of complex systems using queries, tables, and database storage and retrieval using Cloud methodologies for the design, development, implementation, information storage and retrieval, data flow and analysis. Direct the overall database structure to fit into the overall architecture of the system. Create new workflows to take over existing processes as needed as well as provide break/fix requests or updates. Lead development of database structures, database parser software, and database loading software. Direct fulfillment of requirements from a project inception to conclusion. Support the database design, development, implementation, information storage and retrieval, data flow and analysis activities Support the analysis and evaluation of system improvements, optimization, development and/or maintenance efforts Support the development of long and short term requirements for database administration and design Assist in developing databases, database parser software, and database loading software Translate a set of requirements and data into a usable database schema by creating or recreating ad hoc queries, scripts and macros, updates existing queries, creates new ones to manipulate data into a master file Assist in developing database structures that fit into the overall architecture of the system under development Lead development of database structures that fit into the overall architecture of the system under development Lead development of databases, database parser software, and database loading software Develop requirement recommendations from a project's inception to its conclusion for a particular Business and IT subject matter area (i.e. simple to complex systems) Develop a database structure that fits into the overall architecture of the system under development and has to make trades among data volumes, number of users, logical and physical distribution, response times, retention rules, security and domain controls Direct fulfillment of requirements from a project's inception to it conclusion Direct organization of requirements and data into a usable database schema by directing development of ad hoc queries, scripts, macros, updates to existing queries Direct the overall database structure that fits into the overall architecture of the system under development and has to make trades among data volumes, number of users, logical and physical distribution, response times, retention rules, security and domain controls Direct the development of complex systems using queries, tables, Open Database Connectivity and database storage and retrieval using Cloud methodologies Qualifications/Experience Requirements Ten (10) years experience as a DBE in programs and contracts of similar scope, type, and complexity is required. Bachelor's degree in a technical discipline from an accredited college or university is required. Five (5) years of DBE experience may be substituted for a bachelor's degree. Required: Database experience using MongoDB or MariaDB; including deployment and management of the database itself, debugging of optimization issues, and scaling. Desired: Python, Django or Flask, ReST Endpoint Development, Micro-Service Model, Swagger, AWS, C2S or other cloud experience, Docker, Visual Studio Code or similar IDEs, JSON and/or XML serialization, Jira, Confluence, Git version control, Experience working in Agile environment https://c2techsol.applicantstack.com/x/openings Security Clearance with an appropriate agency Polygraph is required. C2 Technology Solutions provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, sexual orientation, gender identity, marital status, national origin, age, veteran status, or disability.",
        "url": "https://www.linkedin.com/jobs/view/3877851272"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "McLean, VA",
        "job_id": 3818816571,
        "company": "Keylent Inc",
        "title": "Software Engineer 21339-1",
        "created_on": 1720638239.3600018,
        "description": "Request-ID: 21339-1 Cognizant *THIS ROLE IS ONSITE IN MCLEAN, VA* Job Title: Software Engineer Experience: 7 to 10 yrs Must Have Skills Python AWS Job Summary: Support Hyperion application end-to-end and troubleshoot any issues within the application. Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability etc. 4+ years of Programming experience (Data Engineer) - outside of college experience required. Roles & Responsibilities 5+ years of experience (Sr-level) Strong Programming experience with object-oriented/object function scripting languages: Python, Spark, Scala 3+ years of working experience in AWS Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using AWS technologies. Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics. Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader. Should have good analytical skills Should be a good team player Should have effective communication skills Should be able to provide technical guidance to team and ensure tasks are closed on-time Should be able to run a medium sized (5 - 7 members) project team independently",
        "url": "https://www.linkedin.com/jobs/view/3818816571"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Fort Meade, MD",
        "job_id": 3787763804,
        "company": "Farfield Systems, Inc",
        "title": "Software Engineer 2",
        "created_on": 1720638241.0899007,
        "description": "About Farfield Systems, Inc At Farfield we are committed to delivering trusted expertise to our government clients. As we grow, our focus is on increasing opportunities for you to grow with us while still delivering the same excellence customers have grown to expect from us. We continually evaluate our environment to provide a place where your career is packed with opportunities to grow and you have the ability to demonstrate your passion to our customers. We focus on building a Team where each employee is a valued member. Farfield provides support to multiple agencies across the United States Government in many locations. That means many different opportunities to follow your career path without changing companies every few years. \"Employee driven...customer focused.\" We build, operate and secure networks and infrastructure. *** Requires a Top Secret/SCI clearance with a polygraph and U.S. Citizenship*** The Software Engineer develops, maintains, and enhances complex and diverse software systems (e.g., processing-intensive analytics, novel algorithm development, manipulation of extremely large data sets, real-time systems, and business management information systems) based upon documented requirements Works individually or as part of a team Reviews and tests software components for adherence to the design requirements and documents test results Resolves software problem reports Utilizes software development and software design methodologies appropriate to the development environment Provides specific input to the software components of system design to include hardware/software trade-offs, software reuse, use of Commercial Off-the-shelf (COTS)/Government Off-the-shelf (GOTS) in place of new development, and requirements analysis and synthesis from system level to individual software components Basic Qualifications: Fourteen (14) years experience as a SWE in programs and contracts of similar scope, type, and complexity is required and Bachelor’s degree in Computer Science or related discipline from an accredited college or university is required Four (4) years of additional SWE experience may be substituted for a bachelor’s degree Analyze user requirements to derive software design and performance requirements Debug existing software and correct defects Provide recommendations for improving documentation and software development process standards Design and code new software or modify existing software to add new features Integrate existing software into new or modified systems or operating environments Develop simple data queries for existing or proposed databases or data repositories Write or review software and system documentation Serve as team lead at the level appropriate to the software development process being used on any particular project Design or implement complex database or data repository interfaces/queries Develop or implement algorithms to meet or exceed system performance and functional standards Assist with developing and executing test procedures for software components Develop software solutions by analyzing system performance standards, confer with users or system engineers; analyze systems flow, data usage and work processes; and investigate problem areas Modify existing software to correct errors, to adapt to new hardware, or to improve its performance Design, develop and modify software systems, using scientific analysis and mathematical models to predict and measure outcome and consequences of design Design or implement complex algorithms requiring adherence to strict timing, system resource, or interface constraints; Perform quality control on team products Implement recommendations for improving documentation and software development process standards Oversee one or more software development teams and ensure the work is completed in accordance with the constraints of the software development process being used on any particular project Confer with system engineers and hardware engineers to derive software requirements and to obtain information on project limitations and capabilities, performance requirements and interfaces Coordinate software system installation and monitor equipment functioning to ensure operational specifications are met Farfield Systems will provide reasonable accommodations to applicants who are unable to utilize our online application system due to a disability. Please send your request to careers@farfieldsystems.com or call us for assistance at 410-874-9363. Farfield Systems is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law. Powered by JazzHR jEmho7wwUx",
        "url": "https://www.linkedin.com/jobs/view/3787763804"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Fort Belvoir, VA",
        "job_id": 3930164924,
        "company": "TekSalt Solutions",
        "title": "Sr. Remedy Database Engineer",
        "created_on": 1720638242.7710578,
        "description": "We are looking for Sr. Remedy Database Engineer in Fort Belvoir, VA. Provides administration, and management of Remedy. Provides pro-active monitoring of Remedy from performance and capability management perspectives. Ensures continuity of operations. If you or know anyone would be interested please reach out to me at praveen.vemula@teksalt.com Job Description Principle Duties and Responsibilities: Senior technical expert for administrating and developing BMC Remedy products and solutions Accountable for all aspects of Remedy services. Provides administration, and management of Remedy. Provides pro-active monitoring of Remedy from performance and capability management perspectives. Ensures continuity of operations. Patch the Remedy Applications monthly or as needed. Strong working knowledge of Remedy environments operating on Microsoft Windows and SQL. BMC Certified Administrator: BMC Remedy AR System 7.5 or higher and provides support database maintenance activities such as backup/recovery, updates to service packs and patches, and optimization. Soft Skills: Excellent verbal and written communication skills should have technical lead experience Should be highly independent and self-motivated. Should be able to work with minimum supervision. Must demonstrate experience as a team player, is collaborative in personal style, and have a history of working well with others. Openness and honesty characterize the ideal incumbent, having unquestioned integrity, self-responsibility, and personal initiative. Education and Certifications Bachelor’s degree or equivalent in IT related field and/or combination of experience and education. Baseline certification: DoD 8570 IAT II certification, Security+, One of the following DoD approved certifications is necessary to meet the category/level IAT-2 (DoD 8570) requirement for this position: GSEC, SSCP, CCNA-Security, CISA, CISSP (or Associate), CASP, GCIH, GCED Computing Environment certification: BMC Certified Administrator: AR System 7.5 or higher Experience Eight years’ hands-on experience in Remedy administration, design, and development. Soft Skills Excellent verbal and written communication skills should have technical lead experience Should be highly independent and self-motivated. Thank you, Praveen TekSalt Solutions praveen.vemula@teksalt.com 612-852-4887",
        "url": "https://www.linkedin.com/jobs/view/3930164924"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Reston, VA",
        "job_id": 3945598883,
        "company": "Walmart Global Tech",
        "title": "Senior IAM Cloud Engineer",
        "created_on": 1720638244.4951446,
        "description": "At Walmart, we prioritize innovation and data security. Our team is dedicated to maintaining a secure operating environment and preserving the trust of our customers, associates, and stakeholders. We combine a range of services and expertise to prevent fraud, detect threats, and manage digital risk and access. Our focus is on mitigating attack risks, securing cloud transformation, and fostering a culture of security and reliability within our team. The Information Security team has the herculean task of assuring that customers can safely shop with peace of mind knowing their data and information will be safe and secure. Solving some of the most unique cyber security problems in the industry, our team members share an elevated level of creativity and ingenuity to secure data for one of the largest e-commerce operations in the world. As a Senior Systems Engineer on the Cloud Identity team, your role will own a project(s), product(s), and/or a service(s) from end to end. The expectation for experience would be hands-on with Active Directory (AD), Azure AD, GCP IAM, and/or Microsoft Identity Manger. Advanced knowledge of SQL, PowerShell and Windows Server. Understanding of infrastructure deployments, cloud and on-premise. Understanding of authentication methods (SSO, MFA, PAP, CHAP, EAP,etc). Familiarity with next generation authentication concepts, (passwordless, ZeroTrust, etc. CISSP, CSSK, CCSP, or Microsoft related certifications would be a plus. What you'll do: Work with one of the largest multi-cloud environments in the world Work directly with Microsoft and Google IAM product teams to plan, scope, test, and sign off on new features. Build monitoring, reporting and service integration into enterprise solutions. Contribute to policies and standards that govern cloud identity use. Utilize scripting tools and other technologies to automate and standardize technical functions. Present internally What you'll bring: Hands on experience with utilizing PowerShell for scripting changes towards Cloud offerings in Azure and GCP. Familiarity with directory services like Active Directory or LDAP. Significant and demonstrable history with complex Identity and Access Management integration and service delivery use cases and requirements. Deep understanding of IAM protocols and standards such as SAML, OAuth, and OIDC. Excellent time management and organizational skills, capable of meeting/driving towards deadlines. Can effectively communicate complex technical concepts to varying audiences. Familiarity with HIPAA, PCI DSS, SOX, and other industry or regulatory compliance Hands on development experience is a plus",
        "url": "https://www.linkedin.com/jobs/view/3945598883"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Annapolis Junction, MD",
        "job_id": 3871104771,
        "company": "Independent Software",
        "title": "Software Engineer, Entry Level",
        "created_on": 1720638246.4081502,
        "description": "What you will be doing! As a Software Engineer you will analyze user requirements to derive software design and performance requirements, debugging existing software and correcting defects. Candidate will also design and write new software or modify existing software to add new features, integrate existing software and writing or reviewing software documentation. Job Requirements Technical experience with Java Web application or RESTful Services Education And Experience No demonstrated experience is required. Bachelor’s degree in Computer Science or related discipline from an accredited college or university is required. Four (4) years of SWE experience on projects with similar software processes may be substituted for a bachelor’s degree. Clearance Required : Must possess an active TS/SCI with Polygraph security clearance to be considered for this role. Independent Software is an Equal Opportunity Employer EOE, M/F/D/V Powered by JazzHR",
        "url": "https://www.linkedin.com/jobs/view/3871104771"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Laurel, MD",
        "job_id": 3913121390,
        "company": "Quevera",
        "title": "Software Engineer 0",
        "created_on": 1720638248.0875127,
        "description": "Job Description Quevera is seeking a to join an exciting, collaborative and innovative team. A place where you are positioned for More than Just a Job. Where leadership partners with you, seek to cultivate and support career development, encouraging growth from within while striving to foster a diverse environment that improves individual and organizational performance. Highlights Of Working For Quevera Are Quevera employees voted Quevera as a TOP EMPLOYER in the Baltimore /DC area for 2020 (ranked #8 out of 150 companies) and 2022 (ranked #5 out of 150 companies). Yearly $5,000 towards education/training. Employees are in control of their career path through our Career Pathway Program. Family and corporate events Excellent health care coverage (100% paid premium option) and 401K matching (up to 4%). And many more! Q-Culture Video Q-Careers \"REQUIRED - MUST have a current TS/SCI Polygraph clearance to apply for role . Only those with a current TS/SCI with Poly clearance will be considered.\" Required Qualifications Experience in either Java or Python IAT Level 2 Certification Preferred Qualifications Experience in Full stack Shell Scripting Docker Typescript, angular 14+ MongoDB, Kubernetes (Docker) Relational data bases (MySQL, Postgres, Oracle, Sybase, etc…) Quevera is an equal opportunity/affirmative action employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected veteran status, age or any other characteristic protected by law.",
        "url": "https://www.linkedin.com/jobs/view/3913121390"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Annapolis Junction, MD",
        "job_id": 3920702088,
        "company": "Independent Software",
        "title": "System Engineer, Junior",
        "created_on": 1720638249.7564936,
        "description": "What you will be doing! As a System Engineer you will implement and maintain operational data flows for a large high performance computing system. The System Engineer shall be responsible for ensuring all data handling requirements are met for data storage, data transport, data security and data compliance, as well as accurate upkeep of knowledge store management. Additional tasking includes working with various mission customers and multiple teams to coordinate on tasks needed, as well as perform exploratory data analysis on raw data to clean, enrich, transform, and convert the raw data into the required formats. Requirements Experience using the Linux CLI Experience implementing and maintaining operational data flows Willingness to learn Apache NiFi to process and distribute data Experience with Corporate data flow processes and tools Experience with Corporate data security and compliance procedures and policies Experience with the Atlassian Tool Suite (i.e. JIRA, Confluence) Manage system requirements and derived requirements to ensure the delivery of production systems that are compatible with the defined system architecture(s) — Department of Defense Architecture Framework (DoDAF), Service-oriented Architecture (SOA), etc. Contribute to the development of sections of systems engineering documentation such as System Engineering Plans, Initial Capabilities Documents, Requirements specifications, and Interface Control Documents Assist with the development of system requirements, functional requirements, and allocation of the same to individual hardware, software, facility, and personnel components Coordinate the resolution of action items from Configuration Control Board (CCB) meetings, design reviews, program reviews, and test reviews that require cross-discipline coordination Participate in an Integrated Product Team to design new capabilities based upon evaluation of all necessary development and operational considerations Allocate real-time process budgets and error budgets to systems and subsystem components Generate alternative system concepts, physical architectures, and design solutions Define the methods, processes, and evaluation criteria by which the systems, subsystems and work products are verified against their requirements in a written plan Develop system design solution that satisfies the system requirements and fulfills the functional analysis Review and provide input to program and contract work breakdown structure (WBS), work packages and the integrated master plan (IMP) Participate in the development of system engineering documentation, such as System Engineering Plans, Initial Capabilities Documents, Requirements Specifications, and Interface Control Documents Participate in interface definition, design, and changes to the configuration between affected groups and individuals throughout the life cycle Derive from the system requirements an understanding of stakeholder needs, functions that may be logically inferred and implied as essential to system effectiveness Derive lower-level requirements from higher-level allocated requirements that describe in detail the functions that a system component must fulfill, and ensure these requirements are complete, correct, unique, unambiguous, realizable, and verifiable Participate in establishing and gaining approval of the definition of a system or component under development (requirements, designs, interfaces, test procedures, etc.) that provides a common reference point for hardware and software developers Develop derived requirements for Information Assurance Services (Confidentiality, Integrity, Non repudiation, and Availability); Basic Information Assurance Mechanisms (e.g., Identification, Authentication, Access Control, Accountability); and Security Mechanism Technology (Passwords, cryptography, discretionary access control, mandatory access control, hashing, key management, etc.) Desired Skills: Experience writing scripts using Bash/Python Experience with time-series visualization tools such as Grafana Education and Experience: Seven (7) years' experience as a SE in programs and contracts of similar scope, type and complexity is required. Bachelor's degree in System Engineering, Computer Science, Information Systems, Engineering Science, Engineering Management, or related discipline from an accredited college or university is required. Five (5) years of additional SE experience may be substituted for a bachelor's degree. Clearance Required : Must possess an active TS/SCI with appropriate Polygraph to be considered for this role. Independent Software is an Equal Opportunity Employer EOE, M/F/D/V Powered by JazzHR",
        "url": "https://www.linkedin.com/jobs/view/3920702088"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Bethesda, MD",
        "job_id": 3971529633,
        "company": "TekWissen ®",
        "title": "Software/Systems Engineer",
        "created_on": 1720638251.284437,
        "description": "Position: Software/Systems Engineer – Legal IT Systems Location: Bethesda, MD 20814 Duration: 3 Months Job Type: Contract Work Type: Onsite Pay Rate: $60 - 61$/hr. on W2 Overview: TekWissen Group is a workforce management provider throughout the USA and many other countries in the world. Our client is a global operator, franchisor, and licensor of hotel, residential, and timeshare properties. The company is primarily focused on management, franchising, and licensing of its lodging properties. It also occasionally develops, acquires or renovates hotel and residential properties, directly and through partnerships, joint ventures, and other business structures with third parties. JOB Description: Client’s Law Department services the Company's lodging and hospitality business and provides legal advice and counsel to client’s internal clients. The Legal Technology, eDiscovery and Records team is responsible for the Law Department’s technology platform to enable and enhance the legal services delivered by the Department. The team partners with attorneys, Senior Law Leadership, and corporate IT discipline leadership to create, oversee, and support the development and implementation of Legal technology strategies and solutions and drives eDiscovery efforts and Law Department Records work. The Software/Systems Engineer will be part of the team working on system and automation/integration projects for the Law Department which includes analysis of request, support in the recommendation/design, work on development and implementation of interfaces, integrations, outputs and inputs for the various systems and databases used by the Law Department maintain the department’s databases (on-premise and cloud) to ensure applications, operating systems and database applications are current, PCI compliant and adhere to standards as dictated by iT contribute to initiatives meant to develop and maintain the department’s front end web pages to collect and maintain data and provide a portal for world-wide clients to the Law Department’s databases provide technical assistance to the Legal Technology Operations team when technical issues arise with the systems and databases used by the departments that cannot be resolved and require escalation, and provide development and implementation of interfaces, integrations, outputs and inputs for various systems and databases for other business units upon requests. This position reports directly to the Vice President of Legal Technology, eDiscovery and Records Management. Specific Expected Contributions The responsibilities of this position include: Automation/Integration Development Support the design, development, and maintenance of software automation/integration across various platforms, such as PIM, B2B, or B2C e-commerce systems, using APIs and message transformation techniques. Ensure that the integrations are scalable, reliable, and secure, and that they meet the business requirements and the industry standards. Execute integration strategies, leverage tools from other developers, ensure ongoing monitoring, and troubleshoot integrations. Collaborate with cross-functional teams, such as developers, engineers, and programmers, to ensure that any automations/integrations are aligned with the technical architecture and the data flow. Monitor and update HotDocs document automation templates/forms to efficiently create first drafts of standard documents. Ensure that the integrations are documented, tested, optimized, and maintained in the Law Department code repository. Database Administration Run database queries, scripts, reports, and updates for assigned Law Department database systems. Interface with the client community to capture user requirements for database systems. Share requirements to support analysis and selection of the appropriate system platform needed to satisfy the requirements. Act as an internal resource on law specific systems (TeamConnect, iManage, Law Manager, Blueprint, LexisNexis, HotDocs. Support the setup, testing and implementation of all Law Department database upgrades. Troubleshoot problems and errors related to database design and architecture. Partner with team and leadership to fix problems minimizing end user effort to correct data. Systems Integration/Implementation Work in close collaboration with a cross-functional group of specialists including developers, engineers, and programmers, and may serve in a coordinator role organizing and managing the comprehensive efforts to align in sync efficiently. Ensure that the integrations are aligned with the business goals and the user needs, and that the stakeholders are informed and engaged. Research new technologies and best practices for system integration, and share insights and ideas with the legal technology team and the users. Stay updated on the latest trends and developments in the legal technology field and provide suggestions for innovation and enhancement. Support client iT to ensure deployment of system upgrades and other functionality. Liaison with client iT to manage servers which are controlled by client iT but host law department data. Install new software or upgrades as instructed to comply with iT requirements and provide client newest functionality. Work with vendors in preparation of installation of hardware and software requiring vendor involvement. Quality Assurance Build integrations that are tested, validated, and verified, and that the quality standards are met. Execute test plans, test cases, and test scripts, and report on the test results and the defects. Ensure that the testing is automated, continuous, and integrated with the development process. Build integrations are compliant with the legal, regulatory, and security requirements, and that the data privacy and the data protection are met. Implement security policies, procedures, and controls, and monitor and report on security incidents and vulnerabilities. Follow all established project management processes. Supplemental Projects (as needed) Participate in hardware/software pilots in support of new technologies. Work with eDiscovery team as necessary to assist in data collection and preservation efforts. Other special technology projects as applicable. Candidate Profile Successful candidates should possess the experience, knowledge, and skills as follows: Minimum of 7 years related experience. 1 year experience with API integration, preferred. Demonstrated knowledge in the following technology; web based tools such as HTML and ASP SQL programming including stored procedures and query development HOTDocs, document assembly preferred Visual Basic and macro development Application deployment and packaging Foundational scripting languages (Powershell, batch, SQL)Working knowledge in the development of various exports/imports using XML (to exchange data between Law Department applications and business applications) Demonstrated experience independently tracking tasks and deliverables with consistent on time delivery. Strong written and verbal communication skills. Solid organizational and project management skills. Excellent technical research and troubleshooting skills Ability to present ideas and information in a concise, organized way. Strong interpersonal skills and ability to interact and communicate effectively with both technical and non-technical staff TekWissen® Group is an equal opportunity Employer supporting workforce diversity.",
        "url": "https://www.linkedin.com/jobs/view/3971529633"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Arlington, VA",
        "job_id": 3837698174,
        "company": "Lovelytics",
        "title": "Tech Lead, Data Engineer",
        "created_on": 1720638253.0536273,
        "description": "Lovelytics is seeking a Technical Lead (Level 4) Consultant with experience delivering strategic Databricks client engagements to join our Data & AI practice! As a Tech Lead, you will gain people management skills in order to develop to the next level. In addition, you will play a key role, often as an engagement lead, on client engagements related to data warehousing, ETL development, data integrations, and data modeling. This is a client-facing and stakeholder management role, focused on using and migrating to our partner technologies; Databricks, AWS, and Azure to name a few. In addition to the technical capabilities for this role, we are looking for someone who wants to work in a collaborative, dynamic, and inclusive environment and has a passion for bringing meaning to data. Role Location: Open to remote candidates in the US in the following states: MD, DC, CA, IL, IA, IN, MA, NC, TX, TN, GA, CO, NY, NJ, VA, FL, PA, OH, OR. As well as Ontario, Canada This role is not eligible for sponsorship at this time. Primary Responsibilities: Utilize consulting and technical skills to be able to work in a client-facing project environment independently Be responsible for your own execution and often others' on client projects, communicating directly with internal and external stakeholders on status updates and potential roadblocks Collaborate with other team members to successfully deliver on projects Work effectively and directly communicate with both internal and client and/or partner teams Develop full ownership of your execution on client engagements, play a role in the project planning and solution stages of engagements as well Successfully lead small Data Engineering projects and workstreams with measurable performance enhancements typically under the management of an engagement lead, or may play the part of the EL Lead the end-to-end implementation of multiple ETL/ELT pipelines, demonstrating efficient data transformation Mentor junior data engineers, and their growth is evident in their project contributions Successfully lead small data warehousing projects with measurable performance enhancements under the management of an engagement lead- may also play the role of an engagement lead Contribute to real-time data processing solutions and managed streaming data Implement security and compliance measures for data pipelines Design and implement version control and branching strategies and integrate them into CI/CD for promoting and testing in higher environments Our Ideal Candidate's Skills and Experiences: B.S. in Computer Science or equivalent 4-6 years' experience in data engineering and big data. 2 years' of professional services experience interacting directly with clients Extensive knowledge of data warehousing concepts and hands-on experience deploying pipelines using Databricks **a must Databricks Solution Architect certification a plus Data modeling and database design skills and knowledge of version control Excellent verbal and written communication skills Experience architecting scalable and fault-tolerant data solutions across Azure, AWS, and Databricks Understands and utilizes Lovelytics tools and client tools What We Promise You: Exciting projects with great clients in varying departments and verticals across the world The ability to work closely with experienced data engineers and quickly grow and expand your skillset The ability to work closely with all sizes of companies, ranging from Fortune 100 to small local businesses A workplace where you are encouraged to challenge the status quo and develop new technologies, methodologies, and processes A diverse team consisting of data gurus, experience seekers, and entrepreneurial minds that are always pushing to be better Lovelytics is an Equal Opportunity Employer. This means you don’t have to worry about whether your application process will be fair. We consider all applicants without regard to race, color, religion, age, ancestry, ethnicity, gender, gender identity, gender expression, sexual orientation, veteran status, or disability. US Salary for this position is $125,000-160,000, however, actual salary is based on a number of various factors including skillset, experience, credentials, etc. Powered by JazzHR wq9ArCu5To",
        "url": "https://www.linkedin.com/jobs/view/3837698174"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Sterling, VA",
        "job_id": 3955653811,
        "company": "Molg",
        "title": "Backend Software Engineer",
        "created_on": 1720638256.4376307,
        "description": "Hours: Full-time Location: Molg HQ in Sterling, VA (Northern Virginia) Salary: $100-120k range, including 401k, equity, and full healthcare benefits OUR MISSION Tackle the fast growing e-waste problem by making electronics manufacturing circular. Molg builds robotics microfactories and software to autonomously assemble and disassemble complex electronic products like laptop PCs, servers, and battery packs. Working with some of the leading computer + server manufacturers as well as industrial companies like Stanley Black and Decker, we are building the circular manufacturing technology to recover existing in-market devices for reuse and recycling as well as helping develop the next generation of circular-focused devices at the design level. IN THIS ROLE YOU WILL: Work with a talented cross-functional team of software, mechanical, electrical, and robotics engineers to develop our core technologies in assembly intelligence from CAD and 3D models as it relates to our robotics and microfactories. As a Software Engineer, you will be responsible for: Design, develop, and maintain scalable and resilient microservices that power Molg's robotic systems. Architect, implement, and manage containerized applications using Docker or similar technologies. Build and maintain APIs to facilitate seamless communication between different components of our systems, ensuring high performance and availability. Collaborate with cross-functional teams including frontend engineers, data scientists, and product managers to integrate new features and improve existing functionality. Optimize system performance and ensure high reliability and availability of backend services. Implement best practices for software development, including code reviews, automated testing, and continuous integration/continuous deployment (CI/CD). Troubleshoot and resolve complex technical issues across the backend infrastructure. WHO YOU ARE: 3+ years of industry experience Proficient in modern software development (e.g. python, ROS, github, docker, etc) Passionate about robotics, automation, and control systems Ability to communicate effectively and efficiently both verbally and in writing WHO WE ARE: We spend our days building robotic systems, developing complex assembly intelligence software, and designing the next generation of circular products for our customers. Given the importance of working hands-on with physical systems, we are a 100% in-person team collaboratively working in our industrial space in Chantilly, VA, down the road from the largest data center market in the world. Our facility includes a variety of robots, CNC milling machines, 3D printers, and all the tools needed to build and test our products. It is important to us that anyone on our team that is interested in learning how to use our various pieces of equipment and machinery is taught and can gain the skills and appreciation for making physical things. While we are primarily funded by our work with Fortune 100 companies, we are also supported by amazing backers like Elemental Excelerator and Techstars. THINGS TO KNOW: We’re a small collaborative team with big ambitions, and there’s a good amount of context-switching. We expect people to be autonomous and drive their own work to completion. We are a profitable business that is primarily funded from customer revenue, which means we are scrappy and looking to build a great sustainable company for years to come. As a growing company and startup, priorities may shift as customer or business requirements change. We strive to empower individuals with context and decision-making power to meet this need.",
        "url": "https://www.linkedin.com/jobs/view/3955653811"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "McLean, VA",
        "job_id": 3936963469,
        "company": "BYTE Systems, LLC",
        "title": "Software Engineer",
        "created_on": 1720638258.1848555,
        "description": "Candidate MUST possess a TS/SCI clearance with Intel Polygraph Required Skills And Demonstrated Experience \" Demonstrated experience developing custom web applications tailored to mission needs. \" Demonstrated experience working within Linux environments. \" Demonstrated experience deploying applications using Amazon web services, cloud infrastructure to include EC2, ECR, RDS, Lambda, S3, Cloud Formation and KMS. \" Demonstrated experience with Python for data extraction and transformation. \" Demonstrated experience using Javascript and CSS for application development. \" Demonstrated experience with modern web development frameworks including Angular and Node.js. \" Demonstrated experience using REST web services. \" Demonstrated experience in deploying and maintaining high-availability for relational and document-based solutions such as PostgreSQL and Elastic Search. \" Demonstrated experience successfully building and deploying applications to serve and process large volumes of data. \" Demonstrated experience in data governance best practices on data sourcing and data compliance management. Highly Desired Skills And Demonstrated Experience \" Demonstrated experience managing network topologies (VPNs, subnets), traffic routing, and firewalls. \" Demonstrated experience performing database administration to include standing up new databases, configuration, user administration, and scripts to backup, restore dev, test, and prod database instances. \" Demonstrated experience with the Sponsors organization or mission. \" Demonstrated experience in Sponsors enterprise capabilities and how to utilize them in delivering modern technology solutions. \" Demonstrated experience interacting with Sponsors authentication services and PKI certificates. \" Demonstrated experience with Sponsors A&A process and supporting systems. \" Demonstrated experience addressing big data problem sets such as entity resolution, text analytics, pattern of life, and social network analysis. \" Bachelors degree in Computer Science.MUST be a US Citizen with a U.S. Government clearance - Intel with Polygraph NOTE: Must have an active TS-SCI with poly. No sponsorships or upgrades are available. Submissions without this requirement will not be considered. H1-B holders will not be considered. Benefits 5 week paid vacation + 10 gov't holidays 15% contribution to 401k LTD, STD disability and life insurance Paid health, dental, and vision for employee and family. $5000 annual training expense reimbursement Computer purchase plan",
        "url": "https://www.linkedin.com/jobs/view/3936963469"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Washington, DC",
        "job_id": 3873034050,
        "company": "Meta",
        "title": "Security Engineer, Investigations - i3",
        "created_on": 1720638259.8755343,
        "description": "The Integrity, Investigations, and Intelligence (i3) teams at Meta are dedicated to protecting the users of our family of applications (e.g. Facebook, Instagram, WhatsApp, Oculus) from a multitude of threats including: criminal organizations, human trafficking and exploitation, and scams/fraud. We are seeking security engineers to investigate sophisticated threat actors, advance investigative methods, automate and mature team processes through tooling, and use innovative approaches to protecting people from offline harm. You will have the opportunity to work on some of the most challenging, complicated, and high-visibility risks the company is facing. The ideal candidate will be an innovative self-starter, who is motivated by our mission and results-driven, and will be able to extract, assimilate, and correlate a wide variety of data in order to surface and disrupt threat actors across multiple spaces. Successful candidates must have a proficiency in scripting languages such as PHP or Python and be comfortable with various technical investigative skills. Security Engineer, Investigations - i3 Responsibilities: Proactively hunt for threats and undetected abuse by leveraging internal data, open-source intelligence, and third party private intelligence. Investigate complex cases to understand how abuse is occurring and implement enforcement strategies to mitigate and/or detect harm. Identify areas to automate investigative tooling and tradecraft. Analyze and interpret large datasets to advance investigations, quantify trends or support findings. Implement effective strategies to prevent and disrupt abuse at scale. Address sensitive content issues, including but not limited to graphic images, videos and writings, offensive or derogatory language, and other objectionable material. This role involves exposure to graphic and/or objectionable content including but not limited to graphic images, videos and writings, offensive or derogatory language, and other potential objectionable material, i.e. child exploitation, graphic violence, self-injury, animal abuse, and other content which may be considered offensive or disturbing. Minimum Qualifications: 3+ years work experience in cyber security, private sector security investigations and intelligence, government, and/or intelligence. Experience working with Python, PHP or similar scripting languages. Proven experience conducting large scale data analysis and utilizing big data tools such as Maltego, Palantir, Tableau, etc. Track record of effective communication and presenting technical findings. Experience investigating and acting on high-impact threats. A combination of investigative skills and engineering skills. Preferred Qualifications: Experience working or managing projects that have enterprise-wide impact and/or multi-organization cross functional stakeholders. Subject matter expertise with either criminal organizations, child safety, human exploitation, or fraud/scams. Familiarity with sophisticated cyber threats. Experience with open source investigation techniques and familiarity with a variety of internet research tools. Experience working with a team spanning multiple locations/time zones. Experience to work within a fast-paced environment where priorities shift and change. Experience prioritizing and executing with minimal direction or oversight. BS/MS or equivalent experience in Computer Science, Information Systems, Intelligence Studies, Cybersecurity or related field. About Meta: Meta builds technologies that help people connect, find communities, and grow businesses. When Facebook launched in 2004, it changed the way people connect. Apps like Messenger, Instagram and WhatsApp further empowered billions around the world. Now, Meta is moving beyond 2D screens toward immersive experiences like augmented and virtual reality to help build the next evolution in social technology. People who choose to build their careers by building with us at Meta help shape a future that will take us beyond what digital connection makes possible today—beyond the constraints of screens, the limits of distance, and even the rules of physics. Meta is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Meta participates in the E-Verify program in certain locations, as required by law. Please note that Meta may leverage artificial intelligence and machine learning technologies in connection with applications for employment. Meta is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com. $105,000/year to $173,000/year + bonus + equity + benefits Individual compensation is determined by skills, qualifications, experience, and location. Compensation details listed in this posting reflect the base hourly rate, monthly rate, or annual salary only, and do not include bonus, equity or sales incentives, if applicable. In addition to base compensation, Meta offers benefits. Learn more about  benefits  at Meta.",
        "url": "https://www.linkedin.com/jobs/view/3873034050"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Annapolis Junction, MD",
        "job_id": 3787914145,
        "company": "Columbia Technology Partners",
        "title": "Software Engineer 0",
        "created_on": 1720638263.6620395,
        "description": "Description: The Software Engineer develops, maintains, and enhances complex and diverse software systems (e.g., processing-intensive analytics, novel algorithm development, manipulation of extremely large data sets, real-time systems, and business management information systems) based upon documented requirements. Works individually or as part of a team. Reviews and tests software components for adherence to the design requirements and documents test results. Resolves software problem reports. Utilizes software development and software design methodologies appropriate to the development environment. Provides specific input to the software components of system design to include hardware/software trade-offs, software reuse, use of Commercial Off-the-shelf (COTS)/Government Off-the-shelf (GOTS) in place of new development, and requirements analysis and synthesis from system level to individual software components. Qualifications: An active TS/SCI clearance + FS polygraph No demonstrated experience is required. Bachelors degree in Computer Science or related discipline from an accredited college or university is required; Four (4) years of SWE experience on projects with similar software processes may be substituted for a bachelors degree Columbia Technology Partners is an Equal Opportunity Employer. We consider applicants without regard to race, color, religion, age, national origin, ancestry, ethnicity, gender, gender identity, gender expression, sexual orientation, marital status, veteran status, disability, genetic information, citizenship status, or membership in any other group protected by federal, state, or local law. Our EEO policy reflects our commitment to ensure equality and promote diversity and inclusion in the workplace. Our policy applies to all employees, job candidates, contractors, stakeholders, partners, and visitors. Powered by JazzHR nEbOhnPqIE",
        "url": "https://www.linkedin.com/jobs/view/3787914145"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Chevy Chase, MD",
        "job_id": 3947461275,
        "company": "GEICO",
        "title": "Senior Staff Engineer – Finance Data - Remote",
        "created_on": 1720638265.4500046,
        "description": "Senior Staff Engineer – Finance Data Position Summary It is an exciting time to be at GEICO as we transform the insurance business and build a tech organization with engineering excellence as its mission, with a culture of psychological safety and continuous improvement. We are seeking experienced engineers with a passion for building high-performance, low maintenance, and zero-downtime systems to help us build a Finance Data Lake from the ground up. Position Description Our Senior Staff Engineer works with our Staff and Sr. Engineers to innovate and build new systems, improve and enhance existing systems and identify new opportunities to apply your knowledge to solve critical problems. You will lead the strategy and execution of a technical roadmap that will increase the velocity of delivering products and unlock new engineering capabilities. The ideal candidate has deep technical expertise. Position Responsibilities As a Senior Staff Engineer, you will: Define the engineering direction and roadmap for the new Finance Data Lake that aligns with business goals. Lead the development of our new Finance Data Lake meeting goals for security, resilience, compliance and correctness. Provide technical guidance and mentorship to team members, sharing knowledge and best practices to foster a culture of constant learning. Hold a high bar on technical decision making, and setting a standard for engineering artifacts that the team creates, such as code and design docs. Stay up-to-date with industry trends and emerging technologies, evaluating their potential impact on the payments engineering domain and making recommendations for adoption. Triage and resolve service disruptions, and build industry standard mechanism for prevention and self-healing. Influence and educate leadership at all levels. Qualifications Exemplary ability to design and develop, perform experiments, and influence engineering direction and product roadmap Advanced experience developing new and enhancing existing data processing (data ingestion, data transformation, data storage, data management, data quality) components Understanding of data warehouse concepts including data modeling and OLAP Experience working with cloud data solutions (Delta Lake, Iceberg, Hudi, Snowflake, Redshift, or equivalent) Experience with data formats such as Delta, Parquet, Avro, ORC, XML, JSON Experience working streaming applications (Spark Streaming, Flink, Kafka, or equivalent) Data processing/data transformation using ETL/ELT tools such as DBT (Data Build Tool), or Databricks Understanding of machine learning Strong working knowledge of SQL and the ability to write, debug and optimize SQL queries and ETL jobs to reduce the execution window or reduce resource utilization Experience with cloud computing (AWS, Microsoft Azure, Google Cloud, Hybrid Cloud, or equivalent) Experience with messaging such as Kafka, ActiveMQ, RabbitMQ, or similar messaging technologies Expertise developing systems that are scalable, resilient, and highly available Knowledge of developer tooling across the software development life cycle (task management, source code, building, deployment, operations, real-time communication) Experience 10+ years of professional experience in data software development, programming languages and developing with big data technologies 8+ years of experience with architecture and design 6+ years of experience in open-source frameworks 4+ years of experience with AWS, GCP, Azure, or another cloud service Education Bachelor’s degree in Computer Science, Information Systems, or equivalent education or work experience Annual Salary $120,000.00 - $261,500.00 The above annual salary range is a general guideline. Multiple factors are taken into consideration to arrive at the final hourly rate/ annual salary to be offered to the selected candidate. Factors include, but are not limited to, the scope and responsibilities of the role, the selected candidate’s work experience, education and training, the work location as well as market and business considerations. GEICO will consider sponsoring a new qualified applicant for employment authorization for this position. Benefits: As an Associate, you’ll enjoy our Total Rewards Program* to help secure your financial future and preserve your health and well-being, including: Premier Medical, Dental and Vision Insurance with no waiting period** Paid Vacation, Sick and Parental Leave 401(k) Plan Tuition Reimbursement Paid Training and Licensures Benefits may be different by location. Benefit eligibility requirements vary and may include length of service. Coverage begins on the date of hire. Must enroll in New Hire Benefits within 30 days of the date of hire for coverage to take effect. The equal employment opportunity policy of the GEICO Companies provides for a fair and equal employment opportunity for all associates and job applicants regardless of race, color, religious creed, national origin, ancestry, age, gender, pregnancy, sexual orientation, gender identity, marital status, familial status, disability or genetic information, in compliance with applicable federal, state and local law. GEICO hires and promotes individuals solely on the basis of their qualifications for the job to be filled. GEICO reasonably accommodates qualified individuals with disabilities to enable them to receive equal employment opportunity and/or perform the essential functions of the job, unless the accommodation would impose an undue hardship to the Company. This applies to all applicants and associates. GEICO also provides a work environment in which each associate is able to be productive and work to the best of their ability. We do not condone or tolerate an atmosphere of intimidation or harassment. We expect and require the cooperation of all associates in maintaining an atmosphere free from discrimination and harassment with mutual respect by and for all associates and applicants.",
        "url": "https://www.linkedin.com/jobs/view/3947461275"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Herndon, VA",
        "job_id": 3823428365,
        "company": "Acclaim Technical Services",
        "title": "Software Engineer - Powershell and Windows Batch Commands (2024-0046)",
        "created_on": 1720638269.135878,
        "description": "Acclaim Technical Services, founded in 2000, is a leading language, operations, and technology services company supporting a wide range of U.S. Federal agencies. We are an Employee Stock Ownership Plan (ESOP) company, which is uncommon within our business sector. We see this as a significant strength, and it shows: ATS is consistently ranked as a top workplace among DC area firms and continues to grow. We are actively hiring a Software Engineer - Powershell and Windows Batch Commands with TS/SCI clearance and polygraph to join our Mission Technology Division working in Herndon, VA. We are looking for experienced Software Developers to join our technology-based program supporting a key Government customer Work with a team engaged in providing technical support in a mission critical Windows-based environment to users world-wide Ensure that applications are compatible with the architecture and aligns to the requirements Analyze requirements, understand the architecture and troubleshoot other data processing problems for applications Engineering (Scripting), Packaging, Testing, maintaining, and monitor applications and their operating systems, including coordinating the installation of the applications MECM application staging and distribution or content knowledge Provide technical guidance to users Proven ability to test, debug and refine software to produce the required product, reduce operating time or improve efficiency RESPONSIBILITIES/ REQUIRED EDUCATION & EXPERIENCE: Must possess an active TS/SCI clearance with polygraph A bachelor’s or master’s degrees in computer science are preferred with a minimum of six (6) years' experience Software Engineering Remote User Support Windows Server 2016 or higher Microsoft Enterprise Configuration Management (MECM) PowerShell Batch/CMD MSI (Windows Installer) Scripting knowledge (PowerShell and Windows Batch commands), SCCM, MECM, Active Directory Tier 3 support- troubleshooting and providing technical resolution for escalated requests Knowledge of software installation of programs, coding and/ scripting skills Proven ability to act resourcefully with the willingness and ability to take initiative Preferred Skills: Windows (MCSA, MCSE) Equal Employment Opportunity / Affirmative Action ATS is committed to a program of equal employment opportunity without regard to race, color, ethnicity, national origin, ancestry, citizenship, sex, pregnancy, marital status, sexual orientation, gender identity, age, religion/creed, hairstyles and hair textures, handicap/disability, genetic information/history, military/veteran status, or any other characteristic or condition protected by federal, state or local law. It is the policy of ATS not merely to refrain from employment discrimination as required by the various federal, state, and local enactments, but to take positive affirmative action to realize for women, people of color, individuals with disabilities and protected veterans full equal employment opportunity. We support the employment and advancement in employment of individuals with disabilities and of protected veterans, and we treat qualified individuals without discrimination on the basis of their physical or mental disability or veteran status. Powered by JazzHR BXo3ZNJ3FB",
        "url": "https://www.linkedin.com/jobs/view/3823428365"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Columbia, MD",
        "job_id": 3911004457,
        "company": "BigBear.ai",
        "title": "Database Engineer",
        "created_on": 1720638270.928686,
        "description": "Overview BigBear.ai is seeking a Database Engineer to support an Authoritative Data and Data Workflow team. This project will be performed full time at a customer site in Laurel. Customer specific polygraph required. What You Will Do Design, build and maintain databases using SQL, PL/SQL, OracleSQL and AWS RDS What You Need To Have A bachelor’s degree and five (5) years' experience as a Database Engineer in programs and contracts of similar scope, type and complexity is required. Ten (10) years of experience required In lieu of bachelor’s degree. Clearance: must possess and maintain a TS with Polygraph clearance Oracle SQL experience and knowledge in areas such as creating tables, SQL queries using unions, joins (inner/outer), complex queries and views Familiarity with Oracle Row Level Security Experience with data ETL What We'd Like You To Have Familiarity with AWS RDS Familiarity with database administrative tasks such as backups, database migrations and cloning and restoring (preferably using AWS RDS) Knowledge of PL/SQL Familiarity with SQL Developer session/sql monitoring tools Experience with the Atlassian Tool Suite Experience working with Git for configuration management Experience working with the JSON format About BigBear.ai BigBear.ai delivers AI-powered analytics and cyber engineering solutions to support mission-critical operations and decision-making in complex, real-world environments. BigBear.ai’s customers, which include the US Intelligence Community, Department of Defense, the US Federal Government, as well as customers in manufacturing, healthcare, commercial space, and other sectors, rely on BigBear.ai’s solutions to see and shape their world through reliable, predictive insights and goal-oriented advice. Headquartered in Columbia, Maryland, BigBear.ai is a global, public company traded on the NYSE under the symbol BBAI. For more information, please visit: http://bigbear.ai/ and follow BigBear.ai on Twitter: @BigBearai.",
        "url": "https://www.linkedin.com/jobs/view/3911004457"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Chantilly, VA",
        "job_id": 3787761899,
        "company": "SOLVARUS",
        "title": "System Engineer",
        "created_on": 1720638272.5861967,
        "description": "Are you looking for an opportunity to make a difference in the government space? What if you could find a position that is tailor-made for your mix of development, engineering, and communication skills? That is why we need you to apply your ingenuity and bring your innovative ideas to a consulting advisory services company. Solvarus Consulting has been dedicated to solving the toughest challenges by providing support to the Intelligence Community and Department of Defense. Help us shorten the time it takes to get critical tools developed and into the hands of warfighters who need them and to quickly aid the people who need it. This component of the Space Program provides Enterprise Systems Engineering and Integration capabilities to a national intelligence acquisition and operations organization. The program supports full lifecycle acquisition across all mission domains of a complex national systems architecture. Duties include: Provide system end-to-end closure of requirements baseline for Space Indications and Warning (I&W) programs and related activities Follow readiness processes for Integration and Closure (I&C) Coordinate with all stakeholders across the enterprise including action tracking and follow-up Provide support to system integration of new initiatives for expanded mission capabilities Assess requirements, Concept of Operations (CONOPs), architecture products, system schedules, and risks. Be the Systems Engineering Technical Advisor (SETA) ‘trusted agent’ for the customer in presenting the assessments to the customer and providing recommendations for way ahead to fulfill the mission. Qualifications: Requires 10 to 12 years with BS/BA or 8 to 10 years with MS/MA or 5 to 7 years with PhD. TS/SCI with Poly. Experience with Intelligence Community (IC) customer or Department of Defense (DoD). Knowledge of Ground systems and overall space and ground architecture. Awareness of mission technical capabilities and system performance analysis and assessment. Experience with Mission Planning, Control or Processing functions in support of space and ground architecture. Experience identifying user & enterprise needs/requirements and gap identification, analysis, and assessment. Experience in requirements engineering, CONOPS development, verification, test, and system integration to accommodate End to End Systems Engineering in support of customer's missions. Proven adaptability and experience with daily interaction on-site with the government customers. Proven adaptability and experience with daily interaction on-site with the government customers. Knowledge of Ground or Space systems. Experience with Indications & Warning (I&W) systems. Experience with DOORs requirements database management/repository. Experience in Rhapsody and SysML applications, Confluence, Jira. A passion for learning, helping others, sharing team knowledge, and contributing value to the mission in any way possible. Desired Skills: Experience with large acquisition systems engineering efforts, methods, and processes. Ability to work in a fast paced, dynamic environment. Good oral and written communication skills; MS Office proficient. Good business acumen: ability to relate professionally inside the work environment. Organized and flexible. Solvarus employment relationships are made without regard to age, race, color, religion, creed, sex, national origin, marital status, veteran status, the presence of any physical or mental disability, genetic information, or any other status or characteristic protected by federal, state, or local law. Discrimination or harassment based upon any of these factors is wholly inconsistent with our company values and will not be tolerated. Powered by JazzHR K8HvbijCA1",
        "url": "https://www.linkedin.com/jobs/view/3787761899"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Washington DC-Baltimore Area",
        "job_id": 3965716946,
        "company": "Armis",
        "title": "Software Engineer Backend & Data",
        "created_on": 1720638274.4941847,
        "description": "Armis is looking for a few of the very best people in their field to join our A-team of big thinkers, doers, movers, and shakers. This unique opportunity truly offers the best of all worlds—start up culture, enterprise level benefits and security, and top pay for the industry. Got your attention yet? Good, keep reading, it only gets better. Ok, so what exactly does Armis do? Connected assets are growing at an explosive rate, across every industry and every geo. In today’s world of smart devices and BYOD, these assets come from every direction and are found in every possible environment and industry. And they’re critical to success for every enterprise. And because every single asset represents a very real potential vulnerability, they’re also the last line of defense against today’s sophisticated cyber criminals. Armis gives companies of every size—across every industry and geo—complete asset visibility, contextual intelligence, and continuous security. We have partnerships and integrations with the planet’s leading tech and cybersecurity players. And we’re building an incredibly smart and diverse global team of thought-leading technologists, creative visionaries and proven game changers who are ready to take Armis to the next level. Location: This is a 100% remote position and we are considering candidates from any major city EST; CST or MST timeZones Overview We are seeking a highly skilled and motivated Software (Data oriented) Engineer to join our dynamic team. This role will sit within the Silk (by Armis) team, our exciting new acquisition, that continues to grow and delight customers. Silk (by Armis) is tackling and solving the biggest pain points in the cyber security industry today- prioritization of vulnerabilities and managing the resolution process - all while taking a very novel and revolutionary approach to these daunting problems. You will play a critical role in designing, developing, and maintaining our data infrastructure to support the analysis and processing of large-scale data sets (and we mean LARGE ). This position offers an exciting opportunity to work at the forefront of cybersecurity, leveraging cutting-edge technologies and methodologies to drive actionable insights from diverse data sources. What You’ll Do Design, build, and maintain scalable data pipelines and infrastructure to support the collection, processing, and analysis of large volumes of data. Develop robust ETL processes to extract, transform, and load data from various sources into our data warehouse. Collaborate with cross-functional teams to understand data requirements and implement solutions to address business needs. Optimize data processing workflows for performance, reliability, and scalability. Implement data quality monitoring and validation processes to ensure accuracy and consistency of data. Work closely with software engineers to integrate data-driven features and functionalities into our products and services. Stay abreast of emerging technologies and best practices in data engineering, and propose innovative solutions to enhance our data infrastructure. What We Expect 5+ years experience as a Software Engineer or similar role, with a focus on building data pipelines and infrastructure. Proficiency in Python programming and experience with relevant libraries and frameworks for data processing (e.g. Pydantic, MongoDB, FastAPI, Redis, Pandas, NumPy, Spark). 3+ years experience with MongoDB and larger data sets i.e. Terabyte's Strong understanding of database systems, with experience in designing and optimizing queries. Hands-on experience with cloud platforms, particularly AWS (Amazon Web Services), and familiarity with services such as S3, ECS, SQS. Experience working with large-scale distributed systems and parallel processing frameworks. Solid understanding of data modeling concepts and techniques. Excellent problem-solving skills and attention to detail. Strong communication and collaboration skills, with the ability to work effectively in a team environment. Salary range guidance for this position is: $163,000- $210,00.00 The salary range listed does not include other forms of compensation or benefits (e.g. i.e. stipend for transit/parking, bonuses, commissions, stocks, health insurance benefits, etc.) offered to candidates. Visit our careers site for more information on benefits at Armis. The choices you make in your career journey matter. You want to do interesting work in an important field while also having time to live your life, which is why we place so much value in your life-work balance. Armis sets you up for success with comprehensive health benefits, discretionary time off, paid holidays including monthly me days, and a highly inclusive and diverse workplace. Put your unique experiences and perspective to work in an environment where they will enable you to thrive, grow, and live your life with integrity. Armis is proud to be an equal opportunity employer. We never discriminate based on race, ethnicity, color, ancestry, national origin, religion, sex, sexual orientation, gender identity, age, disability, veteran status, genetic information, marital status or any other legally protected (or not) status. In compliance with federal law, all persons hired will be required to submit satisfactory proof of identity and legal authorization. Please click here to review our privacy practices.",
        "url": "https://www.linkedin.com/jobs/view/3965716946"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Washington, DC",
        "job_id": 3970181483,
        "company": "ZipRecruiter",
        "title": "Software Development Engineer, AWS Cross Domain Services",
        "created_on": 1720638276.454954,
        "description": "Job Description Join us in building and developing novel solutions to solve big data problems and provide analytic visibility into the AWS Cross Domain Solutions (CDS) workflow. This position requires that the candidate selected be a US and obtain and maintain an active TS/SCI security clearance with polygraph. 10012 Key job responsibilities Amazon Web Services (AWS) Is Seeking a Talented Software Development Engineer To Design And Build An Analytic Platform To Solve Problems And Provide Analytic Visualization Of Operational Data. As a Software Development Engineer On The Pattern Team Within AWS Cross Domain Services (CDS), You Will Work with Data Scientists to develop, train, and validate machine learning models using techniques like clustering, classification, and neural networks for anomaly detection, while evaluating model performance. Build and maintain data pipelines, perform data cleaning and transformation, leverage programming like Python and SQL, and work with databases and data warehouses to enable effective data engineering for anomaly detection. Design and implement scalable data processing and analysis solutions using big data frameworks to handle large datasets and support robust anomaly detection capabilities. Have questions about this role? Start a chat with the recruiter today! Please reach out to Krystan Silva at skrystan@amazon.com for inquiries. About The Team We're a small, independent team within AWS Cross Domain Solutions (CDS) working on providing visibility into the CDS workflow. Our team works with big data to solve the analytical needs of CDS. We need Developers who move fast, are capable of breaking down and solving complex problems, and have a strong will to get things done. We are open to hiring candidates to work out of one of the following locations: Arlington, VA, USA BASIC QUALIFICATIONS- 3+ years of non- professional software development experience 3+ years of non- design or architecture (design patterns, reliability and scaling) of new and existing systems experience Experience programming with at least one software programming language Bachelor's Degree in Computer Science, Mathematics or a related engineering field, or equivalent years of experience in software development QUALIFICATIONS- 5+ years of full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations experience Experience in machine learning, data mining, information retrieval, statistics or natural language processing Experience building complex software systems that have been successfully delivered to customers Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of , , , , sexual , protected veteran status, , , or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en//us.",
        "url": "https://www.linkedin.com/jobs/view/3970181483"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Centreville, VA",
        "job_id": 3914465181,
        "company": "Motion Recruitment",
        "title": "Data Engineer (Top Secret)",
        "created_on": 1720638278.1013765,
        "description": "A leading provider of technical infrastructures is seeking a Data Engineer (ETL-focused), candidates must hold an active Top Secret. In this role, you would support customers to provide systems engineering, software development, and services to enable advanced data discovery. This role would be 5-days on-site. Required Skills US Citizen who holds an active Top Secret or may qualify for a security clearance. Deep knowledge of Python, Java, and SQL. Development and maintenance of back end ETL processes with Informatic and SQL scripts. Strong background in database management solutions, familiarity with databases such as MySQL and Oracle. Experience with cloud computing environment AWS (EC2, VPC, CloudWatch). Responsibilities Active Top Secret Clearance or higher. 8+ years of technical experience. 6+ years with SQL and Python. At least six years with Oracle. Applicants must be currently authorized to work in the US on a full-time basis now and in the future. LI#-DP1 Posted By: Derek Progin",
        "url": "https://www.linkedin.com/jobs/view/3914465181"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Laurel, MD",
        "job_id": 3791740976,
        "company": "SITEC Consulting, LLC.",
        "title": "Microsoft SQL Server Database Engineer",
        "created_on": 1720638279.9335456,
        "description": "About Sitec SITEC is an employee and customer focused Information Technology and Professional Services Firm specializing in design, development, and delivery of state-of-the-art technology solutions, as well as cybersecurity, software and systems engineering services. Summary As a Database Administrator, collaborate with a dynamic and forward-thinking team, contributing to the development, testing, and deployment of a SharePoint 2019 farm for the Enterprise. Assume a pivotal role in leading the customer's migration journey from SharePoint 2013 to SharePoint 2019, with an intermediate stop at SharePoint 2016, while integrating a tailored Attribute Based Access Control (ABAC) solution to fulfill distinctive mission requirements. Engage with team members, such as system administrators, developers, and stakeholders, to discuss project requirements, exchange ideas, and strategize development tasks, with your insights shaping initiatives. Assist in the ongoing maintenance and troubleshooting of the database, monitor performance, diagnose issues, and promptly apply necessary patches and updates. Actively participate in the testing of solutions to ensure their functionality, performance, and compatibility, ensuring a seamless user experience. Demonstrate experience in adding, attaching, detaching, moving, and renaming content databases and service application databases in SharePoint Server. Manage Read-Only databases and Resource-Based Sensitivity (RBS) in SharePoint Server, optimizing performance and data management. Must Be Incredibly Proficient With SharePoint Atlassian Tool Suite C# .NET Framework NET SQL Server Desired Skills DoD 8570 compliance with Information Assurance Technical (IAT) Level II Experience with workflow designer tools Experience with PowerShell scripting language Experience in SharePoint migrations Ability to clearly communicate ambiguous or highly complex concepts to all audiences. Ability to understand the big picture and align the day-to-day activities to support that picture Experience recovering lost information, patching systems with updates, establishing work templates, cataloging business data, administering Web portal maintenance and upgrading SharePoint-related servers. Experience installing SharePoint-related applications, and keeping track of available information space Good communication skills to explain how to use established SharePoint databases. Qualifications Active DoD TS/SCI Security Clearance with poly required. A bachelor's degree in a technical discipline and ten (10) years of experience as a DBE. Fourteen (14) years of experience as a DBE may be substituted in lieu of a degree. Proficiency in managing SQL Server Databases Must be able to work during core hours of M-F 10am-2pm. Exceptions will be considered on a case-by-case basis. How To Apply Qualified candidates will be invited to phone screen with a recruiter and then a formal interview will follow up after if deemed necessary. Job Posted by ApplicantPro",
        "url": "https://www.linkedin.com/jobs/view/3791740976"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Annapolis Junction, MD",
        "job_id": 3733542725,
        "company": "SageCor Solutions",
        "title": "Systems Engineer (EPI - 009)",
        "created_on": 1720638281.6212506,
        "description": "Serving Maryland and the Greater Washington D.C. area, SageCor Solutions (SageCor) is a growing company bringing complete engineering services and true full lifecycle System Engineering services to areas requiring (or desiring) nationally-recognized expertise in high performance computing, large data analytics and cutting edge information technologies. Active TS/SCI W/ Polygraph Required Experience Requirements: Hardware Systems Engineer with a focus on architecture and data modeling documentation. Should have experience with High Performance Computing Application – Special Integrated Circuit requirements, design, test, deployment, tuning, and optimization, including board design, performance analysis, and system integration. Experience developing requirements, design, development and integration of Devices. Experience with large program DoD/IC engineering, design, test, deployment, operations and sustainment milestones and review (ie PDR, CDR, IOT&E, DOT&E, EOA, IOC, FOC, T2Ops). Strong skills in customer support and teaming requirements extraction, teaming with vendors/ industry, and building relationships in support of the USG. Qualifications must include: BS degree in Electrical Engineering, Computer Engineering or a related subject. Proven working experience in installing, configuring, and troubleshooting Unix/Linux based environments. Solid network knowledge (OSI network layers, TCP/IP). SageCor Solutions provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or veteran status, or any other protected class.",
        "url": "https://www.linkedin.com/jobs/view/3733542725"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Arlington, VA",
        "job_id": 3938300245,
        "company": "Lovelytics",
        "title": "Lead Consultant, Data Engineer (Level III)",
        "created_on": 1720638283.380871,
        "description": "Lovelytics is seeking a Sr. Data Engineer (Lead Consultant) with experience delivering strategic Databricks client engagements to join our Data & AI practice! This Lead Consultant will play a key role in delivering on client engagements related to data warehousing, ETL development, data integrations, and data modeling. This is a client-facing role, focused on using and migrating to our partner technologies; Databricks, AWS, and Azure to name a few. In addition to the technical capabilities for this role, we are looking for someone who wants to work in a collaborative, dynamic, and inclusive environment and has a passion for bringing meaning to data. Role Location: Open to remote candidates in the US in the following states: MD, DC, CA, IL, IA, IN, MA, NC, TX, TN, GA, CO, NY, NJ, VA, FL, PA, OH, OR as well as Ontario, Canada This role is not open for work sponsorship at this time. Primary Responsibilities: Utilize consulting and technical skills to be able to work in a client-facing project environment independently Be responsible for your own execution and sometimes lead individual work streams on client engagements as assigned and under supervision of engagement lead Collaborate with other team members to successfully deliver on projects Work effectively and directly communicate with both internal and client and/or partner teams Develop full ownership of your execution on client engagements, you'll become involved in the project planning and solution stages of engagements as well Design and implement complex ETL/ELT pipelines with evidence of improved data processing times Successfully lead small data warehousing projects with measurable performance enhancements under the management of an engagement lead Contribute to real-time data processing solutions and managed streaming data Implement security and compliance measures for data pipelines Design and implement version control and branching strategies and integrate them into CI/CD for promoting and testing in higher environments Our Ideal Candidate's Skills and Experiences: B.S. in Computer Science or equivalent 3-5 years' experience in data engineering and big data. At least 2 years working directly with clients and external stakeholders Extensive knowledge of data warehousing concepts and hands-on experience deploying pipelines using Databricks **A must Data modeling and database design skills and knowledge of version control Excellent verbal and written communication skills Is able to apply technical skills to engagement needs Works with engagement leads and directors to gain exposure in the design and architecture of solutions Understands and utilizes Lovelytics tools and client tools What We Promise You: Exciting projects with great clients in varying departments and verticals across the world The ability to work closely with experienced data engineers and quickly grow and expand your skillset The ability to work closely with all sizes of companies, ranging from Fortune 100 to small local businesses A workplace where you are encouraged to challenge the status quo and develop new technologies, methodologies, and processes A diverse team consisting of data gurus, experience seekers, and entrepreneurial minds that are always pushing to be better Lovelytics is an Equal Opportunity Employer. This means you don’t have to worry about whether your application process will be fair. We consider all applicants without regard to race, color, religion, age, ancestry, ethnicity, gender, gender identity, gender expression, sexual orientation, veteran status, or disability. Powered by JazzHR qWvUsh9HbV",
        "url": "https://www.linkedin.com/jobs/view/3938300245"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Merrifield, VA",
        "job_id": 3970181376,
        "company": "ZipRecruiter",
        "title": "Software Development Engineer, AWS Cross Domain Services",
        "created_on": 1720638285.2533867,
        "description": "Job Description Join us in building and developing novel solutions to solve big data problems and provide analytic visibility into the AWS Cross Domain Solutions (CDS) workflow. This position requires that the candidate selected be a US and obtain and maintain an active TS/SCI security clearance with polygraph. 10012 Key job responsibilities Amazon Web Services (AWS) Is Seeking a Talented Software Development Engineer To Design And Build An Analytic Platform To Solve Problems And Provide Analytic Visualization Of Operational Data. As a Software Development Engineer On The Pattern Team Within AWS Cross Domain Services (CDS), You Will Work with Data Scientists to develop, train, and validate machine learning models using techniques like clustering, classification, and neural networks for anomaly detection, while evaluating model performance. Build and maintain data pipelines, perform data cleaning and transformation, leverage programming languages like Python and SQL, and work with databases and data warehouses to enable effective data engineering for anomaly detection. Design and implement scalable data processing and analysis solutions using big data frameworks to handle large datasets and support robust anomaly detection capabilities. Have questions about this role? Start a chat with the recruiter today! Please reach out to Krystan Silva at skrystan@amazon.com for inquiries. About The Team We're a small, independent team within AWS Cross Domain Solutions (CDS) working on providing visibility into the CDS workflow. Our team works with big data to solve the analytical needs of CDS. We need Developers who move fast, are capable of breaking down and solving complex problems, and have a strong will to get things done. We are open to hiring candidates to work out of one of the following locations: Arlington, VA, USA BASIC QUALIFICATIONS- 3+ years of non- professional software development experience 3+ years of non- design or architecture (design patterns, reliability and scaling) of new and existing systems experience Experience programming with at least one software programming Bachelor's Degree in Computer Science, Mathematics or a related engineering field, or equivalent years of experience in software development QUALIFICATIONS- 5+ years of full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations experience Experience in machine learning, data mining, information retrieval, statistics or natural processing Experience building complex software systems that have been successfully delivered to customers Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of , , gender, gender , , protected veteran status, , , or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en//us.",
        "url": "https://www.linkedin.com/jobs/view/3970181376"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Annapolis Junction, MD",
        "job_id": 3921567695,
        "company": "NiSUS Technologies Corporation",
        "title": "SR006SWE1/2 - Associate to Mid Level System Engineer - Cleared",
        "created_on": 1720638288.8440216,
        "description": "The Software Engineer develops, maintains, and enhances complex and diverse software systems (e.g., processing-intensive analytics, novel algorithm development, manipulation of extremely large data sets, real-time systems, and business management information systems) based upon documented requirements. Works individually or as part of a team. Reviews and tests software components for adherence to the design requirements and documents test results. Resolves software problem reports. Utilizes software development and software design methodologies appropriate to the development environment. Provides specific input to the software components of system design to include hardware/software trade-offs, software reuse, use of Commercial Off-the-shelf (COTS) in place of new development, and requirements analysis and synthesis from system level to individual software components. Requirements TS/SCI with poly required Seven (7) years experience in projects of similar scope, type, and complexity is required. Bachelor's degree in Computer Science or related discipline from an accredited college or university is required. Four (4) years of additional software engineering experience on projects with similar software processes may be substituted for a bachelor's degree Benefits Health & Life Insurance Dental Insurance Disability Insurance 401K Retirement Plan with Matching Tuition Assistance Vacation and Sick Leave Hiring Bonuses Referral Recruitment Program",
        "url": "https://www.linkedin.com/jobs/view/3921567695"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Fort Meade, MD",
        "job_id": 3921706929,
        "company": "SilverEdge",
        "title": "Cloud Software Engineer 2",
        "created_on": 1720638290.5769525,
        "description": "Work with a massively parallel enterprise platform, built with Java on Free and Open-Source Software products including Kubernetes, Hadoop and Accumulo, to enable execution of data-intensive analytics on a managed infrastructure. Seeking a self-motivated Java developer who proactively completes tasks with a strong attention to detail. The candidate must be able to work independently and as part of a team and will be exposed to a variety of technologies depending on customer requirements. The candidate should be familiar with the following Skilled in Java, database technologies (MySQL Postgres etc) and have some experience with Map Reduce programming model and technologies (such as Hadoop, Hive, Pig, etc) Possess additional knowledge of Linux OS development, Docker, Kubernetes, Python, and Ansible Bachelors degree in computer science or related field of study 5 years of experience in software development/engineering in programs of similar scope",
        "url": "https://www.linkedin.com/jobs/view/3921706929"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Columbia, MD",
        "job_id": 3963961158,
        "company": "Merit321, Launching Careers",
        "title": "Backend Software Engineer",
        "created_on": 1720638292.3258395,
        "description": "Position: Backend Software Engineer Location: Columbia, MD Clearance: Active Secret Clearance Summary In this role, you will have the opportunity to work with modern tools and technologies, and you will be encouraged to stay up-to-date with the latest developments in the field. You will be part of a dynamic and collaborative team that is committed to delivering high-quality software solutions to our clients. The work environment is a hybrid one, with a mix of remote and in-person work. We believe in providing our employees with a great work-life balance, so you will have the flexibility to work from home or from our office, depending on your preferences and needs. If you are a self-motivated, creative, and detail-oriented software engineer who is passionate about developing innovative software solutions, we would love to hear from you. Essential Job Responsibilities Must have a strong working knowledge and experience developing Java based software capabilities. Should have an inquisitive nature, responsiveness, and excellent testing skills. Must also possess strong troubleshooting skills and the ability to work under pressure with multiple deadlines. Will work in a fast-paced, small business environment with our talented team. Minimum Qualifications At least 10 years of experience in Software Engineering, Modern Java Frameworks and Libraries (e.g. Spring, Guava) and a Bachelors in related field; 8 years relevant experience with Masters in related field; or High School Diploma or equivalent and 14 years relevant experience. Experience in designing enterprise APIs Experience in RESTful web services Experience in Microservices architecture Experience in Object Oriented Programming (OOP) paradigms Experience with the agile software lifecycle Has a proven ability to learn quickly and works well both independently as well as in a team setting Experience with the Linux operating system Experience with configuration management tools (e.g. Git, Nexus, Maven) Must be able to work a hybrid schedule Desired Skills (Optional) Experience in cloud-based technologies (AWS, Azure) Experience in distributed databases, NoSQL databases, full text-search engines (e.g. Elasticsearch, MongoDB, Solr) Scripting experience is a huge plus. Prior experience or familiarity with our Big Data Platform is a plus. Understanding of AGILE software development methodologies and use of standard software development tool suites. (e.g., JIRA, Confluence, Github Enterprise, etc.) EEO It is the policy of Merit321 to provide equal opportunity in recruiting, hiring, training, and promoting individuals in all job categories without regard to race, color, religion, national origin, gender, age, disability, genetic information, veteran status, sexual orientation, gender identity, or any other protected class or category as may be defined by federal, state, or local laws or regulations.",
        "url": "https://www.linkedin.com/jobs/view/3963961158"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Bethesda, MD",
        "job_id": 3960765263,
        "company": "LanceSoft, Inc.",
        "title": "Software Engineer Level 4",
        "created_on": 1720638296.3525043,
        "description": "Title: Senior Software Engineer– 100% Remote role Location: Bethesda, MD 20817 Duration: 12 Months Contract role This is a part time position (estimated @ 20 hours per week), but full time would be considered. Job Details The software engineer supporting this position must have extensive experience in supporting applications in an UNIX/Linux environment. Specific skills are: Extensive experience with supporting applications that are built on an Oracle data base and hosted in a Linux Environment Extensive experience in supporting and troubleshooting PLSQL code and stored procedures Have the ability to do extensive data analysis to investigate issues with data transferring between systems and stored in Oracle Ability to utilize tool such as Toad to interact with the data base Thorough understanding of Unix/Linux commands and navigating thru the server environment Ability to debug SFTP Experience with Informatica or similar ETL tool is preferred Knowledge of PRO*C would be a plus Software Development 6 Years Advanced Oracle PLSQL 6 Years Advanced Linux application support 6 Years Advanced PRO*C 3 Years Intermediate EEO: All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran. Cell phones are not allowed at the client site, shooting of photos and/or videos are strictly prohibited while inside the client facility.",
        "url": "https://www.linkedin.com/jobs/view/3960765263"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Fort Meade, MD",
        "job_id": 3640353962,
        "company": "Capital Solutions Group (CSG)",
        "title": "AWS or Azure Cloud Software Engineer (TKOSTO-SWE-50.010923p)",
        "created_on": 1720638298.0709646,
        "description": "SECURITY CLEARANCE: TS/SCI with Polygraphs required POSITION: Azure or AWS Cloud Software Engineers and DevOps LABOR CATEGORY: Senior Software Engineer REQUISITION: TKOSTO-SWE-50.010923p LOCATION: Ft. Meade, Maryland ARE YOU experienced in Microsoft Azure and/or AWS cloud? CSG would like to speak with you! \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ LABOR CATEGORY QUALIFICATIONS TABLE for Senior Software Developers / Engineer EDUCATION And WORK EXPERIENCE Minimum Requirement Is Bachelor’s Degree + 14 years of experience, OR No Degree + 18 years of experience CSG, Inc. is an Equal Opportunity / Affirmative Action employer that values the strength of diversity in the workplace. All qualified applicants will receive consideration for employment without discrimination or harassment based on race, color, religion, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity or expression, age, disability, national origin, marital or domestic/civil partnership status, genetic information, citizenship status, veteran status, or any other characteristic protected by law.",
        "url": "https://www.linkedin.com/jobs/view/3640353962"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Annapolis Junction, MD",
        "job_id": 3850210021,
        "company": "Avid Technology Professionals",
        "title": "Software Engineer 1",
        "created_on": 1720638299.6680505,
        "description": "Qualifications Analyze user requirements to derive software design and performance requirements Design and code new software or modify existing software to add new features Debug existing software and correct defects Integrate existing software into new or modified systems or operating environments Develop simple data queries for existing or proposed databases or data repositories Provide recommendations for improving documentation and software development process standards Develop or implement algorithms to meet or exceed system performance and functional standards Assist with developing and executing test procedures for software components Write or review software and system documentation Develop software solutions by analyzing system performance standards, confer with users or system engineers; analyze systems flow, data usage and work processes; and investigate problem areas Serve as team lead at the level appropriate to the software development process being used on any particular project Modify existing software to correct errors, to adapt to new hardware, or to improve its performance Design, develop and modify software systems, using scientific analysis and mathematical models to predict and measure outcome and consequences of design Design or implement complex database or data repository interfaces/queries MANDATORY SKILLS: experience w/ Postgres & React, Git, elasticsearch, Apache Tomcat, Jackson JSON processor for Java, Jira, linux OS, Apache Maven, Jenkins, and Docker Seven (7) years experience as a SWE, in programs and contracts of similar scope, type, and complexity is required. Bachelors degree in Computer Science or related discipline from an accredited college or university is required. Four (4) years of additional SWE experience on projects with similar software processes may be substituted for a bachelors degree. About Avid Technology Professionals Avid Technology Professionals, LLC (ATP) is a premiere provider of software and systems engineering, and acquisition program management services for the community. ATP is actively seeking to pursue contract opportunities with other departments and agencies in the federal government, in state governments, and in the commercial sectors. Delivered by seasoned experts in the IT field, ATP solutions adeptly address the IT concerns manifesting in both the federal and commercial sectors. Employee Benefits The ATP Employee Benefits package includes: A Supportive and Equitable Working Environment that is both Stimulating and Challenging Competitive Hourly Salary Unique Employee Success Sharing Program that allows ATP employees to Share in Company's Successes Automatic Approved Overtime (as long as contract permits) Retirement Pay (401K); 100% company paid, immediately vested with Profit-Sharing Component Company Medical Coverage Plans - HMO, Open Access, PPO plans Company Dental Plan - widely accepted, comprehensive, and flexible Progressive Overtime Policy Flexible Spending Account benefit Lucrative Referral Bonus Policy Holiday Scheduling that Coincides with Government Holidays Robust Professional Expenses & Training Program Computer Allowance Internet Allowance Short and Long Term Disability Life Insurance",
        "url": "https://www.linkedin.com/jobs/view/3850210021"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Annapolis Junction, MD",
        "job_id": 3958008277,
        "company": "Captivation",
        "title": "Software Engineer 1 - Java/Golang/RabbitMQ/Maven/MongoDB",
        "created_on": 1720638300.8901527,
        "description": "Build to something to be proud of. Captivation has built a reputation on providing customers exactly what is needed in a timely manner. Our team of engineers take pride in what they develop and constantly innovate to provide the best solution. Captivation is looking for software developers who can get stuff done while making a difference in support of the mission to protect our country. Description Captivation Software is looking for a mid level software engineer to maintain and expand a Java based machine learning model-building system. The ideal candidate would be able to deploy the application in Kubernetes environment and independently debug issues as they arise. The candidate would be able to take an objective and develop, test, deploy, and debug a solution rapidly. This role requires a passion for new technologies and the ability to think of original solutions. Requirements Security Clearance: Must currently hold a Top Secret/SCI U.S. Government security clearance with a favorable Polygraph, therefore all candidates must be a U.S. citizen Minimum Qualifications: Seven (7) years experience as a SWE, in programs and contracts of similar scope, type, and complexity is required. Bachelor's degree in Computer Science or related discipline from an accredited college or university is required. Bachelor's degree in Computer Science or related discipline from an accredited college or university is required Four (4) years of SWE experience on projects with similar software processes may be substituted for a bachelor's degree. Required Skills: Willing to learn and develop in new technologies as required Experience developing with Java and injection frameworks Experience developing a RESTful API Other tools: Rabbit MQ, Mongodb, Maven Desired Skills: Experience developing in Golang a plus This position is open for direct hires only. We will not consider candidates from third party staffing/recruiting firms. Benefits Annual Salary: $130,000 - $270,000 (Depends on the Years of Experience) Up to 20% 401k contribution (No Matching Required and Vested from Day 1) Above Market Hourly Rates $3,200 HSA Contribution 5 Weeks Paid Time Off Company Paid Employee Medical/Dental/Vision Insurance/Life Insurance/Short-Term & Long-Term Disability/AD&D",
        "url": "https://www.linkedin.com/jobs/view/3958008277"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Annapolis Junction, MD",
        "job_id": 3922294102,
        "company": "Columbia Technology Partners",
        "title": "Cloud Software Engineer 3",
        "created_on": 1720638304.6858099,
        "description": "Description: The Cloud Software Engineer develops, maintains, and enhances complex and diverse Big-Data Cloud systems based upon documented requirements. Directly contributes to all stages of back-end processing, analyzing, and indexing. Provides expertise in Cloud Computing, Hadoop Eco-System including implementing Java applications, Distributed Computing, Information Retrieval (IR), and Object Oriented Design. Works individually or as part of a team. Reviews and tests software components for adherence to the design requirements and documents test results. Resolves software problem reports. Utilizes software development and software design methodologies appropriate to the development environment. Provides specific input to the software components of system design to include hardware/software trade-offs, software reuse, use of Commercial Off-the-shelf (COTS)/Government Off-the-shelf (GOTS) in place of new development, and requirements analysis and synthesis from system level to individual software components. Qualifications: U.S. Citizenship is required for all applicants. CTP is an equal opportunity employer and abides by applicable employment laws and regulations. All applicants and employees are subject to random drug testing in accordance with Executive Order 12564. Employment is contingent upon successful completion of a security background investigation and polygraph Twelve (12) years experience software engineering experience in programs and contracts of similar scope, type, and complexity is required. Bachelors degree in Computer Science or related discipline from an accredited college or university is required; four (4) years of which must be in programs utilizing Big-Data cloud technologies and/or Distributed Computing. Four (4) years of cloud software engineering experience on projects with similar Big-Data systems may be substituted for a bachelors degree. Master in Computer Science or related discipline from an accredited college or university may be substituted for two (2) years of experience. Cloudera Certified Hadoop Developer certification may be substituted for one (1) year of Cloud experience The following Cloud related experiences are required: Two (2) years of Cloud and/or Distributed Computing Information Retrieval (IR) One (1) year of experience with implementing code that interacts with implementation of Cloud Big Table One (1) year of experience with implementing code that interacts with implementation of Cloud Distributed File System One (1) year of experience with implementing complex MapReduce analytics One (1) year of experience with implementing code that interacts with Cloud Distributed Coordination Frameworks One (1) year of experience in architecting Cloud Computing solutions One (1) year of experience in debugging problems with Cloud based Distributed Computing Frameworks One (1) year of experience in managing multi-node Cloud based installation Utility Computing, Network Management, Virtualization (VMWare or VirtualBox), Cloud Computing Multi Node Management and Installation: Management and installation of Cloud and Distributed Computing on multiple nodes, Python, CFEngine, Bash, Ruby or related technologies Experience in Information Assurance: Securing Cloud Based and Distributed applications through industry standard techniques such as Firewalls, PKI Certificate and Server Authentication with experience in Corporate authentication service(s) Object Oriented Design and Programming, Java, Eclipse or similar development environment, MAVEN, RESTful web services Cloud and Distributed Computing Technologies: at least one or a combination of several of the following areas - YARN, J2EE, MapReduce, Zookeeper, HDFS, HBase, JMS, Concurrent Programming, Multi-Node implementation/installation and other applicable technologies Cloud and Distributed Computing Information Retrieval: at least one or a combination of several of the following areas - HDFS, HBASE, Apache Lucene, Apache Solr, MongoDB Ingesting, Parsing and Analysis of Disparate Data-sources and formats: XML, JSON, CSV, Binary Formats, Sequence or Map Files, Avro and related technologies Aspect Oriented Design and Development Debugging and Profiling Cloud and Distributed Installations: Java Virtual Machine (JVM) memory management, Profiling Java Applications, UNIX/LINUX, CentOS Experience in SIGINT: 1. Experience with at least one SIGINT collection discipline areas (FORNSAT, CABLE, Terrestrial/Microwave, Overhead, and ELINT) 2. Geolocation, emitter identification, and signal applications. 3. Joint program collection platforms and dataflow architectures; signals characterization analysis Experience with Other: 1. CentOS, Linux/RedHat, 2. Configuration management tools such as Subversion, ClearQuest, or Razor Columbia Technology Partners is an Equal Opportunity Employer. We consider applicants without regard to race, color, religion, age, national origin, ancestry, ethnicity, gender, gender identity, gender expression, sexual orientation, marital status, veteran status, disability, genetic information, citizenship status, or membership in any other group protected by federal, state, or local law. Our EEO policy reflects our commitment to ensure equality and promote diversity and inclusion in the workplace. Our policy applies to all employees, job candidates, contractors, stakeholders, partners, and visitors. Powered by JazzHR rs21WpAyry",
        "url": "https://www.linkedin.com/jobs/view/3922294102"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Washington, DC",
        "job_id": 3873640175,
        "company": "Meta",
        "title": "Security Engineer, Investigations - i3",
        "created_on": 1720638306.358029,
        "description": "The Integrity, Investigations, and Intelligence (i3) teams at Meta are dedicated to protecting the users of our family of applications (e.g. Facebook, Instagram, WhatsApp, Oculus) from a multitude of threats including: criminal organizations, human trafficking and exploitation, and scams/fraud. We are seeking security engineers to investigate sophisticated threat actors, advance investigative methods, automate and mature team processes through tooling, and use innovative approaches to protecting people from offline harm. You will have the opportunity to work on some of the most challenging, complicated, and high-visibility risks the company is facing. The ideal candidate will be an innovative self-starter, who is motivated by our mission and results-driven, and will be able to extract, assimilate, and correlate a wide variety of data in order to surface and disrupt threat actors across multiple spaces. Successful candidates must have a proficiency in scripting languages such as PHP or Python and be comfortable with various technical investigative skills. Security Engineer, Investigations - i3 Responsibilities: Proactively hunt for threats and undetected abuse by leveraging internal data, open-source intelligence, and third party private intelligence. Investigate complex cases to understand how abuse is occurring and implement enforcement strategies to mitigate and/or detect harm. Identify areas to automate investigative tooling and tradecraft. Analyze and interpret large datasets to advance investigations, quantify trends or support findings. Implement effective strategies to prevent and disrupt abuse at scale. Address sensitive content issues, including but not limited to graphic images, videos and writings, offensive or derogatory language, and other objectionable material. This role involves exposure to graphic and/or objectionable content including but not limited to graphic images, videos and writings, offensive or derogatory language, and other potential objectionable material, i.e. child exploitation, graphic violence, self-injury, animal abuse, and other content which may be considered offensive or disturbing. Minimum Qualifications: 5+ years work experience in cyber security, private sector security investigations and intelligence, government, and/or intelligence. Experience working with Python, PHP or similar scripting languages Proven experience conducting large scale data analysis and utilizing big data tools such as Maltego, Palantir, Tableau, etc. Track record of effective communication and presenting technical findings. Experience prioritizing and executing with minimal direction or oversight. Experience to work within a fast-paced environment where priorities shift and change. A combination of investigative skills and engineering skills. Experience working with a team spanning multiple locations/time zones. Preferred Qualifications: Experience working or managing projects that have enterprise-wide impact and/or multi-organization cross functional stakeholders. Subject matter expertise with either criminal organizations, child safety, human exploitation, or fraud/scams. Familiarity with sophisticated cyber threats. Experience with open source investigation techniques and familiarity with a variety of internet research tools. BS/MS or equivalent experience in Computer Science, Information Systems, Intelligence Studies, Cybersecurity or related field. About Meta: Meta builds technologies that help people connect, find communities, and grow businesses. When Facebook launched in 2004, it changed the way people connect. Apps like Messenger, Instagram and WhatsApp further empowered billions around the world. Now, Meta is moving beyond 2D screens toward immersive experiences like augmented and virtual reality to help build the next evolution in social technology. People who choose to build their careers by building with us at Meta help shape a future that will take us beyond what digital connection makes possible today—beyond the constraints of screens, the limits of distance, and even the rules of physics. Meta is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Meta participates in the E-Verify program in certain locations, as required by law. Please note that Meta may leverage artificial intelligence and machine learning technologies in connection with applications for employment. Meta is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com. $143,000/year to $208,000/year + bonus + equity + benefits Individual compensation is determined by skills, qualifications, experience, and location. Compensation details listed in this posting reflect the base hourly rate, monthly rate, or annual salary only, and do not include bonus, equity or sales incentives, if applicable. In addition to base compensation, Meta offers benefits. Learn more about  benefits  at Meta.",
        "url": "https://www.linkedin.com/jobs/view/3873640175"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "McLean, VA",
        "job_id": 3970735330,
        "company": "ClickJobs.io",
        "title": "Senior Data Engineer (Python, Spark, AWS)",
        "created_on": 1720638308.1907015,
        "description": "Locations: VA - McLean, United States of America, McLean, Virginia Senior Data Engineer (Python, Spark, AWS) Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking a Senior Data Engineer who is passionate about marrying data with emerging technologies. As a Capital One Senior Data Engineer, you’ll have the opportunity to be on the forefront of driving a major transformation within Capital One. What You’ll Do Build and optimize data pipelines for efficient data ingestion, transformation, and loading from various sources while ensuring data quality and integrity Support the design and development of scalable data architectures and systems that extract, store, and process large amounts of data Collaborate with Data Scientists, Machine Learning Engineers, Business Analysts and/or Product Owners to understand their requirements and provide efficient solutions for data exploration, analysis, and modeling Implement testing, validation and pipeline observability to ensure data pipelines are meeting customer SLAs Use cutting edge technologies such as Python, Scala, Spark, and a variety of AWS services to develop modern data pipelines supporting Machine Learning and Artificial Intelligence Basic Qualifications: Bachelor’s Degree At least 4 years of experience in application development (Internship experience does not apply) At least 1 year of experience in big data technologies Preferred Qualifications: 5+ years of experience building data pipelines using Python, Java, or Scala 2+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud) 2+ years of experience using Spark or PySpark 2+ years of data warehousing experience (Redshift or Snowflake) 3+ years of experience with UNIX/Linux including basic commands and shell scripting 2+ years of experience with Agile engineering practices At this time, Capital One will not sponsor a new applicant for employment authorization for this position. Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. This role is expected to accept applications for a minimum of 5 business days. No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City’s Fair Chance Act; Philadelphia’s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries. If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "url": "https://www.linkedin.com/jobs/view/3970735330"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Maryland, United States",
        "job_id": 3964271279,
        "company": "Elsdon Technology",
        "title": "Software Engineer",
        "created_on": 1720638309.9102602,
        "description": "Are you an experienced Full Stack Developer with a breadth of AWS experience and an active TS/SCI with Full Scope Poly and looking for a new position? We have the one for you! Our client are seeking a Full Stack Developer with skills in AWS and data analytics who can leverage experience and expertise across data exploration, engineering, and ETL to explore, architect, develop, and deploying scripts for processing structured and unstructured data into usable data formats for long term storage, search, and analysis. As the ideal candidate, you will have: Experience working with both structured, semi-structured, and unstructured data to include data parsing, transformation, schema definition, and query/analysis. Ability to manage and organize data while identifying trends and inconsistencies that will impact downstream analytics Experience with data pipelines or be willing to learn a pipeline from bottom to top Experience with AWS Cloud Formation Be able to troubleshoot files against an architecture to see where the upload process is failing. Be able to understand unit tests and add to them to increase stability to the entire pipeline. Be prepared to use, GIT, JIRA, Confluence, Anaconda, Spyder, and Microsoft tools. Be prepared to express ideas and solutions and walk together with teammates through coding challenges Experience with AWS should include: Hands on experience with AWS services including CloudFormation, EC2, EMR, IAM, KMS, Lambda, RDS, and S3 Hands on experience with software installation & deployment on AWS (to deploy both in-house applications and Trifacta) Hands on experience with setting up AWS permissions and restrictions Hands on experience with setting up docker & container infrastructure on AWS This is a unique opportunity with a firm with a huge presence in the Government and Federal industry offering fast-tacked progression and a level of job stability that is uncommon in the job market at present. YOU MUST HOLD AN ACTIVE SECURITY CLEARANCE - ( TS/SCI with FSP ) If you are interested in this position or would like more info about roles, responsibilities, or compensation - please click apply now! Feel free to email me at: sophie.roydhouse@elsdonconsulting.com OR connect with me on LinkedIn: https://www.linkedin.com/in/sophie-roydhouse-b21aa0115/",
        "url": "https://www.linkedin.com/jobs/view/3964271279"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Annapolis Junction, MD",
        "job_id": 3787778048,
        "company": "Acclaim Technical Services",
        "title": "Software Engineer - Python / C++ / Perl (2022-0159)",
        "created_on": 1720638311.5408528,
        "description": "Acclaim Technical Services, founded in 2000, is a leading language and intelligence services company supporting a wide range of U.S. Federal agencies. We are an Employee Stock Ownership Plan (ESOP) company, which is uncommon within our business sector. We see this as a significant strength and it shows: ATS is consistently ranked as a top workplace among DC area firms and continues to grow. We are actively hiring a Software Engineer (Python / C++ / Perl) with TS/SCI clearance and polygraph to join our Enterprise Services and Solutions Division team in the Annapolis Junction, MD area. In this role, you will develop / enhance software tools and the associated graphical user interfaces (GUIs) to analyze, annotate and enhance images produced by any of the image acquisition tool systems. You will develop software programs in various programming languages (e.g. Python, C/C++/C#, Perl) in order to perform the following responsibilities RESPONSIBILTIES Implement image processing/manipulation algorithms Analyze digital logic and other circuit schematics Analyze large quantities of hardware-related data from a variety of sources Provide efficient software user interfaces Required Education & Experience Must possess an active TS/SCI clearance with polygraph Candidates should have a minimum of a Bachelor’s Degree in Computer Science or Electrical Engineering (or similar) and ideally 14 years of relevant experience. Equal Employment Opportunity / Affirmative Action ATS is committed to a program of equal employment opportunity without regard to race, color, ethnicity, national origin, ancestry, citizenship, sex, pregnancy, marital status, sexual orientation, gender identity, age, religion/creed, hairstyles and hair textures, handicap/disability, genetic information/history, military/veteran status, or any other characteristic or condition protected by federal, state or local law. It is the policy of ATS not merely to refrain from employment discrimination as required by the various federal, state, and local enactments, but to take positive affirmative action to realize for women, people of color, individuals with disabilities and protected veterans full equal employment opportunity. We support the employment and advancement in employment of individuals with disabilities and of protected veterans, and we treat qualified individuals without discrimination on the basis of their physical or mental disability or veteran status. Powered by JazzHR 2yrtC0yJZw",
        "url": "https://www.linkedin.com/jobs/view/3787778048"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Columbia, MD",
        "job_id": 3928033151,
        "company": "Merit321, Launching Careers",
        "title": "Backend Software Engineer",
        "created_on": 1720638313.1211903,
        "description": "Position: Backend Software Engineer Location: Columbia, MD (Hybrid) Clearance: Current active Secret Clearance Summary In this role, you will have the opportunity to work with modern tools and technologies, and you will be encouraged to stay up-to-date with the latest developments in the field. You will be part of a dynamic and collaborative team that is committed to delivering high-quality software solutions to our clients. The work environment is a hybrid one, with a mix of remote and in-person work. We believe in providing our employees with a great work-life balance, so you will have the flexibility to work from home or from our office, depending on your preferences and needs. If you are a self-motivated, creative, and detail-oriented software engineer who is passionate about developing innovative software solutions, we would love to hear from you. If you are a self-motivated, creative, and detail-oriented software engineer who is passionate about developing innovative software solutions, we would love to hear from you. Essential Job Responsibilities Must have a strong working knowledge and experience developing Java based software capabilities. Should have an inquisitive nature, responsiveness, and excellent testing skills. Must also possess strong troubleshooting skills and the ability to work under pressure with multiple deadlines. Will work in a fast-paced, small business environment with our talented team. Other duties as assigned. Minimum Qualifications At least 5 years of experience in Software Engineering, Modern Java Frameworks and Libraries (e.g. Spring, Guava) and a Bachelors in related field; 3 years relevant experience with Masters in related field; or High School Diploma or equivalent and 9 years relevant experience. Experience in designing enterprise APIs Experience in RESTful web services Experience in Microservices architecture Experience in Object Oriented Programming (OOP) paradigms Experience with the agile software lifecycle Has a proven ability to learn quickly and works well both independently as well as in a team setting Experience with the Linux operating system Experience with configuration management tools (e.g. Git, Nexus, Maven) Must be able to work a hybrid schedule out of our Columbia, MD office. Desired Skills (Optional) DoD 8140 / 8570 compliance certifications may be required in this position as directed by the customer. Experience in cloud-based technologies (AWS, Azure). Experience in distributed databases, NoSQL databases, full text-search engines (e.g. Elasticsearch, MongoDB, Solr) Scripting experience is a huge plus. Prior experience or familiarity with our Big Data Platform is a plus. Understanding of AGILE software development methodologies and use of standard software development tool suites. (e.g., JIRA, Confluence, GitHub Enterprise, etc.) EEO It is the policy of Merit321 to provide equal opportunity in recruiting, hiring, training, and promoting individuals in all job categories without regard to race, color, religion, national origin, gender, age, disability, genetic information, veteran status, sexual orientation, gender identity, or any other protected class or category as may be defined by federal, state, or local laws or regulations. recruiting, hiring, training, and promoting individuals in all job categories without regard to race, color, religion, national origin, gender, age, disability, genetic information, veteran status, sexual orientation, gender identity, or any other protected class or category as may be defined by federal, state, or local laws or regulations.",
        "url": "https://www.linkedin.com/jobs/view/3928033151"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Annapolis Junction, MD",
        "job_id": 3864699247,
        "company": "Columbia Technology Partners",
        "title": "Senior System Engineer",
        "created_on": 1720638314.700106,
        "description": "Description: Applies systems engineering principles throughout the systems life cycle phases: Concept, Development, Production, Utilization, Support, and Retirement. Interacts with the Government regarding Systems Engineering technical considerations and for associated problems, issues or conflicts. Communicates with other program personnel, government overseers, and senior executives. Responsibility for the technical integrity, quality, and completeness of work performed and deliverables associated with one or more of the 25 process areas defined by ISO/IEC15288: Technical Process Area - Stakeholder Requirements Definition, Requirements Analysis, Architectural Design, Implementation, Integration, Verification, Transition, Validation, Operation, Maintenance, and Disposal; Project Process Area - Project Planning, Project Assessment and Control, Decision Management, Risk Management, Configuration Management, Information Management, and Measurement; Enterprise (Organizational Project-Enabling) Process Area - Project Portfolio Management, Infrastructure Management, Lifecycle Model Management, Human Resource Management, and Quality Management; Agreement Process Area - Acquisition and Supply. Qualifications: An Active TS/SCI clearance + FS polygraph. Fourteen (14) years experience as a SWE in programs and contracts of similar scope, type, and complexity is required. Bachelor’s degree in Computer Science or related discipline from an accredited college or university is required. Four (4) years of additional SWE experience on projects with similar software processes may be substituted for a bachelor’s degree. Columbia Technology Partners is an Equal Opportunity Employer. We consider applicants without regard to race, color, religion, age, national origin, ancestry, ethnicity, gender, gender identity, gender expression, sexual orientation, marital status, veteran status, disability, genetic information, citizenship status, or membership in any other group protected by federal, state, or local law. Our EEO policy reflects our commitment to ensure equality and promote diversity and inclusion in the workplace. Our policy applies to all employees, job candidates, contractors, stakeholders, partners, and visitors. Powered by JazzHR CxtdJKi14m",
        "url": "https://www.linkedin.com/jobs/view/3864699247"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "McLean, VA",
        "job_id": 3969097428,
        "company": "CodeHunter",
        "title": "Mid-level Java Software Engineer",
        "created_on": 1720638316.2676678,
        "description": "As a Mid-Level Java Engineer at CodeHunter you will play a critical role in the design, development, and maintenance of our core software application. You will work closely with our cross-functional teams to deliver high-quality solutions that meet our clients' needs. This position offers an exciting opportunity to work on challenging projects, learn new technologies, and make a significant impact on our product. We invite you to be part of our mission in safeguarding digital ecosystems worldwide. This position is primarily located in McLean, Virginia, offering the flexibility of remote work on Fridays. Responsibilities Collaborate with software architects and developers to design and implement Java-based applications Write clean, maintainable, and efficient code Perform code reviews and provide constructive feedback to junior team members Debug and resolve software defects and issues Participate in software testing and quality assurance processes Stay updated with industry best practices and emerging technologies Requirements 3+ years of professional software development experience with Java Strong knowledge of Java, Spring Framework, and object-oriented design principles Experience with RESTful API development Proficiency in database design and SQL Familiarity with version control systems (e.g., Git) Excellent problem-solving and communication skills Ability to work collaboratively in a team environment Preferred Bachelor's degree in computer science or a related field Experience with cloud platforms (e.g., AWS, Azure, GCP) Knowledge of front-end technologies (JavaScript, HTML, CSS) Familiarity with agile development methodologies. (Jira) Benefits CodeHunter offers a creative, team-oriented, and entrepreneurial work environment. Self-starters thrive here. Our employees have the chance to be a part of the organization from the ground level and make a demonstrable impact by bringing an innovative product to the cybersecurity marketplace. CodeHunter offers best-in-class benefits, including: 401K Health coverage Vision and dental coverage Company-sponsored training Parking or metro benefits Catered lunches Generous PTO policy CodeHunter values Diversity, Equity and Inclusion (DEI) and is an equal-opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability or protected veteran status, or any other legally protected basis, in accordance with applicable law.",
        "url": "https://www.linkedin.com/jobs/view/3969097428"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "McLean, VA",
        "job_id": 3965954795,
        "company": "Capital One",
        "title": "Senior Data Engineer",
        "created_on": 1720638317.968172,
        "description": "Center 1 (19052), United States of America, McLean, VirginiaSenior Data Engineer Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking Data Engineers who are passionate about marrying data with emerging technologies. As a Capital One Data Engineer, you’ll have the opportunity to be on the forefront of driving a major transformation within Capital One. What You’ll Do: Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and Snowflake Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance Basic Qualifications: Bachelor’s Degree At least 4 years of experience in application development (Internship experience does not apply) At least 1 year of experience in big data technologies Preferred Qualifications: 5+ years of experience in application development including Python, SQL, Scala, or Java 2+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud) 3+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL) 2+ year experience working on real-time data and streaming applications 2+ years of experience with NoSQL implementation (Mongo, Cassandra) 2+ years of data warehousing experience (Redshift or Snowflake) 3+ years of experience with UNIX/Linux including basic commands and shell scripting 2+ years of experience with Agile engineering practices At this time, Capital One will not sponsor a new applicant for employment authorization for this position. Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website . Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. This role is expected to accept applications for a minimum of 5 business days.No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City’s Fair Chance Act; Philadelphia’s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries. If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com . All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "url": "https://www.linkedin.com/jobs/view/3965954795"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Annapolis Junction, MD",
        "job_id": 3797706876,
        "company": "NiSUS Technologies Corporation",
        "title": "IC12CSWE1 - Associate Cloud Software Engineer - Cleared",
        "created_on": 1720638319.5439668,
        "description": "Support development of a cloud infrastructure; support migration of data to the cloud infrastructure; support development of cloud based tools The Cloud Software Engineer develops, maintains, and enhances complex and diverse Big-Data cloud systems. Provides expertise in cloud computing, Eco-System, including implementing applications, distributed Computing, Information Retrieval (IR), and Object Oriented Design. Works individually or as part of a team. Reviews and tests software components for adherence to the design requirements and documents test results. Resolves software problem reports. Utilizes software development and software design methodologies appropriate to the development environment. Provides specific input to the software components of system design to include hardware/software trade-offs, software reuse, use of Commercial Off the- shelf (COTS). Requirements Five (5) years experience in software development/engineering, including requirements analysis, software development, installation, integration, evaluation, enhancement, maintenance, testing, and problem diagnosis/resolution. Four (4) years of experience developing software with high level languages such as Java, C, C++ . Three (3) years experience developing software for Windows (2000, 2003, XP, Vista), or UNIX/Linux (Redhat versions 3-5) operating systems Three (3) years experience in software integration and software testing, to include developing and implementing test plans and test scripts Ability to work with OpenSource (NoSQL) products that support highly distributed, massively parallel computation needs such as Hbase, CloudBase/Acumulo, Big Table, etc. Shall have demonstrated work experience with the Map Reduce programming model and technologies such as Hadoop, Hive, Pig, etc. Work experience with the Hadoop Distributed File System (HDFS) Work experience with Serialization such as JSON and/or BSON Work experience developing Restful services Work experience in the requirements analysis and design of at least one Object Oriented system Work experience developing solutions integrating and extending FOSS/COTS products Technical writing skills and shall have generated technical documents in support of a software development project In addition, the candidate will have demonstrated experience, work or college level courses, in at least two (2) of the desired characteristics Experience deploying applications in a cloud environment Understanding of Big-Data Cloud Scalability (Amazon, Google, Facebook) Hadoop/Cloud Developer Certification Experience designing and developing automated analytic software, techniques, and algorithms. Experience with taxonomy construction for analytic disciplines, knowledge areas and skills. Experience developing and deploying data driven analytics; event driven analytics; sets of analytics orchestrated through rules engines Experience with linguistics (grammar, morphology, concepts) Experience documenting ontologies, data models, schemas, formats, data element Benefits Health & Life Insurance Dental Insurance Disability Insurance 401K Retirement Plan with Matching Tuition Assistance Vacation and Sick Leave Hiring Bonuses Referral Recruitment Program",
        "url": "https://www.linkedin.com/jobs/view/3797706876"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Annapolis Junction, MD",
        "job_id": 3949816047,
        "company": "Orion Consortium, LLC",
        "title": "System Engineer 2 - Splunk System Engineer",
        "created_on": 1720638328.943473,
        "description": "Position Description: Ensure the Splunk infrastructure functions properly with PKI-based authentication, corporate authorization services, firewalls, and SSL/TLS communications Contribute to development and ongoing improvement of industry best practices and standards for maintaining data analytics enterprise technologies Assist with installing, testing, and deploying hotfixes/patches for Splunk app/product releases to manage enterprise vulnerabilities Assist with development of knowledge articles, documentation, and work instructions used by the Splunk, server, desktop and Information System Security teams, and Tier 2/3 Help Desk technicians Mandatory Skills: Experience managing user authentication within Splunk including RBAC/ABAC Strong organizational, communication, and collaboration skills Desired Skills: Experience reviewing network, host, and firewall security logs Experience with Splunk Machine Learning Toolkit (MLTK) Experience with scripting languages such as CSS, HTML, JavaScript, and Python Knowledge of RMF, Trellix ePO, NESSUS, SCAP, and vulnerability scanning ServiceNow Ticketing System Shell scripting to automate tasks and manipulate data Certification Required: IAT Level II cert required CompTIA Security+ CE Experience: HS or GED + 19 Years experience Bachelors + 14 Years experience Clearance Required: TS/SCI FS poly Position requires a TS/SCI. Powered by JazzHR NUgvD6oetl",
        "url": "https://www.linkedin.com/jobs/view/3949816047"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Arlington, VA",
        "job_id": 3839071917,
        "company": "Govini",
        "title": "Software Engineer",
        "created_on": 1720638332.909007,
        "description": "Company Description Govini builds software to accelerate the Defense Acquisition Process. Ark, Govini’s flagship product, is a suite of AI-enabled applications, powered by integrated government and commercial data, that solves problems across the entire spectrum of Defense Acquisition, including Supply Chain, Science & Technology, Production, Sustainment, and Modernization. With Ark, the Acquisition community eliminates slow, manual processes and gains the ability to rapidly imagine, produce, and field critical warfighting capabilities. Ark transforms Defense Acquisition into a strategic advantage for the United States. Job Description We are seeking a skilled and dedicated forward-deployed software engineer to join our Engineering team. As a Forward Deployed Software Engineer at Govini, you show a clear passion for quality and love to solve client problems using technology. You are energized by the idea of building something new and want to work on problems that matter. You thrive in ambiguous environments and doggedly pursue innovative solutions for our customers. You understand and appreciate the value of face-to-face interactions with end users, and constantly seek to push the boundaries of what is possible. In order to do this job well, you must be a curious and eager problem solver with a hunger for building well-designed, high-quality solutions within highly regulated environments. You have a passion for great work and nothing less than your best will do. You share our intolerance of mediocrity. You’re uber-smart, challenged by figuring things out and producing simple solutions to complex problems. Knowing there are always multiple answers to a problem, you know how to engage in a constructive dialogue to find the best path forward. You’re scrappy. We like scrappy. This role is a full-time position located out of our office in Pittsburgh, PA. This role may require up to 75% travel Scope Of Responsibilities Collaborate with other software engineers, product managers, and client-facing teams to understand client requirements and translate them into technical solutions Lead the deployment and integration of our software and data solutions in client environments, ensuring smooth implementation and adherence to project timelines Actively participate in designing, building and testing scalable data architecture Craft clean, testable, and maintainable code Participate in the end-to-end software development of new feature functionality and design capabilities Optimize processes for maximum speed and accuracy Regularly seek out innovation and continuous improvement, finding efficiency in all assigned tasks Qualifications U.S. Citizenship is required Required Skills Bachelor's degree in Computer Science, Software Engineering, or related field, or equivalent work experience 1+ years of software development experience with a high-level software stack Current possession of a U.S. security clearance, or the ability to obtain one with our sponsorship Ability to communicate technical information to non-technical Intermediate SQL development Strong proficiency with backend API languages such as Java/Spring Boot (preferred) or equivalent Experience performing meaningful tasks in Linux Prior hands-on experience working with data-driven analytics Experience working in agile/scrum teams Ability to work independently with little supervision A burning desire to work in a challenging fast-paced tech environment Desired Skills Experience working within the boundaries of a Federal government accredited software environment Experience in or exposure to the nuances of a startup or other entrepreneurial environment Experience working in a client-facing or consulting role Familiarity with Javascript/ReactJS We firmly believe that past performance is the best indicator of future performance. If you thrive while building solutions to complex problems, are a self-starter, and are passionate about making an impact in global security, we’re eager to hear from you. Govini is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability and protected veterans status or any other characteristic protected by law.",
        "url": "https://www.linkedin.com/jobs/view/3839071917"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Annapolis Junction, MD",
        "job_id": 3670471912,
        "company": "Kaizen Approach, Inc.",
        "title": "Cloud Software Engineer",
        "created_on": 1720638334.5836983,
        "description": "We are currently looking to hire an experienced Cloud Software Engineer to Install, configure, test, administer, monitor, troubleshoot, and sustain the operating system and application software for large, clustered systems based on the CLOUD technologies COPILOT, ACCUMULO, PIG/PIGLET, SPARK, and Hadoop (HDFS and MapReduce). You will work with Map Reduce programming model and technologies such as Hadoop, Hive, Pig, Hadoop Distributed File System (HDFS), etc. You will work with distributed scalable Big Data Store (NoSQL) such as Hbase, Accumulo, Big Table, etc. Requirements: Experience with Serialization data interchange formats such as JSON and/or BSON Experience in in the design and development of at least one Object Oriented system Experience in developing solutions integrating and extending FOSS/COTS products Experience developing and deploying data driven analytics, event driven analytics, and sets of analytics orchestrated through rules engines Experience with linguistics (grammar, morphology, concepts) and developing and deploying analytics that discover and exploit social networks Experience documenting ontologies, data models, schemas, formats, data element dictionaries, software application program interfaces and other technical specifications Characteristics include: Hadoop Cloud Certification Experience deploying applications in a cloud environment Understanding of Cloud Scalability, Apache Nifi, and MongoDB (or strong in other NoSQL) Experience designing and developing automated analytic software, techniques, and algorithms Experience developing and deploying analytics within a heterogeneous schema environment Six years’ experience using Java programming language Bachelor’s degree in Computer Science or related discipline from an accredited college or university is required Active TS/SCI clearance with Polygraph About Kaizen Approach We love what we do, and that drives us to create the best possible results for our customers. We apply years of experience in Cybersecurity and Learning & Development to guide our customers in ongoing improvements that keep their employees’ skills current, and their companies protected. Our reputation is built on the skills and successes of our employees. We care about them as individuals, and we support and encourage them in their professional growth and personal happiness. We sincerely value every member of our team. Premium Healthcare 4 Weeks of PTO 11 Paid Holidays Gifted 401k Profit Sharing Paid Training and Admin Time",
        "url": "https://www.linkedin.com/jobs/view/3670471912"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Annapolis Junction, MD",
        "job_id": 3958450689,
        "company": "Captivation",
        "title": "Software Engineer 1/2 - Python/RESTful/CentOS/Linux/Docker/MariaDB",
        "created_on": 1720638336.3803008,
        "description": "Build to something to be proud of. Captivation has built a reputation on providing customers exactly what is needed in a timely manner. Our team of engineers take pride in what they develop and constantly innovate to provide the best solution. Captivation is looking for software developers who can get stuff done while making a difference in support of the mission to protect our country. Description Captivation Software is looking for mid level software engineer who will be working as a Python Developer and has experience developing Back-End services. Candidate must be prepared to work with a small, mission focused team that frequently interacts with users and the customer. Requirements Security Clearance: Must currently hold a Top Secret/SCI U.S. Government security clearance with a favorable Polygraph, therefore all candidates must be a U.S. citizen Minimum Qualifications: A minimum of seven (7) years of experience providing software engineering support Bachelor's Degree in related technical discipline Required Skills: Python Experience developing Back-End services, RESTful interfaces, sockets Experience interacting with Databases in CentOS 7 and/or Rocky 9 Linux environment Experience with generating and leveraging Docker containers Experience working with MariaDB databases. Desired Skills: Familiarity with JavaScript Familiarity with TCP and UDP multicast This position is open for direct hires only. We will not consider candidates from third party staffing/recruiting firms. Benefits Annual Salary: $130,000 - $270,000 (Depends on the Years of Experience) Up to 20% 401k contribution (No Matching Required and Vested from Day 1) Above Market Hourly Rates $3,200 HSA Contribution 5 Weeks Paid Time Off Company Paid Employee Medical/Dental/Vision Insurance/Life Insurance/Short-Term & Long-Term Disability/AD&D",
        "url": "https://www.linkedin.com/jobs/view/3958450689"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Annapolis Junction, MD",
        "job_id": 3937591829,
        "company": "Raytheon",
        "title": "Junior Software Engineer",
        "created_on": 1720638338.2689366,
        "description": "Date Posted: 2024-02-08 Country: United States of America Location: MD233: 420 National Business Parkway 420 National Business Parkway Suite 400, Annapolis Junction, MD, 20701 USA Position Role Type: Onsite You have been redirected to RTX’s career page as we have recently transitioned from RTX to become a standalone company, which provides us with greater autonomy and opportunities for growth. As a prospective employee of Nightwing, you’ll have the chance to contribute to our continued success and shape the future of our cybersecurity, intelligence, and services offerings. The Software Engineer develops, maintains, and enhances complex and diverse software systems (e.g., processing-intensive analytics, novel algorithm development, manipulation of extremely large data sets, real-time systems, and business management information systems) based upon documented requirements. Works individually or as part of a team. Reviews and tests software components for adherence to the design requirements and documents test results. Resolves software problem reports. Utilizes software development and software design methodologies appropriate to the development environment. Provides specific input to the software components of system design to include hardware/software trade-offs, software reuse, use of Commercial Off-the-shelf (COTS)/Government Off-the-shelf (GOTS) in place of new development, and requirements analysis and synthesis from system level to individual software components. The Level 0 Software Engineer (SWE) shall possess the following capabilities: Analyze user requirements to derive software design and performance requirements. Debug existing software and correct defects. Provide recommendations for improving documentation and software development process standards. Design and code new software or modify existing software to add new features. Integrate existing software into new or modified systems or operating environments. Develop simple data queries for existing or proposed databases or repositories. Required: Current TS/SCI with polygraph. Bachelor’s degree in Computer Science or related discipline from an accredited college or university. Four (4) years of additional software engineering experience on projects with similar software processes may be substituted for a bachelor’s degree. Employee Referral Award Eligibility: This requisition is eligible for an employee referral award. ALL eligibility requirements must be met to receive the referral award. What We Offer: Whether you’re just starting out on your career journey or are an experienced professional, we offer a robust total rewards package with compensation; healthcare, wellness, retirement and work/life benefits; career development and recognition programs. Some of the benefits we offer include parental (including paternal) leave, flexible work schedules, achievement awards, educational assistance and child/adult backup care. Typically requires a University Degree or equivalent experience and less than 2 years prior relevant experience. Engineering/Other Technical Positions: Typically requires a degree in Science, Technology, Engineering or Mathematics (STEM) and a minimum of 2 years of prior relevant experience unless prohibited by local laws/regulations. Previously part of a leading Fortune 100 company and headquartered in Dulles, VA; Nightwing became independent in 2024 but continues to support the nation’s most mission impactful initiatives. When we formed Nightwing, we brought a deep set of credentials and an unfaltering commitment to the mission. For over four decades, our team has been providing some of the world’s most technically advanced full-spectrum cyber, data operations, systems integration and intelligence support services to the U.S. government on its most important missions. At Nightwing, we value collaboration and teamwork. You’ll have the opportunity to work alongside talented individuals who are passionate about what they do. Together, we’ll leverage our collective expertise to drive innovation, solve complex problems, and deliver exceptional results for our clients. Thank you for considering joining us as we embark on this new journey and shape the future of cybersecurity and intelligence together as part of the Nightwing team. The salary range for this role is 53,000 USD - 103,000 USD. The salary range provided is a good faith estimate representative of all experience levels. RTX considers several factors when extending an offer, including but not limited to, the role, function and associated responsibilities, a candidate’s work experience, location, education/training, and key skills. Hired applicants may be eligible for benefits, including but not limited to, medical, dental, vision, life insurance, short-term disability, long-term disability, 401(k) match, flexible spending accounts, flexible work schedules, employee assistance program, Employee Scholar Program, parental leave, paid time off, and holidays. Specific benefits are dependent upon the specific business unit as well as whether or not the position is covered by a collective-bargaining agreement. Hired applicants may be eligible for annual short-term and/or long-term incentive compensation programs depending on the level of the position and whether or not it is covered by a collective-bargaining agreement. Payments under these annual programs are not guaranteed and are dependent upon a variety of factors including, but not limited to, individual performance, business unit performance, and/or the company’s performance. This role is a U.S.-based role. If the successful candidate resides in a U.S. territory, the appropriate pay structure and benefits will apply. RTX anticipates the application window closing approximately 40 days from the date the notice was posted. However, factors such as candidate flow and business necessity may require RTX to shorten or extend the application window. RTX is An Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or veteran status, age or any other federally protected class. Privacy Policy and Terms: Click on this link to read the Policy and Terms",
        "url": "https://www.linkedin.com/jobs/view/3937591829"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Gaithersburg, MD",
        "job_id": 3955041870,
        "company": "Noetic Strategies, Inc.",
        "title": "Mid-Level Software Engineer",
        "created_on": 1720638340.0373154,
        "description": "Job Title: Mid-Level Software Engineer (MUST HAVE ACTIVE TS/SCI) Location : Gaithersburg, MD Clearance : Current Active TS/SCI MINIMUM SKILLS REQUIRED: Strong proficiency with the following technologies: Frontend Web Development: REACT, Angular, Vue.js, or similar JavaScript frameworks Backend Development: Java including Java Spring Boot Cloud Development: AWS, Azure, or similar cloud platforms Relational Database: PostgreSQL with the ability to write SQL Operating System: Linux Version Control System: Git Deep understanding of modern software design patterns and coding standards Experience architecting, developing, and delivering complex software systems into production Working knowledge of Agile development and continuous integration/continuous delivery methodologies and tools Strong, self-motivated desire to learn new programming languages, tools, frameworks, and techniques DESIRED SKILLS: Frontend Web Development: UI/UX Software Architectures: Client-Server, Microservices, Model-View-Controller NoSQL Database: OpenSearch/Elasticsearch Specific AWS Services: S3, SQS, SNS, EC2, Cloud Formation and RDS Automated Software Deployment: Kubernetes, Containerization Pub/Sub Technologies: Message Queues (RabbitMQ, Apache Kafka, AWS SQS) Certifications such as AWS DevOps Engineer or similar Knowledge of continuous integration and delivery tools: Jenkins, GitLab, and Docker PRIMARY DUTIES, RESPONSIBILITIES & ESSENTIAL JOB FUNCTIONS: Resolve various Modernization tasks as prioritized by the team product owner Engage in the complete software development lifecycle, operating within well-defined parameters Take ownership of delivering enhancements within a system or application Break down sizable tasks into manageable units for execution and provide LOE estimations Collaborate with fellow software engineers to create and document optimal technical designs Ensure team compliance with Agile processes and best practices Build software solutions where the solution is not clearly defined but always prioritizing customer needs Resolve obstacles for the development team, working collaboratively with the Product Owner, technical leadership, and other engineers Noetic Strategies Inc. offers a competitive salary, an extensive benefits package and a work environment that encourages excellence. For positions requiring a security clearance, selected applicants will be subject to a government security investigation and must meet eligibility requirements for access to classified information. Noetic Strategies Inc. is an equal opportunity and affirmative action employer that does not discriminate in employment. All qualified applicants will receive consideration for employment without regard to their race, color, religion, sex, age, sexual orientation, gender identity, or national origin, disability or protected veteran status. Noetic Strategies Inc. endeavors to make www.noeticstrategies.com accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact noeticstrategies.com for assistance. This contact information is for accommodation requests only and cannot be used to inquire about the status of applications. Noetic Strategies Inc. offers a competitive salary, an extensive benefits package and a work environment that encourages excellence. For positions requiring a security clearance, selected applicants will be subject to a government security investigation and must meet eligibility requirements for access to classified information. Noetic Strategies Inc. is an equal opportunity and affirmative action employer that does not discriminate in employment. All qualified applicants will receive consideration for employment without regard to their race, color, religion, sex, age, sexual orientation, gender identity, or national origin, disability or protected veteran status. Noetic Strategies Inc. endeavors to make www.noeticstrategies.com accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact noeticstrategies.com for assistance. This contact information is for accommodation requests only and cannot be used to inquire about the status of applications. Powered by JazzHR 6vpxaJbseD",
        "url": "https://www.linkedin.com/jobs/view/3955041870"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "McLean, VA",
        "job_id": 3966529548,
        "company": "Capital One",
        "title": "Senior Data Engineer",
        "created_on": 1720638344.178566,
        "description": "West Creek 4 (12074), United States of America, Richmond, VirginiaSenior Data Engineer Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking Data Engineers who are passionate about marrying data with emerging technologies. As a Capital One Data Engineer, you’ll have the opportunity to be on the forefront of driving a major transformation within Capital One. What You’ll Do: Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and Snowflake Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance Basic Qualifications: Bachelor’s Degree At least 4 years of experience in application development (Internship experience does not apply) At least 1 year of experience in big data technologies Preferred Qualifications: 5+ years of experience in application development including Python, PySPark, SQL, Scala, or Java 2+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud) 3+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL) 2+ year experience working on real-time data and streaming applications 2+ years of experience with NoSQL implementation (Mongo, Cassandra) 2+ years of data warehousing experience (Redshift or Snowflake) 3+ years of experience with UNIX/Linux including basic commands and shell scripting 2+ years of experience with Agile engineering practices At this time, Capital One will not sponsor a new applicant for employment authorization for this position. Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website . Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. This role is expected to accept applications for a minimum of 5 business days.No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City’s Fair Chance Act; Philadelphia’s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries. If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com . All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "url": "https://www.linkedin.com/jobs/view/3966529548"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Washington DC-Baltimore Area",
        "job_id": 3955990510,
        "company": "DIGITALSPEC - A Native Hawaiian Organization - NHO 8(a)",
        "title": "Azure Cloud Engineer",
        "created_on": 1720638346.0887778,
        "description": "Position: Cloud Engineer/Architect Location: Washington, DC (Currently, this program supports 4 days Remote and 1 day onsite per week) Education: BS degree in Computer Science. Engineering, or related field, or equivalent experience Clearance: Top Secret DoD Clearance Required. May consider highly qualified DoD Secret Clearance holders with the ability to obtain a Top Secret. Duties and Responsibilities: As a Cloud Engineer you will play a key role in maintaining and enhancing an enterprise cloud environment servicing more than 10,000 Users. Assist in the engineering, development and administration of a federal enterprise cloud system. Essential Functions: Identify hardware, software, and external service cost estimates and resource requirements. Inform stakeholders of the strategy for deployment of new implementations, as well as, the maintenance and upgrades of existing equipment. Ensure compliance with incident, problem, and change management procedures and execute ticket-based requests with little or no additional help. Participate and manage technical project review meetings to ensure technical designs meet the client’s operational requirements and infrastructure/security standards. Develop a technology roadmap based on established client priorities to meet strategic engineering goals, such as Enterprise Architecture, Quality Assurance, Performance, Operations and Support. Improve infrastructure operations through better use of monitoring, cloud coding, and scripting best practices, Deliver fully functioning network services with secure access points between the client environment and the commercial service provider environment. Understand software development and Agile best practices as it relates to work coordination, acceptance criteria, and deployment management. Lead the development of the technical artifacts, such as an Implementation Plan, System Administration Guide, Operational Turnover Plan, System Security Plan, etc. Automate infrastructure migration to cloud environments and identify on-premise technologies and their cloud counterparts. Support security engineering efforts, including perimeter security, encryption, micro-segmentation, and firewall configuration. Perform network engineering relating to security, segmentation, connectivity, and protocols. Focus on results and be delivery oriented, with a keen interest on quality and the ability to meet deadlines. Certifications (Desired): Google – Associate Cloud Engineer; AWS – Solutions Architect – Associate; Azure – MCSA: Cloud Platform (Infrastructure Focus); VMware – VCP Data Center Virtualization. Qualifications: At least 3 years of technology consulting or other relevant industry experience. Prior professional experience in IT Strategy, IT Business Management, Cloud & Infrastructure, or Systems Engineering. Experience working independently with minimal supervision and guidance. Strong problem solving and troubleshooting skills with experience exercising mature judgment. Proven experience effectively prioritizing workload to meet deadlines and work objectives. Demonstrated ability to write clearly, succinctly, and in a manner that appeals to a wide audience. Proficiency in word processing, spreadsheet, and presentation creation tools, as well as Internet research tools. Must be able to obtain and maintain required clearance for this role. Migration Strategy: Advisory on Migration Disposition/Type (As-Is, Replatform to IaaS, Refactor to PaaS/Cloud Native, etc.) Compute: Infrastructure, Platform Sizing, Consolidation, Tiered and Virtualized Storage, Automated Provisioning, Rationalization, Infrastructure Cost Reduction, Thin Provisioning. Experience with Operating systems and Software: Upgrade Planning and Implementation, Clustering and Grid Computing, Linux-on-Mainframe. Experience with Open Source: Sizing and Performance Analyses, Selection and Implementation, Platform Design and Selection. Experience with Virtualization: Virtualization Server, Storage, Desktop, Network; Reducing Sprawl. Experience with Infrastructure-Based Processes: Monitoring, Capacity Planning, Facilities Management, Performance Tuning, Asset Management, Disaster Recovery, Data Center support. Preferred Certifications and Experience: Experience with Servers, Infrastructure, Platform Sizing, Infrastructure Cost Reduction. Possess one of the following certifications for the design/development of cloud platforms and/or applications using a cloud platform: Pivotal Cloud Foundry – Administrator or Developer Possess any additional certifications (one or more) for the design/development of distributed solutions on a leading infrastructure platform such as: IBM Certified Solution Architect – Cloud Computing Infrastructure v3 Google Cloud Platform Qualified Developer o Microsoft Azure Solutions Architect VMWare Certifications (Data Center Virtualization, Cloud Management and Automation, Network Virtualization) Red Hat Certified Architect AWS certifications Experience with legacy system engineering assessments, discovering the build and details surrounding the system in order to re-build it in a new hosted environment. The engineer will then work with the client to create a Migration Plan, a detailed technical document illustrating the future state architecture of the system in its new environment, supporting the functionality of the system and meeting all required specifications for security, performance, and otherwise as indicated by the client. Corporate Performance Standards: In the performance of their respective tasks and duties all employees are expected to conform to the following: Perform quality work within deadlines with or without direct supervision. Interact professionally with other employees, customers and suppliers. Work effectively as a team contributor on all assignments. Work independently while understanding the necessity for communicating and coordinating work efforts with other employees and organizations. About DIGITALSPEC, LLC DIGITALSPEC, LLC (DSPEC) is a Small Business Administration (SBA) certified 8(a), ISO 9001, 20000, and 27001 company founded in 2005 and headquartered in Fairfax, VA. We are a leading provider of Business Consulting Services and Technical Solution Services delivering true business value and return on investment to Federal clients in the metropolitan Washington, D.C area. DSPEC provides solutions and services in markets such as: Homeland Security and Law Enforcement, Defence and Intelligence, Financial Services, and Social & Citizen Services. We work collaboratively with clients to create solutions that ‘fit’ the client environment and use industry best practices.",
        "url": "https://www.linkedin.com/jobs/view/3955990510"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Fort George G. Meade, MD",
        "job_id": 3813096287,
        "company": "1872 Consulting",
        "title": "Software QA Engineer (junior-mid level)",
        "created_on": 1720638347.7934346,
        "description": "Software QA Engineer (junior-mid level) Active TS/SCI Clearance w/full-scope Polygraph is required SUMMARY We are seeking a Software QA Engineer that provides web application development and support to tools that disseminate data to be conveyed at the absolute first point of relevance allowing mission analytic users to view existing data or contribute new information in a collaborative environment. RESPONSIBILITIES Work with a diverse team of principal, senior, and junior level contributors to provide development and production support for the customer Analytic and Engagement Applications (AEA). Provide direct test support to browser-based web applications written in JavaScript Integrate existing software into new or modified systems or operating environments Provide test support for REST APIs (or web services) between the web application and its related back-end data Understand how to read and query data from back-end sources Collaborate with User Experience design team to increase product's usability and effectiveness Review and test software components for adherence to the design requirements and documents test results Manage system requirements and derived requirements to ensure the delivery of production systems that are compatible with the defined system architecture(s) - Department of Defense Architecture Framework (DoDAF), Service-oriented Architecture (SOA), etc Contribute to the development of sections of systems engineering documentation such as System Engineering Plans, Initial Capabilities Documents, Requirements specifications, and Interface Control Documents Assist with the development of system requirements, functional requirements, and allocation of the same to individual hardware, software, facility, and personnel components Coordinate the resolution of action items from Configuration Control Board (CCB) meetings, design reviews, program reviews, and test reviews that require cross-discipline coordination Participate in an Integrated Product Team to design new capabilities based upon evaluation of all necessary development and operational considerations Allocate real-time process budgets and error budgets to systems and subsystem components Generate alternative system concepts, physical architectures, and design solutions Define the methods, processes, and evaluation criteria by which the systems, subsystems and work products are verified against their requirements in a written plan Develop system design solution that satisfies the system requirements and fulfills the functional analysis Review and provide input to program and contract work breakdown structure (WBS), work packages and the integrated master plan (IMP) Participate in the development of system engineering documentation, such as System Engineering Plans, Initial Capabilities Documents, Requirements Specifications, and Interface Control Documents Participate in interface definition, design, and changes to the configuration between affected groups and individuals throughout the life cycle Derive from the system requirements an understanding of stakeholder needs, functions that may be logically inferred and implied as essential to system effectiveness Derive lower-level requirements from higher-level allocated requirements that describe in detail the functions that a system component must fulfill, and ensure these requirements are complete, correct, unique, unambiguous, realizable, and verifiable Participate in establishing and gaining approval of the definition of a system or component under development (requirements, designs, interfaces, test procedures, etc.) that provides a common reference point for hardware and software developers Develop derived requirements for Information Assurance Services (Confidentiality, Integrity, Non repudiation, and Availability); Basic Information Assurance Mechanisms (e.g., Identification, Authentication, Access Control, Accountability); and Security Mechanism Technology (Passwords, cryptography, discretionary access control, mandatory access control, hashing, key management, etc.) REQUIREMENTS 1-3 years of experience testing web applications Experience testing applications interfacing with REST-based APIs/services TS/SCI clearance w/full-scope Polygraph Required NICE TO HAVES Experience testing web applications which use modern JavaScript front-end frameworks (e.g. React, Angular, Vue) and/or backend run-time environment (Node.js) Experience developing automated test scripts (e.g. Selenium, SoapUI, Katalon) Experience working with applications which use NoSQL (i.e. MongoDb, Elasticsearch), advanced key store (i.e. REDIS), and/or relational (i.e. Oracle, MySQL) databases. Experience with writing Linux based scripts to facilitate application integration using a one or more appropriate server-side languages (i.e. Shell, Python, etc.) Experience working in an Agile software development environment Experience managing software code using Git & MAVEN Experience using Jira and Confluence",
        "url": "https://www.linkedin.com/jobs/view/3813096287"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Bethesda, MD",
        "job_id": 3926412858,
        "company": "Leidos",
        "title": "Data Storage Engineer",
        "created_on": 1720638349.4838648,
        "description": "Description At Leidos, we deliver innovative solutions through the efforts of our diverse and talented people who are dedicated to our customers’ success. We empower our teams, contribute to our communities, and operate sustainable practices. Everything we do is built on a commitment to do the right thing for our customers, our people, and our community. Our Mission, Vision, and Values guide the way we do business. Employees enjoy career enrichment opportunities available through mobility and development and experience rewarding relationships with supportive supervisors and talented colleagues and customers. Your most important work is ahead. If this sounds like the kind of environment where you can thrive, keep reading! Leidos is looking to fill a Storage Engineer position to support the National Media Exploitation Center (NMEC). This role requires technical expertise in storage engineering, system administration, and operations. This role is responsible for optimizing the configuration of the enterprise storage infrastructure to maximize availability, performance, and capacity. This role participates in the lifecycle process of purchasing, installing, maintaining, and replacing storage devices across on-premise and cloud IT environments. This role collects and evaluates metrics to optimize storage operations. This individual identifies risks, performs assessments, and analyzes risk mitigation strategies. Finally, this individual is responsible for developing and maintaining system documentation and procedures in accordance with customer requirements. The Customer utilizes an Agile Framework to plan and successfully complete all initiatives. The work location is in Bethesda at the Intelligence Community Campus. Primary Responsibilities Install, configure, and administer enterprise storage systems from multiple vendors to include, but not limited to, Dell EMC and Quantum Install, configure, and administer Rubrik server and client software. Schedule and monitor backup policies. Generate backup status reports as required Administer Cisco Brocade fibre channel switch fabrics. Configure single initiator zone sets and monitor switch utilization and performance Create and secure SMB file shares Coordinate with server administrators to create, secure, and mount NFS exports Develop and implement Disaster Recovery and Continuity of Operations plans in accordance with customer requirements Install, configure, maintain, and administer Linux and Windows servers in support of storage operations such as Rubrik, Media, DSM and Master servers Build capacity management dashboards that improve visibility into enterprise storage use, health, resilience, and provide ongoing resources for planning for enterprise infrastructure lifecycle replacement Configure Rubrik policies. Troubleshoot and resolve backup job failures Coordinate with vCenter administrators to provision Storage Area Network (SAN) resources to VMWare ESX cluster datastores as required Configure and manage volume snapshots. Assist customers with point-in-time file restores Schedule and monitor volume replication for disaster recovery Monitor health and availability of infrastructure applications and systems Work with vendors to troubleshoot and resolve hardware and software failures Coordinate with Change Management to deconflict and schedule hardware maintenance activities, system and firmware updates, and other configuration changes Execute system changes during scheduled maintenance windows Monitor and evaluate system performance and capacity trends Resolve tier 2 and tier 3 service requests Identify opportunities to automate operations. Develop automated solutions that take advantage of Dell and NetApp system APIs Seek opportunities for continuous improvement to support effective and efficient operations Work with Cloud Storage solutions as needed for Cloud applications Work independently with minimal supervision Mentor junior team members Basic Qualifications A Bachelor of Science degree in Computer Science or similar technical field, program management, or closely related discipline, and at least 8+ years' years of related technical experience. Extensive experience administering enterprise-level data storage systems (Dell Storage Center, Dell Compellent, and Data Domain preferred) with a demonstrated understanding of the following services and technologies SAN administration including provisioning of LUNs, managing fibre channel zoning, and mapping storage to servers NAS administration including NFS and SMB file sharing Integration with VMWare ESX Server Data migration strategies Snapshots Volume replication Managing file and share permissions in an Active Directory environment Storage monitoring and performance tuning Understanding of Cloud storage management Extensive experience managing and administering Veritas Rubrik Experience with scripting technologies such as PowerShell Working knowledge of Unix or Linux based operating systems Working knowledge of Distributed File System (DFS) Excellent communication skills Ability to mentor junior team members Ability to complete complex projects with minimal direction Experience working independently to support a 24/7/365 customer environment Experience contributing to deliverables and meeting performance metrics Experience coordinating with senior management and customers Candidate must, at a minimum, meet DoD 8570.11- IAT Level II certification requirements (currently Security+ CE, CCNA-Security, GSEC, or SSCP along with an appropriate computing environment (CE) certification) Clearance Active TS/SCI clearance with Polygraph required OR active TS/SCI and willingness to get a Poly. US Citizenship is required due to the nature of the government contracts we support. Preferred Qualifications Experience with Disaster Recovery and COOP storage backup and recovery within and across the data centers and the Cloud Ability to analyze cloud migration approaches to support customer decision making for effective and efficient cloud migrations Experience managing Distributed File System (DFS) namespaces Original Posting Date 2024-05-14 While subject to change based on business needs, Leidos reasonably anticipates that this job requisition will remain open for at least 3 days with an anticipated close date of no earlier than 3 days after the original posting date as listed above. Pay Range Pay Range $101,400.00 - $183,300.00 The Leidos pay range for this job level is a general guideline onlyand not a guarantee of compensation or salary. Additional factors considered in extending an offer include (but are not limited to) responsibilities of the job, education, experience, knowledge, skills, and abilities, as well as internal equity, alignment with market data, applicable bargaining agreement (if any), or other law.",
        "url": "https://www.linkedin.com/jobs/view/3926412858"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Washington, DC",
        "job_id": 3843097353,
        "company": "Constellation Technologies, Inc",
        "title": "Software Engineer (Java) - multiple levels",
        "created_on": 1720638353.0023627,
        "description": "About us: Mission Driven, Employee Focused At CTI, you’ll be at the center of an award-winning corporate culture, breaking technological barriers and solving real-world problems for our federal government customers. We are committed to hiring the best of the best, and in return, we offer a world-class, truly unique employee experience that is rare within our industry. If you’re a technical changemaker with a passion for Cyber Operations, Cloud and Data Analytics, or Engineering, we’re looking for you! Love what you do AND where you work - alongside a supportive, innovative team of like-minded individuals. After all, we know that your best work happens when you live your best life, and we do everything we can to make that possible. Are you ready for your best career move? Intel Agency polygraph is strongly preferred. Due to federal contract requirements, United States citizenship and an active TS/SCI security clearance is required for the position. Description: CTI is seeking an experienced TS/SCI polygraph cleared Java Software Engineer to contribute to Java software development activities both individually and as a member of an agile team. Actively participate in program increment planning and related team activities. Promote code reuse and cross-program collaboration while reducing maintenance costs by creating common functions and shared actions for developers and testers. Engineer, author, tune and document automation scripts in a development environment and deploy to the test/production bench. The day-to-day: As a Java Software Engineer you will perform software development activities as a member of an agile development team. Development includes the full range of turning agile stories into implementable concepts, through development, testing, and deployment of the new capabilities in this complex system. There are a range of opportunities for server-side business logic implementation as well as client web application and user interface (UI) development. The qualifications (required): Must be a US Citizen Must have TS/SCI clearance w/ active polygraph Must have demonstrated experience with Java Experience or familiarity with multiple the following: Java/JEE, Python, C/C++, SQL, SOAP, WSDL, WADL, PERL, PowerShell, VBS, Eclipse, Postgres, Oracle, Jenkins Experience with Web Application User Interface Development, knowledge of databases and structures, and/or experience working with JSON, HTML, XML, XSLT Experience with technologies underlying cryptographic systems (symmetric and asymmetric cryptography, ASN.1 encoding, XML canonicalization, digital signatures) Must have documented professional experience with web services Must have a strong understanding of sound Java software development principles and practices Must be an independent thinker, capable of performing high quality work, both independently and with a team in a fast-moving environment The nice-to-haves (desired, not required): Bachelor's degree in Computer Science or a related discipline Experience with the following: JEE (EJB, JPA, JTA, JAX-B, JAX-RS, JAX-WS), SQL, application servers (Tomcat, WebLogic, JBoss), scripting.\\ Experience with high level requirements management including requirements decomposition, secure systems engineering and development, trade-off analysis, interface control, and testing and continuous integration Experience in software development on Agile teams using Agile Developer practices such as Pair Programming, TDD, Refactoring, and ATDD Experience with FITNesse, Mockito, Cucumber, Unified Functional Tester (UFT), Selenium Experience with Behavior Driven Development (BDD) Secure Software development (i.e., Layer 7 Policy) Experience with the Scaled Agile Framework (SAFe) methodology, SAFe Agilest Certification, or experience as a member of an agile team Additional experience in J2EE, Python, C/C++, SQL, SOAP, WSDL, Postgres, Oracle, Mongo, PowerShell a plus The benefits package: Affordable healthcare options with 80% employer paid premium PLUS a company-funded HSA Dental insurance with 100% employer paid premium Vision with 80% employer paid premium Employer paid Life insurance 100% Employer paid Short-term and Long-term disability 100% Annual training, continued education, and professional memberships reimbursement Unlimited access to Red Hat Enterprise Linux and AWS training and accreditation Annual reimbursement for technology i.e. phones, computers, printers, etc 401(k) with company match up to 5% with 100% immediate vesting (after 90 days of employment) The environment and perks: Professional development investment and paid time off for training Contract and work locations in Maryland, Virginia, Colorado, Texas, Utah, California, Florida and Hawaii Team building events throughout the year such as Destination Family Events, Holiday Party, Monthly Get-Togethers Leadership Team engagement and mentorship Performance Recognition Program Complimentary branded apparel Don't see a job opening that's the perfect fit? Apply to our General Position to join our talent pool for consideration for future opportunities. Know someone else who may be a good fit? Refer them through the CTI External Referral Program and you could receive a one-time referral bonus of up to $10,000! Email cti-staffing@cti-md.com for more information. Constellation Technologies is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, religion, creed, color, national origin, ancestry, sex (including pregnancy, childbirth, breastfeeding, or medical conditions related to pregnancy, childbirth, or breastfeeding), age, medical condition, marital or domestic partner status, sexual orientation, gender, gender identity, gender expression and transgender status, mental disability or physical disability, genetic information, military or veteran status, citizenship, low-income status or any other status or characteristic protected by applicable law. Job applicants can submit questions about CTI’s equal employment opportunity policy to cti-hr@cti-md.com. The pay range for this job level is a general guideline only and not a guarantee of compensation or salary. Additional factors considered in extending an offer include (but are not limited to) responsibilities of the job, education, experience, knowledge, skills, and abilities, as well as internal equity, alignment with market data, applicable bargaining agreement (if any), or other law.",
        "url": "https://www.linkedin.com/jobs/view/3843097353"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Columbia, MD",
        "job_id": 3787796178,
        "company": "Quotient",
        "title": "DevOps Engineer",
        "created_on": 1720638355.115007,
        "description": "builds digital experiences that empower and inspire! We’re looking for Engineers with a passion for problem solving. Through our work with America Makes, our team has been building 3D data repositories to support additive manufacturing (3D printing) efforts across the country. We are adding a DevOps Engineer to a multidisciplinary team that’s supporting a revolutionary 3D printing solution, which lets users find and discover models from multiple sources through a single website. The website enables users to search models by varying criteria like material, dimensions, and associated keywords. While each of our developers bring their own set of experiences and expertise, this solution uses Drupal, Oracle DB, and Java in a DOD environment. Candidates: Formulates/defines specifications for complex operating software programming applications or modifies/maintains complex existing applications using engineering releases and utilities from the manufacturer. Designs, codes, tests, debugs, and documents those programs. Provides overall operating system maintenance, such as sophisticated file maintenance routines, large telecommunications networks, computer accounting, and advanced mathematical/scientific software packages. Assists all phases of software systems programming applications. Evaluates new and existing software products. In addition, each team member must meet these DOD requirements: Must be a US Citizen or Permanent resident Must have or have held a Secret Clearance Have a certification from one of the DoD approved IA baseline certification vendors: Global Information Assurance Certification (GIAC) Information Systems Audit and Control Association (ISACA) International Information Systems Security Certifications Consortium (ISC)2 EC-Council Computing Technology Industry Association (CompTIA) Cisco We are an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender, gender identity, national origin, disability, or protected veteran status. Powered by JazzHR LaaB3vj0D0",
        "url": "https://www.linkedin.com/jobs/view/3787796178"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Reston, VA",
        "job_id": 3970185895,
        "company": "ZipRecruiter",
        "title": "Software Development Engineer, AWS Cross Domain Services",
        "created_on": 1720638356.9552586,
        "description": "Job Description Join us in building and developing novel solutions to solve big data problems and provide analytic visibility into the AWS Cross Domain Solutions (CDS) workflow. This position requires that the candidate selected be a US and obtain and maintain an active TS/SCI security clearance with polygraph. 10012 Key job responsibilities Amazon Web Services (AWS) Is Seeking a Talented Software Development Engineer To Design And Build An Analytic Platform To Solve Problems And Provide Analytic Visualization Of Operational Data. As a Software Development Engineer On The Pattern Team Within AWS Cross Domain Services (CDS), You Will Work with Data Scientists to develop, train, and validate machine learning models using techniques like clustering, classification, and neural networks for anomaly detection, while evaluating model performance. Build and maintain data pipelines, perform data cleaning and transformation, leverage programming languages like Python and SQL, and work with databases and data warehouses to enable effective data engineering for anomaly detection. Design and implement scalable data processing and analysis solutions using big data frameworks to handle large datasets and support robust anomaly detection capabilities. Have questions about this role? Start a chat with the recruiter today! Please reach out to Krystan Silva at skrystan@amazon.com for inquiries. About The Team We're a small, independent team within AWS Cross Domain Solutions (CDS) working on providing visibility into the CDS workflow. Our team works with big data to solve the analytical needs of CDS. We need Developers who move fast, are capable of breaking down and solving complex problems, and have a strong will to get things done. We are open to hiring candidates to work out of one of the following locations: Arlington, VA, USA BASIC QUALIFICATIONS- 3+ years of non- professional software development experience 3+ years of non- design or architecture (design patterns, reliability and scaling) of new and existing systems experience Experience programming with at least one software programming Bachelor's Degree in Computer Science, Mathematics or a related engineering field, or equivalent years of experience in software development QUALIFICATIONS- 5+ years of full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations experience Experience in machine learning, data mining, information retrieval, statistics or natural processing Experience building complex software systems that have been successfully delivered to customers Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of , origin, , , sexual , protected veteran status, , , or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en//us.",
        "url": "https://www.linkedin.com/jobs/view/3970185895"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Washington, DC",
        "job_id": 3970730644,
        "company": "ClickJobs.io",
        "title": "Senior Data Engineer (Python, Kafka, Databricks, AWS)",
        "created_on": 1720638358.6260204,
        "description": "Plano 1 (31061), United States of America, Plano, Texas Senior Data Engineer (Python, Kafka, Databricks, AWS) Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking Data Engineers who are passionate about marrying data with emerging technologies. As a Capital One Data Engineer, you’ll have the opportunity to be on the forefront of driving a major transformation within Capital One. What You’ll Do: Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and Snowflake Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance Basic Qualifications: Bachelor’s Degree At least 4 years of experience in application development (Internship experience does not apply) At least 1 year of experience in big data technologies Preferred Qualifications: 5+ years of experience in application development including Python, SQL, Scala, or Java 2+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud) 3+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL) 2+ year experience working on real-time data and streaming applications 2+ years of experience with NoSQL implementation (Mongo, Cassandra) 2+ years of data warehousing experience (Redshift or Snowflake) 3+ years of experience with UNIX/Linux including basic commands and shell scripting 2+ years of experience with Agile engineering practices 2+ years of experience with data streaming applications At this time, Capital One will not sponsor a new applicant for employment authorization for this position. The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. New York City (Hybrid On-Site): $165,100 - $188,500 for Senior Data Engineer Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate’s offer letter. This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan. Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. This role is expected to accept applications for a minimum of 5 business days. No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City’s Fair Chance Act; Philadelphia’s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries. If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "url": "https://www.linkedin.com/jobs/view/3970730644"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Annapolis Junction, MD",
        "job_id": 3949655244,
        "company": "Vibrint",
        "title": "System Engineer SE I",
        "created_on": 1720638360.316749,
        "description": "Vibrint is a trusted provider of mission-critical systems and analysis that transform our customers' capacity and capability in harvesting and harnessing data. Working alongside many of the most talented professionals in public service, we work tirelessly to create and sustain new solutions and services that meet the stringent demands across a variety of customer missions. Our people know they'll be doing work that matters at the heart of the national security mission, exploring new possibilities at the cutting edge of technology. They know they will be well-rewarded and recognized for their commitment. And, they know they will enjoy plentiful opportunities to grow, thrive, and have fun as a member of the Vibrint family. Join Vibrint, where your career is a priority, and your future is our shared goal. The selected Systems Engineer will serve as a site Engineer at our Annapolis Junction, MD location performing remote maintenance and refresh efforts for 8 sites. Travel to sites expected on a yearly basis. Performs daily interaction with partner entities. Position requires an active TS/SCI with Polygraph level clearance. US citizenship is required. Required Education And Years Of Experience A Bachelor's degree in System Engineering, Computer Science, Information Systems, Engineering Science, Engineering Management, or related discipline from an accredited college or university is required. Seven (7) years' experience as a SE in programs and contracts of similar scope, type and complexity is required. Five (5) years of additional SE experience may be substituted for a bachelor's degree. Required Skills The Engineer will need experience in Linux and Ciena to support with system administration, troubleshooting, loading software, configuration of telecom network and software packages. Needs knowledge of cables and collection systems and how they work as well as understand data flow. Engineer will engage SMEs, perform data validation once software is deployed, submit DMRs for data flow path problems/fixes, configure new software updates, and perform troubleshooting. Must be experienced in NiFi and Linux. Call in support is required under this TTO. The Contractor Team shall identify contract personnel who shall serve in a call-in capacity as part of one or more call-in teams, in addition to their regular duties. Call-in personnel shall provide technical assistance and technical support to Government operations at any time, including days, nights, weekends, and holidays. Call-in response times are expected to be within a two (2) hour window on a best effort basis. Upon request the salary range for this job will be made available. Please email your request for information along with the job listing to recruiting@vibrint.com. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, citizenship, family structure, marital status, disability, veteran or military status or any other characteristic protected by law in all phases of the employment process and in compliance with applicable federal, state, and local laws and regulations. An equal opportunity employer/disability/vet. Policy-Statement_EEO-EmployeesAndCandidates.pdf (vibrint.com). Equal opportunity legal notices can be viewed on the following PDFs: Know Your Rights: Workplace Discrimination is Illegal and Pay Transparency Nondiscrimination Provision. We are offering a sign-on bonus!",
        "url": "https://www.linkedin.com/jobs/view/3949655244"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Annapolis Junction, MD",
        "job_id": 3937596012,
        "company": "Raytheon",
        "title": "Database Engineer 3",
        "created_on": 1720638362.3343935,
        "description": "Date Posted: 2023-08-14 Country: United States of America Location: MD233: 420 National Business Parkway 420 National Business Parkway Suite 400, Annapolis Junction, MD, 20701 USA Position Role Type: Onsite You have been redirected to RTX’s career page as we have recently transitioned from RTX to become a standalone company, which provides us with greater autonomy and opportunities for growth. As a prospective employee of Nightwing, you’ll have the chance to contribute to our continued success and shape the future of our cybersecurity, intelligence, and services offerings. Job Summary The Database Engineer provides technical expertise for database design, development, implementation, information storage and retrieval, data flow and analysis. Develops relational and/or Object-Oriented databases, database parser software, and database loading software. Projects long range requirements for database administration and design. Responsible for developing a database structure that fits into the overall architecture of the system under development and has to make trades among data volumes, number of users, logical and physical distribution, response times, retention rules, security and domain controls. The DBE works primarily at the front end of the lifecycle-requirements through system acceptance testing and Initial Operational Capability (IOC). Develops requirements from a project’s inception to its conclusion for a particular business and Information Technology (IT) subject matter area (i.e., simple to complex systems). Assist with recommendations for, and analysis and evaluation of systems improvements, optimization, development, and/or maintenance efforts. Translates a set of requirements and data into a usable document by creating or recreating ad hoc queries, scripts, and macros; updates existing queries, creates new ones to manipulate data into a master file; and builds complex systems using queries, tables, Open Database Connectivity and database storage and retrieval using Cloud methodologies. Role Type This position is an onsite role. Basic Qualifications: Typically requires Ten (10) years of experience as a Database Engineer on programs and contracts of similar scope, type, and complexity and Bachelor’s degree in a technical discipline from an accredited college or university. Five (5) years of database engineering experience may be substituted for a bachelor’s degree. Support the database design, development, implementation, information storage and retrieval, data flow and analysis activities Support the analysis and evaluation of system improvements, optimization, development and/or maintenance efforts Support the development of long and short term requirements for database administration and design Assist in developing databases, database parser software, and database loading software Translate a set of requirements and data into a usable database schema by creating or recreating ad hoc queries, scripts and macros, updates existing queries, creates new ones to manipulate data into a master file Assist in developing database structures that fit into the overall architecture of the system under development (U) Lead development of database structures that fit into the overall architecture of the system under development Lead development of databases, database parser software, and database loading software Develop requirement recommendations from a project’s inception to its conclusion for a particular Business and IT subject matter area (i.e. simple to complex systems) Develop a database structure that fits into the overall architecture of the system under development and has to make trades among data volumes, number of users, logical and physical distribution, response times, retention rules, security and domain controls Direct fulfillment of requirements from a project’s inception to it conclusion Direct organization of requirements and data into a usable database schema by directing development of ad hoc queries, scripts, macros, updates to existing queries Direct the overall database structure that fits into the overall architecture of the system under development and has to make trades among data volumes, number of users, logical and physical distribution, response times, retention rules, security and domain controls Direct the development of complex systems using queries, tables, Open Database Connectivity and database storage and retrieval using Cloud methodologies Active and transferable U.S. government issued TS/SCI security clearance is required prior to start date. Preferred Qualifications: Mongo and Postgres experience What We Offer : Whether you’re just starting out on your career journey or are an experienced professional, we offer a total rewards package that goes above and beyond with compensation; healthcare, wellness, retirement and work/life benefits; career development and recognition programs. Some of the benefits we offer include parental (including paternal) leave, flexible work schedules, achievement awards, educational assistance and child/adult backup care. Employee Referral Award Eligibility: This requisition is eligible for an employee referral award. ALL eligibility requirements must be met to receive the referral award. Previously part of a leading Fortune 100 company and headquartered in Dulles, VA; Nightwing became independent in 2024 but continues to support the nation’s most mission impactful initiatives. When we formed Nightwing, we brought a deep set of credentials and an unfaltering commitment to the mission. For over four decades, our team has been providing some of the world’s most technically advanced full-spectrum cyber, data operations, systems integration and intelligence support services to the U.S. government on its most important missions. At Nightwing, we value collaboration and teamwork. You’ll have the opportunity to work alongside talented individuals who are passionate about what they do. Together, we’ll leverage our collective expertise to drive innovation, solve complex problems, and deliver exceptional results for our clients. Thank you for considering joining us as we embark on this new journey and shape the future of cybersecurity and intelligence together as part of the Nightwing team. The salary range for this role is 118,000 USD - 246,000 USD. The salary range provided is a good faith estimate representative of all experience levels. RTX considers several factors when extending an offer, including but not limited to, the role, function and associated responsibilities, a candidate’s work experience, location, education/training, and key skills. Hired applicants may be eligible for benefits, including but not limited to, medical, dental, vision, life insurance, short-term disability, long-term disability, 401(k) match, flexible spending accounts, flexible work schedules, employee assistance program, Employee Scholar Program, parental leave, paid time off, and holidays. Specific benefits are dependent upon the specific business unit as well as whether or not the position is covered by a collective-bargaining agreement. Hired applicants may be eligible for annual short-term and/or long-term incentive compensation programs depending on the level of the position and whether or not it is covered by a collective-bargaining agreement. Payments under these annual programs are not guaranteed and are dependent upon a variety of factors including, but not limited to, individual performance, business unit performance, and/or the company’s performance. This role is a U.S.-based role. If the successful candidate resides in a U.S. territory, the appropriate pay structure and benefits will apply. RTX anticipates the application window closing approximately 40 days from the date the notice was posted. However, factors such as candidate flow and business necessity may require RTX to shorten or extend the application window. RTX is An Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or veteran status, age or any other federally protected class. Privacy Policy and Terms: Click on this link to read the Policy and Terms",
        "url": "https://www.linkedin.com/jobs/view/3937596012"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Chevy Chase, MD",
        "job_id": 3956943568,
        "company": "Get It Recruit - Finance",
        "title": "Staff Software Engineer - Platform (Finance Data) -- Remote | WFH",
        "created_on": 1720638364.1975589,
        "description": "Position Description Our Staff Engineer plays a pivotal role in our engineering team, collaborating across the organization to ensure a seamless experience for our customers while upholding top-tier standards of security and availability. Our team thrives in delivering high-quality technology products and services, influencing best practices in a dynamic, rapidly evolving environment. The ideal candidate possesses extensive technical expertise, ranging from backend resource management to system reliability, with a strong focus on Platform and Data Engineering. They will lead the creation and management of a Finance Data Lake, integrating multiple edge sources from inception. Position Responsibilities As a Staff Engineer, You Will Take ownership and drive the end-to-end execution of the Data Lakehouse for Finance Data, ensuring its effectiveness and reliability. Provide leadership to engineering teams, guiding multiple areas of focus. Oversee the entire lifecycle of solutions, from inception through maintenance. Collaborate with engineering leadership, team members, and stakeholders to solve complex challenges and build robust business applications. Ensure automation, quality, usability, and performance are central to all solutions. Lead design sessions and code reviews to elevate engineering standards. Utilize Python, SQL, NoSQL databases, and Container Orchestration services (e.g., Terraform, Docker, Kubernetes) along with Azure tools to develop an Event-Driven Big Data Streaming platform for ELT data pipelines. Mentor junior team members to foster their professional growth. Share best practices and improve processes across teams. Qualifications Proficiency in at least two modern programming languages such as Java or Python, including object-oriented design and PowerShell scripting. Experience in architecting and designing scalable systems, including reliability and scaling considerations. Expertise in Event-Driven Big Data streaming infrastructure and ETL/ELT frameworks (e.g., Spark Streaming, Flink, Kafka, Hive, Airflow). Demonstrated ability in deploying highly robust and scalable data pipelines processing petabytes of data. Familiarity with Hadoop, SQL, and NoSQL platforms, as well as various file formats (Iceberg, Avro, JSON, Parquet). Fluency in DevOps concepts, Containerization, Test Automation, CI/CD, and Infrastructure as Code (e.g., GitHub, Kubernetes, Docker, Terraform, Helm, Ansible). Experience with Azure services (e.g., Azure DevOps, Azure Data Lake, Azure Data Factory, Azure Databricks, Azure Storage). Proficiency in observability and monitoring platforms (e.g., Grafana, Azure Monitoring, AppInsights, Dynatrace). Experience in performance tuning for applications handling large datasets. Knowledge of Load Testing and Quality Assurance processes. Strong communication skills, both verbal and written. Experience 6+ years of professional experience in data software development and big data technologies. 4+ years of experience with open-source frameworks. 3+ years of experience in system architecture and design. 3+ years of experience with AWS, GCP, Azure, or another cloud service. Education Bachelor's degree in Computer Science, Information Systems, or equivalent practical experience. Salary Range Annual salary: $110,000.00 - $236,500.00 (Note: Salary to be determined based on candidate qualifications and other factors.) Benefits Comprehensive Total Rewards Program including premier medical, dental, and vision insurance with no waiting period, paid vacation, sick leave, parental leave, 401(k) plan, tuition reimbursement, paid training, and licensures. Benefit eligibility varies by location and may depend on length of service. Equal Employment Opportunity We are committed to providing equal employment opportunities to all individuals regardless of race, color, religion, national origin, ancestry, age, gender, pregnancy, sexual orientation, gender identity, marital status, familial status, disability, or genetic information. Our hiring and promotion decisions are based solely on qualifications relevant to the job. Employment Type: Full-Time",
        "url": "https://www.linkedin.com/jobs/view/3956943568"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Washington, DC",
        "job_id": 3962972738,
        "company": "Get It Recruit - Information Technology",
        "title": "Lead Database Engineer - Remote | WFH",
        "created_on": 1720638370.125744,
        "description": "At our organization, every team member plays a pivotal role in transforming lives through financial empowerment and personalized wealth management strategies. We foster a workplace where job satisfaction thrives, offering you the freedom to support causes that matter and ensuring inclusivity in every aspect of our operations. Your journey to a fulfilling career begins here. Position Overview We are looking for an accomplished Lead Database Administrator with expertise in MySQL, MySQL Aurora, Redshift, and AWS Cloud Services (experience with PostgreSQL and DynamoDB is a plus). The ideal candidate will possess a robust background in managing diverse database systems in production environments, proficiency in cloud-based solutions, and skills in implementing effective sharding strategies. This role involves designing, implementing, and maintaining our expansive database infrastructure, while providing leadership and mentorship to our DBA team. Key Responsibilities Design, implement, and maintain MySQL, MySQL Aurora, Redshift, PostgreSQL, and DynamoDB database schemas, tables, indexes, and relationships in AWS Cloud environments for optimal performance and scalability. Deploy and manage MySQL Aurora instances, RedShift, PostgreSQL instances, and DynamoDB tables to ensure high availability and performance scalability. Configure and optimize Redshift clusters for data warehousing and analytics, focusing on data loading, query optimization, and performance tuning. Implement database sharding strategies for horizontal data partitioning across multiple shards to enhance scalability and performance. Develop and implement backup and recovery strategies across various databases to maintain data integrity and availability during failures or disasters. Manage user access, permissions, and security policies for databases to safeguard sensitive data and comply with regulatory standards. Design and implement high availability and disaster recovery solutions using AWS services such as Aurora Global Database, Multi-AZ deployments, and DynamoDB Global Tables. Lead and mentor a team of database administrators by providing technical guidance, support, and training. Collaborate closely with developers, DevOps engineers, and data engineers to optimize database solutions for various applications and workloads. Stay updated on database technologies, AWS services, and industry trends to drive continuous improvement initiatives. Document database configurations, procedures, and troubleshooting guides for knowledge sharing and reference. Required Skills And Experience Minimum of 8 years of experience as a Database Administrator, specializing in MySQL, MySQL Aurora, and Redshift. Profound understanding of database architectures, replication, clustering, and performance optimization techniques across different database systems. Hands-on experience with AWS Cloud services including RDS, Aurora, Redshift, DynamoDB, EC2, S3, IAM, and CloudWatch. Experience in implementing database sharding techniques for horizontal data partitioning. Strong grasp of database security concepts, including authentication, authorization, encryption, and compliance. Excellent troubleshooting skills with the ability to diagnose and resolve complex database issues. Proven experience in leading and mentoring a team of DBAs. Strong communication and interpersonal skills, adept at collaborating effectively with cross-functional teams. Preferred Qualifications Relevant certifications (e.g., AWS Certified Database - Specialty) would be advantageous. Familiarity with NoSQL databases, PostgreSQL, and additional AWS Cloud services. Note: Applicants must have authorization to work for any employer in the U.S. Sponsorship for employment visas, including CPT/OPT, is not available at this time. Location: Remote Employment Type: Full-Time",
        "url": "https://www.linkedin.com/jobs/view/3962972738"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Reston, VA",
        "job_id": 3819435069,
        "company": "Paradoxum Games",
        "title": "Software Engineer, Infrastructure",
        "created_on": 1720638371.889189,
        "description": "Paradoxum Games is looking for a new infrastructure software engineer to help us move off of the cloud and into the datacenter! As a infrastructure software engineer, you'll work directly with our principal engineers to develop & maintain new, reliable, and performant infrastructure that serves millions of people each month. You're a great fit for this role if you: Required: have 2-3+ years of experience administering and maintaining large-scale container orchestration systems (Kubernetes, HashiCorp Nomad, Docker Swarm) Required: have a solid background (2-3+ years of experience) in low-level systems programming (C++, Rust, Go) Required: are intimately familiar with industry-standard networking protocols (BGP, VRRP, OSPF) Required: are OK with being on-call and ready to respond to incidents Required: have excellent written / verbal communication skills in English Required: are OK with occasional travel to datacenter sites (Ashburn, VA) have great problem solving skills - not every problem is straight forward here! have great organizational / planning skills, with a keen sense of exactly when things need to be re-organized have a passion for both playing and making games have experience debugging complex problems at scale across multiple different components like high impact projects and project ownership, designing infrastructure that hundreds of thousands of players use daily are familiar with next-generation software-defined VPNs (Tailscale, WireGuard, etc) understand the importance of having good metrics / telemetry (OpenTelemetry, Victoria Metrics, Jaeger) don't mind occasionally (and we mean, occasionally) working on the weekends to get something out the door are familiar with common network topologies (Hub/Spoke, Backbone, etc) don't mind pulling out Wireshark to debug a networking issue, or getting your hands dirty and writing a Wireshark dissector for a bespoke network protocol you designed have an appreciation for well-written, thoughtful, and down-right beautiful code know the T568B wiring standard by heart can lift over 50+ lb easily (2U servers are heavy!) You will: Work with other team members located remotely via Slack / Google Meet Collaborating with cross-disciplinary peers, manage complex initiatives that span multiple peers and teams, and leading projects start to finish, including assessing risk, setting goals, and following up with peers to ensure timely delivery Write clean, maintainable, and dependable Rust / Go code powering Paradoxum Games Write Helm charts and deploy services on our Kubernetes cluster Help us stand up and maintain our bare-metal server fleet, located in Ashburn, VA Monitor and respond to any incidents affecting user-facing services Bonus points: Experience working in a self-managed or remote environment Understanding of the Roblox platform and its' limitations Have experience contributing to the Open Source community Have existing code snippets of your current work, either on GitHub or provided as files Please note that this job is only currently available to people that are 18+ and are based in the US! While we'd love to hire from outside the US, currently we're not able to at this time. Please note that this compensation is for employees based near our headquarters in Reston, VA. The actual base pay is dependent upon a variety of job-related factors such as professional background, training, work experience, location, business needs and market demand. Therefore, in some circumstances, the actual salary could fall outside of this expected range. This pay range is subject to change and may be modified in the future.",
        "url": "https://www.linkedin.com/jobs/view/3819435069"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Andrews AFB, MD",
        "job_id": 3960990592,
        "company": "Koniag Government Services",
        "title": "Data Analyst/Power BI Engineer (TS)",
        "created_on": 1720638373.6252306,
        "description": "Kadiak Job ID 24200125 Kadiak, LLC, a Koniag Government Services company, is seeking an experienced Data Analyst/Power BI Engineer with an active Top Secret Clearance to support Kadiak and our government customer at Andrews Air Force Base, MD. Please do not apply for this position if you do not possess the requisite Top Secret Clearance. We offer competitive compensation and an extraordinary benefits package including health, dental and vision insurance, 401K with company matching, flexible spending accounts, paid holidays, three weeks paid time off, and more. Position Summary: Data analysts gather, clean, analyze, visualize, and present existing data to help inform business decisions. An effective data analyst uses data to answer a question and empower decision makers to plot the best course of action. Common tasks for a data analyst might include: Working with business leaders and stakeholders to define a problem or business need. Identifying and sourcing data. Cleaning and preparing data for analysis. Analyzing data for patterns and trends. Visualizing data to make it easier to understand. Presenting data in such a way that it tells a compelling story. General Responsibilities: Experience using Power BI, Tableau, R, and other analytical tools for data visualization and reporting. Strong analytical skills with the ability to interpret complex data sets. Knowledge of SQL and SharePoint data structure for data querying and manipulation. Experience with Excel, including pivot tables and data analysis. Required Skills and Responsibilities: Must have excellent written and verbal communication skills. Must have proficient Microsoft Office Skills (Word, Excel, PowerPoint, MS Project). Must be able to work in a team environment cordially and effectively, as well as having the ability to lead a team if required. Must have excellent interpersonal skills and be able to relate respectfully, cordially and freely with people (customers, employees and vendors inclusive). Must have excellent leadership abilities. Must be able to motivate other employees when needed. Must exhibit a high level of flexibility and an ability to multi-task at all times. Must be able to work with little or no supervision. Must possess good reasoning and problem-solving abilities. Must have the ability to work as a ‘link person’ or middle person amongst customers, employees, and vendors. Must possess at least a basic knowledge of supply chain activities. Must have good time management ability so as to be able to meet up with orders and schedules. Minimum Requirements: BS / BA in Computer Science, Information Systems, Engineering, Business, Program Management, or other related disciplines. Microsoft Office Specialist: Associate (Office 2019). Power BI Microsoft Certification. Demonstrated experience with Confluence. Jira Certification: Managing Jira Projects. DOD 8570 (IAT Level II) Minimum Requirement - Security+ CE Relevant experience can be substituted for education Highly Preferred: MS / MA in Computer Science, Information Systems, Engineering, Business, Program Management, or other related disciplines. Microsoft Office Specialist: Expert Atlassian Certified Expert Seven years of specific experience. Ten years of general experieince. Key Qualifications: Shall provide interactive dashboards and reports using Power BI or similar tools to visualize key metrics and trends. Analyze data to identify patterns, trends, and insights that drive business decisions. Collaborate with cross-functional teams to develop solutions and recommendations based on data. Support ad-hoc analysis and reporting requests as needed. Experience using Power BI, Tableau, R, and other analytical tools for data visualization and reporting. Strong analytical skills with the ability to interpret complex data sets. Knowledge of SQL and SharePoint data structure for data querying and manipulation. Experience with Excel, including pivot tables and data analysis. Excellent communication skills with the ability to present findings to both technical and non-technical audiences. Detail-oriented with a passion for accuracy and data integrity. Shall be familiar with and can maintain, update, and configure software systems (e.g. Microsoft Office Suite, SharePoint, Jira, and Confluence). Clearance and Certifications: TS/SCI Required. Enrolled in CE with a SF86 in the last 12 months (PLEASE DO NOT APPLY IF YOU DO NOT MEET THIS REQUIREMENT). Proficiency in Microsoft Office Products®, (Microsoft® Project©, Power Point©, Microsoft® Word©, and Microsoft® Excel©). Required Tools/Technologies: An advocate for Information Technology specific to BMS and IoT systems. The ability to understand and transition complex requirements into practical effective solutions Previous experience collaborating with various stakeholders to meet project objectives, mission goals and ensure the smooth implementation of projects. To the greatest extent possible, possess the historical and practical institutional knowledge of customer facilities, spaces, and mission critical infrastructure. Ability to apply principles of project management to developing large, complex schedules Experience working closely with a Project Management Office (PMO) to contribute to processes for continuous improvement. Location: Andrews AFB, Maryland. Working Environment & Conditions: This job operates in a professional office environment and has a noise level of mostly low to moderate. This role routinely uses standard office equipment such as computers, phones, photocopiers, filing cabinets and fax machines. This position is primarily indoors, consistent with a standard office position and has a noise level of mostly low to moderate. The incumbent is required to stand; walk; sit; use hands to finger, handle, or feel objects, tools, or controls; reach with hands and arms; talk and hear. The workload may require the incumbent to sit for extended periods of time. The incumbent must be able to read, do simple math calculations and withstand moderate amounts of stress. The incumbent must occasionally lift and/or move up to 25 lbs. Specific vision abilities required by the job include close vision, distance vision, color vision, depth perception, and the ability to adjust focus. Our Equal Employment Opportunity Policy: The company is an equal opportunity employer. The company shall not discriminate against any employee or applicant because of race, color, religion, creed, sex, sexual orientation, gender or gender identity (except where gender is a bona fide occupational qualification), national origin, age, disability, military/veteran status, marital status, genetic information or any other factor protected by law. We are committed to equal employment opportunity in all decisions related to employment, promotion, wages, benefits and all other privileges, terms, and conditions of employment. The company is dedicated to seeking all qualified applicants. If you require an accommodation to navigate or to apply to a position on our website, please contact Heaven Wood via e-mail at accommodations@koniag-gs.com or by calling 703-488-9377 to request accommodations. Koniag Government Services (KGS) is an Alaska Native Owned corporation supporting the values and traditions of our native communities through an agile employee and corporate culture that delivers Enterprise Solutions, Professional Services and Operational Management to Federal Government Agencies. As a wholly owned subsidiary of Koniag, we apply our proven commercial solutions to a deep knowledge of Defense and Civilian missions to provide forward leaning technical, professional, and operational solutions. KGS enables successful mission outcomes for our customers through solution-oriented business partnerships and a commitment to exceptional service delivery. We ensure long-term success with a continuous improvement approach while balancing the collective interests of our customers, employees, and native communities. For more information, please visit www.koniag-gs.com Equal Opportunity Employer/Veterans/Disabled. Shareholder Preference in accordance with Public Law 88-352 Medical Insurance Vision Insurance Dental Insurance 401k Disability Maternity Tuition Assistance",
        "url": "https://www.linkedin.com/jobs/view/3960990592"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Chevy Chase, MD",
        "job_id": 3882069304,
        "company": "GEICO",
        "title": "Software Engineer II",
        "created_on": 1720638375.4403276,
        "description": "Our Software Engineer works with our Distinguished Engineer and Staff Engineers to innovate and build new systems, improve, and enhance existing systems as well as identify new opportunities to apply your knowledge to solve critical problems. You will lead the strategy and execution of a technical roadmap that will increase the velocity of delivering products and unlock new engineering capabilities. The ideal candidate has deep technical expertise in the Open-Source platform domain. Position Responsibilities As a Software Engineer, you will: Collaborate with product managers, team members, customers, and other engineering teams to solve our toughest problems Develop and execute technical software development strategy for the organization including self-service, business continuity, backup/restores, incident response and paging platforms Accountable for the quality, usability, and performance of the solutions Lead projects from the front and interact with clients and sponsors on a regular basis Consistently share best practices and improve processes within and across teams Take on-call and operational support Qualifications Advance knowledge of at least one modern OOP language such as Python or Go (preferred) Understands and applies SRE principles Deep hands-on experience in complex system design and data pipeline and architectures, scale and performance, tuning, with good knowledge on Docker and Kubernetes Strong Test-Driven Development practices (e.g., unit, functional, integration, load, etc.) In-depth knowledge of CS data structures and algorithms Understanding of security best practices (e.g., certificates, encryption) Understand open-source databases like MySQL, PostgreSQL, etc. Familiar with No-SQL databases like Cassandra, MongoDB, etc. Experience in architecting, designing, building automation, workflows, and distributed applications Strong understanding of service integrations / communication standards (e.g., GRPC / REST) Experience partnering with engineering teams and transferring research to production Experience with continuous delivery (CI/CD) and Infrastructure as Code Experience solving analytical problems with quantitative approaches Knowledge of developer tooling across the software development life cycle (task management, source code, building, deployment, test automation and related tools, operations, real-time communication) Knowledge of Kubernetes, containers, and best practices on a K8s environment (including K8s operators) Experience in open-source tools like GIT/Jenkin/CircleCI, and knowledge in Pulumi/Terraform/Ansible is a plus Excellent communication skills Ability to excel in a fast-paced, startup-like environment Experience: 2+ years of professional experience in software development, platform architecture, administration and maintenance of software, and its ecosystem 2+ years of experience with architecture and design 2+ years of experience with AWS, GCP, Azure, or hybrid data center 2+ years of experience in open-source frameworks Education: Bachelor's degree in computer science, Information Systems, or equivalent education or work experience Annual Salary $76,000.00 - $157,000.00 The above annual salary range is a general guideline. Multiple factors are taken into consideration to arrive at the final hourly rate/ annual salary to be offered to the selected candidate. Factors include, but are not limited to, the scope and responsibilities of the role, the selected candidate’s work experience, education and training, the work location as well as market and business considerations. At this time, GEICO will not sponsor a new applicant for employment authorization for this position. Benefits: As an Associate, you’ll enjoy our Total Rewards Program* to help secure your financial future and preserve your health and well-being, including: Premier Medical, Dental and Vision Insurance with no waiting period** Paid Vacation, Sick and Parental Leave 401(k) Plan Tuition Reimbursement Paid Training and Licensures Benefits may be different by location. Benefit eligibility requirements vary and may include length of service. Coverage begins on the date of hire. Must enroll in New Hire Benefits within 30 days of the date of hire for coverage to take effect. The equal employment opportunity policy of the GEICO Companies provides for a fair and equal employment opportunity for all associates and job applicants regardless of race, color, religious creed, national origin, ancestry, age, gender, pregnancy, sexual orientation, gender identity, marital status, familial status, disability or genetic information, in compliance with applicable federal, state and local law. GEICO hires and promotes individuals solely on the basis of their qualifications for the job to be filled. GEICO reasonably accommodates qualified individuals with disabilities to enable them to receive equal employment opportunity and/or perform the essential functions of the job, unless the accommodation would impose an undue hardship to the Company. This applies to all applicants and associates. GEICO also provides a work environment in which each associate is able to be productive and work to the best of their ability. We do not condone or tolerate an atmosphere of intimidation or harassment. We expect and require the cooperation of all associates in maintaining an atmosphere free from discrimination and harassment with mutual respect by and for all associates and applicants.",
        "url": "https://www.linkedin.com/jobs/view/3882069304"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Chantilly, VA",
        "job_id": 3960347477,
        "company": "InSequence, Inc.",
        "title": "System Engineer",
        "created_on": 1720638377.1036987,
        "description": "InSequence Overview: At InSequence, we engineer outcomes for your most demanding technology problems. We hire talented professionals to bridge the technology gap between desired business results and actual implementation. Organizations often face a technology gap between getting a software solution and having it meet a desired business goal. InSequence closes that gap with engineered products and managed services that bridge IT expenditure and their organization goals. Our clients range from large government agencies to enterprises that utilize vast amounts of technology and data. They turn to us for our proven solutions and engineering mindset to manage technology that is predictable, scalable, and reliable. Clearance Requirement: Active TS-SCI Clearance On-Site Support Chantilly, VA Position Overview: We are seeking a highly skilled and detail-oriented System Engineer to join our team. The successful candidate will be responsible for managing the customer’s software asset management tracking initiative using Flexera license management solutions and customizations. This role involves delivering accurate and business-relevant information support to the customer, assisting in audits, and providing guidance on licensing management. Key Responsibilities: Ensure compliance with vendor application license agreements. Utilize reporting tools to monitor software utilization, assist with cost controls, and prevent audits. Administer Flexera License Management products and IT Asset Management (ITAM) concepts. Provide trend and forecasting analysis and reporting of software and hardware lifecycle data. Provide direct support and advisement to customer and partners for creating an ITAM program. Advise customer on process improvements to maintain license compliance, audit readiness, and optimize software purchases. Provide recommendations for software bundling to limit software sprawl where multiple software provides similar functionality. Provide trend reporting and analysis for software version sprawl and provide recommendations for optimization. Coordinate with security teams to tag and report on authorized and unauthorized software on the network. Provide support and training for the customer on Flexera License Management products. Act as a liaison between the SAM customer, internal Flexera Technical Team, and the Project Manager (PM). Support related SPID activities (decommissioning, on-boarding) for Security Impact Determination Requests (SIDR). Collaborate with government Information System Security Officers (ISSO) and SAM teams. Maintain updates and manage various day-to-day operations of FlexNet Manager and Data Platform systems. Assist in the execution of operational activities to ensure the proper operation and delivery of the SAM365® managed service. Generate reports as requested to support customer requirements. Guide users in formulating requirements, advising alternative approaches, and conducting feasibility studies. Incorporate new plans, designs, and systems into ongoing operations. Guide system development and implementation planning through the assessment or preparation of system engineering management plans and system integration and test plans. Interact with the Government regarding Systems Engineering technical considerations and associated problems, issues, or conflicts. Ensure the technical integrity of work performed and deliverables associated with the Systems Engineering area of responsibility. Communicate with other program personnel, government overseers, and senior executives. Required: Excellent analytical, communication, and problem-solving skills. Self-starter with the ability to thrive in a fast-paced environment and manage conflicting priorities with ease. Clear and concise verbal and written communication skills. Ability to assess customer and technology needs/requirements and develop solutions to meet them. Quick adaptability to new technologies. Proficiency in analyzing system requirements and leading design and development activities. Ability to analyze user requirements, concept of operations documents, and high-level system architectures to develop system requirements specifications. Preferred: Experience in software procurement or software asset management Experience in developing system architecture and system design documentation. Customer service-focused with the ability to deliver high performance and customer satisfaction. Ability to work well under pressure with differing levels of management. Capability to maintain confidential information and communications. Proficiency in breaking down complex tasks and explaining solutions. Knowledge of software discovery tools (i.e., BDNA, ADDM, SCCM, etc.). Knowledge of software deployment tools (i.e., SCCM, AppV, Citrix Virtual Apps, etc.). Knowledge of server virtualization technologies (i.e., HyperV, Azure, AWS, VMWare, etc.). Experience with relational databases and SQL is a plus. Experience with Flexera FlexNet Manager and/or Data Platform. Experience with other enterprise software asset management tools is a plus. Experience with ServiceNow is a plus. Certification in Software Asset Management is a plus. ADA Notations: Regular communication. Noise conditions range from very quiet to very noisy. Prolonged use of computer (typing/keyboarding). If you are a proactive and experienced SAM professional with a passion for license management and compliance, we encourage you to apply for this challenging and rewarding position. Salary Range: 100K-135K The final salary can be influenced by a variety of factors, such as Federal Government contract labor categories and wage rates, relevant work experience, specific skills and competencies, educational background, and certifications. Perks: 100% Employer paid healthcare, dental, and vision insurance for employees and their dependents. 4% 401k match 100% Immediate Vesting Profit Sharing & Stock Options 3 weeks of PTO & 11 Federal Holidays Continuous training & education InSequence Inc. is an Equal Opportunity Employer We participate in the E-Verify Employment Verification Program.",
        "url": "https://www.linkedin.com/jobs/view/3960347477"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Arlington, VA",
        "job_id": 3849330847,
        "company": "Marathon TS",
        "title": "Software Engineer",
        "created_on": 1720638378.7695558,
        "description": "Marathon TS is seeking an experienced Software Engineer to join one of our federal projects supporting the Department of Transportation (DOT) Pipeline and Hazardous Materials Safety Administration (PHMSA). The candidate will develop and maintain a portfolio of internal and external applications and support all phases of software engineering including: Requirement Gathering and Requirement Analysis, Application Design and Implementation, Unit Testing and Deployment for the PHMSA Portal System (PPS). This is a fantastic opportunity to leverage previous engineering experience and expertise while supporting a mission critical DOT system. This is a direct-hire, full-time position with salary and benefits. Indev provides a comprehensive benefits package, including Medical, Dental, Vision, 401k with match, Flexible Spending Account, and Paid Time Off (PTO)including vacation and holiday pay. Your Future Duties And Responsibilities Provide technical oversight for mission critical applications. Review and validate system design, development, testing, installation, and implementation of new system components and features. Provide release support for operations and maintenance activities following established configuration management and change control processes. Provide development support for PHMSA PPS. Required Qualifications To Be Successful In This Role BS in Computer Science, Information Systems, Engineering, Business, or other related scientific or technical discipline. A minimum of 5 years of experience of which at least 3 years must be specialized in Full Stack development. Engineering experience should include both on-premises and cloud. Technologies should include: J2EE, AngularJS, AWS (EC2, S3, Lambda), Java, Node JS, Web Services API. Ability to obtain a Public Trust security clearance. Nice To Have's Experience with DOT operating administrations, particularly PHMSA. Current security clearance. Certifications in Oracle, AWS Solutions Architect Assoc. Experience the following technologies: Oracle (OIM, HTTP Server, ADF, RDBMS, JDeveloper) Jquery Oracle Fusion Middleware JavaScript Microservices Marathon TS is committed to the development of a creative, diverse and inclusive work environment. In order to provide equal employment and advancement opportunities to all individuals, employment decisions at Marathon TS will be based on merit, qualifications, and abilities. Marathon TS does not discriminate against any person because of race, color, creed, religion, sex, national origin, disability, age or any other characteristic protected by law (referred to as \"protected status \").",
        "url": "https://www.linkedin.com/jobs/view/3849330847"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Silver Spring, MD",
        "job_id": 3933076544,
        "company": "Tactibit Technologies",
        "title": "Cloud Software Engineer",
        "created_on": 1720638380.5661337,
        "description": "Tactibit Technologies provides innovative information technology, cybersecurity, and cloud support services to the Federal Government. We support some of the nation's most critical and demanding projects including satellite operations, critical infrastructure, and search and rescue. We are a diverse team of hands-on professionals dedicated to solving problems and developing innovative solutions in support of our customers' critical missions. Our success is dependent on our inclusive, collaborative environment with a shared commitment to excellence in everything we do. About The Cloud Software Engineer Position We are looking for a talented software engineer with in-depth experience in AWS DevOps tools and techniques to join our team remotely . You will work closely with government and contractor staff to design and provision enterprise-level cloud services and infrastructure in an AWS cloud computing environment. You will support National Ocean and Atmospheric Administration's (NOAA) cloud initiatives includes services deemed mission critical that ingest, process, disseminate, and store petabytes of environmental data collected from satellites daily. You should be an expert in migrating functions and networks to private, public, and hybrid cloud environments. We expect you to have a passion for creating new cloud deployment and optimizing enterprise systems. You should have a desire to work with satellite data and products for the public and government. Besides, you should be able to perform well working in a team, along with developers, engineers and scientists. Cloud Software Engineer Responsibilities Are Architect, plan, and implement new cloud services and infrastructure based on customer specifications. Migrate functions and on-premise workloads to private, public, and hybrid cloud infrastructures. Act as a subject matter expert for end-to-end cloud systems engineering including identifying cloud providers, networking, provisioning, and management/governance. Design, plan, and implement improvements to the enterprise cloud environment to maximize performance, availability, scalability, and security. Work closely with the Cloud Architect to enhance existing and build new capabilities within the AWS-hosted cloud environment. Collaborate with management, software developers, scientists, system/network administrators, and other technical experts to determine growth plans and identify areas of improvement. Monitor the usage of cloud services and infrastructure to ensure appropriate provisioning and manage cost. Design and implement effective security controls and strategies on cloud network environments. Function in a DevOps environment supporting development, testing, operations, and troubleshooting in a mission-critical environment. Support security testing, hardening, and assessments to meet strict compliance and operational security requirements. Cloud Software Engineer Requirements Are Experience working with Cloud Service Providers, including AWS, Microsoft Azure, and Google Cloud Platforms Computer programming and/or scripting skills with languages including Python, Java, C++, C#, Ruby, Node.js, and/or other common programming languages Experience with Jira and Confluence Experience with migrating and integrating containerized workloads in the cloud using Docker or Kubernetes Strong oral and written communication and interpersonal skills to liaise with coworkers, customers, and other stakeholders Familiarity and experience with large-scale cloud deployments in enterprise environments including DevOps principles and CI/CD pipelines Expert experience analyzing on-premise infrastructure and applications, developing solutions and alternatives for migrating to the cloud, and making recommendations on right-fit cloud solutions and services Experience architecting and building scalable, automated cloud infrastructure and applications Experience with AWS cloud services, infrastructure as code, configuration management, and CI/CD tools and practices Experience with AWS Lambda and similar serverless computing methods Familiarity with key cloud computing concepts, practices, and architecture including Software as a Service (SaaS), Platform as a Service (PaaS), and Infrastructure as a Service (IaaS) Familiarity with modern DevOps principles, practices, methods, and tools Optional experience with Federal government environments and concepts including FedRAMP, NIST Risk Management Framework, NIST SP 800-53 security controls Strong problem solving skills and ability to work under pressure Must be a US Citizen and able to pass a full background investigation to obtain a security badge to enter the applicable government facility Education And Experience BS/BA degree in Computer Science or related discipline Preferred Certifications: AWS Certified Cloud Practitioner, AWS Certified Solutions Architect 6+ years of experience with cloud computing solution development, architecture, or engineering 4+ years of experience supporting the migration of on-premise applications to cloud environments",
        "url": "https://www.linkedin.com/jobs/view/3933076544"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Bethesda, MD",
        "job_id": 3787765166,
        "company": "Vexterra Group",
        "title": "Senior System Integrator Engineer (TS/SCI)",
        "created_on": 1720638382.7379336,
        "description": "Vexterra Group is currently searching for a Senior Systems Integrator Engineer to provide the following systems support in Reston VA or Bethesda, MD office: You will work closely with other infrastructure and network engineers, system engineers and O&M team members on the following key tasks: DUTIES Perform Linux builds, installs, configuration, administration, and troubleshooting of both Linux and Windows platforms to include but not limited to base configuration, security and system patching, performance monitoring, etc. Provide O/S hardening (STIG, OpenSCAP, ACAS) and system patching Develop server builds for virtual and physical environments Manage, maintain, configure, and deploy system integration tools such as Kubernetes, Rancher, Helm Support management and integration of DevOps tools such as Jenkins Deploy, operate, diagnose, and maintain containerized (Docker) microservices across orchestrated (Kubernetes) environments Maintain and orchestrate services and procedures for a big data pipeline which includes kafka, Elasticsearch, and several in house written python/java running on Kubernetes Develop scripts to monitor and check health of the deployed pipeline Maintain and develop DevOps automation using tools (e.g., Saltstack, Ansible, Chef, Puppet) Design, develop, and support scalable, redundant infrastructure to include physical and virtualized environments Set up administrator and service accounts, maintain system documentation, tune system performance, install system wide software, validate and implement critical system patches, and allocate mass storage space Improve existing infrastructure to incorporate latest technology best practices and cross application integrations Participate as needed in all phases of software development with emphasis on the planning, analysis, integration, and documentation phases Manage the code transfer and modification Conduct automated unit and system integration tests to identify and provide feedback on failed modules and components Participate in code design reviews and updates Support CM activities Administer system to system data transfer and integration process Requirements A Bachelors Degree in Information Technology, or a closely related discipline or 4 Years Of Additional Relevant Professional Experience At least 6 years of relevant professional experience Must have an active TS/SCI with Counterintelligence Poly Experience in system integrations testing through a full system development life cycle, including implementing test plans, test cases and test processes Experience with security requirements derivation Experience documenting test results for corrective actions, reporting and audits Excellent verbal and written communication skills Ability to work in a team and also a self-starter who can work on their own Expertise in configuration, administration, and troubleshooting of Linux (RHEL/CentOS, Ubuntu) to include kernel panic recovery; Configuring SSSD, AIDE, SELinux, auditd; and O/S hardening – STIG, OpenSCAP, ACAS. Highly proficient with Patching: yum repositories, APT repositories Highly proficient with Server builds: virtual and physical Experience with installing and deploying DevOps tools, examples could include Chef, Puppet, Git, Docker, Jenkins, Ansible, Salt, etc. Knowledge of Atlassian software such as JIRA, JIRA Service Desk, Confluence, Bitbucket, HipChat, etc. DoD 8570 IAT II Certification (e.g. Security +) Preferred Skills Kubernetes, Ansible, Puppet, Salt, Linux, Git, Python, Elasticsearch, Logstash, IPA, AWS, OpenStack, Java, Kafka, Hadoop, etc. Experience establishing large computer clusters Experience in continuous configuration automation (CCA) methods for providing a flexible, programmatic platform for deploying and managing the configuration of infrastructure and application resources. Experience in applying both agent-based and agentless (e.g. SSH) methods to event-driven orchestration and remote execution for configuration management Experience in applying automated configuration management and deployment tools (e.g. Puppet, Chef, Ansible, and Salt) for configuration automation, cloud control, and event-driven orchestration. Powered by JazzHR 8vBbErYoBZ",
        "url": "https://www.linkedin.com/jobs/view/3787765166"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Arlington, VA",
        "job_id": 3964279377,
        "company": "Elsdon Technology",
        "title": "System Engineer",
        "created_on": 1720638386.3331459,
        "description": "Are you an experienced Systems Engineer holding an active Security Clearance and looking for a new position? We have the one for you! We are seeking a talented Senior Systems Engineer to play a pivotal role in designing and developing software requirements for multiple Artificial Intelligence based support capabilities. You will provide collaborative technical leadership alongside industry and government partners to establish high level data mesh design. patterns and proofs of concept/ prototypes suited to a complex multi-vendor, mutli-organization environment. You will serve as a senior technical advisor in formulating the technical approaches, selection of the tools, diagnostics methods for solving various problems utilizing artificial intelligence. As the ideal candidate, you will: Ensure that the consortium members are proposing and working toward a common interoperable technical vision Proactively identify and develop solutions to identify and mitigate technical risks, ensuring compliance with global standards as well as interoperability with DoD systems. Create and review others' relevant documentation, such as diagrams, detailed assessments and design document. Assist in the evaluation of new products and services; make recommendations for improvements and assist in the development and documentation of network architecture. As the ideal candidate, you will have: U.S. Citizenship Active Secret Clearance Bachelor's Degree in plus 7-10 years experience or a Masters Degree plus 5 years of experience. Education must be in Systems Engineering or closely related discipline such as Modeling and Simulation Engineering. Strong knowledge of proven experience with network capacity planning, network security principles and general network management best practices Experience with network design/development and design of data exchange standards such as APIs, cloud storage, and access-denied environments Great technical writing skills Experienced in Python programming and in industry-standard Software Engineering software (e.g., Cameo). Familiarity and understanding of: Data Mesh Design, Systems of Systems, Systems Engineering Zero Trust Architecture Access Management DoD Classification Implementation Microservice Architecture This is a unique opportunity with a firm with a huge presence in the Government and Federal industry offering fast-tacked progression and a level of job stability that is uncommon in the job market at present. YOU MUST HOLD AN ACTIVE SECURITY CLEARANCE - ( SECRET ) If you are interested in this position or would like more info about roles, responsibilities, or compensation - please click apply now! Feel free to email me at: sophie.roydhouse@elsdonconsulting.com OR connect with me on LinkedIn: https://www.linkedin.com/in/sophie-roydhouse-b21aa0115/",
        "url": "https://www.linkedin.com/jobs/view/3964279377"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Laurel, MD",
        "job_id": 3943047346,
        "company": "WOOD Consulting Services, Inc.",
        "title": "Software Engineer Level 0",
        "created_on": 1720638388.3090281,
        "description": "Software Engineer Level 0 woodcons.com The Software Engineer develops, maintains, and enhances complex and diverse software systems (e.g., processing-intensive analytics, novel algorithm development, manipulation of extremely large data sets, real-time systems, and business management information systems) based upon documented requirements. Works individually or as part of a team. Reviews and tests software components for adherence to the design requirements and documents test results. Resolves software problem reports. Utilizes software development and software design methodologies appropriate to the development environment. Provides specific input to the software components of system design to include hardware/software trade-offs, software reuse, use of Commercial Off-the-shelf (COTS)/Government Off-the-shelf (GOTS) in place of new development, and requirements analysis and synthesis from system level to individual software components. Security Clearance Requirements: This position requires all candidates to be U.S. Citizens and possess an active TS/SCI Security Clearance with a Polygraph. Analyze user requirements to derive software design and performance requirements Debug existing software and correct defects Provide recommendations for improving documentation and software development process standards Design and code new software or modify existing software to add new features Integrate existing software into new or modified systems or operating environments Develop simple data queries for existing or proposed databases or data repositories Required Education & Years of Experience: Bachelor’s degree in Computer Science or related discipline from an accredited college or university is required. Four (4) years of SWE experience on projects with similar software processes may be substituted for a bachelor’s degree. No demonstrated experience is required. Required Skills One (1) or more years of JavaScript programming experience and development of presentation tiers. Familiarity with Java and Spring desired. Experience designing and writing REST-ful applications. Experience writing Interface Control Documents (ICDs). Experience with Microsoft office tools (Word, Excel, Powerpoint). Experience programming on Linux platforms. Experience using one or more of the following revision control applications: git, Subversion (SVN), CVS, ClearCase. Experience with test driven development. WOOD Consulting Services, Inc. is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or protected veteran status and will not be discriminated against on the basis of disability.",
        "url": "https://www.linkedin.com/jobs/view/3943047346"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "McLean, VA",
        "job_id": 3904263471,
        "company": "US Tech Solutions",
        "title": "Data Protection/Security Engineer",
        "created_on": 1720638389.9470203,
        "description": "Data Protection/Security Engineer Location: McLean, VA (3 Days/Week Onsite From Start Of Assignment) Duration: 12 Months Experience in database and folder level security (encryption and data masking); This position will be supporting the Applied Cryptography team within the Information Security Engineering department within the Information Security Unit of the Information Technology Division. This position is primarily responsible for providing administration and engineering support of the encryption-at-rest services within private and public cloud environments. The candidate expectations are a follows: Collaborate with Information Security Leads to implement and support existing data encryption services within private and public cloud environments. Perform the planning, design, implementation and Level 3 support of IT Security solutions related to data-at-rest encryption while onboarding new applications to the enterprise encryption services. Support the security risk assessment of applications and infrastructure; including remediation of incident response, vulnerability analysis and threat intelligence. Provide technical guidance, develop design documents, perform product installation, upgrades and certification, implementation plan, deployment and troubleshooting support. Diagnose, solve and provide root cause analysis for issues related to key and certificate management and data-in-transit. Ensure consistent delivery of superior technical solutions. Champion technology and tools change that improves delivery processes. Act as an agent for change to reflect the latest data encryption management standards in new technologies and tools. Serve as an enterprise subject matter expert (SME) and advocate of IT Security standards and reference architectures related to data-at-rest encryption management. Coordinate with Information Security team to ensure solution assurance and compliance to security policy, procedures, standards and baseline security configurations. Provide training for clients in use of the key and certificate management and access request systems and automated environments. Work closely with technology and business stakeholders to implement data-at-rest encryption and upgrade goals, determine security requirements, design and implement solutions to meet business objectives, IT strategic initiatives, corporate and regulatory requirements. Execute enterprise-wide data encryption management governance processes in collaboration with Information Security and Enterprise Architecture to adequately plan, communicate and deploy enterprise configurations as new data encryption and key management standards are adopted. Communicate effectively with clients to identify needs and evaluate alternative technical solutions and strategies. Protect and secure company resources in the cloud, virtual and physical infrastructures. Stay current with developing technologies, emerging threat landscape and predict impact of changing technologies. Qualifications: Bachelor’s degree in information technology, engineering, computer science, related field or equivalent experience. 5+ years of professional IT experience with implementation and administration of any database and folder-level data encryption solution. Additional 2+ year(s) of experience with implementation and administration of any data encryption and key management security tools and/or cloud services a plus. Experience with System Administration basics with Linux (CentOS, RedHat) and/or Microsoft Windows Server (and associated technologies such as Active Directory and Exchange). Experience operating and maintaining production systems in Linux and public cloud environments. Experience with basic troubleshooting (network commands, cURL or related). Knowledge of AWS cryptography, encryption and key management best practices and policies. Preferred Skills: Knowledge of Vormetric Data Encryption platform, including most recent release. Knowledge of industry leading cloud-based solutions and available native services Experience implementing AWS cryptography, encryption and key management best practices and policies. Experience with scripting tools to automate routine tasks using scripting languages including but not limited to PowerShell and bash scripting. Exposure to various operating systems - UNIX, Linux, Windows",
        "url": "https://www.linkedin.com/jobs/view/3904263471"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Chevy Chase, MD",
        "job_id": 3933758316,
        "company": "GEICO",
        "title": "Staff Engineer- Developer Productivity Engineering (Data)",
        "created_on": 1720638391.700916,
        "description": "Position Summary GEICO is seeking an experienced Staff Engineer with a passion for building high-performance, low maintenance, zero-downtime platforms, and applications. You will help drive our insurance business transformation as we transition from a traditional IT model to a tech organization with engineering excellence as its mission, while co-creating the culture of psychological safety and continuous improvement. Position Description The Staff Engineer is a key member of our engineering team, working across the organization and enterprise to provide a friction-less experience to our developer community and maintain the highest standards of protection and availability. As a Staff Engineer, you will lead the strategy and execution of a technical roadmap that will increase the velocity of delivering products and unlock new engineering capabilities. This role will center around our Developer Productivity Engineering Data spoke, harvesting and providing our userbase with up-to-date, pertinent, and reliable data, to drive decision making and facilitate organizational directives. Our team thrives and succeeds in delivering high-quality technology products and services in a hyper-growth environment where priorities shift quickly. The ideal candidate has broad and deep technical knowledge, typically ranging from front-end UIs through back-end systems and all points in between. Position Responsibilities As a Staff Engineer, you will: Focus on multiple areas and provide leadership to the engineering teams Scope, design, and build scalable, resilient distributed systems Own complete solutions across their entire life cycle Engage in cross-functional collaboration throughout the entire software lifecycle Influence and build vision with product managers, team members, customers, and other engineering teams to solve complex problems for building enterprise class business applications Be accountable for the quality, usability, and performance of your solutions Lead in design sessions and code reviews to elevate the quality of engineering across the organization Build product definition and leverage your technical skills to drive towards the right solution Utilize programming languages like GO, Java, Typescript, React, HTML, Python, C# or other object-oriented languages, SQL, and NoSQL databases, DB engines like Spark and Trino, Container Orchestration services including Docker and Kubernetes, and a variety of Azure tools and services Define, create, and support reusable application components/patterns from a business and technology perspective Mentor team members professionally to help them realize their full potential Consistently share best practices, reduce friction points, and improve processes within and across teams Qualifications Fluency and specialization with at least two modern languages such as GO, Java, C++, Python or C# including object-oriented design Experience building products of micro-services-oriented architecture and extensible REST APIs Experience building data warehouses optimized for OLTP or OLAP Experience with ETL and ELT, preferably using open-source toolsets Experience in analytics and data reporting / aggregated display, using proprietary (PowerBI, Tableau) or OSS (Grafana) toolsets Experience building the architecture and design of new and current systems (architecture, design patterns, reliability, and scaling) Experience writing/consuming metrics from highly complex YAML pipeline/workflow code in GitHub Experience with container orchestration on different compute platforms for data ingestion and transformation Advanced understanding of DevOps Concepts, Cloud Architecture and Azure DevOps Operational Framework Experience leveraging command line shell scripting, bash, PowerShell, etc. Experience with public and private cloud services. Azure, AWS, GCP and equivalent private cloud services running on open-source software stacks. Advanced understanding of security protocols and products: understanding of Azure Active Directory, SAML, OICD/OAUTH In-depth knowledge of CS data structures and algorithms Knowledge of developer tooling across the software development life cycle (task management, source code, building, deployment, operations, real-time communication) Strong problem-solving ability Ability to excel in a fast-paced, startup-like environment Experience 6+ years of professional platform development or general development experience 4+ years of experience in open-source frameworks 3+ years of experience with DW architecture and design 3+ years of experience with AWS, GCP, Azure, or another cloud service Education Bachelor’s degree in computer science, Information Systems, or equivalent education or work experience Annual Salary $110,000.00 - $261,500.00 The above annual salary range is a general guideline. Multiple factors are taken into consideration to arrive at the final hourly rate/ annual salary to be offered to the selected candidate. Factors include, but are not limited to, the scope and responsibilities of the role, the selected candidate’s work experience, education and training, the work location as well as market and business considerations. GEICO will consider sponsoring a new qualified applicant for employment authorization for this position. Benefits: As an Associate, you’ll enjoy our Total Rewards Program* to help secure your financial future and preserve your health and well-being, including: Premier Medical, Dental and Vision Insurance with no waiting period** Paid Vacation, Sick and Parental Leave 401(k) Plan Tuition Reimbursement Paid Training and Licensures Benefits may be different by location. Benefit eligibility requirements vary and may include length of service. Coverage begins on the date of hire. Must enroll in New Hire Benefits within 30 days of the date of hire for coverage to take effect. The equal employment opportunity policy of the GEICO Companies provides for a fair and equal employment opportunity for all associates and job applicants regardless of race, color, religious creed, national origin, ancestry, age, gender, pregnancy, sexual orientation, gender identity, marital status, familial status, disability or genetic information, in compliance with applicable federal, state and local law. GEICO hires and promotes individuals solely on the basis of their qualifications for the job to be filled. GEICO reasonably accommodates qualified individuals with disabilities to enable them to receive equal employment opportunity and/or perform the essential functions of the job, unless the accommodation would impose an undue hardship to the Company. This applies to all applicants and associates. GEICO also provides a work environment in which each associate is able to be productive and work to the best of their ability. We do not condone or tolerate an atmosphere of intimidation or harassment. We expect and require the cooperation of all associates in maintaining an atmosphere free from discrimination and harassment with mutual respect by and for all associates and applicants.",
        "url": "https://www.linkedin.com/jobs/view/3933758316"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Chantilly, VA",
        "job_id": 3927158117,
        "company": "Staffing Resource Group, Inc",
        "title": "Software Engineer",
        "created_on": 1720638393.341957,
        "description": "Job Description Software Engineer Salary: Up to $190k/yr. Location: Chantilly, VA Industry: Department of Defense Clearance: Active TS/SCI with CI Poly Full Time, Direct Hire, Excellent Benefits: 401k, generous PTO, ESOP, full medical, and much more! Qualifications: BS Degree in computer science, data science, engineering, math, statistics, or related field 5-10 years of experience in software and data engineering Proficiency in at least one or more high-level programming languages (Java, Python, C/C++) Experience working with Linux-based servers and systems Experience with at least one modern Big Data (e.g., Hbase), ETL (e.g., Apache NiFi), or Data Science (e.g., MapReduce) technology Experience with containerizing applications (Docker or Kubernetes) Active TS/SCI clearance with CI Poly REQUIRED Responsibilities: Design, implement, and maintain real-time data ingestion and ETL pipelines. Manage and maintain Apache NiFi clusters running as containerized workloads in Kubernetes. Manage other components of our data infrastructure, such as Apache Kafka, as required. Analyze new data sources and integrate them into our data pipeline. Collaborate with other teams to ensure successful integration of data infrastructure with existing systems. EOE/ADA #clearance",
        "url": "https://www.linkedin.com/jobs/view/3927158117"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "McLean, VA",
        "job_id": 3917780295,
        "company": "Capital One",
        "title": "Senior Data Engineer (Python, Spark, AWS)",
        "created_on": 1720638395.198648,
        "description": "Locations: VA - McLean, United States of America, McLean, VirginiaSenior Data Engineer (Python, Spark, AWS) Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking a Senior Data Engineer who is passionate about marrying data with emerging technologies. As a Capital One Senior Data Engineer, you’ll have the opportunity to be on the forefront of driving a major transformation within Capital One. What You’ll Do Build and optimize data pipelines for efficient data ingestion, transformation, and loading from various sources while ensuring data quality and integrity Support the design and development of scalable data architectures and systems that extract, store, and process large amounts of data Collaborate with Data Scientists, Machine Learning Engineers, Business Analysts and/or Product Owners to understand their requirements and provide efficient solutions for data exploration, analysis, and modeling Implement testing, validation and pipeline observability to ensure data pipelines are meeting customer SLAs Use cutting edge technologies such as Python, Scala, Spark, and a variety of AWS services to develop modern data pipelines supporting Machine Learning and Artificial Intelligence Basic Qualifications: Bachelor’s Degree At least 4 years of experience in application development (Internship experience does not apply) At least 1 year of experience in big data technologies Preferred Qualifications: 5+ years of experience building data pipelines using Python, Java, or Scala 2+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud) 2+ years of experience using Spark or PySpark 2+ years of data warehousing experience (Redshift or Snowflake) 3+ years of experience with UNIX/Linux including basic commands and shell scripting 2+ years of experience with Agile engineering practices At this time, Capital One will not sponsor a new applicant for employment authorization for this position. Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website . Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. This role is expected to accept applications for a minimum of 5 business days.No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City’s Fair Chance Act; Philadelphia’s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries. If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com . All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "url": "https://www.linkedin.com/jobs/view/3917780295"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Laurel, MD",
        "job_id": 3912323151,
        "company": "Eqlipse Technologies, a BlueHalo Company",
        "title": "Experienced Python Software Engineer - Fully Cleared",
        "created_on": 1720638396.929625,
        "description": "BlueHalo is a pioneering leader in the defense, space, cyber, and intelligence industries, known for delivering advanced engineering and technology solutions that significantly enhance the capabilities of our clients. At BlueHalo, we are committed to innovation, excellence, and the development of cutting-edge technology solutions that protect and empower our national security and defense communities. This position at BlueHalo offers the opportunity to be at the forefront of technological innovation in critical sectors. If you are a skilled developer looking for a challenging and rewarding career, we encourage you to apply and help us drive progress in our mission-critical projects. Engage in full stack development, utilizing your strong skills in Python and object-oriented programming within a Linux environment. Develop and enhance applications using web frameworks and modern JavaScript technologies, including Angular, as well as NoSQL databases. Implement and manage containerized applications using Docker and Kubernetes, ensuring scalable and efficient software deployment. Contribute to cybersecurity initiatives, applying your knowledge to enhance system security and resilience. Employ machine learning techniques where applicable to optimize processes and data analysis. Collaborate effectively within a team to design, develop, and maintain enterprise-level data services. An educational background in Computer Science or a related field with varying levels of professional experience Bachelor’s degree plus 5 years of relevant experience. Master’s degree plus 3 years of relevant experience. An Associate’s degree plus 7 years of relevant experience. High school diploma/GED plus 9 years of relevant experience for candidates with extensive, directly related experience. Proficiency in Python and experience developing software in a Linux environment. Expertise in full stack development, including JavaScript, NoSQL databases, and the use of Docker and Kubernetes. Experience with Splunk or similar tools. A background in cybersecurity. Proficiency in scripting languages such as Perl and Bash. Experience with machine learning techniques. Ability to work collaboratively in a dynamic team environment. Experience in developing and working with Enterprise Data services. TS/SCI with Poly BlueHalo is committed to hiring and retaining a diverse workforce. We are proud to be an Equal Opportunity/Affirmative Action Employer, making decisions without regard to race, color, religion, creed, sex, sexual orientation, gender identity, marital status, national origin, age, veteran status, disability, or any other protected class. U.S. Citizenship is required for most positions. If you are an individual with a disability and would like to request a reasonable workplace accommodation for any part of the employment process, please send an email to Recruiting@bluehalo.com. Please indicate the specifics of the assistance needed. This option is reserved only for individuals who are requesting a reasonable workplace accommodation. It is not intended for other purposes or inquiries.",
        "url": "https://www.linkedin.com/jobs/view/3912323151"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Herndon, VA",
        "job_id": 3967966037,
        "company": "Amazon Web Services (AWS)",
        "title": "Business Intelligence Engineer II, AWS Infrastructure Services - Science",
        "created_on": 1720638398.5408354,
        "description": "Description AWS Infrastructure Services owns the design, planning, delivery, and operation of all AWS global infrastructure. In other words, we’re the people who keep the cloud running. We support all AWS data centers and all of the servers, storage, networking, power, and cooling equipment that ensure our customers have continual access to the innovation they rely on. We work on the most challenging problems, with thousands of variables impacting the supply chain — and we’re looking for talented people who want to help. You’ll join a diverse team of software, hardware, and network engineers, supply chain specialists, security experts, operations managers, and other vital roles. You’ll collaborate with people across AWS to help us deliver the highest standards for safety and security while providing seemingly infinite capacity at the lowest possible cost for our customers. And you’ll experience an inclusive culture that welcomes bold ideas and empowers you to own them to completion. AWS Infrastructure Services Science (AISS) researches and builds machine learning models that influence the power utilization at our data centers to ensure the health of our thermal and electrical infrastructure at high infrastructure utilization. As a Business Intelligence Engineer, you will be gathering business requirements and creating BI and analytics solutions that have measurable customer impact, and demonstrating exemplary written and verbal communication skills. You will have to set the right vision, strategy and roadmap and work alongside with stakeholders in the organization to make it happen. You know and love working with business intelligence tools, can model multidimensional datasets, and can partner effectively with business leaders to answer key business questions. You will work in a fast-paced environment and deliver artifacts in fast iterations towards an ideal solution. Your ability to work through ambiguity and develop scalable, and high-performance solutions is essential. You will have opportunities to influence software systems and work directly with business stakeholders. In addition, successful candidates will be able to lead by example, balance reporting responsibilities while participating in cross-team efforts including business goals roadmaps and planning. Key job responsibilities Design, develop and maintain scaled, automated, user-friendly systems, reports, dashboards, etc. that will support the needs of the business. Apply deep analytic and business intelligence skills to extract meaningful insights and learning from large and complicated data sets. Be hands-on with ETL to build data pipeline to support automated reporting. Serve as liaison between the business and technical teams to achieve the goal of providing actionable insights into current business performance, and ad hoc analyses to support future improvements or innovations. This will require data gathering and manipulation, problem solving, and communication of insights and recommendations. Build various data visualizations to tell the story of business trends, patterns, and outliers through rich visualizations. Recognize and adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation. Own the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions. About The Team *Why AWS* Amazon Web Services (AWS) is the world’s most comprehensive and broadly adopted cloud platform. We pioneered cloud computing and never stopped innovating — that’s why customers from the most successful startups to Global 500 companies trust our robust suite of products and services to power their businesses. *Diverse Experiences* Amazon values diverse experiences. Even if you do not meet all of the preferred qualifications and skills listed in the job description, we encourage candidates to apply. If your career is just starting, hasn’t followed a traditional path, or includes alternative experiences, don’t let it stop you from applying. *Work/Life Balance* We value work-life harmony. Achieving success at work should never come at the expense of sacrifices at home, which is why we strive for flexibility as part of our working culture. When we feel supported in the workplace and at home, there’s nothing we can’t achieve in the cloud. *Inclusive Team Culture* Here at AWS, it’s in our nature to learn and be curious. Our employee-led affinity groups foster a culture of inclusion that empower us to be proud of our differences. Ongoing events and learning experiences, including our Conversations on Race and Ethnicity (CORE) and AmazeCon (gender diversity) conferences, inspire us to never stop embracing our uniqueness. *Mentorship and Career Growth* We’re continuously raising our performance bar as we strive to become Earth’s Best Employer. That’s why you’ll find endless knowledge-sharing, mentorship and other career-advancing resources here to help you develop into a better-rounded professional. Basic Qualifications 3+ years of analyzing and interpreting data with Redshift, Oracle, NoSQL etc. experience Experience with data visualization using Tableau, Quicksight, or similar tools Experience with data modeling, warehousing and building ETL pipelines Experience in Statistical Analysis packages such as R, SAS and Matlab Experience using SQL to pull data from a database or data warehouse and scripting experience (Python) to process data for modeling Preferred Qualifications Experience with AWS solutions such as EC2, DynamoDB, S3, and Redshift Experience in data mining, ETL, etc. and using databases in a business environment with large-scale, complex datasets Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us. Our compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $89,600/year in our lowest geographic market up to $185,000/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. For more information, please visit https://www.aboutamazon.com/workplace/employee-benefits. This position will remain posted until filled. Applicants should apply via our internal or external career site. Company - Amazon Data Services, Inc. Job ID: A2691454",
        "url": "https://www.linkedin.com/jobs/view/3967966037"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Laurel, MD",
        "job_id": 3788829362,
        "company": "Avid Technology Professionals",
        "title": "Software Engineer 1",
        "created_on": 1720638400.259611,
        "description": "Performing as part of an Agile Software Development team using Jira for requirements tracking, the selected candidate will perform a Software Engineering activities in support of our client. The teams responsibilities revolve around using new technologies to ensure the compliance of interactions and accesses between various enterprise systems. MANDATORY SKILLS: A TS/SCI clearance with a polygraph is required for this role. Demonstrated experience in architecting, designing, developing solutions using the ELK (Elasticsearch, Logstash and Kibana) stack -- not limited to but including analytics and machine learning Demonstrated experience in SQL and writing complex queries with joins and aggregate that deals with large datasets. Demonstrated experience with Python 2.x Demonstrated experience with the Hadoop Distributed File System (HDFS) Experience leading design and implementation of new stories and bug fixes Experience performing thorough design reviews and code reviews Hands on Java development, Junit testing and issue troubleshooting Experience working in an Agile based development environment, using Agile concepts Understanding and comfort with enterprise frameworks for dependency injection, object relational mapping and logging (Spring Framework, Hibernate, SLF4J) Familiarity with Build Management, Continuous Integration, and Automated Testing (Maven, Jenkins) Seven (7) years experience as a SWE in programs and contracts of similar scope, type and complexity is required. Bachelors degree in Computer Science or related discipline from an accredited college or university is required. Four (4) additional years of SWE experience on projects with similar software processes may be substituted for a bachelors degree. OPTIONAL SKILLS: ? Demonstrated experience with Map/Reduce and/or Hadoop, Hive, and/or Pig Architecture and design of ElasticSearch Machine Learning in ElasticSearch About Avid Technology Professionals Avid Technology Professionals, LLC (ATP) is a premiere provider of software and systems engineering, and acquisition program management services for the community. ATP is actively seeking to pursue contract opportunities with other departments and agencies in the federal government, in state governments, and in the commercial sectors. Delivered by seasoned experts in the IT field, ATP solutions adeptly address the IT concerns manifesting in both the federal and commercial sectors. Employee Benefits The ATP Employee Benefits package includes: A Supportive and Equitable Working Environment that is both Stimulating and Challenging Competitive Hourly Salary Unique Employee Success Sharing Program that allows ATP employees to Share in Company's Successes Automatic Approved Overtime (as long as contract permits) Retirement Pay (401K); 100% company paid, immediately vested with Profit-Sharing Component Company Medical Coverage Plans - HMO, Open Access, PPO plans Company Dental Plan - widely accepted, comprehensive, and flexible Progressive Overtime Policy Flexible Spending Account benefit Lucrative Referral Bonus Policy Holiday Scheduling that Coincides with Government Holidays Robust Professional Expenses & Training Program Computer Allowance Internet Allowance Short and Long Term Disability Life Insurance",
        "url": "https://www.linkedin.com/jobs/view/3788829362"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Annapolis Junction, MD",
        "job_id": 3962030997,
        "company": "The Swift Group, LLC",
        "title": "Software Engineer 0",
        "created_on": 1720638401.9001915,
        "description": "Title: Software Engineer 0 Location: Annapolis Junction, MD OPS Consulting is seeking an experienced Software Engineer 0 to work in Annapolis Junction, MD. The Software Engineer (SWE) will develop, maintain, enhance, and resolve diverse and complex software systems. Working individually and as part of a team, the SWE will utilize software development and software design methodologies, provide input to the software components of system design to include hardware/software trade-offs, software reuse, use of Commercial Off-the­ shelf (COTS)/Government Off-the-shelf (GOTS) in place of new development, and requirements analysis. Required Experience A bachelor’s degree in computer science or related discipline from an accredited college or university. Four (4) years of additional SWE experience on projects with similar software processes may be substituted for a bachelor’s degree. Security Clearance A current government clearance, background investigation, and polygraph are required. We are an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran status, disability status, or any other protected class. The Swift Groups is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status, or any other protected class. The Swift Groups is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status, or any other protected class.",
        "url": "https://www.linkedin.com/jobs/view/3962030997"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Rockville, MD",
        "job_id": 3867511658,
        "company": "ConsultNet Technology Services and Solutions",
        "title": "Software Engineer",
        "created_on": 1720638403.701063,
        "description": "Title : Software Engineer Location : Remote anywhere in the continental US (lower 48 states). Cannot work outside of the U.S. Type : Contract to Hire, Long term Contract Job Description: We are looking for a Software Engineer to join a large financial regulator supporting the data validation and corrections of an electronic audit trail. Our client is looking for a senior resource, with at least 6-7 years of experience to help build brand new ETL pipelines for sharing data across the enterprise.  Once ETL pipelines have been created, the individual will handle feature enhancements by building REST APIs using Python. Requirements: 6+ years of experience Bachelor's degree in a technical or analytical field such as computer science, computer/software engineering, or information systems, is required. Hands-on experience with REST API development with Python. Experience in developing Spark applications using PySpark or similar languages. Hands-on experience with ETL processing. Good working knowledge of AWS S3 and RDS. Good understanding and hands-on experience with any document-based database like MongoDB or AWS DocumentDB Experience with EMR clusters. Be a part of the ConsultNet difference. As a leading national provider of IT staffing and solutions, ConsultNet delivers exceptional services to startup, midmarket and Fortune 1000 companies across North America. Since 1996, we've partnered with clients to create rewarding opportunities for our consultants, successfully building teams that have surefire results. In the past two years alone, we have placed more than 1,500 consultants in contract, contract-to-hire, or direct placement opportunities. We understand communication is key to finding the right job that matches your skills and career goals. For us, it's not just the work that we do; it's how we do the work. Our breadth of offerings extends to multiple IT positions in major markets throughout the country, see more at - www.consultnet.com",
        "url": "https://www.linkedin.com/jobs/view/3867511658"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Annapolis Junction, MD",
        "job_id": 3882288455,
        "company": "Belay Technologies",
        "title": "Database Engineer (Mid/Senior Level)",
        "created_on": 1720638407.5643241,
        "description": "Belay Technologies has been voted Baltimore Business Journal's (BBJ) Best Places to Work 2019, runner up in 2020 and a finalist in 2021! Belay Technologies is seeking an Entry-Level or Mid-Level Database Engineer (DBE) Level 1 or 2 to join our intel team. The DBE shall provides technical expertise for database design, development, implementation, information storage and retrieval, data flow and analysis. The DBE is responsible for developing a database structure that fits into the overall architecture of the system under development and has to make trades among data volumes, number of users, logical and physical distribution, response times, retention rules, security and domain controls. Candidates should have the following qualifications: TS/SCI with polygraph is required. Bachelor’s Degree in DBE required, or in similar field or scope The Entry-Level DBE shall have a minimum of 2 years of experience in DBE is required, or in similar field or scope The Mid-Level DBE shall have a minimum of 8 years of experience in DBE is required, or in similar field or scope Candidates are required to have the following skills: Perform database administrative tasks such as granting people access to the DBE and ensuring they have correct permissions System uses Red Hat Linux Based on Oracle Enterprise Edition 12C Uses IBM Software EAFR for analysis of data Coordinate changes with IBM Team Allocates work flow Pulls reports using MySQL on the data, i.e. How many people have entered data Perks and Benefits: 8 weeks paid leave - 4 weeks of personal leave, 3 Yay! days, take off on your birthday,11 paid holidays and optional leave up to 6 days through Belay's volunteer program 10% matching in 401(k) contributions vested on day one $5,000 annual training/tuition Student Loan Repayment Program 100% company funded HSA Rich medical coverage (100% coinsurance) Dental coverage including orthodontia Up to $420,000 in life insurance, premiums 100% company funded Amazon Prime, gym reimbursement, monthly lunches, games and prizes Pet adoption program, generous referral bonus program, fun events, and more! Belay Technologies is a certified Service-Disabled Veteran-Owned Small Business located in Columbia, Maryland (Baltimore/Washington area). Belay Technologies specializes in systems automation and full stack development. Belay Technologies provides leading technology and engineering solutions to the DoD, as well as state-of-the-art commercial products. We hire software engineers, web designers, test engineers, systems engineers, systems administrators, database engineers and other tech services. We are an Equal Opportunity Employer. Employment decisions are made without regard to race, color, religion, national origin, gender, sexual orientation, gender identity, age, physical or mental disability, genetic factors, military/veteran status or other characteristics protected by law. Key words: Full Clearance, Fort Meade, SDVOSB, Service-Disabled Veteran-Owned Small Business, DoD, full scope Powered by JazzHR g3jDzzHBIn",
        "url": "https://www.linkedin.com/jobs/view/3882288455"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "McLean, VA",
        "job_id": 3956003610,
        "company": "Keylent Inc",
        "title": "Software Engineer 21340-1",
        "created_on": 1720638409.3142283,
        "description": "Request-ID: 21340-1 Cognizant *THIS ROLE IS ONSITE IN MCLEAN, VA* Job Title: Software Engineer Experience: 7 to 10 yrs Must Have Skills Python AWS Job Summary: Support Hyperion application end-to-end and troubleshoot any issues within the application. Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability etc. 4+ years of Programming experience (Data Engineer) - outside of college experience required. Roles & Responsibilities 5+ years of experience (Sr-level) Strong Programming experience with object-oriented/object function scripting languages: Python, Spark, Scala 3+ years of working experience in AWS Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using AWS technologies. Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics. Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader. Should have good analytical skills Should be a good team player Should have effective communication skills Should be able to provide technical guidance to team and ensure tasks are closed on-time Should be able to run a medium sized ( 5 - 7 members) project team independentlyith all levels of the organization across all divisions.",
        "url": "https://www.linkedin.com/jobs/view/3956003610"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Bethesda, MD",
        "job_id": 3956712270,
        "company": "Booz Allen Hamilton",
        "title": "Data Engineer, Lead",
        "created_on": 1720638411.298682,
        "description": "Job Number: R0197309 Data Engineer, Lead The Opportunity: Ever-expanding technology like IoT, machine learning, and artificial intelligence means that there’s more structured and unstructured data available today than ever before. As a data engineer, you know that organizing big data can yield pivotal insights when it’s gathered from disparate sources. We need a big data experienced professional like you to help our clients find answers in their data to impact important missions—from fraud detection to cancer research to national intelligence. As a lead big data engineer at Booz Allen, you’ll use your expertise to lead data engineering activities on some of the most mission-driven projects in the industry. You’ll oversee the development and deployment of pipelines and platforms that organize and make disparate data meaningful. Here, you’ll guide and mentor a multi-disciplinary team of analysts, data engineers, developers, and data consumers in a fast-paced, agile environment. You’ll use your expertise in analytical exploration and data examination while you oversee the assessment, design, building, and maintenance of scalable platforms for your clients. Work with us to use big data for good. Join us. The world can’t wait. You Have: 7+ years of experience in application development 7+ years of experience designing, developing, operationalizing, and maintaining complex data applications at enterprise scale 5+ years of experience creating software for retrieving, parsing, and processing structured and unstructured data 5+ years of experience developing scalable ETL and ELT workflows for reporting and analytics Experience developing scripts and programs for converting various types of data into usable formats, supporting project team to scale, and monitoring and operating data platforms Experience supervising others and leading projects and deliverables within a collaborative, cross-functional team environment Ability to obtain a security clearance Bachelor’s degree Nice If You Have: 10+ years of experience using Python, SQL, Scala, or Java 5+ years of experience with a public cloud, including AWS, Microsoft Azure or Google Cloud 5+ years of experience with Distributed data and computing tools, including Spark, Databricks, Hadoop, Hive, AWS EMR, or Kafka 5+ years of experience working on real-time data and streaming applications 5+ years of experience with NoSQL implementation, including MongoDB or Cassandra 5+ years of experience with data warehousing, including AWS Redshift, MySQL, or Snowflake 5+ years of experience with UNIX and Linux, including basic commands and Shell scripting 5+ years of experience with Agile engineering practices TS/SCI clearance with a polygraph Clearance: Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information. Create Your Career: Grow With Us Your growth matters to us—that’s why we offer a variety of ways for you to develop your career. With professional and leadership development opportunities like upskilling programs, tuition reimbursement, mentoring, and firm-sponsored networking, you can chart a unique and fulfilling career path on your own terms. A Place Where You Belong Diverse perspectives cultivate collective ingenuity. Booz Allen’s culture of respect, equity, and opportunity means that, here, you are free to bring your whole self to work. With an array of business resource groups and other opportunities for connection, you’ll develop your community in no time. Support Your Well-Being Our comprehensive benefits package includes wellness programs with HSA contributions, paid holidays, paid parental leave, a generous 401(k) match, and more. With these benefits, plus the option for flexible schedules and remote and hybrid locations, we’ll support you as you pursue a balanced, fulfilling life—at work and at home. Your Candidate Journey At Booz Allen, we know our people are what propel us forward, and we value relationships most of all. Here, we’ve compiled a list of resources so you’ll know what to expect as we forge a connection with you during your journey as a candidate with us. Compensation At Booz Allen, we celebrate your contributions, provide you with opportunities and choices, and support your total well-being. Our offerings include health, life, disability, financial, and retirement benefits, as well as paid leave, professional development, tuition assistance, work-life programs, and dependent care. Our recognition awards program acknowledges employees for exceptional performance and superior demonstration of our values. Full-time and part-time employees working at least 20 hours a week on a regular basis are eligible to participate in Booz Allen’s benefit programs. Individuals that do not meet the threshold are only eligible for select offerings, not inclusive of health benefits. We encourage you to learn more about our total benefits by visiting the Resource page on our Careers site and reviewing Our Employee Benefits page. Salary at Booz Allen is determined by various factors, including but not limited to location, the individual’s particular combination of education, knowledge, skills, competencies, and experience, as well as contract-specific affordability and organizational requirements. The projected compensation range for this position is $96,600.00 to $220,000.00 (annualized USD). The estimate displayed represents the typical salary range for this position and is just one component of Booz Allen’s total compensation package for employees. This posting will close within 90 days from the Posting Date. Work Model Our people-first culture prioritizes the benefits of flexibility and collaboration, whether that happens in person or remotely. If this position is listed as remote or hybrid, you’ll periodically work from a Booz Allen or client site facility. If this position is listed as onsite, you’ll work with colleagues and clients in person, as needed for the specific role. EEO Commitment We’re an equal employment opportunity/affirmative action employer that empowers our people to fearlessly drive change – no matter their race, color, ethnicity, religion, sex (including pregnancy, childbirth, lactation, or related medical conditions), national origin, ancestry, age, marital status, sexual orientation, gender identity and expression, disability, veteran status, military or uniformed service member status, genetic information, or any other status protected by applicable federal, state, local, or international law.",
        "url": "https://www.linkedin.com/jobs/view/3956712270"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Columbia, MD",
        "job_id": 3787767751,
        "company": "Orion Consortium, LLC",
        "title": "Virtualization System Engineer Level 2",
        "created_on": 1720638412.8897452,
        "description": "Description The job duties of the Virtualization System Engineer are as follows: Extensive experience in VMware & Windows administration, troubleshooting, upgrades, migration etc. Excellent Working Knowledge on ESX servers from command line. Monitoring and Performance analysis on ESX Servers and VMs, Patching of ESX servers and VMs with VMware Update manager. Ability to work with VSPHERE / VMware Consolidated Backup / Virtual machine backups. Backup and restore of VMWare servers and virtual sessions. Experience in the Installation, configuration, and administration of components in large VMware environment around best practice to vCenter, VMware update manager, vCenter Operation manager, vCenter site recovery manager. Experience with server consolidation/virtualization for HW reduction and site migration via P2V, V2V. Upgrading experience in Esxi hosts and VM guests using update manager. Excellent analytical skills, identify issues / gaps and recommend solutions. Audit the environment and provide recommendations in technology and process areas for Ops improvement. Develops and applies advanced methods, theories and research techniques in investigation and solution of complex and difficult system design requirements and problems. Deal with hardware & software whenever required & resolve issues by minimizing downtimes Reviews literature, patents, and current practices relevant to the solution of assigned projects. Reviews completion and implementation of systems additions and/or enhancements and recommends corrections in technical application and analysis to management. Evaluates Vendor Capabilities To Provide Required Products Or Services. Consults on technical aspects to other organizations. Maintain security posture. Ensures server data integrity by evaluating, implementing, and managing appropriate software and hardware solutions Conducts regular hardware and software audits of servers to ensure compliance with established standards, policies, and configuration guidelines. Develops and maintains a comprehensive operating system hardware and software configuration database/library of all supporting documentation. Responds to system alerts and engages in troubleshooting and problem resolution. Collaborates with vendors on product capabilities and improvements needed to meet customer requirements. Schedules installations and upgrades and maintains them in accordance with established IT policies and procedures Higher level Installs and monitors security auditing software and remediates defects based on customer security policies Required Qualifications: Fourteen (14) years of experience as an SE in programs and contracts of similar scope, type and complexity is required. Bachelor’s degree in System Engineering, Computer Science, Information Systems, Engineering Science, Engineering Management, or related discipline from an accredited college or university is required. Five (5) years of additional SE experience may be substituted for a Bachelor’s degree. Must have excellent communication, interpersonal, and leadership skills. Required Skills: Should have good communication and customer interaction skills. Must be willing to support after hour activities. Must have a minimum of Security+ certification Position requires a TS/SCI. Powered by JazzHR nsVI2f1K5U",
        "url": "https://www.linkedin.com/jobs/view/3787767751"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Annapolis Junction, MD",
        "job_id": 3958006550,
        "company": "Captivation",
        "title": "Software Engineer 1 - Kubernetes/Kotlin/GitOps",
        "created_on": 1720638414.7046516,
        "description": "Build to something to be proud of. Captivation has built a reputation on providing customers exactly what is needed in a timely manner. Our team of engineers take pride in what they develop and constantly innovate to provide the best solution. Captivation is looking for software developers who can get stuff done while making a difference in support of the mission to protect our country. Description Captivation Software is looking for a mid level software engineer who will assist with daily responsibilities on the program. Requirements Security Clearance: Must currently hold a Top Secret/SCI U.S. Government security clearance with a favorable Polygraph, therefore all candidates must be a U.S. citizen Minimum Qualifications: Seven (7) years experience as a SWE, in programs and contracts of similar scope, type, and complexity is required. Bachelor's degree in Computer Science or related discipline from an accredited college or university is required. Bachelor's degree in Computer Science or related discipline from an accredited college or university is required Four (4) years of SWE experience on projects with similar software processes may be substituted for a bachelor's degree. Required Skills: Kubernetes deployment and maintenance Experience with Kotlin development Desired Skills: Kubernetes networking experience GitOps experience with Kubernetes deployments This position is open for direct hires only. We will not consider candidates from third party staffing/recruiting firms. Benefits Annual Salary: $130,000 - $270,000 (Depends on the Years of Experience) Up to 20% 401k contribution (No Matching Required and Vested from Day 1) Above Market Hourly Rates $3,200 HSA Contribution 5 Weeks Paid Time Off Company Paid Employee Medical/Dental/Vision Insurance/Life Insurance/Short-Term & Long-Term Disability/AD&D",
        "url": "https://www.linkedin.com/jobs/view/3958006550"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Washington, DC",
        "job_id": 3958445724,
        "company": "OneGlobe LLC",
        "title": "Software Engineer - Hybrid",
        "created_on": 1720638416.8001626,
        "description": "Job Opportunity: Our team is seeking a talented Senior Software Engineer to work in our Washington, DC, location (Hybrid). What You'll Get to Do as a Senior DevSecOps Developer: Analyze, design, develop and testing of the enterprise-wide web application using Java- and JavaScript-based technologies. Support a full system development agile lifecycle using CI/CD and DevSecOps practices. Understand and use enterprise standards and best practices around coding, application design and REST/API design. Translate business requirements and context into sound and efficient solutions. Communicate with product owners and teammates to discuss problems and resolution. Proactively identify solutions and work as a member and mentor in a highly productive team. Learn and adopt new technologies quickly and independently and assist teammates with implementation and knowledge sharing. You'll Bring These Qualifications: BS degree with 10 years of experience. Extensive working experience in Agile methodologies such as Kanban, Scrum, or SAFe as part of multi-team organizations. Strong programming skills with Java (8+), Spring Framework (Core, Boot, Data), Vert.x, JavaScript, ReactJS, Redux. Extensive working experience of developing and designing RESTful APIs and web services using XML, JSON, and their associated technologies. Extensive working experience in CI/CD and DevSecOps technologies and practices – Jenkins, Docker, Kubernetes/OpenShift. Working experience in automated QA frameworks such as Selenium, Cucumber, Jest, JUnit/TestNG. Extensive working experience with relational databases such as PostgreSQL. Experience in designing solutions for complex projects, customizable applications, and config- or data-driven features. Experience with BPMN and BPM Engines (Camunda preferred). Extensive experience and ability to mentor others in the use of AWS services (DynamoDB, SQS, S3, EC2, IAM). AWS Certified Developer or Solutions Architect preferred. A bout OneGlobe: OneGlobe LLC was founded in 2005 to provide quality Information Technology solutions that exceed expectations. We focus on IT System Modernization using agile software development practices and DevSecOps to deliver intuitive and maintainable systems that we help our customers improve their processes and capabilities. We provide full service IT solutions and have the skill to identify, plan and perform cost-saving steps throughout the system lifecycle to enhance system efficiency, while optimizing the value that we deliver to our customers. Our team has the drive and right mindset to feel ownership on the projects they work. They partner with our customers to give the extra effort sometimes required to deliver success. We provide highly competitive benefits package to include: extensive medical/dental/vision, 7% of your annual salary toward 401(k), Paid Time Off (PTO), $5K annually toward ongoing education and training, and more. We also have monthly social and tech events! See additional positions at: http://www.oneglobeit.com/#careers OneGlobe is a proud equal opportunity employer. We are a drug free, EEO employer committed to a diverse workforce. We will consider all qualified candidates regardless of race, color, national origin, sex, age, marital status, personal appearance, sexual orientation, gender identity, family responsibilities, disability, political affiliation or veteran status.",
        "url": "https://www.linkedin.com/jobs/view/3958445724"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Maryland, United States",
        "job_id": 3938590255,
        "company": "WOOD Consulting Services, Inc.",
        "title": "Software Engineer Level 0",
        "created_on": 1720638418.5594044,
        "description": "Overview Software Engineer Level 0 woodcons.com The Software Engineer develops, maintains, and enhances complex and diverse software systems (e.g., processing-intensive analytics, novel algorithm development, manipulation of extremely large data sets, real-time systems, and business management information systems) based upon documented requirements. Works individually or as part of a team. Reviews and tests software components for adherence to the design requirements and documents test results. Resolves software problem reports. Utilizes software development and software design methodologies appropriate to the development environment. Provides specific input to the software components of system design to include hardware/software trade-offs, software reuse, use of Commercial Off-the-shelf (COTS)/Government Off-the-shelf (GOTS) in place of new development, and requirements analysis and synthesis from system level to individual software components. Security Clearance Requirements This position requires all candidates to be U.S. Citizens and possess an active TS/SCI Security Clearance with a Polygraph. Responsibilities Analyze user requirements to derive software design and performance requirements Debug existing software and correct defects Provide recommendations for improving documentation and software development process standards Design and code new software or modify existing software to add new features Integrate existing software into new or modified systems or operating environments Develop simple data queries for existing or proposed databases or data repositories Qualifications Required Education & Years of Experience: Bachelor’s degree in Computer Science or related discipline from an accredited college or university is required.  Four (4) years of SWE experience on projects with similar software processes may be substituted for a bachelor’s degree. No demonstrated experience is required. Required Skills One (1) or more years of JavaScript programming experience and development of presentation tiers. Familiarity with Java and Spring desired. Experience designing and writing REST-ful applications. Experience writing Interface Control Documents (ICDs). Experience with Microsoft office tools (Word, Excel, Powerpoint). Experience programming on Linux platforms. Experience using one or more of the following revision control applications: git, Subversion (SVN), CVS, ClearCase. Experience with test driven development. WOOD Consulting Services, Inc. is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or protected veteran status and will not be discriminated against on the basis of disability.",
        "url": "https://www.linkedin.com/jobs/view/3938590255"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Annapolis Junction, MD",
        "job_id": 3904571875,
        "company": "NiSUS Technologies Corporation",
        "title": "TB13SWE3 - Sr Software Engineer - Cleared",
        "created_on": 1720638422.7539616,
        "description": "The Software Engineer develops, maintains, and enhances complex and diverse software systems (e.g., processing-intensive analytics, novel algorithm development, manipulation of extremely large data sets, real-time systems, and business management information systems) based upon documented requirements. Works individually or as part of a team. Reviews and tests software components for adherence to the design requirements and documents test results. Resolves software problem reports. Utilizes software development and software design methodologies appropriate to the development environment. Provides specific input to the software components of system design to include hardware/software trade-offs, software reuse, use of Commercial Off-the-shelf (COTS) in place of new development, and requirements analysis and synthesis from system level to individual software components. Requirements TS/SCI with poly required Twleve (12) years experience as a SWE in projects of similar scope, type, and complexity is required Bachelors degree in Computer Science or related discipline from an accredited college or university is required. Four (4) years of additional SWE experience on projects with similar software processes may be substituted for a bachelors degree Spark SQL Python Data Science Benefits Health & Life Insurance Dental Insurance Disability Insurance 401K Retirement Plan with Matching Tuition Assistance Vacation and Sick Leave Hiring Bonuses Referral Recruitment Program",
        "url": "https://www.linkedin.com/jobs/view/3904571875"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Washington, DC",
        "job_id": 3952949529,
        "company": "Harmonia Holdings Group, LLC",
        "title": "Software Engineer",
        "created_on": 1720638424.4087684,
        "description": "We have an exciting opportunity for a Software Engineer to join our team. The ideal candidate will be a Full Stack Developer with experience performing . NET and SQL development, unit testing, and database management tasks. To perform this job successfully, an individual must possess the knowledge, skills, and abilities listed; meet the education and work experience required; and must be able to perform each essential duty and responsibility satisfactorily. Other duties, in addition to the below, may be assigned as necessary to meet business needs. Essential Job Functions: Write clean, scalable code using .NET programming languages. Design, develop, document, test and debug new and existing software systems, and applications. Understand, design and maintain complex databases, write PL/SQL code, stored procedures and user-defined functions. Ability to diagnose problems and resolve issues across various tiers (application, database, server). Research and fix security vulnerabilities. Ability to implement automation through use of jobs, shell scripts, or other techniques. Follow the team’s established Agile processes which includes supporting short and long-term planning, cross-functional support and demonstrate working software. Minimum Required Qualifications: Bachelor’s degree in computer science or related degree preferred. 5 years of development experience with .NET and C# ​Active Top Secret with past SCI . Employment based sponsorship not available. Knowledge, Skills and Abilities: In-depth knowledge of .Net infrastructure. proficiency in C#, .NET utilizing .NET MVC framework. Web Form, .Net CORE, CSS, JAVASCRIPT, HTML, JQUERY, AJAX, WCF, ADO.NET and client-side framework. Strong knowledge and experience with SQL Server 2019 database and SQL Server technologies (writing SQL/T-SQL, Stored Procedure, UDF, triggers). DevOps experience working with CICD implementation and build/deployment tools (TFS and/or Azure DevOps). Strong customer service/client facing skills. Excellent oral and written communication skills. Strong troubleshooting/performance tuning skills. Excellent organizational and time management skills. Self-driven, flexible and innovative. Preferred Skills: Familiarity with Azure Web Apps and AWS Cloud. Understanding of code versioning tools - such as TFS, Git. Good understanding of Microsoft SharePoint. Location : On-Site in Washington DC.",
        "url": "https://www.linkedin.com/jobs/view/3952949529"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Annapolis Junction, MD",
        "job_id": 3887645125,
        "company": "ClearEdge",
        "title": "Senior Analytics Software Engineer",
        "created_on": 1720638426.2731924,
        "description": "Join ClearEdge and be a part of the team of men and women that solve some of the DoD’s most complex technical challenges. Every day, ClearEdge empowers customers in Government and industry with innovative data driven solutions. Check out our extremely competitive benefits package at clearedgeit.com/benefits which includes a 10k annual training/education benefit, tech bonuses, and state of the art technology lab. We also offer monthly TechConnect and DeepDive collaboration sessions with our entire technical staff, fully paid. Day In The Life Use leading edge technologies to develop analytics pulling data from multiple disparate sources Develop prototype answering new questions and providing better answers to existing questions Using data science methods, develop new ways to represent data and automate processes via machine learning frameworks What we are expecting from you Bachelor’s degree in Computer Science or related field from an accredited college or university 11+ years experience on projects of similar size, scope, or complexity. Advanced degrees will count as years of experience Experience with Java Pig/MapReduce & Python Experience with distributed scalable Big Data Stores such as HBase and Accumulo Experience with Hadoop Distributed File System and related technologies Develpoment experience in a Linux environment Experience with Atlassian Suite of developer tools (Confluence, Jira, etc.) Nice to Have Experience with Apache NiFi Knowledge of IP Protocols and Network Topologies Experience with Maven, Jira, and Git Experience with JavaScript Experience with Jupyter Notebooks About us: Experienced advanced analytic development company providing new Cyber solutions to current and emerging missions. Our Core Values of Honesty, Integrity, Loyalty, Fairness, Respect, Responsibility, and Accountability drive our mission and vision and are the heart of what we do each day. We combine our Core Values with the three key elements of people, technology, and integrity to repeatedly deliver stellar results within our primary competencies. ClearEdge is also pleased to share that we have recently expanded our prime and subcontract presence into new Cyber markets! If this position does not fit your skillset, please visit our Careers page to explore our current openings, or contact a ClearEdge recruiter at careers@clearedgeit.com. ClearEdge is an equal opportunity employer.",
        "url": "https://www.linkedin.com/jobs/view/3887645125"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Reston, VA",
        "job_id": 3959343136,
        "company": "BYTE Systems, LLC",
        "title": "Software Engineer",
        "created_on": 1720638427.9534044,
        "description": "Candidate MUST possess a TS/SCI clearance with Intel Polygraph Overview We are looking for a Software Engineer with full stack devevelopment, cloud engineering/architecture, and containerization experience Software EngineerThe Sponsor seeks software engineering and development support to create custom applications, scripts, or solutions in support of requirements for analysis on high priority intelligence issues. Software solutions experts will create custom tool sets; custom software, data extract, transform and load solutions for a wide variety of datasets and deliver those solutions via various infrastructure configurations such as on cloud infrastructure, on-premises-and-cloud hybrid infrastructure, or on local infrastructure. The Sponsor requires a highly skilled Software Developer that will use coding best practices and adhere to security requirements to develop innovative user interface designs in response to analyst, user analyses, product innovation, technology, and business needs. The Software Developer will support or create databases to connect to applications, as necessary. The individual in this role will ensure user interface guidelines and standards are followed during the development and maintenance of the product. The individual will present design concepts and negotiate with stakeholders to arrive at a design that supports both the business need and a strong user experience. The will conduct and participate in product usability tests. The Software Developer will communicate and work effectively with cross-functional team members including but not limited to analysts, data scientists, management, project managers, and software solutions integrators. The technical professional will maintain existing applications or developed applications per required enterprise or local requirements such as patching or other updates or upgrades. They will create or maintain documentation on technical solutions to include architecture diagrams, minimum viable product templates, user guides, user or functional testing instructions, quick references, development framework guides, APIs, and documentation related to technical solutions developed. (Mandatory) Demonstrated experience with setting up and maintaining Cloud based DevOps functionality. 2. (Mandatory) Demonstrated experience with Java, JavaScript, Python or similar development languages and associated frameworks for applications development such as Flask, ReactJS or similar. 3. (Mandatory) Demonstrated experience working with data in, and developing applications with, Sql and NoSQL databases. 4. (Mandatory) Demonstrated experience documenting system design and architecture. 5. (Mandatory) Demonstrated strong coordination and collaboration skills with ability to work successfully across organizational boundaries. 6. (Mandatory) Demonstrated experience with performance tuning, monitoring, troubleshooting and de-bugging applications and services. 7. (Mandatory) Demonstrated experience developing, deploying and maintaining containerized applications and databases. 8. (Desired) Demonstrated experience with the Sponsors deployment of AWS, experience deploying applications within the Sponsors environment. 9. (Desired) Demonstrated experience working in an Agile software development cycle. 10. (Desired) Demonstrated experience with Sql and NoSQL databases. 11. (Desired) Demonstrated experience with geospatial data, visualization, and relevant analytic methods and tools. 12. (Desired) Demonstrated experience manipulating large datasets. 13. (Desired) Demonstrated experience with open source data engineering tools and processes. MUST be a US Citizen with a U.S. Government clearance - Intel with Polygraph NOTE: Must have an active TS-SCI with poly. No sponsorships or upgrades are available. Submissions without this requirement will not be considered. H1-B holders will not be considered. Benefits 5 week paid vacation + 10 gov't holidays 15% contribution to 401k LTD, STD disability and life insurance Paid health, dental, and vision for employee and family. $5000 annual training expense reimbursement Computer purchase plan",
        "url": "https://www.linkedin.com/jobs/view/3959343136"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Washington DC-Baltimore Area",
        "job_id": 3970744393,
        "company": "Braintrust",
        "title": "Software Engineer - Backend",
        "created_on": 1720638429.6634412,
        "description": "About Us Braintrust is a user-owned talent network that connects top-tier professionals with the world's leading enterprises. We prioritize transparency, eliminating middlemen and high markups, ensuring job-seekers are matched swiftly to innovative roles while clients benefit from unparalleled efficiency and quality. About The Hiring Process The hiring process for this role involves completing your Braintrust profile, applying directly to the role on Braintrust, and undergoing a one-time screening to ensure you meet our vetted talent specifications. After this, the hiring team will contact you directly if they believe you are a suitable match. Our process isn't for everyone, that's intentional. If you believe that you are a top candidate for this job, please join our network to give yourself the opportunity to work with top companies. JOB TYPE: Direct Hire/ FTE Position (no agencies/C2C - see notes below) LOCATION: Work from anywhere - Anytime | No timezone overlap required SALARY RANGE $140,000 – $190,000 /yr ESTIMATED DURATION: 40/week - long term EXPERIENCE: 3-4 years BRAINTRUST JOB ID: 11876 The Opportunity Qualifications 3+ years of professional experience as a Backend Software Engineer, preferably in the fintech/crypto industry. Strong proficiency in backend programming languages such as Java, Python, C#.Net, Scala, Go etc A security first mindset: with good knowledge of common security concepts such as AuthN, AuthZ, SAML, API security, encryption etc An understanding of crypto concepts such as blockchains, RPCs, self custody wallets etc Solid understanding of RESTful API design principles and microservices architecture. Familiarity with cloud platforms such as Azure. Knowledge of DevOps practices and CI/CD pipelines. Excellent communication and collaboration abilities. If you are passionate about building simple solutions to complex problems and are obsessed with creating and building value, we would love to hear from you. Join our dynamic team and contribute to the development of cutting-edge fintech solutions! What You'll Be Working On Job Description We are seeking a talented and experienced Software Engineer with a strong background in backend engineering. As a member of our team, you will play a crucial role in designing, developing, and maintaining our innovative software solutions. We take pride in #building #secure and #seamless products - Do you identify with this? Responsibilities Develop and maintain backend services and APIs Work closely with Azure cloud services for deployment and scalability. Conduct thorough testing, debugging, and prompt resolution of software defects. Stay up-to-date with industry trends and emerging technologies. Contribute to code reviews, architectural decisions, and system design. Create high-quality, efficient, and reliable code following best practices. Optimize performance and scalability of backend systems for handling large data volumes. Stay updated on the latest trends and technologies in fintech/crypto and backend development. Build crypto first and secure products to facilitate traditional financial activity, such as payments, transfers etc. About Mesh Founded in 2020, Mesh’s mission is to build an open, connected and secure financial ecosystem. Mesh is a modern financial operating system that provides enterprise clients with the ability to enable digital asset transfers, crypto payments, account aggregation and registered securities trading, all within their platform. Mesh is creating an embedded financial ecosystem that is more open, connected and secure for businesses and users alike. Mesh has found a particular market need in the digital asset space and is uniquely positioned to provide some much needed simplification. Mesh integrates with 300+ crypto exchanges and self-custody wallets, such as Coinbase, Binance, Kraken, Metamask, Trust Wallet, etc. and offers a seamless, embedded authentication and transfer experience experience that significantly reduces user friction in use cases like asset deposits, crypto payments, crypto payouts, and more. Mesh is funded by notable VCs like Streamlined VC, Motivate VC, B Capital Group, Dreamers VC, SNR, amongst others. Apply Now! Notes Our employers all have varying legal and geographic requirements for their roles, they trust Braintrust to find them the talent that meet their unique specifications. For that reason, this role is not available to C2C candidates working with an agency. If you are a professional contractor who has created an LLC/corp around their consulting practice, this is well aligned with Braintrust and we’d welcome your application. Braintrust values the multitude of talents and perspectives that a diverse workforce brings. All qualified applicants will receive consideration for employment without regard to race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or protected veteran status.",
        "url": "https://www.linkedin.com/jobs/view/3970744393"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Chantilly, VA",
        "job_id": 3966850031,
        "company": "ANSER",
        "title": "System Engineer",
        "created_on": 1720638431.279459,
        "description": "Company Overview ANSER enhances national and homeland security by strengthening public institutions. We provide thought leadership for complex issues through independent analysis, and we deliver practical, useful solutions. ANSER values collaboration, integrity, and initiative and we are client focused in all that we do. Because we were established for the purpose of public service and not for profit, we measure our success in the impact of our service. Specific Job Description ANSER is seeking Senior-level Systems Engineers to guide engineering teams in multi-discipline approaches to requirements engineering, solutions engineering, scheduling, reliability, resiliency, services development, integration, test and evaluation, maintainability and analysis across the National System of Geospatial-intelligence (NSG), and other Federal Agencies to ensure timely and accurate geospatial intelligence. Day to day you will guides mid-level and associate level system engineers in performing requirements engineering, solutions engineering, scheduling, reliability, resiliency, services development, integration, test and evaluation, maintainability, and analysis: Provide hands on guidance and development for planning, analysis/traceability of user requirements, architectures traceability, procedures, and problems to automate or improve existing systems and review cloud service capabilities, workflow, and scheduling limitations. Develop solutions designs based on analysis of requirements and new technology. Assists with translation of customer requirements/needs into systems/capability requirements and solutions. Support the analyses and allocation of requirements to systems architecture components and executing programs. Assists the government agency in performing systems integration activities. Conducts Analysis of Alternatives (AoAs), Course of Actions (CoAs), Trade Studies, and Engineering Assessments. Assists in strategic technical planning, project management, performance engineering, risk management and interface design. Integrate multiple systems, services, processes, and interfaces within a major systems acquisition across organizations and agencies. THIS POSITION REQUIRED ON-SITE WORK 5 DAY A WEEK IN CHANTILLY, VA Required Qualifications And Experience Bachelor’s degree or higher in Systems Engineering or in related technical or scientific fields. Will consider 8 years of experience as a Systems Engineer in lieu of degree. A minimum of 5 years working experience in government or industry in relevant work areas including: Department of Defense or Intelligence Community Acquisition Process, Requirements Process, PPBES Process or system engineering of large complex System of Systems or Service Oriented Architecture/Cloud environments. Demonstrated experience and understanding of systems engineering lifecycle. TS/SCI clearance adjudication or ability to obtain SCI and pass a poly. Understanding of model-based system engineering, processes, tools and languages. Demonstrated understanding of cloud solutions, architecture and applications. Disclaimer In compliance with the Americans with Disabilities Act Amendment Act (ADA), if you have a disability and would like to request an accommodation in order to apply for a position with ANSER, please call 703-416-2000 or e-mail Recruiting@anser.org. ANSER is proud to be an Equal Opportunity Employer. We seek individuals from a broad variety of backgrounds with varying levels of experience who have a desire to do meaningful work. We recruit, employ, train, compensate, and promote regardless of race, color, gender, religion, national origin, ancestry, disability, age, sexual orientation, or any other characteristic protected by law.",
        "url": "https://www.linkedin.com/jobs/view/3966850031"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Rockville, MD",
        "job_id": 3967415098,
        "company": "Unissant",
        "title": "Linux System Engineer",
        "created_on": 1720638433.4951925,
        "description": "Unissant, Inc. is an innovative solutions development and consulting company committed to delivering simplicity through innovation. We meet this commitment by delivering state-of-the-art enterprise computing and processing solutions to both government and commercial clients. Our workforce continually strives to advance (y)our collective intellect through collaborative fortitude, and we are looking for further talent to join that effort! To learn more about our exciting organization, please visit us at www.unissant.com We are seeking a Linux Systems Engineer to join our team in Rockville, MD. Program Mission : This program provides a full range of infrastructure support to a leading federal agency including server architecture design and maintenance, application hosting and administration, providing fixes and upgrade patches and ensuring cyber security compliance protections. You'll join a team balancing process-driven IT operations and Lean engineering with an integrated culture of continuous improvement, coupled with innovation and high-quality support services. Essential Duties and Responsibilities : Responsible for working hands-on designing servers in Oracle Enterprise Linux, CentOS, Ubuntu, and Red Hat Enterprise Linux in a federal agency in both on-premises and cloud environments. Consult and advise the federal client and associated developers to develop Linux-based solutions, plan, install, and act as a mentor and escalation point for the operations and maintenance of a multi-tiered server infrastructure. Lead the testing and deployment of services and applications developed by other contractor teams and provide required functionality to support compliance with federal policy guidelines. Monitor and evaluate, plan for capacity and design enterprise, fault tolerant systems utilizing various Linux flavors, and support development projects. Work closely with teammates in other functional areas, maintain and enhance server security. Work closely with various team members internally and externally for successful completion of various day-to-day tasks and ongoing infrastructure support projects and lead efforts to enhance standard operating procedures and technical documentation. This position will identify hardware and software platform issues and work with teammates to implement proper patches and updates. Support performance tuning and recommend performance improvements, troubleshoot, resolve performance problems and issues, along with other duties as assigned. Work Experience : A minimum of 4 years working with Linux servers is required. Experience with configurations management tools such as Puppet. Experience with patch management tools such as Satellite and Oracle Linux Manager. Experience in compiling and deploying open-source software. Experience performing IT Engineering responsibilities. Experience with secure, tiered, application hosting environments. Familiarity with HTTP, DNS, Internet Protocol, and the OSI Model. Experience with server vulnerability remediation following agency, DHS, and/or DoD STIG Guidelines including working with SSL, DNS, and cyphers issues. Experience with data transfer protocols such as GridFTP. Professional IT experience in a large, distributed data center environment. Experience in administering SSL/TLS certificates for web servers (creating, applying, troubleshooting). Job Skills : Ability to work hands-on to design and continuously improve Linux servers. Ability to script and automate common tasks in Linux environments. Exposure to Amazon AWS or Microsoft Azure engineering and/or administration. Exposure to load balancers such as F5 BigIP. Exposure to server management systems like BigFix or LANrev. Exposure to server and application deployment automation systems like Kickstart, Jenkins, and Docker. Strong troubleshooting skills. Familiarity with performance monitoring and troubleshooting tools. Demonstrated ability to communicate effectively with customers, team members and managers. Strong and Proven Customer service skills. Ability to multi-task and prioritize work within a dynamic environment. In-depth knowledge and experience building, configuring, and monitoring web server system components; LDAP Authentication and compiling open-source applications. Programming or scripting background with one or more of the following: Java, C/C++, PHP and/or any scripting languages (BASH, Python, Ruby, Perl, etc.). Established documentation skills. Education : Bachelor's Degree in Computer Science or a related field is desired Certifications, Licenses and Registrations: One or more of the following certifications is desired: Red Hat Certified System Administrator (RHCSA). Red Hat Certified Engineer (RHCE). ITIL Foundations certification. Must be able to obtain and maintain a Federal Public Trust/Suitability. Communication Skills : Excellent oral and written communications skills. Solid ability to interface, inspire and motivate at various levels of the organization. Ability to convey technical information to non-technical individuals. Demonstrated experience communicating effectively across internal and external organizations. Travel : Availability for occasional after-hours and weekend support. Willing to work onsite as needed Environmental Requirements : Mainly sedentary; in an office environment May be required to lift up to ten (10) pounds Flexible in working extended hours The above statements are intended to describe the general nature and level of work being performed by the individual(s) assigned to this position. They are not intended to be an exhaustive list of all duties, responsibilities, and skills required. Unissant management reserves the right to modify, add, or remove duties and to assign other duties as necessary. In addition, where applicable and available, reasonable accommodation(s) may be made to enable individuals with disabilities to perform essential functions of this position. Please note: Candidate(s) will be required to go through pre-employment screening. Unissant, Inc. is a proud Equal Opportunity Employer! (EOE; M/F/Disability/Vets)",
        "url": "https://www.linkedin.com/jobs/view/3967415098"
    },
    {
        "task_id": "a30229cddd704dc5a7e1d8e38e0dfa0c",
        "keyword": "Data Engineer",
        "location": "Herndon, VA",
        "job_id": 3963012333,
        "company": "Intrepid Technology Group LLC (ITG)",
        "title": "System Engineer",
        "created_on": 1720638435.3169715,
        "description": "Are you a System Engineer ready to revolutionize database management and system optimization? Join us at the forefront of innovation as we shape the future of digital infrastructure! Intrepid Technology Group, LLC, a pioneer in delivering cutting-edge solutions in system and network engineering, artificial intelligence, communications, and satellite communications, driving innovation for a connected world, has an immediate opening for an experienced SQL Systems Engineer. The selected candidate will support multiple domains with a 1000+ user load on each network. Duties will include user account management, account and group policy management, installation and configurations of server and software, scripting, upgrade management, Tier3 and Tier4 O&M troubleshooting, responsible for trouble shooting server network, and system issues. An active TS/SCI w/Poly is a MUST to apply! Location: Herndon, VA Terms: Full-time Schedule: On-site Core Hours: 0930-1430 Qualifications: Bachelor's or Master's Degree is preferred Minimum of 11 years' experience. Ability to demonstrate of Tier 2, Tier 3, and Tier 4 troubleshooting skills. Ability to demonstrate expert experience with Server OS 2008R2, 2012R2, 2016, 2019. Expert knowledge of Active Directory, DNS, and DHCP Demonstrate in-depth knowledge and experience with cloud technologies. Microsoft SCCM/SQL Experience with 1906+ Understanding of Images, Manage Image, Build Image, and Deploy the Image Configure, manage, and deploy software updates to required groups. Experience with scripting in PowerShell, VB, C# Ability to effectively prioritize and execute task. Must be able to work independently or with a team. Must have the ability to adjust and learn new technologies. Experience writing enterprise level technical standards documentation and diagrams. Strong Customer service skills Certification in MCSE/MCITP Preferred Skills: Microsoft Azure Pack PowerShell Understanding of Enterprise Networking best practices​ The successful candidate will need to be proactive self-starter requiring minimal supervision who can monitor existing infrastructure, identify, and coordinate implementation changes to address problems or engineering changes with little to no impact to the end users The candidate will play a vital role in researching and engineering a new system to manage the distribution of updates and hotfixes across domains.",
        "url": "https://www.linkedin.com/jobs/view/3963012333"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3941692465,
        "company": "Foley",
        "title": "Data Engineer",
        "created_on": 1720638445.678973,
        "description": "MISSION Missions are accomplished with people, driving towards a greater purpose. Foley has been a trusted partner to motor carriers of all sizes. Our technologically advanced approach to DOT compliance automates many of the background screening and compliance requirements motor carriers must comply with- making it easier for them to onboard and retain the right driver for their business. At Foley we just don’t help our customers stay compliant – we help them thrive. A positive attitude is everything. At Foley, we’ve created a culture that rewards kindness, enthusiasm, and a can-do attitude. Whether we’re working together internally or helping a customer solve a problem, we approach every challenge with a sense of humor, optimism, and the determination to succeed. Data Engineer Foley is looking for an exceptional Data Engineer with extensive experience in ETL/ELT Tools and Processes, General Database Development, Data Warehousing, Data Delivery Automation and Testing, and foundational engineering patterns and practices to be a key member of Foley’s engineering team. As a Data Engineer at Foley, you will be a key member of the team that drives direction of our data analytics capabilities, evaluating current and prospective tools and technologies, and generally helping to grow Foley’s data product line! You are really excited about joining a team with massive potential for growth. This is a REMOTE option- Those residing in CT, MA, SC, GA, FL, TX are welcome to apply! Who You Will Work With At Foley, we value our employees and treat them with respect while providing them with opportunities to make a difference. You will work with Foley teammates across the organization. Because our departments work so closely together, we are always looking to improve our current process. We welcome new ways to work collaboratively with roles and departments. We cannot wait to hear your ideas! What You Will Do At first, you will Learn how Foley’s products and services work and how Foley can have a positive impact on our customers’ business. Meet and get to know your teammates and stakeholders of our engineering organization. Understand our processes, procedures, and tools for development and deployment. Attend scrum events for all our development teams to understand how our technology and team works. Get up to speed on value of data products at Foley, understanding both internal and external customer needs. Once you get settled, you will Maintain and evolve existing data pipelines using SQL Server Integration Services (SSIS) packages built using Visual Studio. Implement transformation logic in AWS Redshift data warehouse. Work with views and stored procedures to stage data from source systems for new and existing data pipelines. Design, build, test and deploy solutions to connect to data sources such as SQL Server databases, web services/APIs (from government or public databases, service vendors, and software partners), flat files on network and cloud storage, and other data source as needed. Build data extraction solutions using browser automation, robotic process automation, or other technology, as needed. Critically assess current technologies and processes and provide recommendations for improvement. Help establish and grow data governance policies and practice. Help foster a culture of safety, collaboration, and inquisitiveness. What We’d Like You To Have Excellence in teamwork – sharing ownership and inviting criticism. Excellent grasp of fundamental engineering concepts. Experience in and extensive knowledge of Scrum or Agile development. Strong analytical and diagnostic skills, as well as excellent written and verbal communication skills. Desire to continually improve and a strong appetite for change. Ability to quickly adjust to changing priorities. Strong documentation, organizational, and planning skills. Extensive experience building and running data workflows using SQL Server Integration Services (SSIS). Extensive experience developing in a Microsoft SQL Server environment. Experience working within a version control system (branching, merging, pull requests, et al) Knowledge of cloud native technologies and patterns. Experience with infrastructure-as-code tools and products. Working knowledge of data workflow deployment automation and testing technologies. Diverse knowledge of database products and concepts, including Microsoft SQL, redshift, database versioning and deployment models, et al. Experience with Python, C#, or other programming languages. Experience working with web services/APIs to extract data from vendor solutions Experience in non-SSIS ETL/ELT tooling is a huge plus (Airbyte, Fivetran, et al) If you are an analytical and critical thinker with excellent organizational skills, attention to detail, and the ability to self-organize and meet deadlines, we encourage you to apply. What You Will Love About Foley The people! Our employees and customers consistently express the best thing about Foley is our close-knit, exceptionally talented teams. Check out our customers feedback – on Trustpilot Outstanding benefits. 3 medical plans to choose from, 2 level dental, and 2 level vision plans. Generous vacation, sick, and personal time off. 401K plan with a match. We’ve got your back so you can live your best life. It’s about ideas over egos. You will have the freedom to explore new ideas and approaches in an entrepreneurial environment, supported by a collaborative team. Professional growth. We open our roles to our employees first and encourage them to apply for growth opportunities. Our People Operations team is available to discuss career growth and help put a plan in place, helping employees achieve the growth they crave. Our environment! We celebrate success and believe in transparency and teamwork to get us there. We invest in collaboration tools so you can meet with your team face to face. Many of our roles are remote and we want to ensure our employees are engaged and can interact with their peers in a virtual space. What We Do, How We Do It Too often, companies use a piecemeal approach when it comes to screening drivers. They might use one vendor to help recruit. Another to screen. And still, another to address the complex world that is compliance. This approach is inefficient, expensive, and redundant. Not to mention, it makes it too easy for things to fall through the cracks. At Foley, we've built a company that effortlessly manages these three areas under one roof: recruit, screen, comply. Thanks to powerful technology combined with our compliance expertise, we're able to deliver a comprehensive solution to our customers and, as a result, a better overall customer experience. Where We're Headed We're always developing new solutions to slay tomorrow's recruitment, screening, and compliance monsters. At the core of these solutions is our vast collection of data—and the many ways to leverage it, whether that's developing software to calculate a company's compliance risk or implementing predictive analytics to identify the best drivers. What It's Like To Work With Us Diving deep into a niche industry and becoming an expert . . . Continually growing and advancing . . . Making lifelong friends during the process . . . That Sums Up What It's Like To Work For Us. We're a 250+ Person Company On The Verge Of Explosive Growth Thanks To Our AI-powered Technology—and Where It's Headed With Predictive Analytics. If You'd Like To Board Our Rocket Ship, Check Us Out www.foleyservices.com Keywords Data Architect Database Engineer Big Data Engineer Data Systems Engineer ETL Developer Data Warehouse Engineer Business Intelligence Developer Data Specialist Data Platform Engineer",
        "url": "https://www.linkedin.com/jobs/view/3941692465"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Hartford, CT",
        "job_id": 3947990905,
        "company": "Foley",
        "title": "Data Engineer",
        "created_on": 1720638447.579663,
        "description": "MISSION Missions are accomplished with people, driving towards a greater purpose. Foley has been a trusted partner to motor carriers of all sizes. Our technologically advanced approach to DOT compliance automates many of the background screening and compliance requirements motor carriers must comply with- making it easier for them to onboard and retain the right driver for their business. At Foley we just don't help our customers stay compliant - we help them thrive. A positive attitude is everything. At Foley, we've created a culture that rewards kindness, enthusiasm, and a can-do attitude. Whether we're working together internally or helping a customer solve a problem, we approach every challenge with a sense of humor, optimism, and the determination to succeed. Data Engineer Foley is looking for an exceptional Data Engineer with extensive experience in ETL/ELT Tools and Processes, General Database Development, Data Warehousing, Data Delivery Automation and Testing, and foundational engineering patterns and practices to be a key member of Foley's engineering team. As a Data Engineer at Foley, you will be a key member of the team that drives direction of our data analytics capabilities, evaluating current and prospective tools and technologies, and generally helping to grow Foley's data product line! You are really excited about joining a team with massive potential for growth. This is a REMOTE option- Those residing in CT, MA, SC, GA, FL, TX are welcome to apply! Who You Will Work With At Foley, we value our employees and treat them with respect while providing them with opportunities to make a difference. You will work with Foley teammates across the organization. Because our departments work so closely together, we are always looking to improve our current process. We welcome new ways to work collaboratively with roles and departments. We cannot wait to hear your ideas! What You Will Do At first, you will Learn how Foley's products and services work and how Foley can have a positive impact on our customers' business Meet and get to know your teammates and stakeholders of our engineering organization Understand our processes, procedures, and tools for development and deployment Attend scrum events for all our development teams to understand how our technology and team works Get up to speed on value of data products at Foley, understanding both internal and external customer needs Once you get settled, you will Maintain and evolve existing data pipelines using SQL Server Integration Services (SSIS) packages built using Visual Studio Implement transformation logic in AWS Redshift data warehouse Work with views and stored procedures to stage data from source systems for new and existing data pipelines Design, build, test and deploy solutions to connect to data sources such as SQL Server databases, web services/APIs (from government or public databases, service vendors, and software partners), flat files on network and cloud storage, and other data source as needed Build data extraction solutions using browser automation, robotic process automation, or other technology, as needed Critically assess current technologies and processes and provide recommendations for improvement Help establish and grow data governance policies and practice Help foster a culture of safety, collaboration, and inquisitiveness What We'd Like You To Have Excellence in teamwork - sharing ownership and inviting criticism Excellent grasp of fundamental engineering concepts Experience in and extensive knowledge of Scrum or Agile development Strong analytical and diagnostic skills, as well as excellent written and verbal communication skills Desire to continually improve and a strong appetite for change Ability to quickly adjust to changing priorities Strong documentation, organizational, and planning skills Extensive experience building and running data workflows using SQL Server Integration Services (SSIS) Extensive experience developing in a Microsoft SQL Server environment Experience working within a version control system (branching, merging, pull requests, et al) Knowledge of cloud native technologies and patterns Experience with infrastructure-as-code tools and products Working knowledge of data workflow deployment automation and testing technologies Diverse knowledge of database products and concepts, including Microsoft SQL, redshift, database versioning and deployment models, et al Experience with Python, C#, or other programming languages Experience working with web services/APIs to extract data from vendor solutions Experience in non-SSIS ETL/ELT tooling is a huge plus (Airbyte, Fivetran, et al) If you are an analytical and critical thinker with excellent organizational skills, attention to detail, and the ability to self-organize and meet deadlines, we encourage you to apply. What You Will Love About Foley The people! Our employees and customers consistently express the best thing about Foley is our close-knit, exceptionally talented teams. Check out our customers feedback - on Trustpilot Outstanding benefits. 3 medical plans to choose from, 2 level dental, and 2 level vision plans. Generous vacation, sick, and personal time off. 401K plan with a match. We've got your back so you can live your best life It's about ideas over egos. You will have the freedom to explore new ideas and approaches in an entrepreneurial environment, supported by a collaborative team Professional growth. We open our roles to our employees first and encourage them to apply for growth opportunities. Our People Operations team is available to discuss career growth and help put a plan in place, helping employees achieve the growth they crave Our environment! We celebrate success and believe in transparency and teamwork to get us there. We invest in collaboration tools so you can meet with your team face to face. Many of our roles are remote and we want to ensure our employees are engaged and can interact with their peers in a virtual space What We Do, How We Do It Too often, companies use a piecemeal approach when it comes to screening drivers. They might use one vendor to help recruit. Another to screen. And still, another to address the complex world that is compliance. This approach is inefficient, expensive, and redundant. Not to mention, it makes it too easy for things to fall through the cracks. At Foley, we've built a company that effortlessly manages these three areas under one roof: recruit, screen, comply. Thanks to powerful technology combined with our compliance expertise, we're able to deliver a comprehensive solution to our customers and, as a result, a better overall customer experience. Where We're Headed We're always developing new solutions to slay tomorrow's recruitment, screening, and compliance monsters. At the core of these solutions is our vast collection of data—and the many ways to leverage it, whether that's developing software to calculate a company's compliance risk or implementing predictive analytics to identify the best drivers. What It's Like To Work With Us Diving deep into a niche industry and becoming an expert . . . Continually growing and advancing . . . Making lifelong friends during the process . . . That sums up what it's like to work for us. We're a 250+ person company on the verge of explosive growth thanks to our AI-powered technology—and where it's headed with predictive analytics. If you'd like to board our rocket ship, check us out: www.foleyservices.com Keywords: Data Architect Database Engineer Big Data Engineer Data Systems Engineer ETL Developer Data Warehouse Engineer Business Intelligence Developer Data Specialist Data Platform Engineer",
        "url": "https://www.linkedin.com/jobs/view/3947990905"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Salt Lake City, UT",
        "job_id": 3967251141,
        "company": "Jerry Seiner Dealerships",
        "title": "Data Engineer",
        "created_on": 1720638456.6848984,
        "description": "Job Summary Jerry Seiner is looking for an exceptional data engineer with experience in PostgreSQL, Big Query, and Python. As the sole member of the Business Intelligence team, the Data Engineer works closely with multiple departments and vendors throughout the organization to transform data into actionable information. The ideal candidate will have experience crafting visual data representations by using Excel, Vena, Tableau and Looker Studio. This position is in Salt Lake City, Utah and relocation would be required. A hybrid schedule is available after training. Key Responsibilities Gather and document requirements for the data warehouse, cloud database solutions, and technologies. Help maintain and support ETL jobs and coding, pulling data from various source systems and loading data into the data warehouse. Testing new data pipelines or workflow processes. Develop and maintain databases, data systems, dashboards, and reports to facilitate data analysis and visualization. Perform data cleaning, validation, and preprocessing to ensure data accuracy and integrity. Conduct advanced statistical analysis and modeling to support business forecasting and decision-making processes. Participate in developing and implementing data-related projects, including data governance and quality initiatives. Design and implement intuitive and actionable user reporting using data visualization tools such as Tableau, Vena, and Looker Studio. Collaborate with stakeholders to understand and translate business requirements into data analysis solutions Interact with employees as a technical resource to troubleshoot problems with the delivered BI solutions. Ability to explain advanced data concepts to non-technical users and provide customer assistance and ad hoc training. Fulfill ad hoc data requests and analysis on time. Strong workload management skills and abilities, with an emphasis on the ability to prioritize and attention. Other Data Engineering tasks as determined. Required Qualifications Bachelor's degree in Computer Science, Statistics, Mathematics, Economics, or a related field. Two years of data warehouse experience including one year of experience working on projects with multiple deliverables Strong proficiency in data manipulation, analysis, and visualization using Big Query, SQL, Python, R, Excel, or Tableau tools. Experience with data querying, data modeling, and database design. Capable of developing ETL code. Solid understanding of statistical concepts and methods (e.g., hypothesis testing, regression, clustering). Excellent troubleshooting and problem-solving skills, attention to detail, and passion for working with complex datasets. Ability to effectively communicate technical information to both technical and non-technical stakeholders. Ability to quickly learn new tools and technology. Nice To Have One year of experience in data analysis and report design/development using BI software such as Tableau, Looker Studio etc. Applicants must be authorized to work for ANY employer in the U.S. We are unable to sponsor or take over sponsorship of an employment Visa at this time. Family-owned since 1980, The Jerry Seiner Dealerships believes in helping all employees find the career opportunity that is right for them. We encourage the individual growth and development of our employees through our many employee-focused benefits. For more information about our organization, please visit : hYps://seinercareers.com/ Experience \" The Seiner Difference! \" At Jerry Seiner, we are a family-owned business proudly serving the Utah community since 1980 and have recently expanded into Arizona and Nevada. The Seiner culture believes in helping our team members achieve their highest level of success through training, mentoring, and career advancement. We encourage individual growth and development of our employees through our employee-focused benefits. Explore the many career opportunities that come with being a part of the Seiner team. Benefits May Include Paid Time Off (PTO) accumulates from Day 1 Grow with us! Educational reimbursement Health, Dental & Vision Insurance Employer Health Saving Account contribution each month 401k Retirement plan Year-end bonus programs Great discounts on vehicle purchases Discounts on parts and service Referral bonuses for vehicle purchases Closed Sundays! Pre-employment screenings, including but not limited to your background screening, drug test, and motor vehicle record, are required. Salary: $60000 - $100000 per year Job Posted by ApplicantPro",
        "url": "https://www.linkedin.com/jobs/view/3967251141"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3970251650,
        "company": "Infosys",
        "title": "Data Engineer",
        "created_on": 1720638458.4828203,
        "description": "Job purpose Resource will be part of Data Analytics Team (Operations Analytics group) and involved in Database development, Big Data development and ETL activities contributing to Analytics Projects and Paid POVs for Infosys BPO analytics. Duties and responsibilities Understanding requirements provided by client Analyzing the requirements and document the requirement analysis Come up with Development plan and individual task estimation Design and develop Data flow pipelines using RDBMS or Big Data platform Development and Implement Database design, ETL and prepare database for Visualization and analytics. Qualifications Qualifications include: Education : Engineer Background with IT, Computers or any other field with relevant experience Skills : Hands on experience with Data warehouse and database development (5-8 years) using any of the leading database systems. Hands on experience with Database development including ETL (min 5 years), Sql and Analytical Sql Queries with any of the leading databases e.g. Postgres, Oracle, Sql Server etc. Hands on with Cloud ecosystem and tools Advanced working knowledge of Python Fair working knowledge of Python, R programming for Data analysis Familiar with Reporting tools like Tableau, PowerBI etc. Team Player, Good Communication and Analytics skills Experience: 5-8 Years About Us Infosys McCamish Systems,(http://www.infosysbpm.com/mccamish) located in Atlanta, Georgia, is the Life Insurance and Retirement Services subsidiary of Infosys BPM Limited.(www.infosysbpm.com) Infosys McCamish was started in 1985 as a virtual insurance company and went to market as a commercial services provider in 1995.It has an outstanding business perspective and an exemplary track record that no other outsourcer of business solutions can claim – generating US$16 billion of recurring premium in less than five years as a virtual insurance company. Infosys McCamish has expert technology and outsourcing credentials, along with a proven business model for re-engineering systems and performing back-office services at a reduced cost, while reinforcing accuracy, speed and security. Seven of the top ten US insurers are among Infosys McCamish’s many BPM clients. Infosys McCamish has its operations spread across Atlanta GA and Des Moines IA in USA. Infosys is an equal opportunity employer and all qualified applicants will receive consideration without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, spouse of protected veteran, or disability.",
        "url": "https://www.linkedin.com/jobs/view/3970251650"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Scottsdale, AZ",
        "job_id": 3962917183,
        "company": "CVS Health",
        "title": "Associate Data Engineer - Business Intelligence",
        "created_on": 1720638460.0756373,
        "description": "Bring your heart to CVS Health. Every one of us at CVS Health shares a single, clear purpose: Bringing our heart to every moment of your health. This purpose guides our commitment to deliver enhanced human-centric health care for a rapidly changing world. Anchored in our brand — with heart at its center — our purpose sends a personal message that how we deliver our services is just as important as what we deliver. Our Heart At Work Behaviors™ support this purpose. We want everyone who works at CVS Health to feel empowered by the role they play in transforming our culture and accelerating our ability to innovate and deliver solutions to make health care more personal, convenient and affordable. Position Summary As a member of the Business Intelligence COE, you will be part of a team of experienced data and software engineers specializing in PBM client reporting allowing colleagues to provide clients with actionable information. You will contribute to the management of a suite of homegrown applications by applying problem-solving, programming, and project management skills. You will work with team members to develop and implement business solutions. Required Qualifications SQL programming experience in a SQL Server environment and a data warehouse environment (Teradata, Oracle or UDB) Experience working with web-based applications Comprehensive understanding of Microsoft technologies (Windows, SQL Server, Active Directory, IIS, Microsoft Office etc.) Demonstrated experience working successfully in a team environment Preferred Qualifications Public cloud experience with either AWS or GCP Experience/Certification Experience with Tableau Exposure to Agile Methodology Health Care/PBM domain experience Experience with Java, Spring Boot, and Angular Ability to handle multiple projects and activities in a timely, organized manner Strong ability to communicate technical concepts and results to business partners Strong collaboration skills, strong problem-solving skills, and critical thinking ability High levels of self-motivation and attention to detail PMP Certification Education Bachelors Degree in computer science or engineering or equivalent work experience. Pay Range The typical pay range for this role is: $61,800.00 - $131,800.00 This pay range represents the base hourly rate or base annual full-time salary for all positions in the job grade within which this position falls. The actual base salary offer will depend on a variety of factors including experience, education, geography and other relevant factors. This position is eligible for a CVS Health bonus, commission or short-term incentive program in addition to the base pay range listed above. In addition to your compensation, enjoy the rewards of an organization that puts our heart into caring for our colleagues and our communities. The Company offers a full range of medical, dental, and vision benefits. Eligible employees may enroll in the Company’s 401(k) retirement savings plan, and an Employee Stock Purchase Plan is also available for eligible employees. The Company provides a fully-paid term life insurance plan to eligible employees, and short-term and long term disability benefits. CVS Health also offers numerous well-being programs, education assistance, free development courses, a CVS store discount, and discount programs with participating partners. As for time off, Company employees enjoy Paid Time Off (“PTO”) or vacation pay, as well as paid holidays throughout the calendar year. Number of paid holidays, sick time and other time off are provided consistent with relevant state law and Company policies. For more detailed information on available benefits, please visit jobs.CVSHealth.com/benefits We anticipate the application window for this opening will close on: 10/01/2024",
        "url": "https://www.linkedin.com/jobs/view/3962917183"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Goodlettsville, TN",
        "job_id": 3945570350,
        "company": "Dollar General",
        "title": "DATA ENGINEER",
        "created_on": 1720638462.2074163,
        "description": "At Dollar General, our mission is Serving Others! We value each and every one of our employees. Whether you are looking to launch a new career in one of our many convenient Store locations, Distribution Centers, Store Support Center or with our Private Fleet Team, we are proud to provide a wide range of career opportunities. We are not just a retail company; we are a company that values the unique strengths and perspectives that each individual brings. Your difference truly makes a difference at Dollar General. How would you like to Serve? Join the Dollar General Journey and see how your career can thrive. Company Overview Dollar General Corporation has been delivering value to shoppers for more than 80 years. Dollar General helps shoppers Save time. Save money. Every day.® by offering products that are frequently used and replenished, such as food, snacks, health and beauty aids, cleaning supplies, basic apparel, housewares and seasonal items at everyday low prices in convenient neighborhood locations. Learn more about Dollar General at www.dollargeneral.com/about-us.html . The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. This role will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. Job Details Duties & Responsibilities: What major responsibilities does this position have and what percentage of time is spent on completing them? (Typically 5 – 7) Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. Assemble large, complex data sets that meet functional / non-functional business requirements. Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and cloud technologies. Build analytics tools that utilize the data pipelines to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics. Work with stakeholders including the Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Knowledge, Skills and Abilities (KSAs): What KSAs are required to perform this job? Knowledge of programming languages (e.g. Java and Python) Hands-on experience with SQL database design Great numerical and analytical skills Degree in Computer Science, IT, or similar field; a Master’s is a plus Data engineering certification (e.g IBM Certified Data Engineer) is a plus Experience with big data tools: Hadoop, Spark, Kafka, etc. Experience with relational SQL and NoSQL databases, including Postgres and Cassandra. Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc. Experience with Snowflake/Azure cloud services: EC2, EMR, RDS, Redshift Experience with stream-processing systems: Storm, Spark-Streaming, etc. Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc Qualifications Work Experience &/or Education: What are the minimum education and/or experience requirements necessary to perform this job? Degree in information technology or computer science with additional vendor-specific certification. BS or MS degree in Computer Science or a related technical field 4+ years of Python or Java development experience 4+ years of SQL experience (No-SQL experience is a plus) 4+ years of experience with schema design and dimensional data modeling Ability in managing and communicating data warehouse plans to internal clients Experience designing, building, and maintaining data processing systems Experience working with a cloud platform such as Snowflake / Azure or Databricks",
        "url": "https://www.linkedin.com/jobs/view/3945570350"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Redmond, WA",
        "job_id": 3940378315,
        "company": "Microsoft",
        "title": "Data Engineer",
        "created_on": 1720638464.0346067,
        "description": "The Azure Core Organization is responsible for creating the foundation of Microsoft’s Cloud Platform for utility computing. This platform is one of the lowest levels of the services software/hardware stack and includes an efficient, virtualized computational substrate, a fully automated service management system, and a comprehensive set of highly scalable storage services. The Azure Compute Capacity and Efficiency (AC2E) team is the team in Azure Core tasked with managing all aspects of Compute capacity and efficiency management across the fleet. Capacity Management needs to ensure that on the one hand, there is sufficient capacity across all regions, allocation domains, and hardware infrastructure to meet all customer demand, while on the other hand ensuring that capacity is provisioned efficiently thereby avoiding overspending and COGS/CAPEX impact. At the scale of Azure’s business, managing this trade-off across the entire Azure Compute fleet is an enormously complex task, where improvements can make the difference between customer allocation failures on the one hand, and gargantuan savings on the other. As a Data Engineer in the team, you will work closely with our software engineers, program managers, and data scientists across different teams within Azure Core. You will also collaborate with a variety of internal partner teams across Azure and Microsoft. You will build reliable, secured, highly scalable, performant data pipelines to enhance the Azure Compute allocation, deliver capacity management and efficiency improvements. The value of your work will be reflected in improvements to the Azure platform, Azure service capacity fulfillment rate, customer satisfaction, and various efficiency metrics, including COGS reduction. You will have opportunities for mentorship, accelerate your career growth, and work on truly high-business impact areas. Microsoft’s mission is to empower every person and every organization on the planet to achieve more. As employees we come together with a growth mindset, innovate to empower others, and collaborate to realize our shared goals. Each day we build on our values of respect, integrity, and accountability to create a culture of inclusion where everyone can thrive at work and beyond. In alignment with our Microsoft values, we are committed to cultivating an inclusive work environment for all employees to positively impact our culture every day. Responsibilities Data Engineering, insights generation and analytics with deep domain knowledge understanding. With detailed instructions, you’ll implement code to extract raw data, validate its quality, and ensure the correct data is ingested within your area of work after cooking, data transformation. You will follow data modeling and handling procedures to maintain compliance with all applicable laws and policies across your assigned workstreams. Integrating analytics intelligence into the Azure platform Advocate for best practices in data engineering. Other Embody our Culture and Values Qualifications Required Qualifications: Bachelor's Degree in Computer Science, or related field OR equivalent experience. Other Requirements Ability to meet Microsoft, customer and/or government security screening requirements are required for this role. These requirements include, but are not limited to the following specialized security screenings: Microsoft Cloud Background Check: This position will be required to pass the Microsoft Cloud background check upon hire/transfer and every two years thereafter. Additional / Preferred Qualifications Bachelor's Degree in Computer Science, Math, Software Engineering, Computer Engineering , or related field AND 1+ year(s) experience in business analytics, data science, data modeling or data engineering work OR Master's Degree in Computer Science, Math, Software Engineering, Computer Engineering, or related field OR equivalent experience. Experience in business analytics, data science, software development, data modeling OR data engineering work. Data Engineering IC2 - The typical base pay range for this role across the U.S. is USD $76,400 - $151,800 per year. There is a different range applicable to specific work locations, within the San Francisco Bay area and New York City metropolitan area, and the base pay range for this role in those locations is USD $100,300 - $165,400 per year. Certain roles may be eligible for benefits and other compensation. Find additional benefits and pay information here: https://careers.microsoft.com/us/en/us-corporate-pay Microsoft will accept applications and processes offers for these roles on an ongoing basis. #Azurecorejobs Microsoft is an equal opportunity employer. Consistent with applicable law, all qualified applicants will receive consideration for employment without regard to age, ancestry, citizenship, color, family or medical care leave, gender identity or expression, genetic information, immigration status, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran or military status, race, ethnicity, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable local laws, regulations and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application process, read more about requesting accommodations.",
        "url": "https://www.linkedin.com/jobs/view/3940378315"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Philadelphia, PA",
        "job_id": 3953760111,
        "company": "Perpay Inc.",
        "title": "Junior Data Engineer",
        "created_on": 1720638465.7742543,
        "description": "About Us: We believe everyone deserves a chance to improve their financial future. We’re dedicated to building simple and inclusive financial products that help our members create healthy habits and achieve economic stability. Some things we’re excited about: $500 million in spending power used by our members Increasing members’ credit by 36 points (on average) within the first 3 months Backed by First Round Capital and JP Morgan Products we’ve built to make an impact: Perpay Marketplace: Combines interest-free payments and modern e-commerce to reduce cost of ownership and promote healthy repayment behavior. Perpay+: Leverages Marketplace repayment history to help members monitor and build credit with all 3 credit bureaus. Perpay Credit Card: Expands access to the flexibility and benefits of a World Mastercard by removing common barriers like high security deposits and low approval odds. The Perpay team is a motivated group of creative problem solvers who love getting things done and making an impact. Located in Center City, Philadelphia, our one-of-a-kind space promotes a collaborative work environment, unites our team, and feels like a home away from home. About the Role: As a Junior Data Engineer at Perpay, you will assist in building and maintaining our data pipelines and architectures, supporting our data products and insights. You will work alongside data scientists, analysts, and other team members to help drive Perpay’s mission of creating inclusive financial products that improve the lives of our members. With the launch of our credit card, the need for effective data engineering resources has increased to meet our growing modeling, reporting, and analytical requirements. You will gain exposure to a wide variety of projects across different business domains, ensuring our data infrastructure supports essential functions in risk, commerce, marketing, operations, and more. Your work will directly impact our customers by enabling automated and efficient data-driven services. We are looking for a Junior Data Engineer who is a quantitative, eager learner with a passion for data and a willingness to develop new skills. The ideal candidate has a foundational understanding of data engineering principles and is excited to grow their expertise in building and maintaining data pipelines, implementing ETL processes, and supporting data governance initiatives. You should be comfortable working in a fast-paced, entrepreneurial environment and handling multiple tasks with various stakeholders. Why You’ll Love It Here: Learning Opportunities: Gain hands-on experience and learn from experienced professionals in the field. Variety: Work on a diverse set of projects that will expose you to different areas of data engineering and business functions. Growth: Opportunities for career advancement and professional development. Collaborative Environment: Join a team that values collaboration and continuous improvement. Our greatest strength is our people and we’d love for you to be one of them! Responsibilities: Assist in the development and maintenance of ETL pipelines using tools like AWS Glue, Apache Airflow, and Fivetran Support data producers in understanding data sources and contribute to the design and implementation of data models using Redshift and Snowflake Implement basic data governance practices, including metadata management and data lineage tracking with tools such as Apache Atlas Collaborate with team members to develop scalable data solutions, ensuring data quality and reliability Help identify and resolve data-related issues, applying optimization techniques like indexing and partitioning Learn and contribute to the ongoing development of a modern data architecture, gaining exposure to advanced data engineering practices Stay current with industry trends and best practices, continuously developing technical skills in data engineering What You’ll Bring: Bachelor’s degree in a quantitative/technical field (Computer Science, Statistics, Engineering, Mathematics, Physics, Chemistry) 0-2 years of experience in data engineering or related fields, with a strong eagerness to learn and grow Basic proficiency in SQL and Python, with a willingness to learn cloud data platforms such as AWS, GCP, or Azure Familiarity with data warehouse solutions like Redshift or Snowflake and data orchestration tools like Apache Airflow is a plus Understanding of data modeling and ETL processes, with a keen interest in data governance and quality practices Strong problem-solving skills and the ability to work collaboratively in a team environment Excellent communication skills and a proactive approach to learning and development Hey, we know not everybody checks all the boxes, so if you’re interested, please apply because you could be just what we’re looking for! What We’ll Bring: Competitive salary + company equity 401k with company match Medical / Dental / Vision insurance Flexible Spending Account (FSA) Relocation assistance Pre-tax commuter benefit Student loan repayment match Gym subsidy with City Fitness Cell phone plan Paid parental leave Unlimited PTO Additional Perks: Opportunity to gain experience at one of the fastest-growing financial startups in the country Work on both e-commerce & fintech customer-facing products Collaborate cross-functionally with product, design, marketing, operations, data teams, and more This is not a remote opportunity; it is 100% onsite () () () Perpay is proud to be an equal opportunity employer. We value diversity in all its forms and are committed to creating an inclusive environment. We do not discriminate on the basis of race, religion, color, national origin, gender identity, sexual orientation, sex (including pregnancy), marital status, political affiliation, age, veteran status, disability status or other non-merit factor. Please contact us at careers@perpay.com to request accommodation.",
        "url": "https://www.linkedin.com/jobs/view/3953760111"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3948068356,
        "company": "Invitae",
        "title": "Senior Data Engineer, Data Solutions",
        "created_on": 1720638467.56768,
        "description": "Invitae is a leading medical genetics company trusted by millions of patients and their providers to deliver timely genetic information using digital technology. We aim to provide accurate and actionable answers to strengthen medical decision-making for individuals and their families. Invitae's genetics experts apply a rigorous approach to data and research, serving as the foundation of their mission to bring comprehensive genetic information into mainstream medicine to improve healthcare for billions of people. By joining Invitae, you’ll work alongside some of the world’s experts in genetics and healthcare at the forefront of genetic medicine. We’ve crafted a culture that empowers our teams and our teammates to have the biggest impact and to explore their interests and capabilities. We prize freedom with accountability and offer significant flexibility, along with excellent benefits and competitive compensation in a fast-growing organization! We are looking for a reliable and motivated Senior Data Engineer to join our Data Solutions Team in developing the data ingestion pipelines and data platform architecture that supports the analytical and reporting needs of data scientists, our bioinformatics team, customers, and internal stakeholders. What you’ll do: Support and collaborate with multiple teams to gather requirements, design software, and implement features that support multiple teams and use cases across Data Science, Bioinformatics, and Finance. Design and implement reliable, scalable and efficient data framework, data driven products and software solutions for external and internal customers. Create a secure, flexible and powerful world-class Health Data Platform for medical research and treatment. Enhance existing systems to automate and use latest technologies and tools. Ability and passion for data to become the Subject Matter Expert working with users on databases, tables, schemas and meta-data. Follow and contribute to agile best practices within the organization, looking for ways to streamline, automate and reduce redundancy and costs. Support and respond to teammate and user questions in a fast-paced, collaborative environment in a timely manner. What you bring: Minimum of 8 years of related experience with a Bachelor’s degree, 2 years and a Master’s degree or beyond. Skilled in one or more high-level languages (Scala preferred and/or Python). Willingness to learn new languages and technologies. Proficient in AWS, Azure, or Google Cloud Platform (AWS preferred). Experience with Relational and Columnar databases. Experience with one or more containerization tools, especially Docker and Kubernetes. Experience with messaging/queuing or stream processing systems (Kafka strongly preferred). Experience (or aptitude and interest) in contributing to and maintaining DevOps/Cloud Infrastructure. Preferred Skills: Hands-on experience with troubleshooting, debugging, log collection, and alerting systems. Hands-on experience with Snowflake. Hands-on experience working with large datasets, ETL pipelines, and modern warehouse technologies. Hands-on functional programming in Scala or other language. Hands-on parallel programming in Spark or other platforms. Experience with maintaining and administering Kubernetes clusters. Experience with build automation and CI/CD pipelines (e.g. GitHub Actions). Experience with one or more data visualization tools (Looker preferred). Nice to have: Demonstrated experience with data modeling/dimensional modeling. Demonstrated experience with database performance tuning. Familiarity with data lineage/data governance. Demonstrated understanding of security principals including OAuth, Role-Based Access Control and encryption. Experience with Snowflake Security and Data Governance. This salary range is an estimate, and the actual salary may vary based on a wide range of factors, including your skills, qualifications, experience and location. This position is eligible for benefits including but not limited to medical, dental, vision, life insurance, disability coverage, flexible paid time off, Spring Health, Carrot Fertility, participation in a 401k with company match, ESPP, and many other additional voluntary benefits. Invitae also offers generous paid leave programs so you can spend time with your new child, recover from your own illness or care for a sick family member. USA National Pay Range $138,400—$173,000 USD Please apply even if you don’t meet all of the “What you bring” requirements noted. It’s rare that someone checks every single item, it’s ok, we encourage you to apply anyways. Join us! At Invitae, we value diversity and provide equal employment opportunities (EEO) to all employees and applicants without regard to race, color, religion, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. We will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of the San Francisco Fair Chance Ordinance. We truly believe a diverse workplace is crucial to our company's success and to better serve our diverse patients. Your input is especially valuable. We’d greatly appreciate it if you can take a quick moment to make your selection(s) below. Submissions will be anonymous. You can find a detailed explanation of our privacy practices here.",
        "url": "https://www.linkedin.com/jobs/view/3948068356"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Atlanta, GA",
        "job_id": 3962080841,
        "company": "Intuit",
        "title": "Data Engineer 2 (Mailchimp)",
        "created_on": 1720638469.2196608,
        "description": "Overview As a part of the Observability Platforms team, you’ll join us in maintaining, scaling, optimizing, and migrating our Observability and data pipeline infrastructure. It consists of Kafka and PubSub, which pushes up to 10 billion events per day, and Splunk, Elasticsearch and OpenSearch clusters that ingest 1 trillion documents. We provide our internal and external customers with highly available, approachable data pipelines and discovery tools such as Wavefront, Splunk, and Grafana. Together with the rest of the Observability Platforms, we enable Mailchimp’s understanding of business-critical processes and support the tools and features that Mailchimp’s customers use to find and fine-tune their audiences. What you'll bring Some experience writing APIs, tools, and scripts (Python, Golang, Bash or PHP) Some experience with cloud technologies (AWS, Google Cloud) Familiarity with other pieces of our technical stack (Kafka, PubSub, Kubernetes, Wavefront, Splunk, Prometheus, and PagerDuty) Experience with git or other version control tools Ability to utilize automation and orchestration tools to build large scalable systems Experience with Splunk, Wavefront, Grafana, and creating Dashboards and visualizations How you will lead Collaborate across teams to support the organization's data pipeline priorities Automate processes, create tools, and build features that will improve everyone's experience with our data pipeline infrastructure Guide engineers on how they can best utilize the data streaming platform to gain confidence that their services are performing as expected Develop, communicate, and enforce standards Participate in an on-call schedule, and assist in mitigating issues that may arise Troubleshoot and maintain existing infrastructure to ensure the smooth operation of data pipelines, observability tools and cloud platform Contribute to technical documentation of our infrastructure and systems Track work in assigned tickets and contribute to project planning",
        "url": "https://www.linkedin.com/jobs/view/3962080841"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Dallas, TX",
        "job_id": 3969316291,
        "company": "CBC",
        "title": "Data Engineer",
        "created_on": 1720638470.984952,
        "description": "POSITION Data Engineer LOCATION Remote- must be local to DFW, TX DURATION 6 Month CTH INTERVIEW TYPE Video VISA RESTRICTIONS Only GC or Citizens Required Skills Must be proficient in Python Must be able to integrate data APIs - need to know how to work with APIs in general. 5+ years of experience in data engineering with a proven track record in the Azure ecosystem. A strong focus on building and maintaining data pipelines and infrastructure. Strong proficiency in Azure Synapse Analytics, Azure DevOps, and Apache Spark pools. Senior level knowledge of data modeling, processes, and data warehouse concepts Cloud BC Labs Inc is a digital transformation organization aimed at creating seamless solutions for clients to effectively manage their business operations. The company specializes in Business and Management Consulting, AI/ML, Data Analytics & Visualization, Cloud Data Warehouse Migration, Snowflake Implementation, Informatica Implementation & Upgrade, Staffing Services and Data Management Solutions",
        "url": "https://www.linkedin.com/jobs/view/3969316291"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Cincinnati, OH",
        "job_id": 3970271433,
        "company": "Fifth Third Bank",
        "title": "Data Engineer I",
        "created_on": 1720638474.6905801,
        "description": "Make banking a Fifth Third better® We connect great people to great opportunities. Are you ready to take the next step? Discover a career in banking at Fifth Third Bank. General Function The data engineer designs and builds platforms, tools, and solutions that help the bank manage, secure, and generate value from its data. The person in this role creates scalable and reusable solutions for gathering, collecting, storing, processing, and serving data on both small and very large (i.e. Big Data) scales. These solutions can include on-premise and cloud-based data platforms, and solutions in any of the following domains ETL, business intelligence, analytics, persistence (relational, NoSQL, data lakes), search, messaging, data warehousing, stream processing, and machine learning. Responsible and accountable for risk by openly exchanging ideas and opinions, elevating concerns, and personally following policies and procedures as defined. Accountable for always doing the right thing for customers and colleagues, and ensures that actions and behaviors drive a positive customer experience. While operating within the Bank's risk appetite, achieves results by consistently identifying, assessing, managing, monitoring, and reporting risks of all types. Essential Duties And Responsibilities Responsible for design, Development, and Support of data solutions, APIs, tools, and processes to enable rapid delivery of business capabilities. Work closely with IT application teams, Enterprise architecture, infrastructure, information security, and LOB stakeholders to translate business and technical strategies into data-driven solutions for the Bank. Act as a technical Expert addressing problems related to system and application design, performance, integration, security, etc. Conduct research and Development based on current trends and technologies related to the banking industry, data engineering and architecture, data security, and related topics. Work with developers to Build CI/CD pipelines, Self-service Build tools, and automated deployment processes. Evaluate software products and Provide documented recommendations as needed. Provide Support and troubleshooting for data platforms. Must be willing to Provide escalated on-call Support for complicated and/or critical incidents. Participate in the planning process for hardware and software. Plan and work on internal projects as needed, including legacy system replacement, Monitoring and analytics improvements, tool Development, and technical documentation. Provide technical guidance and mentoring for other team members. Manage and prioritize multiple assignments. SUPERVISORY RESPONSIBLITIES: None Minimum Knowledge, Skills, And Abilities Required Bachelor's degree in Computer Science/Information Systems or equivalent combination of education and experience. Must be able to communicate ideas both verbally and in writing to management, business and IT sponsors, and technical resources in language that is appropriate for each group. Fundamental understanding of distributed computing principles Knowledge of application and data security concepts, best practices, and common vulnerabilities. Conceptual understanding of one or more of the following disciplines preferred big data technologies and distributions, metadata management products, commercial ETL tools, Bi and reporting tools, messaging systems, data warehousing, Java (language and run time environment), major version control systems, continuous integration/delivery tools, infrastructure automation and virtualization tools, major cloud, or rest API design and development. Data Engineer I LOCATION -- Cincinnati, Ohio 45202 Fifth Third Bank, National Association is proud to have an engaged and inclusive culture and to promote and ensure equal employment opportunity in all employment decisions regardless of race, color, gender, national origin, religion, age, disability, sexual orientation, gender identity, military status, veteran status or any other legally protected status.",
        "url": "https://www.linkedin.com/jobs/view/3970271433"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Phoenix, AZ",
        "job_id": 3967438798,
        "company": "CBC",
        "title": "Data Engineer",
        "created_on": 1720638476.8648968,
        "description": "POSITION Data Engineer LOCATION Hybrid- Phoenix, AZ DURATION CTH 9 Month conversion INTERVIEW TYPE Video VISA RESTRICTIONS Only GC or Citizens Required Skills Needs STRONG GCP experience Proficiency in SQL and experience with relational databases (e.g., MySQL, PostgreSQL). Proven experience as a Data Engineer or in a similar role. Bachelor's or Master's degree in Computer Science, Information Technology, Engineering, or a related field. Proficiency in programming languages such as Kotlin and Java. Familiarity with front-end technologies, particularly React.js, for data visualization and UI integration. Experience with Google Cloud Platform and its data-related services (e.g., BigQuery, Dataflow, Pub/Sub). Strong understanding of data warehousing concepts and experience with data warehouse solutions (e.g., BigQuery). Cloud BC Labs Inc is a digital transformation organization aimed at creating seamless solutions for clients to effectively manage their business operations. The company specializes in Business and Management Consulting, AI/ML, Data Analytics & Visualization, Cloud Data Warehouse Migration, Snowflake Implementation, Informatica Implementation & Upgrade, Staffing Services and Data Management Solutions",
        "url": "https://www.linkedin.com/jobs/view/3967438798"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "New Jersey, United States",
        "job_id": 3971250050,
        "company": "Hexaware Technologies",
        "title": "Data Engineer",
        "created_on": 1720638480.8077562,
        "description": "What Working at Hexaware offers: Hexaware is a dynamic and innovative IT organization committed to delivering cutting-edge solutions to our clients worldwide. We pride ourselves on fostering a collaborative and inclusive work environment where every team member is valued and empowered to succeed. Hexaware provides access to a vast array of tools that enhance, revolutionize, and advance professional profile. We complete the circle with excellent growth opportunities, chances to collaborate with highly visible customers, chances to work alongside bright brains, and the perfect work-life balance. With an ever-expanding portfolio of capabilities, we delve deep into and identify the source of our motivation. Although technology is at the core of our solutions, it is still the people and their passion that fuel Hexaware’s commitment towards creating smiles. “At Hexaware we encourage to challenge oneself to achieve full potential and propel growth. We trust and empower to disrupt the status quo and innovate for a better future. We encourage an open and inspiring culture that fosters learning and brings talented, passionate, and caring people together.” We are always interested in, and want to support, the professional and personal you. We offer a wide array of programs to help expand skills and supercharge careers. We help discover passion—the driving force that makes one smile and innovate, create, and make a difference every day. The Hexaware Advantage: Your Workplace Benefits Excellent Health benefits with low-cost employee premium. Wide range of voluntary benefits such as Legal, Identity theft and Critical Care Coverage Unlimited training and upskilling opportunities through Udemy and Hexavarsity. Role: Data Engineer with Databricks Hybrid FTE Sal: $130k Responsible for designing, developing, and maintaining data processing pipelines using Databricks platform. Working closely with data engineers and data scientists to implement data solutions that meet business requirements, to help client with their cloud migration journey. It includes the following responsibilities: Designing and developing data pipelines using Databricks platform. Writing efficient and optimized code in languages such as Python, Scala, or SQL. Collaborating with data engineers to ensure data quality and integrity. Implementing data transformations and aggregations to support analytics and reporting. Working with data scientists to deploy machine learning models on Databricks. Troubleshooting and resolving issues related to data pipelines and Databricks environment. Optimizing performance and scalability of Databricks jobs. Documenting technical specifications and maintaining code repositories. Keeping up-to-date with the latest Databricks features and best practices. Participating in code reviews and providing feedback to improve code quality. He/she should have a strong understanding of distributed computing concepts and experience with cloud platforms such as Azure. They should also possess good problem-solving skills and be able to work in a collaborative team environment.",
        "url": "https://www.linkedin.com/jobs/view/3971250050"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3944312077,
        "company": "Confluent",
        "title": "Data Engineer",
        "created_on": 1720638482.9211712,
        "description": "With Confluent, organizations can harness the full power of continuously flowing data to innovate and win in the modern digital world. We have a purpose that drives us to do better every day – we're creating an entirely new category within data infrastructure - data streaming. This technology will allow every organization to create experiences and use the power of data in ways that profoundly impact the way we all live. This impact is our purpose and drives us to do better every day. One Confluent. One team. One Data Streaming Platform. Data Connects Us. About The Role The mission of the Data Science/Data Engineering team at Confluent is to serve as the central nervous system of all things data for the company: we build data and analytics infrastructure, insights, models and tools, to empower data-driven thinking, and optimize every part of the business. This position offers limitless opportunities for an ambitious data engineer to make an immediate and meaningful impact within a hyper growth start-up, and contribute to a highly engaged open source community. This is a partnership-heavy role. As a member of the Data team, you will enable various functions of the company, i.e. Product, Engineering, Go-to-Market, etc.,, to be data-driven. As a Data Engineer, you will take on big data challenges in an agile way. You will build data pipelines that enable data scientists, operation teams, and executives to make data accessible to the entire company. You will also build data models to deliver insightful analytics while ensuring the highest standard in data integrity. You are encouraged to think out of the box and play with the latest technologies while exploring their limits. Successful candidates will have strong technical capabilities, a can-do attitude, and are highly collaborative. What You Will Do Design, build and launch extremely efficient and reliable data pipelines to move data across a number of platforms including Data Warehouse and real-time systems. Developing strong subject matter expertise and managing the SLAs for those data pipelines. Set up and improve BI tooling and platforms to help the team create dynamic tools and reporting. Partnering with Data Scientists and business partners to develop internal data products to improve operational efficiencies organizationally. What You Will Bring 3+ years of experience in a Data Engineering role, with a focus on data warehouse technologies, data pipelines and BI tooling. Bachelor or advanced degree in Computer Science, Mathematics, Statistics, Engineering, or related technical discipline Knowledge of SQL and of relational & cloud database systems and concepts. Strong knowledge of data architectures and data modeling and data infrastructure ecosystem. Experience with enterprise business systems such as Salesforce, Marketo, Zendesk, Clari, Anaplan, etc. Experience with ETL pipeline tools like Airflow, DBT, and with code version control systems like Git. The ability to communicate cross-functionally, derive requirements and architect shared datasets; ability to synthesize, simplify and explain complex problems to different types of audiences, including executives. The ability to thrive in a dynamic environment. That means being flexible and willing to jump in and do whatever it takes to be successful. What Gives You An Edge Experience with Apache Kafka Knowledge of batch and streaming data architectures Product mindset to understand business needs, and come up with scalable engineering solutions Come As You Are At Confluent, equality is a core tenet of our culture. We are committed to building an inclusive global team that represents a variety of backgrounds, perspectives, beliefs, and experiences. The more diverse we are, the richer our community and the broader our impact. Employment decisions are made on the basis of job-related criteria without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status, or any other classification protected by applicable law. At Confluent, we are committed to providing competitive pay and benefits that are in line with industry standards. We analyze and carefully consider several factors when determining compensation, including work history, education, professional experience, and location. This position has an annual estimated salary of $132,300 - $155,430, an annual bonus, and a competitive equity package. The actual pay may vary depending on your skills, qualifications, experience, and work location. In addition, Confluent offers a wide range of employee benefits. To learn more about our benefits click HERE . Click HERE to review our Candidate Privacy Notice which describes how and when Confluent, Inc., and its group companies, collects, uses, and shares certain personal information of California job applicants and prospective employees.",
        "url": "https://www.linkedin.com/jobs/view/3944312077"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3960708042,
        "company": "Paradigm",
        "title": "Data Engineer",
        "created_on": 1720638484.636521,
        "description": "Paradigm is rebuilding the clinical research ecosystem by creating a platform that enables equitable access to trials for all patients while enhancing trial efficiency and reducing the barriers to participation for healthcare providers. Incubated by ARCH Venture Partners and backed by leading healthcare and life sciences investors, Paradigm aims to break down barriers across the trial ecosystem through one seamless infrastructure implemented at healthcare provider organizations, bringing potentially life-saving therapies to patients faster. Our team is diverse in its experience and committed to the company’s mission to create equitable access to clinical trials for any patient, anywhere. Join us, and bring your expertise, passion, creativity, and drive as we work together to realize this mission. The Data Engineering team at Paradigm is dedicated to constructing the infrastructure for clinical care data. We ingest data from multiple data sources: Electronic Health Records, Lab providers, Health Information Exchanges (and more). We enrich, harmonize, and make it available for multiple consumers and product verticals. As our population, research, and health system footprint grows we will scale our architecture to support parallel data processing. Our efforts support ML & Data Science teams in optimizing and developing new classification models. We collaborate with Data Scientists and Analysts to enhance patient-matching heuristics, address trial & research site feasibility queries, and unearth insights for clinical trial opportunities. Additionally, we engage with application consumer teams to optimize data for Clinical Trial Matching workflows and Study Conduct processes. As an Engineer IC on the Data team, you will work closely with product and data consumers to enhance the breadth, quality, and completeness of our data. You and your team will take ownership of critical system components, focusing on designing them for performance, correctness, and reliability. Your responsibilities will span integrating with data providers, managing data lineage, developing data derivation algorithms, implementing parallel processing, establishing automated monitoring and data quality procedures, and refining data warehouse architecture and data modeling. What you’ll do: Integrate data feeds from various healthcare data providers Collaborate closely with product managers, data scientists, analysts, and engineering colleagues to translate complex business problems into efficient technical solutions Develop and maintain robust batch and streaming pipelines Create tools and processes to facilitate data accessibility across products and data consumers Enhance the maturity of monitoring systems and processes to improve visibility and failure detection within our infrastructure Cultivate expertise on various datasets and tooling within the organization Work alongside privacy & security teams to ensure responsible data governance What you’ll bring: 3-5 years of experience in data and/or software engineering Demonstrated ability to drive technical projects, prioritize work, identify dependencies, facilitate technical decisions, and engage in cross-functional team discussions Proficiency in SQL and data analysis Experience in system design and data workflow optimization Proficient coding skills in Python, Java, Kotlin, Scala or other relevant languages Experience with ETL orchestration tools like Apache Airflow and Dagster Knowledge of data transformation tools like DBT or dataform Experience with data warehouse DBs (such as BigQuery, Redshift, Snowflake, etc…) Experience in designing and implementing scalable, available, maintainable, and performant systems Nice To Have, But Not Required Experience in the healthcare data domain Experience with Epic Systems At Paradigm, we are committed to providing equal employment opportunities to all qualified individuals. We believe in creating a diverse and inclusive workplace that values the contributions of every employee, regardless of their race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, disability, veteran status, or any other characteristic protected by law. We are an equal-opportunity employer and do not discriminate against any employee or applicant for employment based on any of the aforementioned factors. We encourage and welcome candidates from diverse backgrounds and perspectives to apply for our open positions. We strive to provide reasonable accommodations to qualified individuals with disabilities and to ensure that all employment decisions are based on job-related factors such as skills, experience, and qualifications.",
        "url": "https://www.linkedin.com/jobs/view/3960708042"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Scottsdale, AZ",
        "job_id": 3946665341,
        "company": "Persistent Systems",
        "title": "Data Engineer",
        "created_on": 1720638486.3530738,
        "description": "About Persistent We are a trusted Digital Engineering and Enterprise Modernization partner, combining deep technical expertise and industry experience to help our clients anticipate what’s next. Our offerings and proven solutions create a unique competitive advantage for our clients by giving them the power to see beyond and rise above. We work with many industry-leading organizations across the world including 14 of the 30 most innovative US companies, 80% of the largest banks in the US and India, and numerous innovators across the healthcare ecosystem. Our disruptor’s mindset, commitment to client success, and agility to thrive in the dynamic environment have enabled us to sustain our growth momentum by reporting $282.9M revenue in Q1FY24, delivering 17.1% Y-o-Y growth. In addition, our total employee count reached 23,130 people this quarter, located in 21 countries across the globe. We’re also pleased to share that Persistent has been named the fastest-growing Indian IT Services brand by Brand Finance. Acknowledging our vertical industry expertise, we were placed as a Leader in Everest Group’s Payments IT Services PEAK Matrix® Assessment 2023. We were also recognized as a Leader in the ISG Provider Lens™ Digital Engineering Services Quadrants U.S. 2023 and the Salesforce Ecosystem Partners 2023 ISG Provider Lens™ Study. Throughout this market-leading growth, we’ve maintained strong employee satisfaction - over 94% of our employees approve of the CEO, and 89% would recommend working at Persistent to a friend. About Position: Role: Data Engineer (Data Masking) Location: Scottsdale AZ (Day 1 onsite) Experience: 8+ What You'll Do: Must have experience hands-on experience in Scala , Python, Spark, Must have experience in Aerospike or Solr. Highly analytical and data oriented. Experience in SQL, NOSQL Database Data masking of on prem PII data. Should be a having good hands-on working experience in Python and Scala Should have deep understanding of big data systems. Ansible knowledge is good to have You have experience with development tools and agile methodologies Expertise You'll Bring: Must have experience hands-on experience in Scala , Python, Spark, Must have experience in Aerospike or Solr. Highly analytical and data oriented. Experience in SQL, NOSQL Database Data masking of on prem PII data. Should be a having good hands-on working experience in Python and Scala Should have deep understanding of big data systems. Ansible knowledge is good to have You have experience with development tools and agile methodologies Benefits: Competitive salary and benefits package Culture focused on talent development with quarterly promotion cycles and company-sponsored higher education and certifications. Opportunity to work with cutting-edge technologies. Employee engagement initiatives such as project parties, flexible work hours, and Long Service awards Annual health check-ups Insurance coverage: group term life, personal accident, and Mediclaim hospitalization for self, spouse, two children, and parents Our company fosters a values-driven and people-centric work environment that enables our employees to: · Accelerate growth, both professionally and personally · Impact the world in powerful, positive ways, using the latest technologies · Enjoy collaborative innovation, with diversity and work-life wellbeing at the core · Unlock global opportunities to work and learn with the industry’s best Let’s unleash your full potential at Persistent - persistent.com/careers",
        "url": "https://www.linkedin.com/jobs/view/3946665341"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Texas, United States",
        "job_id": 3951757784,
        "company": "Tapcheck",
        "title": "Data Intelligence Engineer",
        "created_on": 1720638488.2637274,
        "description": "Tapcheck is looking for a Data Intelligence Engineer to join our growing Data & Analytics team. In this role, you will work throughout the entire data lifecycle, from requirements gathering with business stakeholders, to pipeline design, development, and deployment, to BI modeling and report development. You will play a crucial role in shaping the future of Tapcheck by enabling data-driven decision-making across the organization. What You'll Do: Collaborate with business stakeholders to gather and translate their requirements into actionable data solutions Design, develop, and maintain data pipelines using Azure Data Factory, with SQL queries as the primary transformation steps Implement data quality checks to ensure data accuracy and consistency Utilize Omni Analytics, our BI tool, to build insightful reports and dashboards Perform data analysis and reconciliation, supporting business users in their reporting needs Contribute to the modern data stack and drive the migration towards a formal data warehouse What You'll Bring: 3+ years of experience as a BI / data engineer, preferably in a smaller company or start-up environment Strong attention to detail and ability to work with complex data sets Proficiency in SQL, including the mastery of subqueries, CTEs, and window functions Experience with data pipeline and ETL tools (Azure Data Factory, DBT, Fivetran) Familiarity with reporting and visualization tools such as Looker and Omni Analytics Knowledge of modern data warehousing concepts and dimensional modeling Excellent communication and interpersonal skills This is a remote-friendly role. Ideally, candidates will sit in the following states: AL, AZ, CA, CO, DC, DE, FL, GA, ID, IL, LA, MA, MI, MO, NC, NH, NJ, NV, NY, PA, OR, OH, RI, SC, TX, VA, WA, WI About Tapcheck: Tapcheck is a digital platform offering an easy and convenient way to access on-demand earnings early. Available at no cost to employers, our app-based on-demand pay solution helps relieve the financial stress that many employees experience on a daily basis. The Tapcheck team is passionate about our mission to improve financial wellness and boost business productivity. By giving workers the ability to transfer wages they've earned directly to their bank account or pay card without waiting for payday, Tapcheck eliminates the need for high-interest payday loans or employer-funded cash advances. How We Get Things Done: Our core values act as a steadfast guide, directing our decisions and anchoring our actions. We consider these values non-negotiable, especially when it comes to our hiring process. Humility: We believe in the power of humility. We value team players who are down-to-earth, respectful, and open to learning from others. Our employees approach challenges with a positive attitude, acknowledging their strengths and weaknesses while celebrating the achievements of their colleagues Grit: We admire individuals with grit - those who demonstrate unwavering determination and resilience in the face of obstacles. At Tapcheck, we take pride in overcoming challenges together, pushing the boundaries of what is possible, and embracing failure as an opportunity for growth Raising the Bar: Continuous improvement is at the heart of our culture. We are committed to setting high standards and pushing ourselves to exceed them. We seek employees who are innovative and strive for excellence, constantly seeking ways to enhance our products, services, and processes Striving for Growth: We foster an environment that encourages personal and professional development. Our employees are driven to learn, grow, and adapt to new circumstances. We support individuals who take initiative, seek out new challenges, and actively contribute to their own growth and the growth of the company Why Join Tapcheck? Competitive base Paid Time Off Health Insurance Dental Insurance Vision Insurance 401K Match Compensation: $105,000 - $115,000. The actual base salary will depend on numerous factors such as: location, experience, training, knowledge. and skills. Tapcheck reserves the right to amend, change, alter, and revise pay ranges and benefits offerings at any time. All applicants acknowledge that by applying to this position you understand that this specific pay range is contingent upon meeting the qualifications and requirements of the role, and for the successful completion of the interview selection and process. It is at the Company's discretion to determine what pay is provided to a candidate within the range associated with the role. Equal Employment Opportunity Policy Tapcheck, Inc. provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.",
        "url": "https://www.linkedin.com/jobs/view/3951757784"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Corvallis, OR",
        "job_id": 3969220318,
        "company": "Conch Technologies, Inc",
        "title": "Data Engineer",
        "created_on": 1720638489.7709343,
        "description": "Hi, Fulltime job Data Engineer – more than 12 Years Exp Location: Corvallis, OR – 5 Days Onsite Role Full time Job Description At least 12+ years’ experience in data engineering, AI and ML and Aws/Azure Architect, Design and implement distributed data processing pipelines using Spark, Python, SQL and other tools and languages prevalent in the Big Data/Lakehouse ecosystem. Experience in AI and ML on AWS/AZURE Analyzes design and determines coding, programming, and integration activities required based on general objectives. Play the technical lead role representing deliverables from vendor team resources at onsite and offshore locations. Lead the technical co-ordination and Business Knowledge transition activities to offshore team. Reviews and evaluates designs and project activities for compliance with architecture, security and quality guidelines and standards. Writes and executes complete testing plans, protocols, and documentation for assigned portion of data system or component; identifies defects and creates solutions for issues with code and integration into data system architecture. Collaborates and communicates with project team regarding project progress and issue resolution. Works with the data engineering team for all phases of larger and more-complex development projects and engages with external users on business and technical requirements. Collaborates with peers, engineers, data scientists and project team. What You Bring Bachelor's or Master's degree in Computer Science, Information Systems, Engineering or equivalent. 6+ years of relevant experience with detailed knowledge of data technical architectures, infrastructure components, ETL/ ELT and reporting/analytic tools. 2+ years of experience with Cloud based DW such as Redshift, Snowflake etc. 1+ years’ experience in Big Data Distributed ecosystems (Hadoop, SPARK, Unity Catalog & Delta Lake) 1+ years’ experience in Big Data Distributed systems such as Databricks, AWS EMR, AWS Glue etc. Leverage monitoring tools/frameworks, like Splunk, Grafana, CloudWatch etc. Experience with container management frameworks such as Docker, Kubernetes, ECR etc. Experience working on CI/CD processes such as Jenkins, Codeway etc. and source control tools such as GitHub, etc. Strong experience in coding languages like Python, Scala & Java Thank you & Regards V S Durga Prasad | I T Recruiter E: vprasad@conchtech.com | T: 901-466-4708 | 615-922-1491 Conch Technologies | www.conchtech.com",
        "url": "https://www.linkedin.com/jobs/view/3969220318"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Phoenix, AZ",
        "job_id": 3947703790,
        "company": "PrePass",
        "title": "Data Engineer",
        "created_on": 1720638491.4650664,
        "description": "About PrePass PrePass® is North America's most utilized and technologically advanced weigh station bypass and toll payment platform. Proven PrePass technologies enable safe, qualified motor carriers to bypass inspection facilities at highway speeds, saving them time, fuel, and money while reducing emissions. As the only provider to offer bypass and tolling solutions, PrePass technology allows fleets to regain control of toll costs, eliminate toll violations, and automatically resolve max toll disputes. PrePass is the only preclearance system developed, owned, and operated in the United States of America as well as the American Trucking Associations' only Endorsed Corporate Partner. That's why more than 105,000 fleets subscribe over 750,000 commercial vehicles to PrePass services. Position Description Embark on a transformative career journey as a Data Engineer in the thriving transportation sector. This role offers a unique chance to immerse yourself in the world of cutting-edge cloud solutions, pivotal for data processing and analytics. We are looking for a visionary developer with a solid foundation in programming back-end data systems and a zest for mastering the intricacies of modern data ecosystems through state-of-the-art technology. If you're a forward-thinker, eager to explore and shape the future of data engineering, we invite you to join us on this remarkable endeavor. This is a hybrid position based in Phoenix. Your Key Responsibilities Develop and manage cloud-native data pipelines using Databricks and Microsoft Azure services Contribute to developing incremental and real-time data ingestion to data lake storage with a medallion architecture Formulate complex queries for transforming and collating data across multiple sources Develop features with scalability, maintainability, and testability in mind Troubleshoot data integration and data quality issues, diagnose failures, and implement optimal solutions Participate in proof-of-concept exercises and prototyping. Provide analysis and recommendations based on your findings Requirements Qualifications Required Bachelor's degree in computer science or equivalent experience Minimum 5 years of experience in designing and building data solutions Advanced SQL skills Coding experience using Python for the purpose of data analytics Experience creating and maintaining ETL pipelines and automation Understanding of Data Lake architecture and design principles Exceptional collaborative abilities, with a talent for navigating fluid environments and embracing change A commitment to staying abreast of emerging technologies and industry best-practices to drive continuous improvement in development methodologies Preferred Proficiency in Azure Cloud and ETL cloud tooling Proficiency with reporting and data visualization tools, specifically PowerBI Experience coding with an OOP language, such as .Net Knowledge of event driven architecture - queues, batches, and pub/sub models Familiarity with DevOps CI/CD implementation methodologies Familiarity with No-SQL databases Benefits How We Will Take Care of You Robust benefit package that includes medical, dental, and vision that start on date of hire Paid Time Off, to include vacation, sick, holidays, and floating holidays 401(k) plan with employer match Company-funded \"lifestyle account\" upon date of hire for you to apply toward your physical and mental well-being (i.e., ski passes, retreats, gym memberships) Tuition Reimbursement Program Employee Assistance Program (available at no cost to you) Voluntary benefits, to include but not limited to Legal and Pet Discounts Company-sponsored and funded \"Culture Team\" that focuses on the Physical, Mental, Professional well-being of employees Community Give-Back initiatives Culture that focuses on Employee Development initiatives",
        "url": "https://www.linkedin.com/jobs/view/3947703790"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Wichita, KS",
        "job_id": 3970928181,
        "company": "Patterned Learning Career",
        "title": "Junior Data Engineer",
        "created_on": 1720638495.0717254,
        "description": "This is a remote position. Junior Data Engineer - Remote Job, 1+ Year Experience Annual Income: $58K - $76K A valid work permit is necessary in the US About us: Patterned Learning is a platform that aims to help developers code faster and more efficiently. It offers features such as collaborative coding, real-time multiplayer editing, and the ability to build, test, and deploy directly from the browser. The platform also provides tightly integrated code generation, editing, and output capabilities. Are you a passionate and detail-oriented individual with a knack for problem-solving? Do you thrive in a fast-paced, collaborative environment? If so, then this Junior Data Engineer role at CVS Health could be your perfect match! In this exciting role, you'll play a key role in building and scaling our cutting-edge AIOps platform. You'll work alongside a talented team to develop machine learning and AI solutions that will revolutionize CVS Health's IT operations. Here's what you'll do: Design, implement, and manage data pipelines to extract, transform, and load data for analysis and insights. Develop and automate data cleaning, transformation, and preparation processes to ensure data quality and consistency. Integrate data from various sources to create a unified view of our IT infrastructure and applications. Leverage big data technologies like Kafka to handle large data volumes efficiently. Implement data security measures to safeguard sensitive information. Create and maintain documentation for data processes, data flows, and system configurations. Continuously monitor and optimize data pipelines and systems for performance, scalability, and cost-effectiveness. To be successful, you'll need: 2+ years of programming experience in Python, Java, and SQL. 2+ years of experience with ETL tools and database management (relational and non-relational). 2+ years of experience with data modeling techniques and tools for designing efficient data structures. Strong skills in data quality assessment, cleansing, and validation. Excellent communication, collaboration, and problem-solving skills. A meticulous attention to detail and a strong work ethic. Bonus points if you have: Knowledge of big data technologies and cloud platforms (e.g., Azure Synapse). Experience with PySpark and Data-bricks. We offer: The opportunity to work on a cutting-edge AIOps platform that's transforming healthcare. A collaborative and supportive work environment where you can learn and grow. A chance to make a real impact on the success of CVS Health. Please note: While this role is fast-paced, we value work-life balance and offer a comprehensive benefits package. Why Patterned Learning LLC? Patterned Learning can provide intelligent suggestions, automate repetitive tasks, and assist developers in writing code more effectively. This can help reduce coding errors, improve productivity, and accelerate the development process. Pattern recognition is particularly relevant in the context of coding. Neural networks, especially deep learning models, are commonly employed for pattern detection and classification tasks. These models simulate human decision-making and can identify patterns in data, making them well-suited for tasks like code analysis and generation.",
        "url": "https://www.linkedin.com/jobs/view/3970928181"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3948553989,
        "company": "WinMax",
        "title": "Data Engineer",
        "created_on": 1720638496.7578177,
        "description": "Title: Data Engineer Duration: 6+ months Location: remote Job Description The Main function of a Data Engineer is to collect data requirements, costs, access, usage, and availability for business scenarios. They design a data model, which translates into design specifications to model the flow of data across various pipelines. Leads conversations with data scientists, specialists, and makes improvements to data models and schemas to connect, ingest, and enable analytical requirements. This person will be working with the Hololens Hardware – Design Validation Engineering team, which develops test solutions (hardware, software, and data infrastructure) to improve the design of the product. They will be working with multiple disparate datasets, coming from different teams and locations, and will be standing up secure services to make a common data view available to the program. Specific skills: Microsoft Synapse, SQL, Python, DAX, PowerBI. Experience with Microsoft’s portfolio of services, data infrastructure utilizing cloud services. Experience with data interpretation layers: Power BI content, R (programming language for statistical analysis), JMP, Kusto, manipulating data, data deployments Microsoft synapse or Microsoft SQL Azure based tools Candidates submitted to this role must be US Citizens because one of the projects they’ll be supporting utilizes trade restricted data for the US Army.",
        "url": "https://www.linkedin.com/jobs/view/3948553989"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Washington DC-Baltimore Area",
        "job_id": 3963388260,
        "company": "Tech Consulting",
        "title": "Junior Data Engineer",
        "created_on": 1720638498.6208274,
        "description": "What This Role Requires: 1-4 years of programming experience after your degree Must have coding experience in both Python and SQLIt is preferred that you have experience in at least one of the following additional languages: Java, C#, C++, ScalaFamiliarity with Big Data technology in cloud and on-premises environments: Hadoop, HDFS, Spark, NoSQL Databases, Hive, MongoDB, Airflow, Kafka, AWS, Azure, Dockers or Snowflake Good understanding of object-oriented programming (OOP) principles & concepts Familiarity with advanced SQL techniques Familiarity with data visualization tools such as Tableau or Power BI Familiarity with Apache Flink or Apache Storm Understanding of DevOps practices and tools for (CI/CD) pipelines. Awareness of data security best practices and compliance requirements (e.g., GDPR, HIPA). To Qualify: You should be willing to relocate anywhere in the US on a client project-to-project basis, as this is an onsite, in-office position . Strong English communication skills, both written and verbal. Bachelor’s Degree in Computer Science, Information Systems, Electrical Engineering, Mathematics, or a related quantitative field.",
        "url": "https://www.linkedin.com/jobs/view/3963388260"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Bellevue, WA",
        "job_id": 3966474728,
        "company": "IntePros",
        "title": "Data Engineer II",
        "created_on": 1720638502.553774,
        "description": "Compensation Range: $45.00 - $53.00/hr Welcome to IntePros, a certified woman-owned company specializing in innovative and results-oriented recruiting and staffing solutions. We take immense pride in genuinely understanding what drives and inspires exceptional individuals like you. Your success is our priority, and we are dedicated to actively shaping your long-term career journey. At IntePros, we believe in comprehensive well-being. You have access to our medical, dental, vision, and mental health programs, ensuring your health and wellness are taken care of. To support your continuous growth, we also provide a $1,500 per year education and professional certification fund. Diversity and inclusion are cornerstones of our company ethos. IntePros is proud to be an equal opportunity employer. We do not discriminate in employment on the basis of race, color, religion, sex, pregnancy, gender identity, national origin, sexual orientation, disability, age, veteran or military status, retaliation, or any other characteristic protected by law. We celebrate the rich tapestry of backgrounds and perspectives that make us stronger as a team. Please note that only qualified individuals being considered will be contacted. We appreciate your interest and look forward to potentially embarking on a transformative journey together. Data Engineer III IntePros is looking for a Data Engineer, to work 3 days a week on-site and 2 days a week remotely for a team located in Bellevue, WA. This is an exciting opportunity that will work with various tools and technologies automating manual tasks to reduce operational burden on business teams. Responsibilities: Work in a complex data warehouse environment. Developing and supporting analytic technologies. Design, implement, and support an analytical infrastructure providing ad-hoc access to large datasets and computing power. What are the top three MUST HAVE skill sets (technical) that are required? 3+ years of Data Engineering experience. Strong SQL Skills Strong Python Skills What are the top three PREFERRED skill sets (technical)? AWS technologies like redshift, S3, AWS Glue, EMR, etc. BI report development experience. Soft Skill requirements (team fit/personality requirements) Effective communication skills Strong MS Excel skills Data analysis skills",
        "url": "https://www.linkedin.com/jobs/view/3966474728"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Plano, TX",
        "job_id": 3936180113,
        "company": "Public Storage",
        "title": "Data Engineer II",
        "created_on": 1720638504.2404153,
        "description": "Company Description Since opening our first self-storage facility in 1972, Public Storage has grown to become the largest owner and operator of self-storage facilities in the world. With thousands of locations across the U.S. and Europe, and more than 170 million net rentable square feet of real estate, we're also one of the largest landlords. We've been recognized as A Great Place to Work by the Great Place to Work Institute. And, our employees have also voted us as having Best Career Growth , ranked us in the Top 5% for Work Culture , and in the Top 10% for Diversity and Inclusion . We're a member of the S&P 500 and FT Global 500. Our common and preferred stocks trade on the New York Stock Exchange. Workplace One of our values pillars is to work as OneTeam and we believe that there is no replacement for in-person collaboration but understand the value of some flexibility. Public Storage teammates are expected to work in the office five days each week with the option to take up to three flexible remote days per month. Our office is based in Plano, east of Interstate 75 near E. Park Blvd, just North of Historic Downtown Plano. , Job Description As a key member of our leading-edge, full-stack team, the Data Engineer II role offers an unparalleled opportunity to advance your career in a stable, S&P 500 company renowned for its innovative spirit, collaborative team culture and commitment to technical excellence. In this elevated position, you are instrumental in advancing our cutting-edge data infrastructure; optimizing data flow and storage, and spearheading data science and data engineering initiatives that drive our company's growth this year and for many years after. This role is best suited for those who thrive in a culture of mentoring, performance, accountability, and technical leadership, offering a prime environment to refine various skills @ data engineering, machine learning operations, and learn/apply advanced leadership skills. Responsibilities: Advanced Collaboration: Define and lead projects in collaboration with Data Scientists and Engineers to enhance data workflows, implementing cutting-edge solutions to meet complex data challenges. Applied System Architecture Development: Take charge of designing and executing significant enhancements to our data systems to support advanced analytical capabilities and meet evolving business needs. Pipeline Management: Architect, build, and manage sophisticated data pipelines from a variety of sources, ensuring scalability and reliability. Data Governance: Spearhead the development and enforcement of data management practices, ensuring the highest quality of data in our data lake and compliance with data privacy standards. ML Ops Leadership: Play a critical role in the strategy and execution of Machine Learning Operations, driving the adoption of best practices and innovative solutions. Code Excellence: Set the standard for code maintainability, performance, and best practices, mentoring junior engineers and leading by example. Strategic Documentation: Create, maintain, and evolve detailed documentation of data architectures, systems, and processes, facilitating knowledge sharing and operational efficiency. , Qualifications Required Qualifications: A BS in computer science 5+ years of experience OR a Master’s degree in Computer Science with 3+ years relevant experience as a Data Engineer. Must possess have multiple years of experience of hands on technical programming skills in SQL and Python. Possess exceptional communication skills – written and verbal. 3-5 years of experience in developing and deploying production-grade code in cloud environments, with a proven track record in engineering best practices applied to machine learning. Advanced proficiency in relational database modeling, Data Mart design, SQL development, and performance tuning. Must be expert at SQL coding and troubleshooting. Experience (4+ years) with Python/SQL for sophisticated data processing and API development. Experience (3-5 years) in managing analytical data warehouses and advanced columnar data stores, with a strong preference for experience in Big Query. Demonstrable skills, including advanced query optimization, version control systems, code review processes, and comprehensive documentation. Proven track record (2+ years) in designing, implementing, and maintaining scalable ETL data architectures using advanced tools like Airflow, DBT, Luigi, or Azkaban. Desired Qualifications: Solid experience (2+ years) with advanced Data and ML Orchestration, Containerization, and GPU Compute technologies such as Docker, Kubernetes, Spark, or Dask. Experience (2+ years) in a senior DevOps or MLOps role, leading the development of machine learning infrastructure with tools like Terraform or Google Cloud Deployment Manager/GKE. Proficiency (2+ years) with advanced dashboarding tools, with a preference for Looker ML. Experience (2+ years) in developing and deploying complex ML solutions in public cloud environments like Google Cloud Platform, AWS, or Azure. This role represents a significant step forward for those eager to influence the future of data engineering, offering a dynamic, supportive environment where innovation and collaboration are not just valued but essential to our success. If you are passionate about pushing the boundaries of data engineering and ready to take on this challenging yet rewarding role, we invite you to apply. Additional Information All your information will be kept confidential according to EEO guidelines.REF1766R",
        "url": "https://www.linkedin.com/jobs/view/3936180113"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Bellevue, WA",
        "job_id": 3962966045,
        "company": "LTIMindtree",
        "title": "Data Engineer",
        "created_on": 1720638505.8946316,
        "description": "About Us: LTIMindtree is a global technology consulting and digital solutions company that enables enterprises across industries to reimagine business models, accelerate innovation, and maximize growth by harnessing digital technologies. As a digital transformation partner to more than 700+ clients, LTIMindtree brings extensive domain and technology expertise to help drive superior competitive differentiation, customer experiences, and business outcomes in a converging world. Powered by nearly 90,000 talented and entrepreneurial professionals across more than 30 countries, LTIMindtree — a Larsen & Toubro Group company — combines the industry-acclaimed strengths of erstwhile Larsen and Toubro Infotech and Mindtree in solving the most complex business challenges and delivering transformation at scale. For more information, please visit www.ltimindtree.com. Job Title: Data Engineer (US Citizen or GC only) Work Location Bellevue, WA ROLE DESCRIPTION •Perform data analysis and provide actionable insights. •Suggest the data visualization team in building required KPI’s and visualization. •Perform ad-hoc analysis and present findings. • Ability to create excel or power bi reports as needed. • Perform data validation to ascertain data quality and data integrity before commencing any data analysis. • Coordinate and communicate between business users and the data engineering teams. •Manage stakeholder communications and helping implement solutions/features on time. •Ability to guide junior developers in the team. Ability to organize and lead meetings with business and operational data-owners. • Work closely with business stakeholder and data engineering team to document business processes. • Help form data management and governance processes within the data engineering team. • Strong ability to troubleshoot and resolve data issues. SKILLS •8+ years in data analytics and SQL •6+ years of experience building Power BI reports including data import, transformations, mapping. •6+ years of experience in working with SSAS Tabular / Multi-dimensional cubes. •6+ years of experience in managing pipelines using SSIS and ADF •Nice to have Azure Synapse experience •Ability to analyze, model and interpret data. •Apply methodical and logical approach to solving business problems. •A high level of mathematical ability •Good problem-solving skills. •Good understanding of SQL •Ability to build Power BI reports (understanding of DAX would be preferable) •The ability to plan work and meet deadlines. •Accuracy and attention to detail •Good Interpersonal skills Skills Power BI Benefits/perks listed below may vary depending on the nature of your employment with LTIMindtree (“LTIM”): Benefits and Perks: · Comprehensive Medical Plan Covering Medical, Dental, Vision · Short Term and Long-Term Disability Coverage · 401(k) Plan with Company match · Life Insurance · Vacation Time, Sick Leave, Paid Holidays · Paid Paternity and Maternity Leave The range displayed on each job posting reflects the minimum and maximum salary target for the position across all US locations. Within the range, individual pay is determined by work location and job level and additional factors including job-related skills, experience, and relevant education or training. Depending on the position offered, other forms of compensation may be provided as part of overall compensation like an annual performance-based bonus, sales incentive pay and other forms of bonus or variable compensation. Disclaimer: The compensation and benefits information provided herein is accurate as of the date of this posting. LTIMindtree is an equal opportunity employer that is committed to diversity in the workplace. Our employment decisions are made without regard to race, color, creed, religion, sex (including pregnancy, childbirth or related medical conditions), gender identity or expression, national origin, ancestry, age, family-care status, veteran status, marital status, civil union status, domestic partnership status, military service, handicap or disability or history of handicap or disability, genetic information, atypical hereditary cellular or blood trait, union affiliation, affectional or sexual orientation or preference, or any other characteristic protected by applicable federal, state, or local law, except where such considerations are bona fide occupational qualifications permitted by law. Safe return to office : In order to comply with LTIMindtree’ s company COVID-19 vaccine mandate, candidates must be able to provide proof of full vaccination against COVID-19 before or by the date of hire. Alternatively, one may submit a request for reasonable accommodation from LTIMindtree’s COVID-19 vaccination mandate for approval, in accordance with applicable state and federal law, by the date of hire. Any request is subject to review through LTIMindtree’s applicable processes",
        "url": "https://www.linkedin.com/jobs/view/3962966045"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3969077996,
        "company": "Cogstate",
        "title": "Data Engineer",
        "created_on": 1720638507.6653962,
        "description": "We’re a cognitive science company on a mission to optimize the measurement of brain health to advance the development of new medicines and to enable earlier clinical insights in healthcare. That’s why we’re seeking a Data Engineer responsible for building and maintaining data pipelines in Azure Databricks to enable Cogstate’s data strategy. The candidate will have a central critical role for establishing and maintaining the Azure Databricks infrastructure, provide best practices and recommendations for using these tools, and will working closely with data scientists, data managers, clinical scientists, and other business stakeholders as a point of contact for the data platform infrastructure. *Salary Range: $125,000 - $155,000 per year (commensurate with experience). Benefits listed below. Core Responsibilities Design, develop, and maintain scalable data pipelines using Azure Data Factory, and integrate data from various sources and systems into a coherent data ecosystem. Utilize Azure Databricks for complex data transformation and processing tasks, ensuring high performance and alignment with business requirements. Manage Azure Functions for efficient data ingestion and processing, optimizing for performance and cost. Implement and maintain data storage solutions in Azure Data Lake Storage Gen2 (ADLS2), applying best practices for data lifecycle management, security, and access control. Collaborate with cross-functional teams to define data requirements, deliver comprehensive data solutions, and support data-driven decision-making processes. Employ GitHub for source control management, adhering to best practices for code reviews, branching, and versioning. Facilitate CI/CD pipelines using GitHub Actions to automate testing, integration, and deployment processes, enhancing productivity and ensuring high-quality deployments. Monitor, troubleshoot, and optimize data systems and pipelines for performance, reliability, and scalability. Stay abreast of industry trends and advancements in cloud and data engineering technologies, continuously seeking opportunities for innovation and improvement. Maintain Data Governance infrastructure to ensure appropriate data access rights and restrictions for internal and external users. Qualifications BS/BA in Computer Science, Data Science, or a related field or relevant experience 5+ years’ experience as a data engineer. Must have 2+ years experience in implementing data engineering solutions with Apache Spark, preferably Databricks. Must have deep expertise in one of the programming languages for data processes (Python, Scala). Experience with Python, PySpark, Hadoop, Hive and/or Spark to write data pipelines and data processing layers. Direct experience with relational databases like SQL Server. Excellent SQL experience for writing complex SQL transformations. Strong knowledge on Databricks configuration, troubleshooting and performance tuning. Experience with development tools for CI/CD, unit and integration testing, automation and orchestration, including GitHub, Azure Data Factory and Azure functions. Familiarity with Agile workflow methodologies, demonstrating an ability to work effectively in a dynamic, iterative development environment and manage tasks with varying priorities. Benefits Remote Work Practices: Cogstate is a virtual first company. Cogstate employees can work from anywhere where Cogstate is registered to business within the United States, Australia, or the United Kingdom! Generous Paid Time-off: Cogstate employees receive 20 days of vacation leave, 10 days of personal leave and 10 paid public holidays, unused days roll over year after year. 401(k) Matching: As you invest in yourself and your future, Cogstate invests in you too: we match up to 3% of your yearly salary in Cogstate’s 401k program Competitive Salary: We offer competitive base salaries plus additional earning opportunities based on the position. Health, Dental & Vision Coverage: We've invested in comprehensive health & dental insurance options with competitive company contributions to help when you need it most. We also offer free vision insurance for all full-time employees. Short-Term & Long Term Disability Life Insurance: 100% employer sponsored Pre-Tax Benefits: Healthcare and Dependent Care Flexible Spending Accounts Learning & Development Opportunities: Cogstate offers a robust learning program from mentorships to scholarships to Coursera to improve knowledge or obtain certifications in applicable areas of interest. Why We Do What We Do Our mission at Cogstate is to optimize the measurement of brain health to advance the development of new medicines and to enable earlier clinical insights in healthcare. We are driven by the notion that through our work we impact the health of communities by delivering solutions that combine breakthrough science with advanced technologies. We are inspired by the dedication of researchers and the resilience of patients, and we are strengthened by our 20-year heritage supporting them. We are always working to simplify the complex with solutions that offer insights and hope for the future of healthcare, particularly dementia-related diseases and rare and pediatric disorders. Commitment to Diversity & Inclusion Cogstate is committed to building and maintaining a fair, diverse and inclusive workplace where the personal worth of each employee is recognized, and all are respected and valued for their differences. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and certain state or local laws. If you need assistance in applying please email PeopleandCulture@cogstate.com.",
        "url": "https://www.linkedin.com/jobs/view/3969077996"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3949668973,
        "company": "CareQuest Institute for Oral Health",
        "title": "Data Engineer",
        "created_on": 1720638509.4282694,
        "description": "ABOUT US: CareQuest Institute for Oral Health® is a national nonprofit championing a more equitable future where every person can reach their full potential through excellent health. We do this through our work in philanthropy, analytics and data insights, health transformation, policy and advocacy, and education as well as our leadership in dental benefits and innovation advancements. We collaborate with thought leaders, health care providers, patients, and local, state, and federal stakeholders, to accelerate oral health care transformation and create a system designed for everyone. To learn more, visit carequest.org. JOB SUMMARY: The Data Engineer will lead activities in expanding and optimizing our data pipeline architecture as well as optimizing the data flow. Design and build out data models used for analytical reporting and data science needs. Support data analysts and data scientists on projects and will ensure data are structured and delivered in a consistent manner throughout ongoing work. Manage the operational aspects of data platform solutions, including administering accessing controls, performance tuning, and automation of tasks. Develop and implement standard operating procedures associated with data engineering in line with organizational goals. Perform routine and ad hoc data extractions, create and deliver reports in a timely manner, and other analysis summaries as requested. Additional responsibilities include delivering routine information/data and assisting in the preparation & configuration of Azure Cloud environment. PRIMARY JOB RESPONSIBILITIES: Setup and configure data warehouse of dental and medical claims, practice management systems, electronic health records data, and other structured and unstructured health data. Strong knowledge of Business Intelligence and Data Warehouse development methodologies Experience with the following tools and platforms: Azure Data Factory, Azure Storage Account, Azure DevOps, Azure Storage Explorer, Databricks, and Snowflake. Experience with data ingestion from various third-party data sources Develop and implement standard operating procedures to ensure compliance with licensing, legal, and ethical requirements Recommend the best practices for management, monitoring & optimization of data Ensure that data pipelines and data stores are high-performing, efficient, organized, and reliable, given a specific set of business requirements and constraints. Process and analyze extracts of data from practice management system such as Dentrix, EagleSoft, Curve Dental, EPIC, Ace Dental Prepare routine and ad hoc data extractions, reports, and analysis summaries Research and design Electronic Data Interchange processes Serve as technical resource for all department staff Lead technical implementation projects Develop and maintain database and dashboard applications for monitoring measures by CareQuest Institute staff. Handle technical application development projects as assigned Perform new and existing set-up and maintenance processes Other duties as needed or required. JOB QUALIFICATIONS: Required: Bachelor’s degree in computer science or a related information technology field 1-3 years prior related technical and business experience required 1-3 years of experience with SQL programming 1-2 years of experience with cloud database technologies, such as Azure, AWS, Google Cloud, Databricks, or Snowflake Proficiency in the following technical applications/programs necessary Relational Databases (SQL Server preferred) Knowledge of one or more data science languages/programs (Python or R preferred) Preferred: Experience with medical or dental claims data, electronic health records, and/or dental practice management software preferred Experience using data analytics and/or report development tools like PowerBI and Tableau Hands-on expertise with SQL, Python, Azure data services, and Snowflake experience building out a complex ETL/ELT pipeline Experience implementing security and compliance requirements and working with different data modeling techniques. Experience designing and implementing data warehouse methodologies PHYSICAL DEMANDS: Incumbent must be able to communicate effectively. Manual dexterity and sitting is required in carrying out position own position responsibilities (i.e., use of personal computer). Ability to travel or move about within and outside serviced facilities required. Incumbent works primarily in either a private or shared office environment. The specific statements shown in each section of this description are not intended to be all-inclusive. They represent typical elements and criteria necessary to successfully perform this position. ** In accordance with CareQuest Institute for Oral Health’s Compliance Plan, all employees must conduct CareQuest Institute for Oral Health business and activities in accordance with applicable laws, regulations, professional standards and ethical standards and report potential compliance or ethical issues to CareQuest Institute for Oral Health’s designated Compliance Officer. ** CareQuest Institute for Oral Health’s Affirmative Action Program affirms our commitment to make reasonable accommodation for known physical or mental limitation of otherwise-qualified individuals with disabilities or special disabled veterans, unless the accommodation would impose an undue hardship on the operation of our business and activities. Please see Human Resources for additional information regarding this program.",
        "url": "https://www.linkedin.com/jobs/view/3949668973"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3944750345,
        "company": "QuantumBricks",
        "title": "Data Engineer",
        "created_on": 1720638511.2430577,
        "description": "Job Title: Data Engineer Location: Multiple Location (Onsite) Required Qualifications / Skills 10 years of strong SQL skills; SQL Server and PostgreSQL is preferred. experience in any other RDBMS is plus.. Proficiency in the Python programming language Ability to prepare, analyze, and effectively communicate the findings of data analysis to customers, RTC leadership, and outside stakeholders. Strong written/verbal communication skills. Must be able to obtain and maintain a Secret Clearance Python and spark knowledge is plus. Knowledge about Database engineering and Data warehousing Concepts. Experience with Agile based project methodology. Ability to identify risks/issues for the project and manage them accordingly. Responsibilities Write secure and high-quality code and maintains algorithms that run synchronously with appropriate systems. Develop Extract, Transform, Load (ETL) pipelines for data Work directly in support of Army modernization priorities including working directly on and around helicopters, missiles, and sensors.",
        "url": "https://www.linkedin.com/jobs/view/3944750345"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3971398500,
        "company": "AllCloud",
        "title": "Data Engineer",
        "created_on": 1720638512.8856611,
        "description": "Data Engineer Location: US / Canada (Eastern Time) - Home based Job Type: Full-time, Permanent About AllCloud AllCloud is a global professional services company providing organizations with cloud enablement and transformation. Through a unique combination of expertise and agility, AllCloud accelerates cloud innovation and helps organizations fully unlock the value received from cloud technology and data and analytics. As an AWS Premier Consulting Partner and audited MSP, a Salesforce Platinum Partner and Snowflake Premier Partner, AllCloud helps clients connect their front office and back office by building a new operating model that allows them to harness the benefits of cloud technology. AllCloud is supported by a robust ecosystem of technology partners, proven methodologies, and well-documented best practices. Thereby elevating customers by achieving operational excellence on the cloud, within a secure environment, at every milestone of the journey to becoming cloud first. With years of experience and a portfolio of thousands of successful cloud deployments, AllCloud serves clients across the globe. AllCloud has offices in Israel, Europe and North America Job Summary Are you passionate about data and delivering solutions for clients that turn data into valuable, actionable information for their business? We are hiring Data Engineers with strong experience across the entire Cloud Data stack. The ideal candidate will have extensive experience in data pipelines (ELT/ETL), data replication, data warehousing and dimensional modeling, and curation of data sets for Data Scientists and Business Intelligence users. This candidate will also have excellent problem-solving ability dealing with large volumes of data. How You'll Make Your Mark: Building scalable Cloud data solutions using MPP Data Warehouses (Snowflake, Redshift, or Azure Data Warehouse/Synapse), data storage (S3, Azure Blob Storage, Delta Lakes, or AWS Lake Formation) and analytics platforms (i.e. Spark, Databricks, etc.) Creation of data pipelines and transformations Knowledge of ETL tools such as– Matillion, FiveTran, Informatica, Talend, SSIS, etc. Data Replication - Golden Gate, Qlik, etc. Transformations – dbt Load historical data to a data warehouse Scripting in Python, Java Script or Shell Workflow Orchestrations using Apache Airflow, AWS Step Functions, etc. Familiarity with automated promotions, SCM tools, and CICD best practices Modeling and curation of data for visualization and predictive modeling users Design and implementation of AWS and/or Azure services such as Lambda, SNS, etc. Creating data integrations with scripting languages such as Python Writing complex SQL queries, stored procedures, etc. Summary of Requirements & Experience Bachelor’s degree, or equivalent experience, in Computer Science, Engineering, Mathematics or a related field. Commensurate work experience will be considered in lieu of degree 2+ years’ experience building scalable Cloud data solutions using MPP Data Warehouses (Snowflake, Redshift, or Azure Data Warehouse/Synapse), data storage (S3, Azure Blob Storage, Delta Lakes, or AWS Lake Formation) and analytics platforms (i.e. Spark, Databricks, etc.) 3+ years with complex SQL queries and scripting 3+ years’ experience building data pipelines via Python, Spark, or GUI Based tools 3+ years’ experience loading historical data to data warehouses 3+ years’ experience with AWS and/or Azure Cloud 3+ years developing, and deploying scalable enterprise data solutions (Enterprise Data Warehouses, Data Marts, ETL/ELT workloads, etc.) 3+ years of supporting business intelligence and analytic projects 2+ years’ DevOps experience 2+ years’ experience working in an environment with automated promotions to production Familiar with ETL/ELT patterns and methodologies. Good understanding of code repositories such as GIT Excellent written and oral communication skills Pluses Experience with a Business Intelligence tool such as Tableau, PowerBI, Sigma, etc. Experience working at a consulting company Experience with Data Vault architecture Matillion Associate Certification Snowflake SnowPro AWS Data & Analytics Specialty AWS Database Specialty AWS Solutions Architect Associate AWS Developer Associate AWS Glue Why work for us? Our team inspires progress in each other and in our customers through our relentless pursuit of excellence; you will work with leaders who promote learning and personal development. AllCloud is an Equal Opportunity Employer and considers applicants for employment without regard to race, color, religion, sex, orientation, national origin, age, disability, genetics or any other basis forbidden under federal, provincial, or local law.",
        "url": "https://www.linkedin.com/jobs/view/3971398500"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3960676382,
        "company": "Cortex",
        "title": "Data Engineer",
        "created_on": 1720638514.6969724,
        "description": "Job Description 🔎 #Conheça um pouco sobre a área: Estamos à procura de um Data Engineer Pleno para integrar nosso time de dados. O profissional será responsável por desenvolver e manter pipelines de dados robustos e escaláveis, garantir a qualidade e a integridade dos dados, além de colaborar com diferentes equipes para fornecer soluções de dados eficientes. Se você tem paixão por dados, habilidades técnicas avançadas e experiência com grandes volumes de dados, queremos conhecer você! Responsibilities and assignments 🎯 #Desafios que você vai encontrar: Desenvolver, manter e otimizar pipelines de dados utilizando Apache Spark. Implementar e gerenciar soluções de dados na plataforma Databricks. Projetar e implementar arquitetura de dados na AWS, utilizando serviços como S3, Redshift, Glue, Lambda, entre outros. Garantir a qualidade e integridade dos dados através de práticas de validação e monitoramento. Colaborar com equipes de análise de dados, ciência de dados e outros stakeholders para entender necessidades de dados e fornecer soluções eficazes. Participar na definição e implementação de boas práticas de engenharia de dados. Documentar processos, pipelines e arquitetura de dados de forma clara e detalhada. Requirements And Qualifications ✔️ # Conhecimentos importantes para a posição: Formação em Ciência da Computação, Engenharia de Dados, Sistemas de Informação ou áreas relacionadas. Experiência comprovada como Data Engineer ou em funções similares. Conhecimento avançado em Apache Spark. Experiência com Databricks e suas funcionalidades. Experiência em arquitetura de dados na AWS, incluindo serviços como S3, Redshift, Glue e Lambda. Sólidos conhecimentos em SQL e na modelagem de dados. Experiência com ferramentas de ETL/ELT. Capacidade de trabalhar de forma colaborativa em um ambiente ágil. Boas habilidades de comunicação e capacidade de traduzir requisitos de negócio em soluções técnicas. ⭐ #Diferencial para a posição: Experiência com outras plataformas de nuvem como Azure ou Google Cloud. Conhecimento em linguagens de programação como Python ou Scala. Experiência com ferramentas de orquestração de workflows, como Apache Airflow. Certificações em AWS, Databricks ou outras relacionadas. Additional information 💜 #PraVocê Nossos Beneficios: PraVocê No Dia a Dia 🌞 Vale Refeição ou Alimentação; Gympass; Auxílio home-office; Flexibilidade de horários; PraVocê & Família👩‍👩‍👧‍👦 Plano de Saúde e Plano Odontológico; Auxílio creche (até os 6 anos completos da criança): Licença-Maternidade, Paternidade e Adotante Estendidas (#todasasfamíliasimportam); Seguro de Vida; Day Off de Aniversário (Um dia de folga para tirar no dia ou durante o mês do seu aniversário.); Dia da Família (Um dia de folga para mamães e papais tirarem entre os meses de maio e agosto e aproveitar como quiserem.); Pausa Mental (Uma semana corrida de folga em JANEIRO para que descansem e recarreguem as baterias.). PraVocê AINDA MAIS🔝 Senso de propósito ao fazer parte de um time que está construindo algo que será perene e trará frutos para a sociedade, seja a Cortex em si enquanto \"startup\" em rápido crescimento que gerará cada vez mais empregos, seja o produto Plataforma Cortex, que será usado por cada vez mais usuários no Brasil e exterior. Ambiente de trabalho descontraído, jovem, empreendedor e meritocrático, sem espaço para política.. ;-) Oportunidade de desenvolvimento de carreira e crescimento numa empresa que não para de crescer. Valorizamos, cultivamos e respeitamos as diferenças, por isso proporcionamos um ambiente aberto e inclusivo. Todas as nossas posições são elegíveis para pessoas com deficiência. Process stages Step 1: Registration1Registration Step 2: Screening People 📞2Screening People 📞 Step 3: Talk People 💬3Talk People 💬 Step 4: Avaliação Técnica 📝4Avaliação Técnica 📝 Step 5: Match Cultural 💜5Match Cultural 💜 Step 6: Proposta 💰6Proposta 💰 Step 7: Hiring7Hiring Tecnológica, inquieta e ousada, bem-vindo à Cortex!🚀 Somos a Cortex, a plataforma brasileira líder em inteligência de Go-To-Market e o maior investimento em Inteligência Artificial aplicada a negócios na América Latina. Nosso propósito é transformar com inteligência o caminho de pessoas e negócios. Usamos IA e Ciência de Dados para promover uma gestão completa e contínua do Go-to-Market de marcas líderes no país . Nossa plataforma é a única que coleta e normaliza dados externos, sejam eles firmográficos, de mídia ou de localização, e os unifica com dados internos de clientes para criar painéis de analytics flexíveis, análises geográficas e fluxos de trabalho baseados em dados. Grandes investidores acreditam no nosso negócio. Temos como parceiros alguns dos maiores e principais fundos de investimento em tecnologia, como Lightrock, Riverwood Capital e SoftBank. VEM FAZER PARTE DO TIME! Aqui na Cortex você encontrará um ambiente receptivo às diferenças e às necessidades das pessoas. Crescemos rápido porque crescemos juntos. Tudo isso faz com que ser cortexiane seja uma experiência única de descobertas, transformação e crescimento a partir de um ambiente colaborativo de criação e realização. Vamos juntos? #BeCortex",
        "url": "https://www.linkedin.com/jobs/view/3960676382"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Atlanta, GA",
        "job_id": 3964714634,
        "company": "Rev.io",
        "title": "Data Engineer- Atlanta, GA ONLY",
        "created_on": 1720638516.336193,
        "description": "About Rev.io Rev.io provides configurable software for billing, customer management, business management, payments, analytics, provisioning, and automation to service providers in telecommunications, Wireless & IoT, managed IT services, A/V, security integration, and other related industries. Our modern, cloud-based software delivers the industry’s most complete quote-to-cash experience, enabling our clients to grow their revenue efficiently. Rev.io is an Atlanta-based company with employees based in more than 20 states. We have more than 21 years’ in business serving our clients. While we are very experienced in what we do, we are still growing rapidly and we are looking for exceptional people who are excited to join us on a career-defining journey. In all of our daily work, each of us are guided by our mission, vision and “ACT TOP” values. Our Mission: To help clients grow revenue efficiently. Our Vision: To be the best billing & back-office software company in the world by providing innovative solutions and extraordinary service to our clients and end users. Our ACT TOP Values: Achieving Extraordinary Results: Doing more than expected and pursuing a higher standard of work without compromising your job responsibilities. Caring For Employees, Clients and Community: Demonstrating empathy and concern by helping those who are most in need. Take Responsibility and Act Like An Owner: Being accountable and doing the right thing even when no one is watching. Transparent Leadership: Communicating openly, regardless of title or situation Opportunity To Make a Positive Difference: Recognizing situations and taking action to produce a better outcome. Passion For Innovation: Enthusiastically finding new solutions to improve efficiency. In a About The Role: We are looking for a Data Engineer to join our team in Atlanta, GA. The purpose of a Data Engineer is to migrate and curate a client's data from their legacy system into the Job Objectives and Responsibilities: Work with client teams of Billing and Mediation Specialists, and Database Administrators responsible for all clients billing and rating operations Migration of legacy data to specified SQL Server database structure Configure database and DBMS parameters. Monitor successful completion of all scheduled jobs for implementation; troubleshooting any job failures and DBMS issues. Communicate and update on task due dates, requirement needs and overall project status to the Implementation Team and Vice President of Operations. Provide hands-on assistance in research and resolution of client billing issues and inquiries Create, modify and test custom SQL queries and stored procedures to provide maximum visibility into billing details and accuracy and customer billing formats to large wholesale and enterprise customers Manage complex call records mediation and rating engine to ensure all usage elements are properly applied to customer accounts Validate results by testing Dual bill run testing and rating to ensure the customer is billing correctly out of Analyzing rating, MRCs, NRCs for accurate billing between a customers legacy system and Assist clients with guidance on data migration standards and best practices for utilizing Foster ideas on how to improve client relationships and operational efficiencies Develop a foundation and the recommendation to steam line and define the data migration process by product type and monthly recurring revenue. Skills Needed: A Bachelor’s degree in Computer Science, Business, or Information Technology recommended Knowledge and working experience of SQL Server and DTS packages are essential. Familiarity with CDR formats (e.g. BAF, EMI), types and jurisdictions Familiarity with telecom tax engines and data feeds (e.g. SureTax, Avalara) Experience of analysis of data structures and development of tools to parse and restructure data Client-facing skills together with the ability to work independently in the field interpreting specific requirements In-depth understanding of SQL queries, statements, and stored procedures Experience of problem solving, database installation and management procedures in development and production environments. Knowledge of ODBC/ADO and accessing remote systems using various methods. T-SQL experience and programming in SQL Server, SQL scripting and SQL database design. Experience of applying data modeling tools would be an advantage. Management of database systems. Benefits and Perks: Excellent medical, dental and vision coverage, with rates comparable to larger companies Company paid for life and disability insurance 401k with generous company match and immediate vesting Unlimited PTO Monthly tech and fitness reimbursements Professional development allowance At Rev.io no employee or applicant will be treated less favorably on the grounds of their sex, marital status, race, color, nationality or ethnic or national origin, disability, gender, sexual orientation, gender identity, age, pregnancy or maternity, marital or civil partner status, or religion or belief. By clicking submit below, you consent to allow Rev.io to store and process the personal information submitted above. Powered by JazzHR Pra3a5AnsO",
        "url": "https://www.linkedin.com/jobs/view/3964714634"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Kansas City, MO",
        "job_id": 3969331836,
        "company": "Sunflower Bank, N.A.",
        "title": "Data Engineer",
        "created_on": 1720638517.9616086,
        "description": "Description Data Engineer Hybrid/Remote Opportunity MUST live and be based out of KS, TX, MO, NM, MI, or AZ ONLY MUST have experience implementing and managing on-premise Microsoft SQL Server and Snowflake. Great Compensation Plan with Amazing Benefits! Sunflower Bank, N.A. is looking for a highly motivated individual to fill the full-time position of Data Engineer. The Data Engineer is responsible for developing and maintaining data pipelines that help enable the Bank to use data as an asset. The Data Engineer is also responsible for assisting in creating data models, managing metadata, and acting as an expert in data reporting and analytics. Assembling large, complex sets of data to meet functional business requirements. Identifying, designing, and implementing data pipelines using on-premise and cloud ETL/ELT tools and data platforms. Responsible for implementing, maintaining, and managing the Banks on-premise and cloud data platforms. Build required infrastructure for optimal extraction, transformation, and loading of data from various data sources across the Bank. Build analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer retention/acquisition. Work with stakeholders to deliver data-related solutions that empower the Bank to use data as an asset. Devise, develop, and deploy required data queries in response to business user needs. Education / Experience Preferred Bachelor’s degree in Computer Science, a related field, or 5+ years of IT experience. Strong understanding of both on-premise and cloud ETL/ELT methodologies, tools, and frameworks. Experience and understanding of data models, structures, theories, principles, and practices. Direct experience with data management techniques. Proven experience implementing and managing on-premise Microsoft SQL Server and Snowflake. Proven experience implementing and managing reporting and analytics platforms like Power BI, Tableau, etc. Sunflower Bank Benefits Employees Enjoy Outstanding Benefits, Including 401(k) Plan with 6% Match Health/Dental/Vision Insurance Company-paid Life Insurance Tuition Reimbursement Fitness Reimbursement Paid Time Off Volunteer Leave Paid Holidays Plus many more employee perks & incentives! People choose to “bank” with us, but for those we serve, we’re more than a bank. We strive to be the financial backbone of their lives and we know that starts with our team. If you qualify, apply online at www.sunflowerbank.com/careers. You’ve never worked anyplace like Sunflower Bank! EOE/AA: Minorities/Females/Disabled/Vets Open until filled; early application encouraged. This vacancy announcement may be used to fill similar positions within 90 days. If you are a California resident, you may be entitled to certain rights regarding your personal information, which is information that identifies, relates to, or could reasonably be linked with a particular California resident or household. Additional information about our data collection practices and location specific notices is available on our privacy policy.",
        "url": "https://www.linkedin.com/jobs/view/3969331836"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Audubon, PA",
        "job_id": 3963528417,
        "company": "Synkriom",
        "title": "Data Engineer",
        "created_on": 1720638519.5778675,
        "description": "Job Description Title: Data Analyst Location: Audubon, PA Job type: 24+ months Contract Client: Direct Client Responsibilities: Identifying, creating, and preparing data required for analysis and visualization Assuring that data is cleansed, mapped, transformed, and optimized for storage and use according to business and technical requirements Automating tasks and deploying production standard code Loading transformed data into storage and reporting structures such as data warehouses, real-time reporting systems and analytics applications Building data models and an understanding of learning algorithms Required Technical Skills: Minimum 5 years of relevant experience; 7+ years of relevant experience preferred Experience in data engineering Experience building and optimizing 'big data' data pipelines, architectures, and data sets Knowledge of SQL and Python/PySpark Open mindset, ability to quickly adapt to new technologies and learn new practices Required Functional Skills: Communication skills Collaboration skills Presentation skills Optional Skills: Knowledge of Oracle PL/SQL, Java is a plus Performance/security-oriented data modeling and data design, including partitions, different type of indexes, views, and most effective data management practices Azure Synapse Analytics, Azure DevOps Knowledge of Purview, Snowflake, Qlik, or Tableau is a plus Education & Certifications: Bachelor’s Degree in Data Engineering, Data Science or Computer Engineering Company Description Founded in 2015 and backed by two decades of hands-on experience, our strong insight on the enterprise ecosystem and its dynamics helps to offer a complete end-to-end solution. We automate recruitment processes with intelligent solutions to save time and money. Synkriom leads the industry with experience authentication of candidates. Founded in 2015 and backed by two decades of hands-on experience, our strong insight on the enterprise ecosystem and its dynamics helps to offer a complete end-to-end solution. We automate recruitment processes with intelligent solutions to save time and money. Synkriom leads the industry with experience authentication of candidates.",
        "url": "https://www.linkedin.com/jobs/view/3963528417"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "New Jersey, United States",
        "job_id": 3970972247,
        "company": "Hexaware Technologies",
        "title": "Data Engineer (Databricks)",
        "created_on": 1720638521.2477133,
        "description": "What Working at Hexaware offers: Hexaware is a dynamic and innovative IT organization committed to delivering cutting-edge solutions to our clients worldwide. We pride ourselves on fostering a collaborative and inclusive work environment where every team member is valued and empowered to succeed. Hexaware provides access to a vast array of tools that enhance, revolutionize, and advance professional profile. We complete the circle with excellent growth opportunities, chances to collaborate with highly visible customers, chances to work alongside bright brains, and the perfect work-life balance. With an ever-expanding portfolio of capabilities, we delve deep into and identify the source of our motivation. Although technology is at the core of our solutions, it is still the people and their passion that fuel Hexaware’s commitment towards creating smiles. “At Hexaware we encourage to challenge oneself to achieve full potential and propel growth. We trust and empower to disrupt the status quo and innovate for a better future. We encourage an open and inspiring culture that fosters learning and brings talented, passionate, and caring people together.” We are always interested in, and want to support, the professional and personal you. We offer a wide array of programs to help expand skills and supercharge careers. We help discover passion—the driving force that makes one smile and innovate, create, and make a difference every day. The Hexaware Advantage: Your Workplace Benefits Excellent Health benefits with low-cost employee premium. Wide range of voluntary benefits such as Legal, Identity theft and Critical Care Coverage Unlimited training and upskilling opportunities through Udemy and Hexavarsity Role: Data Engineer (Databricks) Location: Morristown, New Jersey Work Mode: Hybrid Salary Range: $130K - $140K Role: Databricks Developer Responsible for designing, developing, and maintaining data processing pipelines using Databricks platform. Working closely with data engineers and data scientists to implement data solutions that meet business requirements, to help client with their cloud migration journey. It includes the following responsibilities: Designing and developing data pipelines using Databricks platform. Writing efficient and optimized code in languages such as Python, Scala, or SQL. Collaborating with data engineers to ensure data quality and integrity. Implementing data transformations and aggregations to support analytics and reporting. Working with data scientists to deploy machine learning models on Databricks. Troubleshooting and resolving issues related to data pipelines and Databricks environment. Optimizing performance and scalability of Databricks jobs. Documenting technical specifications and maintaining code repositories. Keeping up to date with the latest Databricks features and best practices. Participating in code reviews and providing feedback to improve code quality. He/she should have a strong understanding of distributed computing concepts and experience with cloud platforms such as Azure. They should also possess good problem-solving skills and be able to work in a collaborative team environment. Privacy Statement: The information you provide will be used in accordance with the terms of our Privacy Policy and will be used specifically for the business/processing purpose of the event. You should be aware that we may share your details with our approved vendors for this event to be handled successfully.",
        "url": "https://www.linkedin.com/jobs/view/3970972247"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Orlando, FL",
        "job_id": 3905990140,
        "company": "Kolter Solutions",
        "title": "Data Engineer",
        "created_on": 1720638522.9612446,
        "description": "Kolter Solutions is seeking a Data Engineer Location: Remote Job Description: As a Data Engineer you will play a key role in building data solutions including high performance OLTP and OLAP databases. Additional Responsibilities Include: Develop high performance code using T-SQL for SQL Server environment, including creating tables, views, stored procedure, functions and SSIS packages Strong understanding SQL Server Change Tracking and Change Data Capture Experience working with Cloud as infrastructure (Azure) or other Cloud Platform based on IaaS and PaaS Solutions and ability to integrate data from multiple data sources Strong domain knowledge of data warehousing principles, tools and technologies Proven ability to work directly with users and management to gather requirements, provide status updates, and build good relationships Must be proactive, innovative, and creative in meeting client/customer needs Ability and willingness to work in a team environment and adopt a culture of ownership and initiative, and promote such within the team Knowledge and familiarity with database development concepts and best practices Experience working with both ETL and ELT solutions Proven knowledge of both on-prem and cloud data warehouse solutions To fulfill this role successfully, you must possess the following minimum qualifications and experience: Bachelor's Degree in Computer Science, Engineering and/or Information Systems. Basic understanding of data warehousing design/concepts Expertise in Microsoft SQL Server RDBMS systems Basic understanding of cloud data warehouse solutions such as Azure Synapse, etc., Ability to comprehend and troubleshoot Ad-hoc SQL queries Proficiency in MS Office Suite Demonstrated understanding of problem management and change control processes. Comfortable contributing and debating in a casual and friendly team environment Demonstrated collaboration skills with the ability to handle conflict and to work with a distributed team Superior critical thinking skills with the ability to develop completely new problem-solving approaches and formulate innovative solutions. Ability to work independently in a fast-paced environment, and manage workload prioritization to deliver high quality work products on time with minimal direction Ability to adapt and collaborate in changing circumstances. Effective analytical and problem-solving skills Excellent verbal and written communication skills Minimum 1-3 years' experience in building Data Integration jobs using tools such as SSIS, Talend or Informatica Minimum 3 plus years' experience using Microsoft SQL Server Minimum 3 plus years' experience creating tables, views, stored procedure, functions, and SSIS packages 2-4 years' experience creating data warehouse using Kimball methodology 2-4 years' experience using SQL Server Change Tracking and Change Data Capture Kolter Solutions is a leading professional staffing company based in Central Florida. We place highly skilled individuals on a contract, contract-to-hire and direct hire positions at clients nationwide. Kolter Solutions has proudly been recognized as the   \"2023 Best Places to Work ” by the Orlando Business Journal and    \"2022 Best Staffing Companies to Work for  \" by Staffing Industry Analysts (SIA).  We are also in the Fast 50 – 2022 Fastest growing companies in Central Florida ! We offer: Full Health Benefits Vision Dental 401 (k) Pet Insurance Life Insurance Supplemental Benefits such as short-term disability, accidental insurance, and supplemental dental and vision. Employee Discounts Referral Program Kolter Solutions is an Equal Opportunity Employer. We believe in hiring a diverse workforce and sustaining an inclusive, people-first culture. We are committed to non-discrimination on any protected basis, such as disability and veteran status, or any other basis covered under federal, state or local applicable law.",
        "url": "https://www.linkedin.com/jobs/view/3905990140"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Malvern, PA",
        "job_id": 3969091500,
        "company": "SkilzMatrix Digital",
        "title": "data engineer",
        "created_on": 1720638528.7955217,
        "description": "Responsibilities Integrate AWS Services such as Lambda, DynamoDB, EC2, RDS, S3, Athena, Data pipeline, API gateway, Glue, EMR etc. in reporting application to improve the accessibility/efficiency Write scripts and troubleshoot for performance using relevant programming languages like SQL, Spark and Python Develop and Implement ETL jobs using AWS Glue to extract, transform, and load data from various sources Design and implement scalable data solutions using AWS, PySpark, Python and other related services Work closely with Data and Analytics team to build Data Lake using various AWS services like EMR, S3, DynamoDB, Lambda etc., Apache Airflow, PySpark, and HIVE Use AWS data pipeline for Data Extraction, Transformation and Loading from homogeneous or heterogeneous data sources Work on migration of PySpark framework into AWS Glue for enhanced processing Design and Develop web applications using Java EE, Python, Web Services and AWS Qualified candidates will be responsible for API design, implementation, testing and prod support. They will work on a data modernization project which will utilize multiple AWS Services Tech needs: Java AWS Services - Glue, EMR API development SpringBoot services They are aws tech experience. dynamo and/or azure db. Knowledge of Resiliency patterns and chaos testing. Kafka knowledge Nice To Have Architecture experience in cloud or legacy systems. Building large scale systems. Java 11 understanding. spring boot experience and jersey framework knowledge, micro service architecture, graph ql implementation, Performance tuning of Java rest APIs. Design, Model, Develop, Test and Implement using Big Data technologies like Python, Pyspark, Hadoop, UNIX and SQL",
        "url": "https://www.linkedin.com/jobs/view/3969091500"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Cincinnati, OH",
        "job_id": 3849332514,
        "company": "Syntricate Technologies",
        "title": "Data Engineer",
        "created_on": 1720638530.4937916,
        "description": "Data Engineer Location: Cincinnati, OH Fulltime only 7+ plus years of experience in solutioning data pipeline for large enterprise data Warehouse applications using Azure Data Factory, Data bricks, Data Ingestion, Data Transformation /Computation, Orchestration, Reporting & Data Analytics Knowledge of Azure Services such as ADF, Databricks, Event Hub, Delta Lake, Data Lake Strong command on Python, Pyspark , SQL & Power BI Experience in GitHub , Git Action , Jira end to end data pipeline. Strong knowledge in CICD Pipeline for automatic deployments Experience in Terraform for Infrastructure Provisioning Strong knowledge on design and integration patterns Proficient in technical artifacts e.g., Application Architecture, Solution Design Documents, etc Strong at analytical and problem-solving skills Experience working with multi-vendor, multi-culture, distributed offshore and onshore development teams in dynamic and complex environment. Experience in Retail is desired, not mandatory. Must have excellent written and verbal communication skills",
        "url": "https://www.linkedin.com/jobs/view/3849332514"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Audubon, PA",
        "job_id": 3964667073,
        "company": "Turnberry Solutions",
        "title": "Data Engineer",
        "created_on": 1720638532.2656963,
        "description": "Duration: 12+ Months Location: PA-Audubon, Hybrid 3-4 days on site Overview: Every Turnberry consultant belongs to a practice, an internal group of consultants and leaders with shared experience and expertise. Each of these practices aligns to one of the core services Turnberry offers to clients. As a Data Engineer, you will join Turnberry's Data Strategy and Intelligence practice and service. This service provides insights into company data, advanced analytics solutions, robust data governance, and tailored solutions for specific data challenges. Responsibilities ﻿Identify, create, and prepare data required for analysis and visualization Assure that data is cleansed, mapped, transformed, and optimized for storage and use according to business and technical requirements Automate tasks and deploy production standard code Load transformed data into storage and report structures such as data warehouses, real-time reporting systems and analytics applications Build data models and understand learning algorithms Qualifications Minimum 5 years of relevant experience; 7+ years of relevant experience preferred Experience in data engineering Experience building and optimizing 'big data' data pipelines, architectures, and data sets Knowledge of SQL and Python/PySpark Open mindset, ability to quickly adapt to new technologies and learn new practices Communication skills Collaboration skills Presentation skills Bachelor’s Degree in Data Engineering, Data Science or Computer Engineering Preferred Qualifications Knowledge of Oracle PL/SQL, Java is a plus Performance/security-oriented data modeling and data design, including partitions, different type of indexes, views, and most effective data management practices Azure Synapse Analytics, Azure DevOps Knowledge of Purview, Snowflake, Qlik, or Tableau is a plus The salary range for this role is $80,000 to $160,000 or the hourly equivalent. Pay is based on several factors including but not limited to education, work experience, certifications, etc. In addition to your salary, Turnberry Solutions offers benefits such as a comprehensive healthcare package (medical, dental, vision), disability and group term life insurance, health and flexible spending accounts, a utilization bonus, 401(k) with match, flexible time off for salaried employees, parental leave for salaried employees, and flexible work arrangements (all benefits are subject to eligibility requirements). No matter where or when you begin a career with Turnberry, you'll find a far-reaching choice of benefits and incentives. At Turnberry, inclusion is one of our core values. We are fully invested in and focused on hiring and growing a diverse team of high performers. We're committed to creating a positive and connected work environment for all. We believe that uniqueness in ideas, experiences, and backgrounds make us a better Turnberry: Turnberry is an Equal Employment Opportunity/Affirmative Action employer, and recruits, employs, trains, compensates, and promotes regardless of age, ancestry, family medical or genetic information, gender identity and expression, marital, military, or veteran status; national and ethnic origin; physical or mental disability; political affiliation; pregnancy; race; religion; sex; sexual orientation; and any other protected characteristics.",
        "url": "https://www.linkedin.com/jobs/view/3964667073"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Memphis, TN",
        "job_id": 3940951643,
        "company": "Monogram Foods",
        "title": "Data Engineer",
        "created_on": 1720638533.8568125,
        "description": "Are you passionate about data and innovation? Monogram Foods is looking for a Data Engineer to join our IT team, supporting our co-manufacturing, private label, and foodservice channels across the United States. In this role, you will build data processing pipelines and analytics solutions using Azure cloud tools, transform data, detect and extract changes, and create models for business insights. This position requires collaboration with business analysts and stakeholders to meet their data needs. If you have a background in data engineering, proficiency in Azure tools, and a drive for excellence, we encourage you to apply! Essential Job Duties & Responsibilities: Ingest data from various sources using Azure Data Factory. Transform data visually with data flows or by using compute services. Monitor and optimize ETL processes and data pipelines for performance, reliability, and scalability. Collaborate with business analysts, stakeholders, and other teams to understand their data needs. Write complex SQL queries, stored procedures, and views to support data processing and reporting requirements. Migrate data and processes to Microsoft Fabric to leverage its advanced data management and analytics capabilities. Utilize Python, PySpark, and other programming languages for data processing and analytics within Azure services. Education & Experience: The ideal candidate will have a bachelor's degree in computer science, information technology, mathematics, or a related field. 3 years of experience in data engineering or analytics. Proficiency with Azure Data Factory, Azure SQL Analytics, and SQL Server is required. Experience with Microsoft Fabric is a plus. The candidate should also be skilled in programming languages such as SQL, Python, or Scala, and possess knowledge of parallel processing and data architecture patterns. Excellent communication and problem-solving skills are essential for success in this role. Must be 18 years or older. Competencies & Skills: Strong expertise in Azure Data Factory, Azure SQL Analytics, and SQL Server. Proficiency in SQL, Python, and/or Scala for data processing and analytics. Experience with Logic Apps, Azure Storage, REST APIs",
        "url": "https://www.linkedin.com/jobs/view/3940951643"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3949350622,
        "company": "Steneral Consulting",
        "title": "Data Engineer",
        "created_on": 1720638535.4844701,
        "description": "Title : Data Engineer Location : Remote Duration : 6+ months NO VIOP OR GOOGLE VOICE NUMBERS NEED LINKEDIN NEED DL COPY NEED DOB (MM/DD) NEED SSN LAST 4 DIGJTS ATTACHED IS THE EXCEL SHEET THE CANDIDATE HAS TO FILL Profile: Data Analyst will work closely with Architects and leads in the Interflow team, key business stakeholders including program staff to support the organizations effort in the areas of data Analysis and information management. Role should have strong experience in writing SQL queries to extract data for reports. Candidate should have experience working with large set of data. As these reports are very critical for the company and we are looking for an expert who is capable of handling, consuming and extracting large sets of data for analysis and reports. All Skills Should Be Mentioned On The Resume Key words as must haves: AWS - Athena AWS - S3 Glue hands-on development of reporting applications msExcel RedShift SQL Tableau Desktop /Creator Nice AWS - Lambda AWS - SNS/SQS Tableau API Integration Team: Interflow /DAP Reporting: DAP Reporting is responsible for reports, analytics data on the digital test administration. Interflow works on generating reports for all the digital test administration. These reports are used to understand the health of the administration and report stats for leadership. This team is responsible for providing live reports to check on the status of the exams and provide canned post admin reports to analyze the admins in detail. What You’ll Do Gather, analyze, and interpret data to solve specific use cases. Develop and participate in the review of requirements, data mappings, database specifications documentation and other artifacts and clearly communicate to both the business and technical team members the interpretation of the data. Demonstrate the ability to access, trace, analyze, and remediate data issues leveraging SQL and applicable AWS tools such as Athena and Redshift. Good analytical skills and experience in handling large sets of data Expertise to understand complex queries, proof test, and tweak for the intended use. Critical analysis skills to extract data and analyze the data for inconsistencies and issues. Design and manage information systems, conduct analysis, and generate accurate and comprehensive reports on operational data. Very strong in documentation of data artifacts, design and reports details. Support Architects, Leads and product owners to ensure that all aspects of the information analysis and requirements gathering process are completed with the highest degree of accuracy and quality, including the development and socialization of key project artifacts. Support ad hoc data requests as required as part of DevOps team. Review, extract, and analyze data and information on system processes and procedures. Experience developing testing strategies and identifying comprehensive test scenarios based on business requirements. Demonstrated experience with AWS Lambda, NodeJS, SNS/SQS, S3, IAM, CloudWatch, RDS, DynamoDB, Redshift and Athena. Willingness and ability to perform in multiple roles on a team, including testing and production support in addition to data analysis. Strong ability to understand and internalize the big picture and broader implications. Communicate and understand business goals and requirements, and work to create data solutions that add value to the business. Support team with functional, regression and end to end testing efforts with a focus on flow of data ensuring that features deliver the expected functionality with high quality. Self-starter with ability to set priorities, work independently and attain goals. Develop data-driven reporting solutions from concept to deployment. Strong SQL skills and ability to work with different data sources. Focus on designing and developing effective user-friendly reporting assets including interfaces and outputs. Cultivate and maintain knowledge of College Board products, data systems and information assets. Work with a highly proficient team using Tableau/Tableau Server, SQL, Relational/Columnar Databases, Data Modeling, MS Office Develop scripts to extract data in Athena, Timestream and Redshift. Automate the data extract pull and able to visualize these on charts/graphs/Tableau Exhibit good communication and practical decision-making skills, a believer in good feedback and documentation. About You BA/BS required (major in an analytical field desired) A minimum of 5 to 10+ years work-related experience. Experience in hands-on development of reporting applications for the Web in a professional environment The Ethos of continuous improvement and interest in learning new things. More About You Strong analytical thinking and structured problem-solving ability Ability to handle multiple projects and assignments simultaneously and effectively in a cross-functional team environment. Versed on the agile methodology and best practices. Excellent interpersonal and collaboration skill with the ability to work with a diverse set of colleagues, across functions, from different organizations, disciplines, etc. Self-starter, ability to set priorities, work independently and attain goals",
        "url": "https://www.linkedin.com/jobs/view/3949350622"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Redmond, WA",
        "job_id": 3942855423,
        "company": "Centific",
        "title": "Data Engineer",
        "created_on": 1720638537.1388278,
        "description": "About Centific: Centific expertly engineers platforms and curates multimodal, multilingual data to empower the ‘Magnificent Seven’ and enterprise clients with safe, scalable Artificial intelligence (AI) deployment. Our team includes over 150 PhDs and data scientists, along with more than 4,000 AI practitioners and engineers. We leverage an integrated ecosystem comprised of industry-leading partnerships, and 1.8 million vertical domain experts across 230 locales, to create high-quality pre-trained datasets, fine-tuned industry-specific Large Language Models (LLMs), and Retrieval-Augmented Generation (RAG) pipelines supported by vector databases. Our innovations can reduce Generative Artificial Intelligence (Gen AI) costs by up to 80% and bring Gen AI solutions to market 50% faster. Our mission is to bridge the gap between AI creators and industry leaders by bringing best practices in Generative AI to unicorn innovators and enterprise customers. We aim to help these organizations unlock significant business value by leveraging Generative AI at scale, ensuring they stay at the forefront of technological advancement and maintain a competitive edge in their respective markets. Website - https://www.centific.com/ Job Title: Data Engineer Job Description: Proven experience (3+ years) as a Data Engineer or in a similar role. Experience with big data tools and technologies (e.g., Databricks, Hadoop, Spark, Kafka). Proficiency in SQL and experience with relational databases (e.g., MySQL). Experience with cloud platforms (e.g., Azure, AWS , Google Cloud) and their data services. Strong programming skills in languages such as Python, C#, Java, or Scala. Experience with ETL tools (e.g., Azure Data Factory, Azure Batch, Databricks, etc.). Knowledge of data modeling, data architecture, and schema design. Excellent problem-solving skills and attention to detail. Strong communication and collaboration skills. Ability to work in a fast-paced, agile environment. Benefits offered - comprehensive healthcare, dental & vision, 401k plan, PTO, etc. Centific is an equal-opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, national origin, ancestry, citizenship status, age, mental or physical disability, medical condition, sex (including pregnancy), gender identity or expression, sexual orientation, marital status, familial status, veteran status, or any other characteristic protected by applicable law. We consider qualified applicants regardless of criminal histories, consistent with legal requirements.",
        "url": "https://www.linkedin.com/jobs/view/3942855423"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Jersey City, NJ",
        "job_id": 3938110891,
        "company": "Open Systems Technologies",
        "title": "Data Engineer",
        "created_on": 1720638538.8606923,
        "description": "A financial firm is looking for a Data Engineer to join their team in Jersey City, NJ. Compensation: $115K Responsibilities: Design and develop next generation Client and Corporate applications analytics platforms Work with business partners and as analysts to understand the data requirements, analyze & design the necessary data flows and database design Developing systems for downloading data from various internal and external sources Developing robust quality control processing, monitoring and workflow dashboards Integration of risk and quantitative models Simplification and automation of existing manual processes Provide support for overnight batch jobs Work with a team of frontend and backend engineers, product managers and analysts Qualifications: A Bachelor's or Master's Degree in Computer Science, Engineering, Physics, Math, or related work experience 5+ years of experience programming in SQL queries, stored procedures, query optimization performance tuning (Microsoft SQL Server, PostgreSQL or Oracle) 4+ years of experience in Python 3+ years of hands-on data architecture and data modelling experience 2+ years of Core Java experience 2+ years of experience building highly scalable data solutions using Snowflake, Hadoop, Spark, Databricks 2+ years of experience working in cloud environments (AWS and/or Azure) Create and maintain Conceptual, Logical and Physical data models Experience in architecting, designing, integrating, and understanding the characteristics and specificities of data-intensive systems Identify opportunities to reuse data and reduce redundancy across the enterprise Experience with Git/GitHub Knowledge with DevOps tools like DevOps - Jira, Confluence and CI/CD pipelines (Jenkins) Required Skills: Strong analytical skills Must be willing to take full ownership of projects, covering discovery, analysis, technical design and implementation, testing, and deployment tasks Good communication skills and be comfortable working closely with senior business partners, product owners and in cross-functional teams A strong desire to document and share work done to aid in long term support Must be a self-starter, a dependable partner, and team player Preferred Skills: Experience working in the finance industry Experience with market data vendors - Bloomberg, Markit, ICE/Client, Client, Intex, KBRA Experience working on distributed system and handling & processing of large-scale data (trades, risk, market data etc.)",
        "url": "https://www.linkedin.com/jobs/view/3938110891"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Pennsylvania, United States",
        "job_id": 3959016753,
        "company": "Quantum Technology Recruiting Inc. (QTR)",
        "title": "Data Engineer (Python)",
        "created_on": 1720638540.954441,
        "description": "Exciting Opportunity: Fully remote Data Engineer Are you a skilled Data Engineer looking to make an impact in the Casino Gaming industry? Our client, a top player in this rapidly growing sector, is searching for talented individuals to join their team. This company is renowned for developing innovative games across land-based, online, mobile, and social platforms, enjoyed by players in over 150 countries. They provide a vibrant, multidisciplinary environment that fosters professional growth and collaboration. As a Data Engineer, you will play a key role in creating and maintaining efficient data pipelines, robust databases, and real-time player interaction systems. You will collaborate with diverse teams to ensure data is seamlessly integrated, enabling data-driven insights and enhancing the performance of gaming platforms. Responsibilities: Develop and manage scalable ETL pipelines to handle data extraction, transformation, and loading. Ensure databases are optimized for performance, security, and integrity. Integrate data from various sources to provide a comprehensive organizational view. Monitor data system performance, addressing issues and ensuring scalability. Work with data analysts and business intelligence teams to fulfill data requirements. Implement real-time data processing systems to support interactive player experiences. Qualifications: Bachelor’s degree in Computer Science, IT, or a related field. Proven experience in a Python development role, specifically with data engineering. Proficient in Python and its libraries/frameworks. Strong SQL skills and experience with query optimization. Knowledgeable in data modeling, database architecture, and data warehousing. Experience with ETL processes and relevant tools. Familiarity with cloud services (e.g., AWS, GCP, Azure). Experience with big data technologies (e.g., Hadoop, Spark) is a plus. Understanding of AI tools like Gemini and ChatGPT is a plus. Perks: Work remotely from anywhere (must work during EST timezone) Engage in a stimulating, multidisciplinary work environment. Collaborate with a team of diverse and talented professionals. Contribute to cutting-edge social casino gaming projects. Opportunities for career advancement and professional development. Competitive 401k program",
        "url": "https://www.linkedin.com/jobs/view/3959016753"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3954486547,
        "company": "O'Reilly Auto Parts",
        "title": "Data Engineer II",
        "created_on": 1720638543.043967,
        "description": "Job Summary: We are seeking an experienced and highly skilled Data Engineer II to join our dynamic team. The ideal candidate will possess extensive expertise in data warehousing, ETL processes, cloud-based data platforms, SQL, Python programming, data visualization, and orchestration/scheduling tools. You will play a critical role in designing, developing, and maintaining our data infrastructure, ensuring the seamless integration and transformation of data from various sources to support robust analytics and business intelligence. Key Responsibilities: Data Architecture Design: Lead the design, implementation, and maintenance of scalable data warehouse solutions to meet the organization's data storage and retrieval needs. ETL Process Management: Oversee the development and management of ETL processes using Informatica Intelligent Cloud Services (IICS)/FiveTran to ensure efficient and accurate data transfer between systems. Cloud Data Platforms: Architect and manage cloud data warehouses using Snowflake/BigQuery, ensuring optimal performance, security, and scalability. Data Transformation: Utilize dbt (data build tool) to model, transform, and document data pipelines, ensuring high data quality and consistency. SQL Development: Develop, optimize, and maintain complex SQL queries and scripts to support data extraction, transformation, and loading processes. Python Development: Develop, optimize, and maintain data processing scripts and applications using Python to support various data engineering tasks. Data Visualization: Create and manage data visualizations using tools such as DOMO, Tableau, Power BI, or Looker to help stakeholders interpret complex data insights. Orchestration and Scheduling: Implement and manage orchestration/scheduling tools such as Apache Airflow, Prefect, or similar to automate and monitor data workflows. Team Leadership: Mentor and guide junior data engineers, fostering a culture of continuous learning and improvement. Cross-functional Collaboration: Work closely with data analysts, data scientists, and other stakeholders to understand data requirements and deliver robust data solutions. Performance Tuning: Monitor and enhance the performance of data systems to ensure high availability and fast query responses. Documentation and Standards: Maintain comprehensive documentation for data models, ETL processes, and data infrastructure while establishing best practices and standards. Security and Compliance: Implement and manage data security protocols and ensure compliance with relevant regulations and best practices. Required Skills and Qualifications: Educational Background: Bachelor’s degree in Computer Science, Information Technology, Engineering, or a related field. Experience: Minimum of 5-7 years of experience in data engineering, data warehousing, and ETL development, with at least 2 years. Technical Proficiency: Expertise in designing and managing data warehouses. Advanced proficiency in ETL tools like Informatica Intelligent Cloud Services (IICS), Fivetran, Azure Data Factory, Google Cloud Dataflow etc. Extensive hands-on experience with Snowflake for data warehousing and management. Proficient in dbt for data modeling and transformation. Strong knowledge of Google BigQuery and its ecosystem. Proficiency in SQL and database management. Advanced skills in developing, optimizing, and maintaining complex SQL queries and scripts. Advanced skills in Python for data processing and automation. Experience with data visualization tools such as DOMO, Tableau, Power BI, or Looker. Experience with orchestration/scheduling tools such as Apache Airflow, Prefect, or similar. Experience with data observability tools to monitor data pipeline health and ensure data quality. Knowledge of data governance principles and experience in implementing governance policies and procedures. Experience with CI/CD practices for data engineering workflows. Leadership Skills: Proven ability to lead and mentor a team of engineers, manage projects, and deliver results. Analytical Skills: Strong problem-solving skills with the ability to analyze complex data requirements and develop efficient solutions. Communication Skills: Excellent written and verbal communication skills to effectively collaborate with team members and stakeholders. Preferred Qualifications: Experience with other cloud platforms (e.g., GCP, AWS, Azure) and their data services. Knowledge of data governance and data quality frameworks. Experience with version control systems like Git.",
        "url": "https://www.linkedin.com/jobs/view/3954486547"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3943543072,
        "company": "Cypris",
        "title": "Data Engineer",
        "created_on": 1720638544.8227627,
        "description": "About Cypris: At Cypris, we're building the single ecosystem for global innovation data. Cypris is an AI-powered research tool that centralizes data sources like scientific papers, global patents, market news, and company data into one platform. Cypris arms users with access to unique insights from 500M+ global data points, answering key questions about their market, competitors, core technologies & more to support new product development, commercial strategy & accelerate global innovation. We're connecting R&D teams to the global innovation landscape like the Bloomberg Terminal did for the finance world & Pitchbook for venture capital. Current users include leading R&D & innovation teams at mid-size to Fortune 100 companies operating in emerging markets like aerospace, genomics, cancer research, autonomous vehicles, and more. About the Role: In this role, you will be responsible for designing, building, and maintaining scalable data pipelines and systems to support our data-driven platform. You will work closely with other members of our engineering team to ensure the availability and quality of data required to supply innovation analytics and insights. This is an exciting opportunity to contribute to our data infrastructure and help shape the future of our data capabilities. In This Role You Will: Design, develop, and optimize robust data pipelines to process and transform large datasets from various sources Optimize data stores' performance in terms of index response times and query response times Implement and maintain ETL processes to ensure data accuracy and integrity Collaborate with cross-functional teams to understand data requirements and deliver effective data solutions Develop and maintain data warehouses and data lakes to support business intelligence and analytics Monitor and troubleshoot data pipeline performance and reliability, implementing improvements as needed Ensure data security and compliance with relevant regulations and standards Stay up-to-date with the latest technologies and best practices in data engineering and incorporate them into our processes Requirements A Key Candidate Will Have: 7+ years of proven experience as a Data Engineer or in a similar role Bachelor's or Master's degree in Computer Science, Engineering, or a related field Proficiency in programming languages like Python, Java, or Scala Experience with cloud platforms such as GCP (preferred), AWS, Google Cloud, or Azure Hands-on experience with big data technologies such as Hadoop, Spark, or similar frameworks Knowledge of data warehousing concepts and experience with tools like Redshift, BigQuery, or Snowflake Familiarity with ETL tools and processes Strong problem-solving skills and attention to detail The desire to contribute and grow at an early-stage startup Technologies We Use: Python GCP Apache Beam MongoDB Elasticsearch Benefits Through this role, you will get: A strong base salary, bonus structure + equity To have your voice & opinion heard Proper training to be armed with the right knowledge to find success in our market To help drive the future growth of our platform and team How do I get in touch? Please apply & we'll be in touch! If your resume and background appear like a good match, we will reach out for an initial phone screen. Where are Cypris's offices located? Cypris has offices in New York, Los Angeles, and Boulder. What are Cypris' future hiring plans? We're always looking for the best talent across departments. We will regularly post new job opportunities on the Cypris Careers page, Wellfound, or LinkedIn. How does Cypris make money? We sell annual access to our dashboard to R&D, innovation & IP teams looking to better understand the innovation landscape. We also build custom reports for objective-oriented projects. Both can be included in one package. What does the interview process look like? The process depends on the role. For all positions, we'll start with an introduction call with our Talent team. From there, the interview process will vary by role. We typically do a longer call with one of our team members who can appropriately vet your skills & \"speak your language\" - on the engineering side, this might mean some technical questions, while on the sales side, we'll ask you to build a quick presentation about yourself. Finally, we'll check over a few references. If you're near one of our offices, we may give you the opportunity to meet in person.🚀",
        "url": "https://www.linkedin.com/jobs/view/3943543072"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Roanoke, VA",
        "job_id": 3969220215,
        "company": "Titan America",
        "title": "Data Engineer",
        "created_on": 1720638548.4918933,
        "description": "Data Engineer Roanoke, VA Titan America LLC, a leading company in the heavy building materials industry in the eastern United States and the North American operating subsidiary of the TITAN Group, is on a growth journey towards becoming the best-in-class vertically integrated cement producer, driving innovation and operating excellence. In the last ten years Titan America has grown from $600MM to 1.6BN in revenue and we expect to continue this trajectory through organic growth and acquisition. TITAN was founded in Greece in 1902, are now publicly listed and operate across 15 countries in five strategic regions—Greece & Western Europe, U.S.A., Southeastern Europe, Eastern Mediterranean, and Brazil. At the heart of Titan America's operations is the production of low-carbon cement, construction aggregates, a broad range of high-performance ready-mix concrete products, and concrete block. We also play a pivotal role in beneficiating, processing and distributing fly ash for industrial applications, underlining our commitment to sustainable practices. Our strong values-based culture helps our employees shape their interactions with customers, suppliers, and the communities we proudly operate in. We are committed to the environment and the mitigation of climate change through our extensive decarbonization efforts and the development of green products. We are the number one player in the key markets in Florida, North and South Carolina, Virginia, and have leading positions in Metropolitan New York and New Jersey. Our team, now numbering just over 2,500, is the driving force behind our success. Our assets span the United States, with two cement production facilities, three cement import terminals, a network of rail distribution terminals, six aggregate quarries/mines, over eighty company-owned ready-mix concrete plants, and ten fly ash processing/distribution sites. Embracing a high-performance culture, Titan America champions respect for people, society, and the environment. Our vision is clear—to become the best-in-class vertically integrated cement producer in our served markets. Title: Data Engineer Reporting: Sr. Manager of Industrial Digitalization Products Location: Roanoke, VA Position Overview: We are looking for a skilled and motivated Data Engineer to join our team in Roanoke, VA. This position will be responsible for designing, developing and maintaining scalable data pipelines and transformations for business-critical use cases. Key Responsibilities: Design, develop and maintain scalable data pipelines for extracting, transforming, and loading data from various sources Collaborate with cross-functional teams to identify data needs and determine the best data solutions Develop and implement data models to support business requirements and ensure data quality Ensure the security and privacy of sensitive data by implementing appropriate access controls Monitor and optimize data pipeline performance to ensure timely and accurate data delivery Document data pipeline processes, data dictionaries, and data storage solutions Qualifications: Bachelor's degree in Computer Science, Computer Engineering, or a related technical field Minimum of five years of professional experience working as a Data Engineer or Software Developer Strong hands-on experience with data warehouse and transformation solutions, i.e. Domo, Snowflake or similar Proficient in at least one scripting language such as Python, JavaScript, or R Understanding of data modeling, data integration and data quality processes Familiarity with cloud platforms such as AWS, Azure, or Google Cloud Platform Strong analytical and problem-solving skills Full Stack Software Development experience in a professional setting is highly desired, but not required What We Offer: Company Values: Be a part of a company that values integrity, sustainability, and ethical business practices. We are committed to making a positive impact on our industry and the world. Competitive Compensation: We believe in recognizing and rewarding your contributions. Comprehensive Benefits: We care about your health and well-being. Our benefits package includes comprehensive medical, dental, and vision coverage, as well as a 401(k) plan with company matching. Supportive Culture: Our inclusive and collaborative culture fosters teamwork and creativity. We value diversity and provide a platform for all voices to be heard. Community Engagement: We believe in giving back. You'll have opportunities to participate in volunteer programs and community outreach initiatives. Success begins with hiring the right people to partner with us as we grow and develop our businesses. People are central to everything we do. It is through their efforts and talents that Titan has been successful for over 100 years. Join our team and help us lead the way in environmentally responsible cement manufacturing! Titan America is an Equal Employment Opportunity (EEO) / Affirmative Action employer and welcomes all qualified applicants. Applicants will receive fair and impartial consideration without regard to race, sex, color, religion, national origin, age, disability, veteran status, genetic data, or other legally protected status.",
        "url": "https://www.linkedin.com/jobs/view/3969220215"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Fountain Hills, AZ",
        "job_id": 3952570115,
        "company": "Zortech Solutions",
        "title": "Data Engineer (Data Masking)",
        "created_on": 1720638550.2349916,
        "description": "Role: Data Engineer (Data Masking) Location: Scottsdale AZ (100% Onsite) Duration: 6+ Months Job Description Must have : Python , Scala , Spark , SQL (Aerospike or Solr) Should be a having good hands-on working experience in Python and Scala Should be having good hands-on working knowledge on spark framework, spark SQL, spark streaming and spark performance tuning Should be having experience in Aerospike or Solr. Should have deep understanding of big data systems. Ansible knowledge is good to have.",
        "url": "https://www.linkedin.com/jobs/view/3952570115"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Greater Boston",
        "job_id": 3956387885,
        "company": "BlueSnap",
        "title": "Data Engineer",
        "created_on": 1720638551.8427012,
        "description": "BlueSnap is a rapid growth international FinTech company, headquartered in Waltham, MA, with offices in Israel, Ireland, and the UK. We are thrilled to be one of the fastest growing companies in the payments industry. Our team works collaboratively building a world-class all-in-one payments orchestration platform that provides our clients with extensive capabilities, technologies, and services, all with only one integration. BlueSnap is looking for an enthusiastic Data Engineer who will work as a member of our global Business Intelligence team. This is a hybrid role based from our Waltham, MA office. In this role you will have the opportunity to: Develop and maintain BlueSnap's Business Intelligence and Safeguarding solutions Implement and optimize ETL processes to ensure efficient data integration from various sources Explore ways to enhance data quality, reliability, and performance Monitor and troubleshoot data issues, ensuring data accuracy Analyze and organize raw and structured data from different sources Build algorithms and prototypes Develop analytical tools and programs to automate data processing Collaborate with BI Engineers and Finance team members on several projects Write and maintain technical documentation on JIRA, ensuring clear and comprehensive records of all development activities Work with teams across the globe to build efficient and scalable solutions Be available for on-call responsibilities as needed to address urgent issues and ensure system availability Qualifications: Degree in Information Systems or any Business Intelligence related discipline 4+ years professional experience as a Data Engineer or BI Developer Technical expertise with data models, data mining, and segmentation techniques Advanced SQL skills for complex queries and database management. Hands-on experience with data integration tools like Oracle Data Integrator, Airflow etc. Proficiency in Groovy for data manipulation, analysis, and efficient ETL coding Experience in Cloud DevOps, including storage management, bucket configuration, and overall cloud monitoring Good analytic skills Ability to independently troubleshoot Strong team player skills and the ability to work harmoniously with diverse employees Good oral and written communication skills, including ability to communicate complex ideas in a simple way Experience in Oracle’s BI and EPM product suites is an advantage Experience with Oracle Account Reconciliation (ARCS) is an advantage Database maintenance skills is an advantage As a regular full-time BlueSnap team member you will receive a competitive salary along with an excellent benefits package which will include BlueCross BlueShield medical and dental insurance, FSA, HRA, vision, life, disability and more! You will have the opportunity to save for retirement through our 401K. We find some of our best team members through employee referrals, which is why we provide you with the opportunity to earn significant referral bonuses. In addition, we provide our US team members with a flexible time off plan that will help you enjoy a nice work/life balance. These are just a few of the great benefits we offer. We look forward telling you more during the interview process with BlueSnap! BlueSnap is an equal opportunity employer. We celebrate differences in both background and perspective. All our applicants are considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran, or disability status. We support equality of treatment in employment and are committed to having procedures to determine equal pay for our employees that do not discriminate and are free from bias.",
        "url": "https://www.linkedin.com/jobs/view/3956387885"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Miami, FL",
        "job_id": 3932049861,
        "company": "hear.com",
        "title": "Data Engineer",
        "created_on": 1720638553.5437446,
        "description": "We're hear.com, a tight-knit crew of tech enthusiasts, creators, and innovators dedicated to shaking things up in hearing care. We're not your average company—we thrive on creativity, collaboration, and a passion for making a real impact. Forget the corporate jargon and stiff office culture; we believe in flexibility, growth, and always having fun while doing what we love. As a Data Engineer at hear.com you will play a pivotal role in architecting, building, and optimizing our data infrastructure. You will collaborate with a cross-functional team of experts to design and implement robust, scalable, and high-performance data solutions that empower our decision-making and contribute to our mission of providing better hearing care. What You’ll Do Design and implement scalable and reliable data pipelines, incorporating streaming data processing with Apache Pulsar to handle real-time data efficiently. Optimize Data Systems: Work closely with peers, analysts, and product teams to improve data reliability, efficiency, and quality. Innovate: Stay ahead of the curve by researching and implementing cutting-edge technologies that can enhance our data capabilities and leverage our business. Data Governance and Strategy: Play a key role in defining and implementing data governance and security policies to ensure data integrity and compliance. Technology Stack Languages & Frameworks: Python, SQL Data Warehousing & Streaming: Snowflake, Pulsar, Kinesis Workflow Management: Airflow Databases: PostgreSQL, MongoDB Containers & Orchestration: Docker Infrastructure as Code: Terraform Cloud: AWS What You’ll Need Experience: 5+ years of experience in data engineering with a proven track record of building and optimizing data systems. Technical Expertise: Strong programming skills in Python and SQL. Hands-on experience with our technology stack is highly desirable. Problem-Solving Skills: Ability to tackle complex data challenges and deliver innovative solutions. Team Player: Excellent communication skills and the ability to work effectively in a collaborative environment. Continuous Learner: Passion for learning and adapting to new technologies and methodologies. Why You’ll Love Working With Us Global Vision and Growth: Be part of a company with a long-term vision and robust growth trajectory. Customer Impact: Work with happy and grateful customers every single day. Creative Environment: An open-minded and international working environment that fosters creativity. Growth Mindset: Access to courses, conferences, and more to support your continual learning and development in an innovative, fast-moving company. Autonomy and Responsibility: Enjoy a high degree of autonomy and responsibility from day one. Unique Culture: Innovative, driven, and family-like work culture. Hybrid Schedules: Enjoy the flexibility of working 2 days remotely and 3 days in our beautiful office in Coral Gables, Florida or Denver, Colorado. Excellent Benefits and Compensation: Full medical, dental, vision, open PTO, paid company holidays and sick time, paid parental leave, and matching 401K program. Base salary range $140k-$150k, dependent on experience. Great Perks: Weekly lunch, a fully stocked kitchen, and a small, close-knit team that values ownership and collaboration. At hear.com, we're not just shaping the future of hearing care; we're redefining it. Join us in our mission to help everyone hear well to live well.",
        "url": "https://www.linkedin.com/jobs/view/3932049861"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Charlotte, NC",
        "job_id": 3919052266,
        "company": "Syntricate Technologies",
        "title": "Data Engineer",
        "created_on": 1720638555.2577884,
        "description": "Hi, Hope you are doing well Number of position : 5 Only Full Time Full-time I, Shakib (i3 infotek) would like to share a job opportunity as Data Engineer based in Charlotte, NC (Onsite) location for a Full-time position. In case, if you are not comfortable with this location, please share your preference with contact details for further requirements *** Kindly find the JD below and let me know if you are available for the same. Job tittle - Data Engineer Duration: Full-time Location - Charlotte, NC (Onsite) Job Description Role: Data Engineer In this role you will serve as a senior dedicated technical resource responsible for Oracle PL/SQL development for our Data Warehouse running on a Linux based Oracle 11g RAC 2 node cluster. The Data Warehouse is currently under development and you will be an integral part of its design and implementation. You will support key strategic initiatives, including the integration of new lines of business, design and implementation of data marts and subject areas, and key decisions around the future of the data warehouse. You will also be involved with the implementation of new reporting tools, new ETL routines and other projects that are core to developing and enhancing the data warehouse environment. Essential Job Functions Utilize excellent hands-on PL/SQL skills to design and implement data warehouse subject areas. Develop high performing extraction, transformation and loading (ETL) routines for loading data from our production database into the data warehouse. Create PL/SQL that performs well with large data sets, is written to scale for future growth and adheres to business rules. Effectively integrate new lines of business into the DW requiring the addition of new data sources into existing tables and data marts. Share knowledge and PL/SQL expertise and mentor team members in the design of efficient reporting queries. Team with engineering, IT and business stakeholders to support projects of varying complexity; gather reporting and data warehousing requirements, evaluate best approach and propose solutions. Please reply me with your updated resume and required details: Full Name Best number to reach you: Work Authorization/Visa Status Current Location: Expected Compensation Best time to call you: Waiting for your earliest response Sincerely, Mohd Shakib Sr. Technical Recruiter Direct: 781-896-2153 Address: 1500 District Avenue, Ste. 4135, Burlington, MA 01803",
        "url": "https://www.linkedin.com/jobs/view/3919052266"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3945561637,
        "company": "Compass Group USA",
        "title": "Data Engineer",
        "created_on": 1720638557.2222393,
        "description": "Who We Are Compass Technology is a dedicated internal team for Compass Group delivering enterprise-wide initiatives that support our diverse customer base and enhance our business operations. Our domain encompasses a vast spectrum of opportunities, from hands-on desk support to Cybersecurity, Cloud Engineering, AI, and Modern Application development. We are committed to building robust IT infrastructures, driving digital transformation, and much more. Compass Group is the leading foodservice management and support services company, with $26 billion in revenue in 2023. In 2023, Compass Group was named one of Forbes’ America’s Best Large Employers along Springbuk’s Healthiest 100 Workplaces in America (since 2019). Job Summary We're looking for a hands-on, collaborative Data Engineer to join our Data Services team. In this role you'll have the opportunity to build and troubleshoot ingestion pipelines for multiple sources of data. You will also work closely with our business intelligence and reporting solutions team to help develop valuable data models and visualizations for the business. We’re currently leveraging AWS cloud and using a range of AWS services like S3, DynamoDB, Athena, Redshift, SQS, Lambda and Glue. However, we are migrating to Snowflake as our primary data warehouse platform for new projects and developing a new data pipeline architecture Job Responsibilities Work with data analysts to build pipelines for required projects and requests Define, build, test, and implement scalable data pipelines using python and SQL Monitor pipeline performance and efficiency Troubleshoot data related issues including missing data, poor data quality, etc. Collaborate with analytics and reporting teams to develop data models that feed business intelligence tools Maintain code via CI/CD process as defined in our Azure DevOps platform Job Qualifications 4+ years in an ETL or Data Engineering roles; building and implementing data pipelines and modeling data Leverage Python to create maintainable, reusable, and complex functions Strong SQL skills to write efficient queries and improve inefficient ones Familiarity with AWS data services: S3, DynamoDB, Athena, Redshift, Lambda, Glue, SQS, SNS, and API Gateway Experience managing Snowflake instances, including data ingestion and modeling Experience with IBM DataStage designing, building, and maintaining data pipelines Highly self-motivated and directed with an attention to detail Persuasive and professional communication skills (presentations, documentation, and emails)",
        "url": "https://www.linkedin.com/jobs/view/3945561637"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Charlotte, NC",
        "job_id": 3951771595,
        "company": "HatchPros",
        "title": "Data Engineer",
        "created_on": 1720638558.884826,
        "description": "W2 only (USC/GC) Zoom Hybrid Data Engineer (ETL/Lambda/Glue/Kafka/SQL/AWS) Raleigh, NC or Charlotte NC – MUST BE LOCAL TO EITHER MARKET (Raleigh and Charlotte) Please Fill The Skill Matrix YEARS OF EXPERIENCE (required to include per the manager) ETL Kafka Lambda Glue AWS SQL Coding/SSIS Python (nice to have) On call schedule: There is a 24x7 on call rotation that works out to be about 5 weeks a year. This person will be the escalation support which is typically 1 ticket a week during on call. Required Qualifications Ideal candidates have been an ETL Developer previously….but that is not required Must be proficient in the following with 8+ years of IT experience: 7+ years experience AWS cloud programming language experience in Kafka – Python is nice to have Must have hands on experience with the following: Glue, AWS, Lambda, Kafka and ETL 7 years experience in ETL/API Writing and SQL coding / SSIS Bachelor degree in Computer Science or a related discipline. Must be able to Lift and shift existing data products with Glue",
        "url": "https://www.linkedin.com/jobs/view/3951771595"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Greater Chicago Area",
        "job_id": 3970344894,
        "company": "Tardis Tech",
        "title": "Data Engineer",
        "created_on": 1720638560.5041313,
        "description": "Company: A prominent financial technology firm, committed to leveraging data and technology to enhance their platform. They are driven by a passion for data, technology, and finance, and are seeking a talented Data Engineer who shares our enthusiasm. Role Overview: As a Data Engineer you will play a crucial role in developing and improving the data product. You will be responsible for designing and implementing ETL pipelines, optimizing data warehouse architecture, streamlining data processing, generating reports, and automating data quality assurance. We’re looking for individuals who are not only proficient in data engineering but also have a keen interest in financial markets. Job Responsibilities: Build critical data pipelines for our trading platform, focusing on automation to enhance efficiency. Conduct research to understand various financial datasets and historical trading data. Collaborate with trading desks, clients, and data vendors to meet data requirements effectively. Automate data collection, validation, reconciliation, and quality checks to ensure data integrity. Identify and implement new processes to enrich data within the data request process. Take ownership of technical projects, leading their design and development to successful outcomes. Requirements: Bachelor's or advanced degree in a quantitative field such as Mathematics, Economics, Physics, Computer Science, or Electrical Engineering. Proficiency in Python, C#, WPF, SQL, with experience in AWS, cloud computing, Hadoop/HDFS, PySpark, Hive, and Airflow preferred. Familiarity with data management and ETL pipelines development is desirable. Understanding of large-scale applications in distributed environments is advantageous. Experience with financial datasets and financial reporting, particularly FINRA CAT, is highly preferred.",
        "url": "https://www.linkedin.com/jobs/view/3970344894"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Ohio, United States",
        "job_id": 3960599705,
        "company": "Vernovis",
        "title": "Data Engineer",
        "created_on": 1720638562.2181444,
        "description": "Job Title: IT Data Engineer Location/ Work Structure : Remote Who we are: Vernovis is a Total Talent Solutions company that specializes in Technology, Cybersecurity, Finance & Accounting functions. At Vernovis, we help these professionals achieve their career goals, matching them with innovative projects and dynamic direct hire opportunities in Ohio and across the Midwest. Client Overview: Vernovis partnering with a global IT Platform services company to onboard an IT Data Engineer. This resource will work heavily on enterprise migrations work and the creation/development of new data models and platforms. If interested please contact Travis at tbrush@vernovis.com What You'll Do: *Heavy data migration from previous platforms into new internal platforms (CRM) *Heavy SQL coding for the retrieval of enterprise data *Platform modeling Work with the IT Data Engineering Team and assist in developing the next generation data and analytics infrastructure. Must have strong SQL modeling skills (dbt is a plus) Write high quality SQL code to retrieve and analyze data from database tables (primarily Databricks) Develop high quality SQL models for ad-hoc requests, as well as ongoing reporting / dashboarding. Work directly with business stakeholders to translate between data and business needs. Continually improve SQL models through automating or simplifying self-service support for datasets What Experience You'll Have: Python, SQL, Spark and Databricks experience Experience with data migration and platform modeling Visualization tools experience Agile environment experience What Experience is Nice to Have: Specific experience with Data Warehouse/Data Lake configuration and development using Databricks platform. Experience with Tableau / Sigma Computing Experience operating in an Agile development environment. Familiarity with usage of Agile tools (JIRA / Confluence) Understanding of CI/CD deployment models and release strategy as well as SCM tools (Git preferred) and code management best practices. Experience in AWS environment. Experience with cloud ELT platforms such as AWS Glue, Talend Stitch, or FiveTran",
        "url": "https://www.linkedin.com/jobs/view/3960599705"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3942965372,
        "company": "Envision",
        "title": "Data Engineer",
        "created_on": 1720638564.1511722,
        "description": "NO C2C and NO SPONSORSHIP AVAILABLE The Data Engineer plays a leading role designing and running multiple data platforms across the enterprise. This is a ‘hands on’ role and is responsible for leading the transformation and execution of data solutions from disparate legacy systems to modern cloud-based solutions with a focus on enterprise data management. This position leverages cloud-based (Azure) infrastructure to implement technology solutions that are scalable, resilient, and efficient. This role collaborates with Technology Product Owners, Solution Architects, DBAs, and other cross-functional teams and business leaders. In addition, the Lead Data Engineer/Architect will plan, design, implement and operate data solutions, using an Agile methodology. RESPONSIBILITIES Leads the development of future state data architecture designs, standards, guidelines, and principles. Works with Technology Product Owners to understand functional requirements and interact with other cross-functional teams to architect data solutions. Executes design sessions to gather requirements, review, approve, and communicate design artifacts with stakeholders. Utilizes Microsoft Azure technologies to solve business problems with a focus on enterprise data management. Designs and manages data models associated with Relational DBs and optionally NoSQL DBs (Azure Cosmos, MongoDB, etc.). Designs, implements, and maintains database solutions, manages data access, and resolves database performance, capacity, and security issues. Performs problem-solving of application issues and production errors, including high level critical production issues that require immediate attention. Develops Data Flow Diagrams (DFDs), Data Dictionaries, and database schemas with a focus on enterprise data management. Designs and codes SQL queries and maintenance of SSIS packages. Participates in an on-call support rotation and provide non-standard work hour support Participates in brainstorming and discussion sessions to help development teams better understand and utilize data technologies. SKILLS Bachelor's Degree preferably in Computer Science, Information Technology, or related IT discipline or equivalent experience. 7+ years of experience developing Data Flow Diagrams, Data Models, and Data Architectures both on-prem and in the cloud, with at least 3-5 years utilizing Microsoft Azure. 5-7 years of experience designing and supporting Azure hosted data services. Successful completion of the Microsoft Azure Data Engineer Associate Certification. Additional Azure Certifications and/or Exams are preferred. 3-5 years of experience working in a multi-business unit organization. Experience with API code integrations with external vendors to push/pull data. Experience with SQL Server and Microservices. Ability to create and maintain organized technical documentation. Advanced organizational skills with the ability to manage multiple assignments. Strong interpersonal, written, and verbal communication and presentation skills. Reasoning ability to solve practical problems and deal with a variety of variables in many different situations. Experience with Azure Cloud Technologies including a track record of learning new technologies and architecting them to solve business problems. Demonstrated commitment to continuous learning within the enterprise architecture field.",
        "url": "https://www.linkedin.com/jobs/view/3942965372"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Boulder, CO",
        "job_id": 3965694976,
        "company": "Inceed",
        "title": "Data Engineer",
        "created_on": 1720638566.1372824,
        "description": "Compensation: $110K - $150K Location: [Boulder, CO] Data Engineer Inceed has partnered with a city municipality here in the Denver Metro to help find a skilled Data Engineer to join their team! In this role, you will support the analytic, application and reporting data workflows while also design and strategizing for enterprise database systems. You will be the expert in standards for maintenance, operations, programming, and security in both on-premises and cloud data environments. Responsibilities: Data engineering- lead database and data flow design and implementation Improve data management, data integration, and application integration functionality across the enterprise Provide expert advice and support in database administration Required Qualifications & Experience: Expert level in T-SQL or PL-SQL Azure Synapse and Microsoft Azure data warehouse solutions SSIS Development experience Knowledge of Python Data Pipelines Data Warehousing Comfortable working with legacy systems Nice to Have Skills & Experience: Experience in government, non-profit, or educational entity Understanding of backup and recovery Strong troubleshooting/ performance tuning Ability to mentor and train DBA Perks & Benefits: Hybrid remote (2 days a week onsite) Other Information: Background Check Completion of Criminal Justice Information Systems certification post offer If you are interested in learning more about the Data Engineer opportunity, please submit your resume for consideration. Our client is unable to provide sponsorship at this time. We are Inceed, a staffing and direct placement firm who believes in the possibility of something better. Our mission is simple: We’re here to help every person, whether client, candidate, or employee, find and secure what’s better for them. Inceed is an equal opportunity employer. Inceed prohibits discrimination and harassment of any type and affords equal employment opportunities to employees and applicants without regard to race, color, religion, sex, sexual orientation, gender identity or expression, pregnancy, age, national origin, disability status, genetic information, protected veteran status, or any other characteristic protected by law.",
        "url": "https://www.linkedin.com/jobs/view/3965694976"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3952360342,
        "company": "DivIHN Integration Inc",
        "title": "Data Engineer (REMOTE)",
        "created_on": 1720638567.9613345,
        "description": "DivIHN (pronounced “divine”) is a CMMI ML3-certified Technology and Talent solutions firm. Driven by a unique Purpose, Culture, and Value Delivery Model, we enable meaningful connections between talented professionals and forward-thinking organizations. Since our formation in 2002, organizations across commercial and public sectors have been trusting us to help build their teams with exceptional temporary and permanent talent. Visit us at https://divihn.com/find-a-job/ to learn more and view our open positions. Please apply or call one of us to learn more For further inquiries regarding the following opportunity, please contact our Talent Specialist Lavanya at 224 369 0873 Title: Data Engineer (REMOTE) Location: Remote Job Description Client is seeking a dynamic, motived, energetic and driven Data Engineer! Purpose And Scope Design, develop and implement enterprise data movement, transformation, and storage to facilitate application data migration / transfer and reporting structures. Utilizes technologies to facilitate enterprise data pipelines, spark notebooks, dataflows, data lakes, SQL Serverless, and data warehouse processes and artifacts from internal and external application data sources. Essential Responsibilities Focuses on data as an entity and does analysis on data in terms of content, integrity, and security. Works on design and implementation of data flows/pipelines – focuses on control and optimization of data movement and transformation. Works on data integration between different systems or source/sink requirements. Specific Job Duties Design, develop, test, and manage the overall framework to facilitate analysis and processing of enterprise data working closely with the Data Architect to direct and optimize the flow of data within the framework and ensure consistency of data delivery and utilization across multiple projects. Design how data will be stored, accessed, used, integrated, and managed by different data regimes and digital systems, working with data users to determine, create, and populate optimal data architectures, structures, and systems. Recommend and implement ways to improve data reliability, efficiency, and quality; shall evaluate, compare, and improve the different approaches including the design patterns innovation, data lifecycle design, data ontology alignment, annotated datasets, and elastic search approaches. Process, clean, and verify the integrity, accuracy, completeness, and uniformity of enterprise data sets, integrating external or new datasets into existing datasets, as required. Design, implement, and operate data management systems for business intelligence needs. Plan, design, and optimize data throughput and query performance. Build data and analytics proofs that will offer deeper insight into datasets, allowing for critical discoveries surrounding key performance indicators and user activity. Perform research and analysis of large data sets to include operational data and perform data validation and visualization and other statistical analyses. Support change management activities for enterprise data analysis. Document all processes, models, and activities. Perform all other position related duties as assigned or requested. Work Environment, Physical Demands, And Mental Demands Typical office environment with no unusual hazards, occasional lifting to 20 pounds, constant sitting while using the computer terminal, constant use of sight abilities while reviewing documents, constant use of speech/hearing abilities for communication, constant mental alertness, must possess planning/organizing skills, and must be able to work under deadlines. Quality - Quality is the foundation for the management of our business and the keystone to our goal of customer satisfaction. It is our policy to consistently provide services that meet customer expectations. Accordingly, each employee must conform to the Amentum Quality Policy and carry out job activities in compliance with applicable Amentum Quality System documents and customer contracts. Each employee must read and understand his/her Quality Management and Customer Satisfaction responsibilities. Procedure Compliance - Each employee must read, understand, and implement the general and specific operational, safety, quality and environmental requirements of all plans, procedures and policies pertaining to his/her job. Minimum Position Knowledge, Skills, And Abilities Required Bachelor's degree in Computer Science or related field and 2-4 years of experience. Excellent communications and analytical skills; demonstrated working knowledge and experience of several of the following technologies/tools is desired: Microsoft SQL (T-SQL) Oracle SQL (PL-SQL) Azure Synapse Azure ADLS Azure Pipelines Azure Spark Notebooks (SCALA, Python, Spark SQL) REST API JSON Files Parquet Files Delta Lake Files Data Vault 2.0 SQL Serverless Synapse Data Warehouse (MPP - Dedicated) Microsoft DevOps and GitHub Agile Development Data Encryption / Decryption About Us DivIHN , the 'IT Asset Performance Services' organization, provides Professional Consulting, Custom Projects, and Professional Resource Augmentation services to clients in the Mid-West and beyond. The strategic characteristics of the organization are Standardization, Specialization, and Collaboration. DivIHN is an equal opportunity employer. DivIHN does not and shall not discriminate against any employee or qualified applicant on the basis of race, color, religion (creed), gender, gender expression, age, national origin (ancestry), disability, marital status, sexual orientation, or military status.",
        "url": "https://www.linkedin.com/jobs/view/3952360342"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Lakewood, CO",
        "job_id": 3970550821,
        "company": "Inceed",
        "title": "Data Engineer",
        "created_on": 1720638569.618843,
        "description": "$125k-$140k/yr (DOE) Lakewood, CO (Hybrid- 2x a week onsite) Data Engineer Inceed has partnered with a great company to help find a skilled Data Engineer to join their team! This person will be responsible for the development and maintenance of enterprise systems that collect and store data. Responsibilities: Create, test and maintain data pipeline architectures and workflows Develop, design and implement the overall data strategy Leverage WhereScape and other ETL tools to store data into data warehouses Train and mentor new employees Required Qualifications & Experience: 5+ years of related experience Experience using WhereScape or other similar ETL tools Oracle database experience Perks & Benefits: Health, dental, and vision insurance 401k Volunteer opportunities Great PTO policy Amazing growth opportunity within the company Other Information: Named one of the top places to work in Colorado several years in a row Hybrid work schedule (Mondays and Tuesdays in office) This position requires a background check and credit check If you are interested in learning more about the Data Engineer opportunity, please submit your resume for consideration. Our client is unable to provide sponsorship at this time. We are Inceed, a staffing and direct placement firm who believes in the possibility of something better. Our mission is simple: We’re here to help every person, whether client, candidate, or employee, find and secure what’s better for them. Inceed is an equal opportunity employer. Inceed prohibits discrimination and harassment of any type and affords equal employment opportunities to employees and applicants without regard to race, color, religion, sex, sexual orientation, gender identity or expression, pregnancy, age, national origin, disability status, genetic information, protected veteran status, or any other characteristic protected by law.",
        "url": "https://www.linkedin.com/jobs/view/3970550821"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Brentwood, TN",
        "job_id": 3958835034,
        "company": "Ncontracts",
        "title": "Data Engineer",
        "created_on": 1720638571.4704301,
        "description": "Data Engineer Remote | Product and Development | Full-Time At Ncontracts we help financial institutions make better decisions. This is achieved by creating tools that help them understand their risks and compliance concerns and provide pathways for them to mitigate them. We also have teams of industry leading experts to walk them through the process. This is an individual contributor role, working closely with other data team members to complete the building out of ETL pipelines and Operational Data Store (ODS). Your day-to-day tasks will include extracting data from Ncontracts applications, transforming it according to business logic, and efficiently loading it into our Operational Data Store (ODS). You will work with members from the cross-functional team to help them with the consumption of the data. You will be expected to use your expertise to optimize the process for performance and accuracy. Your commitment to continuous improvement will drive enhancements in our data architecture and ensure that our data system can scale. A successful Data Engineer at Ncontracts will: Contribute to building out ETL pipeline to include data from all Ncontracts products Write performant and maintainable code Ensure the quality of ETL process Communicate well with other members of the data team and across the functional area Advocate for best practices and continuous improvement Participate in architecting and building a scalable data system Mentor team members on their area of expertise It is expected you will have the following skills and experience: 3+ Years of experience as Data Engineer designing and implementing data warehouse and data lake solutions on Microsoft Azure. Experience with Azure Data Factory is required Azure Data Lake is a plus Exposure to Microsoft Fabric is a plus Very strong background in MS SQL Server is required Expertise in data architecture using different database types and data formats Expertise in building data pipelines to clean, enrich, and transform data Experience with Python is a plus and highly desirable Expertise in database design and tuning techniques Strong understanding of ETL process and tooling Experience with version control systems (Azure DevOps, Git) Familiar with CI/CD concept It is helpful for you to have at least some of the following: Experience working in horizontally scaling systems Exposure to Azure SQL/warehouse/data lake products such as Azure SQL, Synapse, Databricks, ADLS Exposure to Microsoft Fabric is a plus Experience with other cloud data stacks (Google, AWS) is a plus Familiarity with message/event driven architecture patterns and distributed systems architecture Familiarity with systems integration An automation mindset A Data Engineer at Ncontracts is expected to exhibit the following behaviors: Intentional mentorship:&#8239;Ncontracts is dedicated to teaching and growing talent and expects everyone to help those less experienced. Honesty: Whether reviewing someone’s code, participating in retrospectives, or working with your team on what direction to take a project we expect openness and honesty. Honesty creates trust, and we believe that all great teams are built on trust. Low Ego: Have confidence in your skills and experience but be willing to alter your opinions and ideas when another, better one comes along. Have strong opinions, but loosely held. Deep Curiosity:&#8239;You will be expected to research new and exciting technologies, perfect the use of existing technologies, and discover new libraries and tools that can affect change across the organization. Motivation:&#8239;You are a natural self-starter, and you enjoy solving problems. You can solve the problem with minimal instruction and figuring out what should be done. WE OFFER A fun, fast-paced work environment Responsible PTO Plan that meets or exceeds state and local medical and family leave laws 11 paid holidays Community and social events to keep you connected and engaged Mental Health Benefits Medical, Dental and Vision insurance Company-paid Group Life Insurance, Short- and Long-Term Disability Flexible Spending Account & Health Savings Account Aflac Benefits – Critical Illness, Cancer Protection, & Hospital Choice Pet Insurance 401 (k) with company match with eligibility on Day 1 of employment 2 Paid Volunteer Time Off Days And much more! Compensation Information Pursuant to state and local law disclosure requirements, the pay range for this role, with final offer amount dependent on education, skills, experience and location is $130,000 to $150,000 per year. This position may be eligible for an annual discretionary incentive award. The incentive award amount is dependent upon company performance and your personal performance and is not guaranteed. AAP/EEO Statement Ncontracts provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state, or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training. Other Duties Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.",
        "url": "https://www.linkedin.com/jobs/view/3958835034"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3965902470,
        "company": "Inclusion Cloud",
        "title": "Data Engineer",
        "created_on": 1720638574.855619,
        "description": "About Inclusion Cloud: Inclusion Cloud is a leading provider of digital transformation services with a focus on integrating cutting-edge technologies to drive operational efficiency and enhance business performance. We specialize in delivering tailored solutions across various industries, leveraging our expertise in IT Service Management (ITSM), IT Operations Management (ITOM), and HR Service Delivery (HRSD) to empower organizations to achieve their goals. Job Description: We are seeking a talented and motivated Data Engineer to join our AWS Partner Intelligence team. This role involves working directly with Software Engineering, Business Intelligence, Data Science, and Product teams to continuously improve our data infrastructure, design, tools, and pipelines. Your work will directly influence and drive organizational insights, customer-facing features, and machine learning models. This is a fully remote position. Key Responsibilities: Architecture Design: Implement next-generation data pipelines and BI solutions. AWS Resource Management: Manage AWS resources including EC2, RDS, Redshift, Kinesis, EMR, Lambda, etc. Data Architecture: Build and deliver high-quality data architecture and pipelines to support business analysts, data scientists, and customer reporting needs. Data Integration: Interface with other technology teams to extract, transform, and load data from a wide variety of data sources. Process Improvement: Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers. A Day in the Life: Collaborate with Software Engineers, Product Managers, Data Scientists, and Business Intelligence Engineers to design, plan, and deliver on high-priority data initiatives serving internal stakeholders and AWS customers. Build automated, fault-tolerant, and scalable data solutions leveraging state-of-the-art technologies including but not limited to Spark, EMR, Python, Redshift, Glue, and S3. Continuously evaluate and improve our strategy, architecture, tooling, and codebase to maximize performance, scalability, and availability. Qualifications: Basic Qualifications: 1+ years of data engineering experience. Experience with data modeling, warehousing, and building ETL pipelines. Experience with one or more query languages (e.g., SQL, PL/SQL, DDL, MDX, HiveQL, SparkSQL, Scala). Experience with one or more scripting languages (e.g., Python, KornShell). Preferred Qualifications: Experience with big data technologies such as Hadoop, Hive, Spark, EMR. Experience with any ETL tool like Informatica, ODI, SSIS, BODI, Datastage, etc. Why Join Us: Innovative Environment: Work in a dynamic and innovative environment where your ideas are valued and encouraged. Professional Growth: Access to abundant training and certification opportunities to enhance your skills and advance your career. Impactful Work: Contribute to projects that directly impact and improve the way organizations operate and deliver services. Inclusive Culture: Be part of a diverse and inclusive team that fosters collaboration and creativity. How to Apply: If you are ready to take on exciting challenges and make a difference in the world of digital transformation, we want to hear from you! Apply now by submitting your resume and a cover letter outlining your relevant experience and qualifications. Inclusion Cloud is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.",
        "url": "https://www.linkedin.com/jobs/view/3965902470"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Corvallis, OR",
        "job_id": 3942667481,
        "company": "HireKeyz Inc",
        "title": "Data Engineer",
        "created_on": 1720638576.5275822,
        "description": "Role: Data Engineer Location: Corvallis, OR – 5 Days Onsite Role Type: Contract/W2 Job Description At least 8 to 10+ years’ experience in data engineering , AI and ML and Aws/Azure Architect, Design and implement distributed data processing pipelines using Spark, Python, SQL and other tools and languages prevalent in the Big Data/Lakehouse ecosystem. Experience in AI and ML on AWS/AZURE Analyzes design and determines coding, programming, and integration activities required based on general objectives. Play the technical lead role representing deliverables from vendor team resources at onsite and offshore locations. Lead the technical co-ordination and Business Knowledge transition activities to offshore team. Reviews and evaluates designs and project activities for compliance with architecture, security and quality guidelines and standards. Writes and executes complete testing plans, protocols, and documentation for assigned portion of data system or component; identifies defects and creates solutions for issues with code and integration into data system architecture. Collaborates and communicates with project team regarding project progress and issue resolution. Works with the data engineering team for all phases of larger and more-complex development projects and engages with external users on business and technical requirements. Collaborates with peers, engineers, data scientists and project team. What You Bring Bachelor's or Master's degree in Computer Science, Information Systems, Engineering or equivalent. 6+ years of relevant experience with detailed knowledge of data technical architectures, infrastructure components, ETL/ ELT and reporting/analytic tools. 2+ years of experience with Cloud based DW such as Redshift, Snowflake etc. 1+ years’ experience in Big Data Distributed ecosystems (Hadoop, SPARK, Unity Catalog & Delta Lake) 1+ years’ experience in Big Data Distributed systems such as Databricks, AWS EMR, AWS Glue etc. Leverage monitoring tools/frameworks, like Splunk, Grafana, CloudWatch etc. Experience with container management frameworks such as Docker, Kubernetes, ECR etc. Experience working on CI/CD processes such as Jenkins, Codeway etc. and source control tools such as GitHub, etc. Strong experience in coding languages like Python, Scala & Java Knowledge And Skills Fluent in relational based systems and writing complex SQL. Fluent in complex, distributed and massively parallel systems. Strong analytical and problem-solving skills with ability to represent complex algorithms in software. Strong understanding of database technologies and management systems. Strong understanding of data structures and algorithms Database architecture testing methodology, including execution of test plans, debugging, and testing scripts and tools. Strong analytical and problem-solving skills. Nice to Have Experience with visualization tools such as PowerBI, Tableau etc. Experience with transformation tools such as dbt. Have experience in building realtime streaming data pipelines Experience in pub/sub streaming technologies like Kafka, Kinesis, Spark Streaming etc.",
        "url": "https://www.linkedin.com/jobs/view/3942667481"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3944226008,
        "company": "Syntricate Technologies",
        "title": "Data Engineer",
        "created_on": 1720638579.8471324,
        "description": "Position : Data Engineer Location : Houston, TX (Onsite) Duration : Contract Experience : 10+ Years Job Description : Skills | Must-Haves: .NET and C# experience Spark experience experience with large data sets PowerBI experience experience using CI/CD DevOps pipelines Job Description Skills/Domain: Microsoft Azure, Synapse, Spark, Python, Angular, C#, .NET, DevOps, Azure Function, Microservice/API Development, Power BI Roles and Responsibilities: Promotes and adheres to Client's architectural standards and guidelines and works closely with the Enterprise Architects to develop target architectures and technology standards Provides explicit architecture consulting to project teams, across corporate functions, understands and frames the business problem to be solved. Identify available problem-solving approaches and methods As part of an Agile delivery approach, works with the product owner to write epics and stories to be delivered and participate in Agile SAFe delivery via standups, sprint planning and PI planning. Develop solutions by utilizing Microsoft Synapse, .Net and Spark to retrieve data from on-premises systems, manipulate large datasets, and perform intricate data wrangling including logging, error handling and rerun capabilities Implement role based and OBO authentication to enhance security management for the business. Develop CI/CD DevOps pipelines using ansible roles to enable continuous deployment and regression tests. Develop Data Manager Application using Angular and NgRx. Design synapse data warehouse using data modelling guidelines to enable optimal performance for Power BI reporting and develop Power BI reports Develop Rest APIs following REST guidelines for interacting with source systems and ingesting data into Azure Data Manager for Energy.",
        "url": "https://www.linkedin.com/jobs/view/3944226008"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Denver, CO",
        "job_id": 3959113442,
        "company": "Wise Skulls",
        "title": "Data Engineer",
        "created_on": 1720638583.9818964,
        "description": "Title: Data Engineer Location: Denver, CO (On-site) Duration: 6+ months Implementation Partner: Infosys End Client: To be disclosed Jd 8+ years of experience as a Data Engineer Strong experience with AWS Proficiency in Snowflake Expertise in GIT Strong skills in handling large datasets Excellent communication and coordination skills for managing onshore and offshore teams",
        "url": "https://www.linkedin.com/jobs/view/3959113442"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Providence, RI",
        "job_id": 3933222136,
        "company": "Noblesoft Technologies",
        "title": "Data Engineer",
        "created_on": 1720638586.1193259,
        "description": "Role: Data Engineer Location: Providence, Rhode Island/ Remote Mandatory Skills Gen AI, Data Engineering, ETL Jobs, Snowflake, Azure Cloud Job Description Design, develop, and maintain scalable data pipelines for ingesting, processing, and transforming large volumes of structured and unstructured data. Implement efficient data processing workflows to support the training and evaluation of solutions using large language models, ensuring reliability, scalability, and performance. Addressing issues related to data quality, pipeline failures, or resource contention, ensuring minimal disruption to systems. Integrate Large Language Model into data pipeline for natural language processing tasks. Working with Snowflake ecosystem Deploying, scaling, and monitoring AI solutions on cloud platforms like Snowflake, Azure, AWS, GCP Communicating technical and non-technical stakeholders and collaborate with cross-functional teams. Cloud cost management and best practices to optimize cloud resource usage and minimize costs. Preferred Qualifications Experience working within the Azure ecosystem, including Azure AI Search, Azure Storage Blob, Azure Postgres and understanding how to leverage them for data processing, storage, and analytics tasks. Experience with techniques such as data normalization, feature engineering, and data augmentation. Ability to preprocess and clean large datasets efficiently using Azure Tools /Python and other data manipulation tools. Expertise in working with healthcare data standards (ex. HIPAA and FHIR), sensitive data and data masking techniques to mask personally identifiable information (PII) and protected health information (PHI) is essential. In-depth knowledge of search algorithms, indexing techniques, and retrieval models for effective information retrieval tasks. Familiarity with search platforms like Elasticsearch or Azure AI Search is a must. Familiarity with chunking techniques and working with vectors and vector databases like Pinecone. Experience working within the snowflake ecosystem. Be able to implement efficient data processing workflows to support the training and evaluation of solutions using large language models, ensuring reliability, scalability, and performance. Ability to proactively identify and address issues related to data quality, pipeline failures, or resource contention, ensuring minimal disruption to systems. Best Regards, RESUME@NOBLESOFT.COM :: Phone (972) 845 8400 USA :: México :: India :: UK www.noblesoft.com :: Fax : (972) 845 8053 Inc500 :: Fastest Growing 500 Private Companies in USA Follow us on Linkedin",
        "url": "https://www.linkedin.com/jobs/view/3933222136"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Atlanta Metropolitan Area",
        "job_id": 3964926678,
        "company": "Agile Resources, Inc.",
        "title": "Data Engineer",
        "created_on": 1720638588.0376618,
        "description": "Title: Data Engineer Compensation: $120k - $140k Benefits: Medical, Dental, Vision, 401k, and more Location: Atlanta Metro Area (hybrid) Our growing client in the Atlanta Metro area is looking for a strong Data Engineer to expand their team. The role requires 1-2 days in office. The ideal candidate will have: Experience at smaller to mid-sized companies ($20M - $125M revenue). Strong knowledge of SQL for managing data. Experience with Power BI for creating visualizations and using DAX, M, and Power Query. Experience in BI/analytics for retail or DTC metrics like inventory, sales reporting, and lifetime value. Experience with data warehouses like BigQuery or Snowflake. KEYWORDS: Data Engineer, SQL, Power BI, Data Management, Retail Analytics, Data Warehouse",
        "url": "https://www.linkedin.com/jobs/view/3964926678"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3943499105,
        "company": "Zortech Solutions",
        "title": "Data Engineer-Developer - US",
        "created_on": 1720638589.9264688,
        "description": "Role: Date Engineer-Developer Location: NYC preferred or remote Duration: 6+ Months Job Description Strong in Data engineering Strong developer profile Hand-on experience on:- Azure / AWS / Data bricks (Good to have) / Coding skills / Analytics skills. Experience- 7-12 years.",
        "url": "https://www.linkedin.com/jobs/view/3943499105"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3943490159,
        "company": "Wizcom Corporation (Formerly Dextro Software Systems)",
        "title": "Data Analytics Engineer",
        "created_on": 1720638591.6738594,
        "description": "Job Title: Data Analytics Engineer Job Location: Fully Remote – in the US, Core Working hours – 9AM to 5PM EST Duration: Contract-to-hire Work Authorization: GC/US Only (no EAD’s) W2 only (No C2C) Day to Day: Take complex business request / questions.. create a data product or dashboard for users to visualize these business requests/questions that arise. Top skills/ How are they applied? Strong complex SQL skills BI Tools: Tableau is strongly preferred, open to others Data Analysis Skills, End User Design, Data Modeling (ETL or SSIS) VERY Strong Communicator that can work with Stakeholders Minimum Qualifications Education Bachelor's degree in the healthcare field, analytics, biostatistics, informatics, computer science, business, or engineering required or Master's degree in a related field preferred Experience 3-4 years in data collection, management, analysis, and interpretation. Experience supporting customer groups through data analysis, report creation, and graphical presentation. required and Tableau Desktop experience preferred and Database development and maintenance. Metrics implementation, performance benchmarking, and data/metrics governance required and Mathematical process modeling and simulation.. Operational and/or clinical data in healthcare field and knowledge of the healthcare industry preferred",
        "url": "https://www.linkedin.com/jobs/view/3943490159"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Greater Chicago Area",
        "job_id": 3929208566,
        "company": "Glocomms",
        "title": "Data Engineer",
        "created_on": 1720638594.5158226,
        "description": "Our client is a specialized private investment firm that is helping companies innovate their respective industries. They are looking to bring on a strong Data Engineer/Software Engineer to bolster their internal team. In this role you will: Help create, design, and deploy ELT and ETL Pipelines Build and maintain the data infrastructure to help support internal products. Be a key contributor to the future of the team in bringing your own innovative ideas. The Ideal Candidate will have: B.S. and/or M.S. in Mathematics, Engineering, Computer Science or any other related fields Proven expereince working the construction and implementation of data solutions. Experience in owning projects for their entire lifecycle Coding skills in Python for the automtion and processing of Data. Ideal Technical Experience Includes: Experience in GCP, or AWS or Azure Big Data Experience including Spark or PySpark Experience using Docker for architecture, deployment and managment of infrastructure. Additional Relevant Experience May Include: Working knowledge of Kubernetes Cluster Experience wearing multiple hats, being adapatble and resourceful.",
        "url": "https://www.linkedin.com/jobs/view/3929208566"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Atlanta Metropolitan Area",
        "job_id": 3945842254,
        "company": "Agile Resources, Inc.",
        "title": "Data Engineer",
        "created_on": 1720638596.1956952,
        "description": "Data Engineer Location: Hybrid with 1-2 days per week onsite in Atlanta Type: Contract-to-Hire Pay: $55-$70/hour A top fashion retailer in Atlanta is seeking a Data Engineer to join their team. Here's what you'll be doing: Developing data strategy and delivering solutions Building tools to automate reporting Collaborating with business units Here's what our ideal candidate has: 5+ years of Data Engineering experience Strong SQL and relational database skills Experience building and managing big data pipelines Experience utilizing cloud data warehouse tools (BigQuery) Experience with DBT Visualization and Analytics experience with Power BI Benefits: Medical, Dental, Vision Key Words: SQL, GCP, BigQuery, Data Warehouse, Big Data, Power BI, Dashboard, DBT, Data Build Tool, Relational Database, Data Pipeline, Data Transformation, Data Structures, Metadata, Dependency,",
        "url": "https://www.linkedin.com/jobs/view/3945842254"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Orlando, FL",
        "job_id": 3959542974,
        "company": "GlobalLogic",
        "title": "Data Engineer",
        "created_on": 1720638599.4593377,
        "description": "Description: The project mission is to transform advertising and Disney’s Ad platform with data and AI across TV and streaming video. We build solutions to measure and optimize every aspect of the advertising life cycle. Our tenant is a strong cross-domain team to deliver E2E solutions covering tech areas ranging from machine learning, big data, microservices to data visualization. Our team is seeking a senior software engineer who will be a core team member for our advertising data platform engineering group. This engineering group focuses on big data infrastructure, operational data, audience solution, inventory forecasting and full funnel measurements as a foundation layer for Disney’s addressable Ad Platforms. Requirements: Bachelor or above in computer science or EE 4+ years of professional programming in Java, Scala, Python, and etc. 3+ years of big data development experience with technical stacks like Spark, Flink, Singlestore, Kafka, Nifi and AWS big data technologies Knowledge of system, application design and architecture Experience of build industry level high available and scalable service Passion about technologies, and openness to interdisciplinary work Experience with processing large amount of data at petabyte level Demonstrated ability with cloud infrastructure technologies, including Terraform, K8S, Spinnaker, IAM, ALB, and etc. Experience with ClickHouse, Druid, Snowflake, Impala, Presto, Kinesis, etc. Experience in widely used Web framework (React.js, Vue.js, Angular, etc.) and good knowledge of Web stack HTML, CSS, Webpack Job Responsibilities: Build components of large-scale data platform for real-time and batch processing, and own features of big data applications to fit evolving business needs Build next-gen cloud based big data infrastructure for batch and streaming data applications, and continuously improve performance, scalability and availability Contribute to the best engineering practices, including the use of design patterns, CI/CD, code review and automated test Chip in ground-breaking innovation and apply the state-of-the-art technologies As a key member of the team, contribute to all aspects of the software lifecycle: design, experimentation, implementation and testing. Collaborate with program managers, product managers, SDET, and researchers in an open and innovative environment",
        "url": "https://www.linkedin.com/jobs/view/3959542974"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "New Jersey, United States",
        "job_id": 3940992522,
        "company": "Open Systems Technologies",
        "title": "Data Engineer",
        "created_on": 1720638601.1732557,
        "description": "A law firm is looking for a Data Engineer to join their team. This role is remote. Compensation: $120-130k The Data Engineer must have a strong technical background in the Azure stack, including Azure Synapse Analytics, Azure Data Factory, Azure Databricks, and other Azure services. As the firm implements an aggressive and exciting cloud strategy, this role will facilitate and develop streamlined processes and work collaboratively with cross-functional teams of devops engineers, product and project managers, business analysts, and infrastructure architects. This individual must thrive in a fast-paced environment and be self-motivated with a passion for problem-solving and innovation. This IT department is project-focused, undergoing a complete digital business transformation for the firm. This position will drive new data engineering strategies and opportunities for the firm. Responsibilities Ability to diagnose and analyze dataflows and applications. Create queries for the business team that translate custom in-house applications into useful reports or outputs for discussions. Strong data modeling and data warehousing skills with the ability to optimize report and query performance. Designing and implementing scalable and secure data processing pipelines using Azure Data Factory, Azure Databricks, and other Azure services. Automating data pipelines and workflows to streamline data ingestion, processing, and distribution tasks. Experience with DevOps + automation is a big plus. Code management with Git. Qualifications At least 3 - 5 years of experience working on cloud data platforms, preferably in the Azure stack. Power BI proficiency, data modeling, and DAX experience. Managing and optimizing data storage using Azure Data Lake Storage and Azure SQL. At least 2 years of experience with: Python / PySpark / Scala Azure Synapse Analytics Agile Scrum principles and practices SQL Query, SP, and SSIS packages Azure Cosmos DB Azure SQL solutions (Azure SQL DB, Managed Instances, etc.) 1 to 2 years of Boomi ETL is a plus. An eye for automation and templating for reusable solutions. Self-starter. Curious problem solver who is willing to investigate on their own. Strong desire for quality + best practice improvement. Likes to collaborate across teams. EDUCATION Bachelor of science degree (or equivalent) in computer science, engineering, or relevant field",
        "url": "https://www.linkedin.com/jobs/view/3940992522"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Linthicum Heights, MD",
        "job_id": 3945093603,
        "company": "ExpediteInfoTech, Inc.",
        "title": "Data Engineer (Junior)",
        "created_on": 1720638602.8948655,
        "description": "Responsibilities Participate in data policy development for the various DHS programs as well as in policy and program evaluation efforts as well as workgroups for data review and reporting Work with executives and other business leaders to identify opportunities for improvement. Create reports for internal teams and/or external clients. Collaborate with team members to collect and analyze data. Use graphs, infographics, and other methods to visualize data. Establish KPIs to measure the effectiveness of business decisions. Structure large data sets to find usable information. Work with a team of analysts and other associates to process information. Create presentations and reports based on recommendations and findings. Required 4+ years of relevant experience within the data science or computer science industry to include data management, data cleaning, statistical analysis, data visualization, and report writing. (Master’s degree and/or Data Analysis Certifications in programming languages or other areas related to data analytics can be considered in lieu of work experience) Bachelor’s degree in a field like mathematics, statistics, economics, finance, or computer science Advanced speaking and report-writing skills for effective communication. Attention to detail and a commitment to accuracy. Experience designing methods and strategies to capture, store, and manage data. Good working knowledge of SAS, STATA, RStudio, Python, Java, C, C++, and Jupyter software (or a combination of at least three of the following programs of choice: Microsoft Excel and Google sheet for data organization; STATA, SAS (Statistical Analysis Software), SQL, Matlab, R, or Python for data analysis Tableau or PowerBI, Tableau, Qlik Sense, Looker, Zoho Analytics, or Domo for data visualization and reporting Experience with computing cluster environments. Solid working knowledge of office processes and related computer software (such as Microsoft Office and web-based applications) and electronic communications tools. Strong analytical and critical thinking skills. Ability to set and meet deadlines. Ability to work in high-pressure situations. Strong judgment and discretion in dealing with confidential information. Desired Master’s degree About: ExpediteInfoTech, Inc. (EIT) is a SBA 8(a) certified small business. Headquartered in Rockville, MD since 2012, EIT has provided specialized technical, cybersecurity, IT, and financial advisory solutions to the Federal, State and County governments. Our clients include the US Department of Education, US Department of Transportation, US Department of Justice, US Department of Health & Human Services, Montgomery County government, Prince George's County Government, the governments of the State of Maryland and the District of Columbia. EIT is appraised at level 3 for CMMI Services & CMMI Development, as well as ISO 9001:2015, ISO 20000-1:2018 and ISO 27001:2013. EIT offers a competitive benefits package including medical, dental, vision and prescription drug coverage, paid time off, federal holidays, matching 401K plan, and tuition/professional development reimbursement benefits. EIT is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by applicable law.",
        "url": "https://www.linkedin.com/jobs/view/3945093603"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Austin, Texas Metropolitan Area",
        "job_id": 3967731126,
        "company": "Formula.Monks",
        "title": "Data Engineer",
        "created_on": 1720638604.548632,
        "description": "Data Engineer Location: Austin, TX (hybrid-on site 3 days per week) Job Type: This is a Full-Time W2 position with us here at Formula.Monks where you will be working as a consultant at this top FAANG company. Since you’re a full time-employee, you will receive an annual salary, full benefits, PTO, etc. This position is on a high impact team working on a long-term engagement. You are also able to explore employment opportunities at this tech company while working via Formula.Monks! Description: You will develop, and test large scale data solutions, to provide efficient analytical and reporting capabilities across the clients global and regional sales and finance teams. You will help develop highly scalable data pipelines to load data from various source systems leveraging cloud tools to schedule and monitor the workflows. Build generic and reusable solutions that can scale and utilize various technologies and frameworks to solve our complex business requirements. Extend and build out existing data reporting warehouses and capabilities. You will be required to understand existing solutions, fine-tune them and support them as needed. Data quality is our goal and we expect you to meet our high standards on data and software quality. We are a rapidly growing team with plenty of interesting technical and business challenges to solve. We seek a self-starter, who is willing to learn fast, adapt well to changing requirements and work with cross functional teams. Key Qualifications: 5+ years of hands-on database engineering and data warehousing experience. Proficiency in advanced SQL including query performance tuning Understanding of materialized views, stored procedures, table design and data mart design 3+ years of experience in OOO programming skill sets Python a strong plus. Experience developing and working with custom ETL pipelines using SQL or other scripting languages (Python, Bash scripting) Development experience with SQL based cloud data warehouses like Snowflake, Redshift, BigQuery etc Experience with version control system Git and virtualization tools specifically Docker a must. Understanding of continuous integration and delivery (CI/CD) practice. Experience with cloud computing platforms like AWS, Google Cloud is a plus. Job workflow management with Airflow is also a plus. Ability to learn and adapt to new tools and technologies in a fast paced environment. An analytical and mathematical mind capable of evaluating and solving various complex data related problems. Ability to take directions as part of a team as well as deliver independent work. Excellent oral and written communication skills.",
        "url": "https://www.linkedin.com/jobs/view/3967731126"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Cleveland, OH",
        "job_id": 3943070863,
        "company": "Capgemini",
        "title": "AWS Data Engineer- Python",
        "created_on": 1720638606.1710272,
        "description": "Job Title : AWS Data Engineer- Python Job Location : OH, Cleveland Required Skills/Experience Strong proficiency in Python; with extensive experience in design implementation and maintenance of data pipelines using Python, PySpark on Databricks; with the ability to write and complete complex queries to perform curation and build views required by end users single and multifaceted Experience in: Building and handling Key Management Services Building S3 file systems via code pipelines, creating and using IAM roles Creating and using AWS transfer Family and Storage Gateway mechanism Building and using CloudFormation templates Standing up and maintaining EC2 instances and Lambda services Standing up Kubernetes clusters and maintenance Previous experience with data visualization tools: Power BI, Tableau As well as previous experience data architecture and data modeling on Databricks Good experience in GIT and CI/CD Pipelines Databricks Extensive experience in Databricks Data engineering Job Runs Data Ingestion and Delta Live Tables setting up and maintaining SQL Warehouses SQL Editor and alerts Good experience in setting up and maintaining workspaces catalogs workflows and Compute Experienced with purpose compute Job computes SQL Warehouse Vector search Pools and Policies Experienced in setting up and maintaining the Unity catalog Experienced in crafting and maintaining Databricks schema objects Notebooks scheduler and Job clusters building Notebooks with complex code structures building and maintaining the Autoloader process Solid experience in the usage of Table ACL Row and Column Level Security With Unity Catalog Should be good in Data Ingestion Storage Harmonization and curation Demonstrable experience in performance and tuning to ensure jobs are running at efficient levels and no performance bottleneck Experienced in data ingestion using various methods API Direct database reads AWS Cross account data copy third party vendor data using FTP process etc Life at Capgemini: Capgemini supports all aspects of your well-being throughout the changing stages of your life and career. For eligible employees, we offer: Flexible work Healthcare including dental, vision, mental health, and well-being programs Financial well-being programs such as 401(k) and Employee Share Ownership Plan Paid time off and paid holidays Paid parental leave Family building benefits like adoption assistance, surrogacy, and cryopreservation Social well-being benefits like subsidized back-up child/elder care and tutoring Mentoring, coaching and learning programs Employee Resource Groups Disaster Relief About Capgemini: Capgemini is a global business and technology transformation partner, helping organizations to accelerate their dual transition to a digital and sustainable world, while creating tangible impact for enterprises and society. It is a responsible and diverse group of 340,000 team members in more than 50 countries. With its strong over 55-year heritage, Capgemini is trusted by its clients to unlock the value of technology to address the entire breadth of their business needs. It delivers end-to-end services and solutions leveraging strengths from strategy and design to engineering, all fueled by its market leading capabilities in AI, cloud and data, combined with its deep industry expertise and partner ecosystem. The Group reported 2023 global revenues of €22.5 billion. Get the future you want | www.capgemini.com Disclaimer: Capgemini is an Equal Opportunity Employer encouraging diversity in the workplace. All qualified applicants will receive consideration for employment without regard to race, national origin, gender identity/expression, age, religion, disability, sexual orientation, genetics, veteran status, marital status or any other characteristic protected by law. This is a general description of the Duties, Responsibilities and Qualifications required for this position. Physical, mental, sensory or environmental demands may be referenced in an attempt to communicate the manner in which this position traditionally is performed. Whenever necessary to provide individuals with disabilities an equal employment opportunity, Capgemini will consider reasonable accommodations that might involve varying job requirements and/or changing the way this job is performed, provided that such accommodations do not pose an undue hardship. Capgemini is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to your recruiting contact. Click the following link for more information on your rights as an Applicant http://www.capgemini.com/resources/equal-employment-opportunity-is-the-law",
        "url": "https://www.linkedin.com/jobs/view/3943070863"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3929020106,
        "company": "AddSource",
        "title": "Data Engineer",
        "created_on": 1720638607.888809,
        "description": "Sr Data Engineer with Python 12+ Years of experience Remote Mandatory Skills: Python, SQL Job Description Must have 5-7+ years of experience Building a brand new internal app for accuracy dept SQL strong Database modeling skills Data movement from different data sources Demonstrate technical and creative skills Data storage Design of data environment Python Google cloud platform experience preferred Cloud platform experience preferred Cloud architecture and development preferred R-shiny skills preferred",
        "url": "https://www.linkedin.com/jobs/view/3929020106"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3968660224,
        "company": "Personify Health",
        "title": "Data Engineer I",
        "created_on": 1720638609.714302,
        "description": "Overview Now is the time to join us! We’re Personify Health. We’re the first and only personalized health platform company to bring health, wellbeing, and navigation solutions together. Helping businesses optimize investments in their members while empowering people to meaningfully engage with their health. At Personify Health, we believe in offering total rewards, flexible opportunities, and a diverse inclusive community, where every voice matters. Together, we’re shaping a healthier, more engaged future. Responsibilities Who are you? Data Engineer I perform development activities with the guidance of another member of the data engineering team. You will work closely with account management, ETL, data warehouse, business intelligence, and reporting teams as you develop data pipelines and enhancements and investigate and troubleshoot issues. In this role you will wear many hats, but your knowledge will be essential in the following: Extracting, cleansing, and loading data. Building data pipelines using SQL, Kafka, and other technologies. Triage incoming bugs and incidents. Perform technical operation tasks. Investigate and troubleshoot issues with data and data pipelines. Participation in sprint refinement, planning, and kick-off to help estimate stories, raise awareness and additional implementation details. Help monitor areas of the data pipeline and raise awareness to team when issues arise. Performing quality assurance work to verify the accuracy of code and data results You’ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in. Qualifications What you bring to the Personify Health team: In Order To Represent The Best Of What We Have To Offer You Come To Us With a Multitude Of Positive Attributes Including 1 – 2 years or less experience in data engineering SW certification or degree in IT related field Solid grasp of modern relational and non-relational models and differences between them. Proficiency in writing SQL, the use of Excel, and some analytical tools. Understanding of REST API. Understanding of JSON. Detail oriented and able to examine data and code for quality and accuracy. Knowledge of Agile environments, including Scrum and Kanban methodologies Python / R / programming language experience preferred ETL experience preferred AWS Lambda / Console experience preferred Git experience preferred No candidate will meet every single desired qualification. If your experience looks a little different from what we’ve identified and you think you can bring value to the role, we’d love to learn more about you! Personify Health is an equal opportunity organization and is committed to diversity, inclusion, equity, and social justice. In compliance with all states and cities that require transparency of pay, the base compensation for this position ranges from $65,000 to $76,000. Note that salary may vary based on location, skills, and experience. This position is eligible for 10% target bonus/variable compensation as well as health, dental, vision, mental health and other benefits. We strive to cultivate a work environment where differences are celebrated, and employees of all backgrounds are empowered to thrive. Personify Health is committed to driving Diversity, Equity, Inclusion and Belonging (DEIB) for all stakeholders: employees (at each organization level), members, clients and the communities in which we operate. Diversity is core to who we are and critical to our work in health and wellbeing.",
        "url": "https://www.linkedin.com/jobs/view/3968660224"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Beaverton, OR",
        "job_id": 3959388487,
        "company": "Kforce Inc",
        "title": "Data Engineer",
        "created_on": 1720638611.3462703,
        "description": "Responsibilities Kforce has a client that is seeking a Data Engineer to work onsite in Beaverton, OR. Summary: The Data Engineer will collaborate with product owners, developers, database architects, data analysts, visual developers and data scientists on data initiatives and will ensure optimal data delivery and architecture is consistent throughout ongoing projects. In this role, you must be self-directed and comfortable supporting the data needs of the product roadmap. The right candidate will be excited by the prospect of optimizing and building integrated and aggregated data objects to architect and support our next generation of products and data initiatives. Key Tasks: Data Engineer will create and maintain optimal data pipeline architecture Assemble large, complex data sets that meet functional/non-functional business requirements Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing for greater scalability Comprehensive documentation and knowledge transfer to Production Support Work with Production Support to analyze and fix Production issues Participate in an Agile/Scrum methodology to deliver high -quality software releases every 2 weeks through Sprint As a Data Engineer, you will refine, plan stories, and deliver timely Analyze requirement documents and Source to target mapping Requirements 5+ years of experience in Big Data stack environments like AWS EMR, Cloudera, Hortonworks 3+ years of SPARK in batch mode 3+ years of experience in scripting using Python 3+ years of experience working on AWS Cloud environment Experience building cloud scalable high-performance data lake solutions Experience with relational SQL & tools like Snowflake Experience with source control tools such as GitHub and related dev processes Experience with workflow scheduling tools like Airflow The pay range is the lowest to highest compensation we reasonably in good faith believe we would pay at posting for this role. We may ultimately pay more or less than this range. Employee pay is based on factors like relevant education, qualifications, certifications, experience, skills, seniority, location, performance, union contract and business needs. This range may be modified in the future. We offer comprehensive benefits including medical/dental/vision insurance, HSA, FSA, 401(k), and life, disability & ADD insurance to eligible employees. Salaried personnel receive paid time off. Hourly employees are not eligible for paid time off unless required by law. Hourly employees on a Service Contract Act project are eligible for paid sick leave. Note: Pay is not considered compensation until it is earned, vested and determinable. The amount and availability of any compensation remains in Kforce's sole discretion unless and until paid and may be modified in its discretion consistent with the law. This job is not eligible for bonuses, incentives or commissions. Kforce is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.",
        "url": "https://www.linkedin.com/jobs/view/3959388487"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Miami, FL",
        "job_id": 3969248814,
        "company": "Spot Pet Insurance",
        "title": "Data Engineer",
        "created_on": 1720638613.0153816,
        "description": "Spot Pet Insurance is the fastest growing pet insurance company in North America. Our commitment to an exceptional end-to-end customer experience and our data-driven approach have quickly established us as a leading pet insurance provider. We're dedicated to providing pet parents with peace of mind by offering accessible and comprehensive coverage so their furry companions can lead happier, healthier lives. To demonstrate this, we recently joined forces with MrBeast to find homes for 100 homeless pets and committed to giving each of them pet insurance for life! Along the way, we’ve created a company culture that allows our employees to thrive, with perks like daily free meals, a pet-friendly office, and ridiculously fun company events every quarter. Our dedication to fostering a positive and rewarding work environment for our team has even earned us a Great Place to Work certification. The ideal candidate is passionate about designing cost efficient and robust data pipelines. The data engineer will work on SPOT’s backend team and work directly with the backend engineers on building and improving data warehouses, access, and processes. They will also be expected to provide guidance and direction for improving or replacing existing data workflows for the data team. Duties and Responsibilities Create data pipelines with Python, SQL, and Spark Identify opportunities for improving existing systems to ensure scalability Improve visibility into existing data, making it easy to query and visualize Design and build schema and access plans for backend features that require an internal data store Perform QA with all code, including unit, integration, and / or regression tests Create data quality checks for ingested and post-processed data Ensure alerting and monitoring of automated pipelines Maintain and enhance existing data products Bring new perspectives to data engineering problems and take initiative Maintain a clean and secure python environment Skills Fluent in SQL Proficient with python; understands the importance of maintaining Comfortable with python dependency management tools (conda, venv, etc.) Capable of building and running cost-efficient Spark jobs Experience with Cloud Technologies (AWS, Azure, or GCP), AWS preferred (Kinesis, SQS, Redshift, Athena, S3, and Lambda) Understands database systems, both relational (postgres, mysql, etc.) and non-relational (mongodb, cassandra, etc.) Familiarity with event systems (kafka, SQS, etc.) Capable of auditing and securing data Experience with data analytics platforms like Databricks or equivalent platform Proficient with git, GitHub, pull requests, continuous integration and continuous deployment. What we offer: A collaborative and supportive work environment, recognized as a Great Place to Work. Cell phone allowance of $100 Monthly parking pass reimbursed Medical, dental, and visions benefits Life insurance Unlimited PTO Bring your pet to work Your pet insurance is covered (up to $100) 401k with Company match Annual performance-based bonus",
        "url": "https://www.linkedin.com/jobs/view/3969248814"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3952357127,
        "company": "Zortech Solutions",
        "title": "Data Engineer-Developer - US",
        "created_on": 1720638614.7206335,
        "description": "Role: Data Engineer-Developer Location: NYC preferred or remote Duration: Fulltime Job Description Strong in Data engineering Strong developer profile Hand-on experience on:- Azure / AWS / Data bricks (Good to have) / Coding skills / Analytics skills. Experience- 7-12 years.",
        "url": "https://www.linkedin.com/jobs/view/3952357127"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "New Jersey, United States",
        "job_id": 3971392724,
        "company": "DRUM UPSKILL",
        "title": "Data Engineer",
        "created_on": 1720638616.261807,
        "description": "Data Engineer New Jersey About the Role: Our client, a renowned consumer products company, is seeking a skilled Data Engineer to join their dynamic team. The company has recently implemented Snowflake as their new data lake and is sourcing data from various points. This role will play a crucial part in integrating and managing these data sources efficiently. Responsibilities: Design, develop, and maintain scalable data pipelines and systems. Work closely with data architects to implement data models on Snowflake. Integrate data from multiple data sources into Snowflake. Develop and maintain ETL processes using AWS services. Ensure data quality and integrity through best practices and automation. Collaborate with cross-functional teams to understand data needs and deliver solutions. Monitor and optimize data pipeline performance and troubleshoot any issues. Document processes and create technical documentation for data integration. Requirements: 2 years of experience as a Data Engineer. Proficiency in Snowflake and AWS services. Strong SQL skills and experience with ETL tools. Knowledge of data warehousing concepts and best practices. Experience with data integration from various sources. Excellent problem-solving skills and attention to detail. Ability to work collaboratively in a team and communicate effectively. Preferred Qualifications: Experience in the consumer products industry. Familiarity with other data lake solutions. Knowledge of Python or other programming languages. Experience with data visualization tools.",
        "url": "https://www.linkedin.com/jobs/view/3971392724"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Scottsdale, AZ",
        "job_id": 3965910786,
        "company": "Persistent Systems",
        "title": "AWS Data Engineer",
        "created_on": 1720638617.8948646,
        "description": "About Persistent We are a trusted Digital Engineering and Enterprise Modernization partner, combining deep technical expertise and industry experience to help our clients anticipate what’s next. Our offerings and proven solutions create a unique competitive advantage for our clients by giving them the power to see beyond and rise above. We work with many industry-leading organizations across the world including 14 of the 30 most innovative US companies, 80% of the largest banks in the US and India, and numerous innovators across the healthcare ecosystem. Our disruptor’s mindset, commitment to client success, and agility to thrive in the dynamic environment have enabled us to sustain our growth momentum by reporting $282.9M revenue in Q1FY24, delivering 17.1% Y-o-Y growth. In addition, our total employee count reached 23,130 people this quarter, located in 21 countries across the globe. We’re also pleased to share that Persistent has been named the fastest-growing Indian IT Services brand by Brand Finance. Acknowledging our vertical industry expertise, we were placed as a Leader in Everest Group’s Payments IT Services PEAK Matrix® Assessment 2023. We were also recognized as a Leader in the ISG Provider Lens™ Digital Engineering Services Quadrants U.S. 2023 and the Salesforce Ecosystem Partners 2023 ISG Provider Lens™ Study. Throughout this market-leading growth, we’ve maintained strong employee satisfaction - over 94% of our employees approve of the CEO, and 89% would recommend working at Persistent to a friend. About Position: Role: AWS Data Engineer Location: Scottsdale AZ (Day 1 onsite) Experience: 8+ Must have : AWS Certified Solutions Architect , AWS , redshift , kinesis , glue , IAM , python ,Spark What You'll Do: Utilize AWS services such as Kinesis, S3, Glue, Redshift, and RDS SQL Server for data processing and storage. Implement data ingestion processes to handle streaming and batch data. Ensure data quality and integrity through robust ETL processes. Collaborate with other data engineers and the Cloud engineering team to develop and deploy data pipelines in AWS. Optimize and tune data processing workflows for performance and cost efficiency. Monitor and troubleshoot data pipeline issues to ensure continuous data flow and reliability. Document data architecture, processes, and workflows. AWS Certified Data Analytics – Specialty or AWS Certified Solutions Architect. Experience with other AWS services such as Lambda, Cloudwatch, Kinesis, Firehose, Event bridge, Redshift, DynamoDB, IAM, RDS SQL Server Familiarity with big data technologies like Apache Spark or Hadoop. Experience with reporting and visualization tools like Tableau Knowledge of DevOps practices and tools for CI/CD such as Jira and Harness Expertise You'll Bring: Bachelor's or master’s degree in computer science, Engineering, or a related field. - Minimum of 5 years of experience in data engineering, with a focus on AWS technologies. - Proven experience with AWS services including Kinesis, S3, Glue, Redshift, and RDS SQL Server. - Strong proficiency in SQL and experience with database design and optimization. - Expertise in ETL/ELT processes and tools. - Familiarity with data warehousing concepts and best practices. - Experience with data modeling and schema design. - Proficiency in programming languages such as Python, Java, or Scala. - Knowledge of data governance and security best practices in a cloud environment. - Excellent problem-solving skills and the ability to work independently with minimal supervision. Strong communication and collaboration skills. Benefits: Competitive salary and benefits package Culture focused on talent development with quarterly promotion cycles and company-sponsored higher education and certifications. Opportunity to work with cutting-edge technologies. Employee engagement initiatives such as project parties, flexible work hours, and Long Service awards Annual health check-ups Insurance coverage: group term life, personal accident, and Mediclaim hospitalization for self, spouse, two children, and parents Our company fosters a values-driven and people-centric work environment that enables our employees to: · Accelerate growth, both professionally and personally · Impact the world in powerful, positive ways, using the latest technologies · Enjoy collaborative innovation, with diversity and work-life wellbeing at the core · Unlock global opportunities to work and learn with the industry’s best Let’s unleash your full potential at Persistent - persistent.com/careers",
        "url": "https://www.linkedin.com/jobs/view/3965910786"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Miami, FL",
        "job_id": 3963703048,
        "company": "Quirch Foods",
        "title": "Data Engineer",
        "created_on": 1720638619.8679543,
        "description": "Essential Duties and Responsibilities: Develop, test and maintain custom business solutions by leveraging state of the art software development practices using an agile methodology Evaluate business requirement and help define problems and develop solutions Participate in design and architecture discussions with business leaders, end users and IT team members Document business requirements and solution, document code and provide support for creation of end user documentation Update business knowledge, technical skills and soft skills, leverage educational opportunities, participate in professional organizations Experience: MS SQL Programming Visual Studio/C#/WPS Experience Microsoft SSAS, SSIS, SSRS desired but not required Biztalk experience desired but not required Skills and Requirements: Bachelor's degree in Computer Science, Engineering, Math or equivalent and/or related experience and training preferred Strong analytical and problem-solving skills Strong collaborator and team player Great organizational skills, attention to detail and follow thru Effective oral and written communication skills Extensive familiarity with data management principles Benefits: Professional growth and developmental opportunities to grow your skills using state of the art technology Comprehensive benefits package that includes: Medical, Dental, Prescription Drug Plan, Disability Plan, Life insurance Plan 401K savings Plan Paid Holidays Personal Time off Employee Discounts Quirch Foods is an Equal Opportunity Employer (EOE). Qualified applicants are considered for employment without regard to age, race, color, religion, sex, national origin, sexual orientation, disability, or veteran status. All applicants must be eligible to work in the United States. #J-18808-Ljbffr",
        "url": "https://www.linkedin.com/jobs/view/3963703048"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3944218942,
        "company": "Syntricate Technologies",
        "title": "Snowflake Data Engineer (Databricks, Python, SQL , SSRS, SSIS)",
        "created_on": 1720638621.599769,
        "description": "Job Title: Snowflake Data Engineer (Databricks, Python, SQL , SSRS, SSIS) Location: Remote Duration:W2 JD CDSM Sr. Data Engineer Role Snowflake + Databricks + Azure Data Factory with Python+ Java Design, develop & maintain data platform services for varied use cases such as Data Extraction, Data Processing, Data Ingestion, Data Observability and Data Discovery. Good hands-on experience on Snowflake Data Warehouse Should be able to implement Row and Column level security in Snowflake. Should have good hands-on experience in writing stored procedures and complex sql queries in snowflake. Experience in performing ETL using Databricks on Azure Cloud. Excellent command of one or more programming languages, preferably Python or PySpark Strong knowledge & experience of architecture & internals of Apache Spark using Python. Proficient in Spring framework and Spring Boot Proficient in SQL and Query tuning Prior experience in the healthcare domain working directly with Healthcare End Users (customers) Comfortable working in Agile methodology with flexible attitude towards learning new technologies. Knowledge of Rally is desirable. Required: Cloud Data Engineers - 7 to 10 years' experience - Solid experience in cloud based modern DW stack - Snowflake, Databricks, Python, SQL , SSRS, SSIS - Familiarity with Java is great - Familiarity with Airflow and DevOps Thank you, Pallavi Verma, Sr. Technical Recruiter Syntricate Technologies Inc. Phone#: (781) 552-4333| Email to:  pallavi@syntricatetechnologies.com MBE & E-Verify Certified IT Services & Solutions | IT, Non-IT & Medical Staffing services Explore our more positions on  Website Connect with us on  LinkedIn This e-mail message may contain confidential or legally privileged information and is intended only for the use of the intended recipient(s). Any unauthorized disclosure, dissemination, distribution, copying or the taking of any action in reliance on the information herein is prohibited. Please notify the sender immediately by email if you have received this email by mistake and delete this e-mail from your system.You have received this email as we have your email address in our member(s) or subscriber(s) list. If you do not want to receive any further emails or updates, please reply and request to unsubscribe.",
        "url": "https://www.linkedin.com/jobs/view/3944218942"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Wisconsin, United States",
        "job_id": 3962955297,
        "company": "Element6Talent",
        "title": "Data Engineer",
        "created_on": 1720638623.3500657,
        "description": "Element6's client in Brookfield, WI is looking for a Data Engineer to join their team on a contract basis. The Data Engineer is a key technical resource responsible for understanding a diverse data landscape, designing, and developing data solutions in response to project and support requests. The Data Engineer will have experience with relational and/or non-relational databases (e.g., MSSQL, Oracle, Snowflake) and business intelligence tools (e.g., SSIS, SSRS, Power BI). The Data Engineer must be self-motivated with excellent attention to detail and the ability to work with moderate supervision. The Data Engineer reports to the Manager, Data Engineering. REQUIREMENTS : Bachelor’s degree with a minimum of 1 year of professional experience, or at least 3 years of experience developing data solutions (e.g., ETL, reports, data warehouse). Equivalent combination of education and experience will be considered. Strong SQL skills. Experience using data tools/services to deliver data driven solutions, including ETL, data integration, and/or reporting solutions. Experience with designing, modeling, and building databases and data structures. Experience with Visual Studio and SSDT. Knowledge of scripting languages (e.g., C#, Python) as a data tool. Solid analytical, problem solving, and troubleshooting skills. Familiarity with source control and CI/CD pipelines. Knowledge of database administration tasks (e.g., backup, restore, logging). Strong communication skills, both written and verbal. Experience working in an iterative delivery process. Must have a thorough understanding of company policies and procedures as they relate to this position. Must understand and comply with all job-related State and Federal laws and regulations. PRINCIPAL ACCOUNTABLITIES: Collaborate with Business Analysts and/or other Data Engineers to validate requirements and technical capabilities. Design data solutions that meet project goals and minimize risk. Create documentation per project and organization standards. Participate in the estimating of projects and support work. Participate in defining data development processes and tools. Participate in research efforts and recommend new data technologies that align to Company goals. Participate in solution design activities. Provide clear and timely communication on issues, risks, and changes to the development team and stakeholders. Perform other duties as assigned.",
        "url": "https://www.linkedin.com/jobs/view/3962955297"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3934825856,
        "company": "Avani Tech Solutions Private Limited",
        "title": "Data Engineer",
        "created_on": 1720638625.5124927,
        "description": "Title:: Data Engineer Working Model: Remote. Pay range: $58.77-$62.82 Looking for W2 resources Job Description We are seeking a skilled Data Engineer to join our Data Science team under the Consumer organization. The Data Engineer will work with data from all parts of the company like netops, field ops, call centers, sales, and finance. The ideal candidate will build automated data pipelines and data wrangle Big Data. Responsibilities Design, build, maintain, debug, and improve data pipelines Assemble large, complex data sets Create business intelligence dashboards Build, improve and maintain data infrastructure documentation and data dictionaries Support the Data Science team by providing clean, reliable, and timely data for building predictive models for churn, customer lifetime value, upgrade propensity, and customer satisfaction Required Qualifications 2+ years of experience as a Data Engineer in a similar role Advanced working knowledge of SQL (e.g., Microsoft SQL Server, PostgreSQL, MySQL, etc.) Experience building automated ETLs (e.g., Alteryx, Apache Airflow/Beam/NiFi, dbt, Dataiku, AWS Glue, etc.) Experience building BI dashboards (e.g., PowerBI, Tableau, Sisense, etc.) Experience working in data warehouses & data lakes (e.g., Teradata, DataBricks, Snowflake, etc.) Experience working in cloud platforms (e.g., AWS, Azure, GCP) Familiar with Python and Jupyter notebooks Experience wrangling with large, messy, and undocumented datasets Excellent communication and teamwork abilities Preferred Qualifications The ideal candidate would be familiar with this specific tech stack below. The more of this specific tech stack the candidate already knows, the better. Microsoft SQL Server Alteryx PowerBI DataBricks AWS (especially S3, Lambda, IAM) Experience supporting a Data Science team. Familiarity with data involving telecoms, mobile providers, ISPs, or cable companies.",
        "url": "https://www.linkedin.com/jobs/view/3934825856"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Seattle, WA",
        "job_id": 3969278291,
        "company": "Amazon",
        "title": "Data Engineer",
        "created_on": 1720638627.1853359,
        "description": "Description The AWS Network Finance and Data Transfer Data Engineering team provides foundational and centralized data platform for AWS Finance (supporting AWS Networking, AWS CloudFront, AWS Direct Connect) to identify financial insights for better understanding of our customers and costs. Our teams take on some of the hardest scalability, performance, and distributed computing challenges. We process big data and provide tools for customers to interactively understand the copious amounts of data we store. We are looking for experienced, self-driven Data Engineer. In this role, you will be building complex data engineering and business intelligence applications using AWS big data stack. You should have deep expertise and passion in working with large data sets, data visualization, building complex data processes, performance tuning, bringing data from disparate data stores and programmatically identifying patterns. You should have excellent business acumen and communication skills to be able to work with business owners to develop and define key business questions and requirements. You will provide guidance and support for other engineers with industry best practices and direction. Amazon Web Services (AWS) has culture of data-driven decision-making, and demands timely, accurate, and actionable business insights. Key job responsibilities Design, implement, and support data warehouse/ data lake infrastructure using AWS bigdata stack, Python, Redshift, QuickSight, Glue/lake formation, EMR/Spark, Athena etc. Develop and manage ETLs to source data from various financial, AWS networking and operational systems and create unified data model for analytics and reporting. Creation and support of real-time data pipelines built on AWS technologies including EMR, Glue, Redshift/Spectrum and Athena. Collaborate with other Engineering teams, Product/Finance Managers/Analysts to implement advanced analytics algorithms that exploit our rich datasets for financial model development, statistical analysis, prediction, etc. Continual research of the latest big data and visualization technologies to provide new capabilities and increase efficiency. Use business intelligence and visualization software (e.g., QuickSight) to develop dashboards those are used by senior leadership. Empower technical and non-technical, internal customers to drive their own analytics and reporting (self-serve reporting) and support ad-hoc reporting when needed. Working closely with team members to drive real-time model implementations for monitoring and alerting of risk systems. Manage numerous requests concurrently and strategically, prioritizing when necessary Partner/collaborate across teams/roles to deliver results. Mentor other engineers, influence positively team culture, and help grow the team. Basic Qualifications 3+ years of data engineering experience Experience with data modeling, warehousing and building ETL pipelines Experience with SQL Bachelor's degree in computer science, engineering, analytics, mathematics, statistics, IT or equivalent Experience of designing and developing data engineering systems from grounds up and maintaining/supporting existing systems. Experience in dealing with large and complex data sets and performance tuning Proficiency in one of the scripting languages - Python, Ruby, or similar Experience operating large data warehouses or data lakes Experience in designing data models that supports structures and unstructured data Experience in gathering requirements and formulating business metrics for reporting Ability to deal with ambiguities and competing priorities. Preferred Qualifications Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions Experience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases) Master's degree in computer science, engineering, analytics, mathematics, statistics, IT or equivalent Prior experience in programming using Python Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us. Our compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $118,900/year in our lowest geographic market up to $205,600/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. For more information, please visit https://www.aboutamazon.com/workplace/employee-benefits. This position will remain posted until filled. Applicants should apply via our internal or external career site. Company - Amazon.com Services LLC Job ID: A2692660",
        "url": "https://www.linkedin.com/jobs/view/3969278291"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Plano, TX",
        "job_id": 3967439730,
        "company": "Stellent IT",
        "title": "Data Engineer",
        "created_on": 1720638628.8649068,
        "description": "Ridgefield Park, NJ or Plano, Texas(4 days on site a week) Phone and Video Long Term Job Description: Location: Ridgefield Park, NJ or Plano, Texas Team is hybrid (4 in office, Friday remote) 4 years of experience in the Big data Solutions on GCP. Expertise in Python, Bigquery, Kubernetes and Airflow is a must have. Summary The main function of the Data Engineer is to develop, evaluate, test and maintain architectures and data solutions within our organization. The typical Data Engineer executes plans, policies, and practices that control, protect, deliver, and enhance the value of the organization's data assets. Necessary Skills And Attributes 3+ years of code based ETL development using python and SQL 3+ years of experience writing complex SQL queries. 3+ years of Python development experience 2+ years of experience on GCP services such as Bigquery, Kubernetes and Composer 2+ years of working experience in Apache Airflow Experience in developing high-performance, reliable and maintainable code. Analytical and problem-solving skills, applied to Big Data domain. Experience and understanding of Big Data engineering concepts. End to End exposure and understanding of Data engineering projects. Experience on spark and Dataproc is a plus. Proven understanding and hands on experience with github, development IDEs such as VS code. B.S. or M.S. in Computer Science or Engineering Bachelors or Masters in Computer Engineering. Pankaj Kumar IT Technical Recruiter Phone : 321-766-9495 Email: pankaj@stellentit.com Gtalk: pankaj@stellentit.com",
        "url": "https://www.linkedin.com/jobs/view/3967439730"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Irving, TX",
        "job_id": 3921603214,
        "company": "Syntricate Technologies",
        "title": "Data engineer",
        "created_on": 1720638630.4587018,
        "description": "Position-Data Engineer Duration-Contract Location- IRVING, TX JD 10+ years of experience in solutioning data pipeline for large enterprise data Warehouse applications using AWS, Data Lake, Data Ingestion, Data Transformation, /Computation, Orchestration, Reporting & Data Analytics. Good experience with Schema design, ETL setup, Batch jobs setup / custom scripting, data curation and aggregation. Postgres, MongoDB, Strong knowledge in CICD Pipeline for automatic deployment. Strong knowledge on design and integration patterns Proficient in technical artifacts e.g., Application Architecture, Solution Design Documents, etc .Strong at analytical and problem-solving skills, Experience working with multi-vendor, multi-culture, distributed offshore and onshore development teams in dynamic and complex environment. Must have excellent written and verbal communication skills? Experience working in Agile delivery Mongo DB , AWS Regards, Pallavi Verma Sr. Technical Recruiter | Syntricate Technologies Inc. Direct : 781-552-4333 | Email : pallavi@syntricatetechnologies.com | Web: www.syntricatetechnologies.com We're hiring! connect with us on LinkedIn and visit our Jobs Portal Minority Business Enterprise (MBE) Certified | E-Verified Corporation | Equal Employment Opportunity (EEO) Employer This e-mail message may contain confidential or legally privileged information and is intended only for the use of the intended recipient(s). Any unauthorized disclosure, dissemination, distribution, copying or the taking of any action in reliance on the information herein is prohibited. Please notify the sender immediately by email if you have received this email by mistake and delete this e-mail from your system. You have received this email as we have your email address shared by you or from one of our data sources or from our member(s) or subscriber(s) list. If you do not want to receive any further emails or updates, please reply and request to unsubscribe .",
        "url": "https://www.linkedin.com/jobs/view/3921603214"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Seattle, WA",
        "job_id": 3956421403,
        "company": "Ascendion",
        "title": "Data Engineer",
        "created_on": 1720638632.51387,
        "description": "About Ascendion : Ascendion is a full-service digital engineering solutions company. We make and manage software platforms and products that power growth and deliver captivating experiences to consumers and employees. Our engineering, cloud, data, experience design, and talent solution capabilities accelerate transformation and impact for enterprise clients. Headquartered in New Jersey, our workforce of 6,000+ Ascenders delivers solutions from around the globe. Ascendion is built differently to engineer the next. Ascendion | Engineering to elevate life We have a culture built on opportunity, inclusion, and a spirit of partnership. Come, change the world with us: Build the coolest tech for world’s leading brands Solve complex problems - and learn new skills Experience the power of transforming digital engineering for Fortune 500 clients Master your craft with leading training programs and hands-on experience Experience a community of change makers! Join a culture of high-performing innovators with endless ideas and a passion for tech. Our culture is the fabric of our company, and it is what makes us unique and diverse. The way we share ideas, learning, experiences, successes, and joy allows everyone to be their best at Ascendion. About the Role: Position: Data Engineer Location: Remote Summary: This Candidate will be helping to build the data pipeline for personalization and AI experience for players. These services include but are not limited to: Create Cosmos streams and write scope queries to read and extract data from Cosmos, write queries to create Power BI and Geneva Dashboards, ensure data pipeline are stable, supportable, extensible, and trusted by business partners. Vendor will implement data processing and monitoring standards to address data latency & data quality. Vendor will help author data ETL pipelines on Azure SQL database and collaborate with engineers and PMs to provide technical oversight, ensuring clarity with the right results are being met. Vendor will assist in marching towards a well-defined, long-term architecture that will allow Team to better solve business problems. Top Skills: Minimum 5 years experience with Cosmos. Minimum 5 years experience with Scope queries. Minimum 5 years experience with Data ETL pipelines. Qualifications: • Bachelor's degree in a technical field such as computer science, computer engineering or related field required. MBA or other related advanced degree preferred • 5-7 years experience required • Experience with broad technology delivery, including custom development using various development languages • Experience with providing technical leadership, mentoring on software engineering design, development, and frameworks, analyzing alternatives, and presenting technical options to leads and clients • Experience with driving technical planning and solutions for small software portfolio, including authoring or managing the development of software designs, managing vendor evaluations, and justifying technical recommendations • Ability to apply fundamental concepts, processes, practices, and procedures on technical assignments • Requires practical experience and training. Salary Range: $130,000 - 143,000 Annually - Factors that may affect pay within this range may include geography/market, skills, education, experience and other qualifications of the successful candidate. Benefits: The Company offers the following benefits for this position, subject to applicable eligibility requirements: [medical insurance] [dental insurance] [vision insurance] [09-10 days/hours of paid time off] Want to change the world? Let us know. Tell us about your experiences, education, and ambitions. Bring your knowledge, unique viewpoint, and creativity to the table. Let’s talk!",
        "url": "https://www.linkedin.com/jobs/view/3956421403"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Seffner, FL",
        "job_id": 3957772338,
        "company": "Take2 Consulting, LLC",
        "title": "Data & Analytics Engineer (Contract-to-Hire)",
        "created_on": 1720638634.26203,
        "description": "Data & Analytics Engineer Tasked with working collaboratively to define and develop custom reports and dashboards that deliver information and insights to support all areas of the business. As an Analytics Engineer, you will play a crucial role in designing, developing, and maintaining our advanced analytics solutions within the Microsoft Azure ecosystem. As a member of the Data Management team, there is an expectation that you will support application development teams as a subject matter specialist in the areas of SQL and report development. The ideal candidate will have the ability to support multiple simultaneous projects, possess a passion for operational excellence and possess excellent interpersonal and soft skills. Responsibilities: Design and develop end-to-end analytics solutions using Microsoft Azure technologies, including SQL Server Management Studio (SSMS)/ Azure Data Studio, Azure Data Factory (ADF), Databricks, and Power BI. Collaborate with business users, data engineers, and other stakeholders to gather requirements, understand data sources, and define data models. Extract, transform, and load (ETL) data from various sources into the Azure data platform, ensuring data quality and integrity. Develop and maintain data pipelines and workflows using Azure Data Factory, Databricks, and other relevant tools to ensure efficient and timely data processing and availability. Create visually compelling and interactive dashboards, reports, and data visualizations using Power BI, enabling stakeholders to gain actionable insights from the data. Monitor and optimize the performance of Azure-based data solutions, identifying and resolving issues to ensure smooth and efficient operation. Stay up to date with the latest trends and advancements in the Microsoft Azure ecosystem and business intelligence domain, identifying opportunities for improvement and innovation. Assist in troubleshooting production code integration issues. Qualifications: Bachelor's degree in Computer Science, Management Information Systems, or a related field. Certifications with strong portfolio exemplifying expertise. Proven experience as an Analytics Engineer, Business Intelligence Developer, or similar role, with expertise in the Microsoft Azure stack. Strong proficiency in Azure Data Factory, SSMS, Databricks, and Power BI. Strong proficiency in SQL and experience with relational databases and data modeling concepts. Understanding Python is a plus. 3 years or more hands-on technical experience with T-SQL development. Experience in designing and implementing ETL processes and data pipelines. Familiarity with data warehousing principles, dimensional modeling, and data integration techniques. Excellent analytical and problem-solving skills, with the ability to work with large and complex datasets. Strong understanding of data visualization principles and best practices. Ability to collaborate effectively with cross-functional teams and communicate complex technical concepts to non-technical stakeholders. Benefits: Medical, dental, and vision insurance 401(k) with company match Associate discounts including furniture Company paid life and disability insurance Paid time off Employee Assistance Program Wellness Programs And more! We do not discriminate in hiring or employment against any individual on the basis of race, color, gender, national origin, ancestry, religion, physical or mental disability, age, veteran status, sexual orientation, gender identity or expression, marital status, pregnancy, citizenship, or any other factor protected by anti-discrimination laws Applicants must be authorized to work in the U.S.",
        "url": "https://www.linkedin.com/jobs/view/3957772338"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Memphis, TN",
        "job_id": 3928851666,
        "company": "Syntricate Technologies",
        "title": "Data Engineer",
        "created_on": 1720638635.9386857,
        "description": "Job Title: Data Engineer - Must Have Aws experience Location: Memphis, TN - Onsite / Hybrid Type: Fulltime Job Summary: The data engineer's main responsibility is to design, develop, and maintain data pipelines and infrastructure on the AWS platform. Working with large volumes of data, ensuring its quality, reliability, and accessibility. Tasks may include data ingestion, transformation, and storage, data sharing and consumption as well as implementing data security and privacy measures. This role is crucial in enabling efficient and effective data-driven decision-making within the company. Essential Duties And Responsibilities Build and maintain scalable and reliable data pipelines, ensuring the smooth flow of data from various sources to the desired destinations in AWS cloud environment. Work closely with stakeholders to understand their data requirements and designing data solutions that meet their needs. This includes understanding data models/schemas, and implementing ETL (Extract, Transform, and Load) processes to transform raw data into a usable format in destination. Responsible for monitoring and optimizing the performance of data pipelines, troubleshooting any issues that arise, and ensuring data quality and integrity. Qualifications & Technology Proficient in programming languages such as Python, as well as SQL for database querying and manipulation. Strong understanding of AWS services related to data engineering, such as Amazon S3, Amazon Redshift, Amazon Aurora Postgres, AWS Glue, AWS Lambda, AWS Step Function, AWS Lake Formation, Amazon Data Zone, Amazon Kinesis, MSK and Amazon EMR. Knowledge of database design principles and experience with database management systems Experience with data storage technologies like relational databases (e.g., SQL Server, PostgreSQL) and distributed storage systems (e.g., PySpark) is essential. Understanding of Extract, Transform, Load (ETL) processes and experience with ETL tools like AWS Glue and SQL Server Integration Services is important. You should be skilled at integrating disparate data sources and ensuring data quality and consistency. Understanding and experience with orchestration tools like Apache Airflow, AWS Glue Workflows, AWS Step Functions and notification services. Familiarity to IAC such as Terraform, git and DevOps pipelines Awareness of data governance practices, data privacy regulations, and security protocols is crucial. Experience implementing data security measures and ensuring compliance with relevant standards is desirable. Strong analytical thinking and problem-solving abilities are essential to identify and resolve data-related issues effectively. You should be able to analyze complex data sets, identify patterns and derive actionable insights. Education And Experience Minimum Required: Bachelor's Degree in Computer Science or related field or equivalent experience",
        "url": "https://www.linkedin.com/jobs/view/3928851666"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3970059125,
        "company": "Wilson Language Training",
        "title": "Data/Analytics Engineer",
        "created_on": 1720638637.7510934,
        "description": "Here at Wilson Language Training, we are committed to working together for our mission to achieve literacy for all. We believe literacy is a fundamental right and should be attainable for all people. We strive to reflect this belief in our work. The success of our team members is no less paramount. We’re dedicated to ensuring that every Wilson employee experiences truly satisfying professional development while feeling inspired to bring their authentic selves to work. Are you ready to be a changemaker? Wilson Language training is growing and is looking to hire a Data/Analytics Engineer. As an Analytics Engineer you will bridge the gap between data engineering and data analysis. You will be responsible for building and maintaining robust data pipelines, transforming raw data into actionable insights, and collaborating with various teams to support data-driven decision-making. Key Responsibilities Data Pipeline Development: Design, develop, and maintain scalable data pipelines to ingest, process, and store large volumes of data from various sources. Data Modeling: Create and manage data models to support analytics and reporting needs, ensuring data accuracy and consistency. ETL Processes: Implement ETL (Extract, Transform, Load) processes to ensure the smooth flow of data from source systems to data warehouses. Collaboration: Work closely with data analysts and business stakeholders to understand data requirements and deliver data solutions that meet their needs. Data Quality: Implement data quality checks and monitoring to ensure the reliability and accuracy of data. Optimization: Optimize data processing and storage for performance and cost efficiency. Documentation: Maintain comprehensive documentation of data pipelines, models, and processes. AdHoc Querying & Data Integrations: Assist Business Systems team with adhoc querying and reporting from ERP/CRM systems. Help with data integrations between these systems and our custom built applications. Innovation: Stay up-to-date with industry trends and best practices in data engineering and analytics, and apply them to improve existing processes. Understand and display WLT’s values. Other duties as assigned Qualifications Education: Bachelor’s degree in Computer Science, Data Science, Information Technology, or a related field. A master’s degree is a plus. Experience: 3+ years of experience in data engineering, analytics, or a related field. Technical Skills: Proficiency in Microsoft SQL, dBT, Python, and/or R. Experience with data pipeline tools (e.g., Apache Airflow, Luigi) and data warehousing solutions (Azure SQL, Azure Fabric, Azure Synapse). Tools: Familiarity Microsoft Power BI. Cloud Platforms: Experience with cloud platforms like Azure Fabirc. Problem-Solving: Strong analytical and problem-solving skills with a keen attention to detail. Communication: Excellent communication skills with the ability to explain complex technical concepts to non-technical stakeholders. Preferred Qualifications Knowledge of machine learning and predictive analytics. Experience with version control systems (e.g., Git). Understanding of data governance and security best practices. Wilson has identified the anticipated pay range for this role based on the many factors that we consider in defining compensation levels for our roles, including market data, and internal equity considerations. Actual pay, and allocation between base and any target discretionary bonus, will vary based on geographic location, education, work experience, skills, market data, and internal equity considerations. Wilson offers competitive benefits, including: Medical, dental, vision, and Life & Disability Insurance 401k plan with partial employer match Paid Time Off Paid holidays Tuition reimbursement “O’Connor days,” which refers to a company-wide office closure between Christmas and New Year’s Eve, as well as other perks. Anticipated Salary range: $117,000 - $152,000. Wilson Language Training is an Equal Opportunity, Drug-Free Employer Committed to Diversity in the Workplace. M/W/D/V",
        "url": "https://www.linkedin.com/jobs/view/3970059125"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Dallas, TX",
        "job_id": 3967444689,
        "company": "Trident Consulting",
        "title": "Data Engineer",
        "created_on": 1720638639.4628625,
        "description": "Trident Consulting is seeking a \" Data Engineer \" for one of our clients in \" Dallas, TX \" A global leader in business and technology services. Role: Data Engineer Location: Dallas, TX Duration: Contract Roles & Responsibilities Possess a strong background in Abinitio, SQL, Python, AWS, and Unix, with hands-on experience in these technologies. Demonstrate excellent project management skills, with a proven track record of successfully delivering complex projects. Exhibit strong analytical and problem-solving abilities, with the capacity to make data-driven decisions. Show exceptional communication and interpersonal skills, with the ability to collaborate effectively with diverse teams. Display a proactive and results-oriented mindset, with a focus on achieving business objectives. About Trident Trident Consulting is an award-winning IT/engineering staffing company founded in 2005 and headquartered in San Ramon, CA. We specialize in placing high-quality vetted technology and engineering professionals in contract and full-time roles. Trident's commitment is to deliver the best and brightest individuals in the industry for our clients' toughest requirements. Some of our recent awards include \" 2022, 2021, 2020 Inc. 5000 fastest-growing private companies in America \" 2022, 2021 SF Business Times 100 fastest-growing private companies in Bay Area",
        "url": "https://www.linkedin.com/jobs/view/3967444689"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Raleigh, NC",
        "job_id": 3925159194,
        "company": "Mayne Pharma",
        "title": "Data Engineer",
        "created_on": 1720638641.254554,
        "description": "Description Position Overview: We are seeking a talented Data Engineer to join our team. The ideal candidate will have a strong foundation in data analysis, experience with data warehousing techniques, proficiency in SQL for data manipulation and querying, and expertise in data visualization using Power BI. The Data Engineer will play a crucial role in extracting insights from our data assets to support business decision-making processes. Key Responsibilities: Work closely with cross-functional teams to understand data requirements and develop analytical solutions. Design and maintain data models within the data warehouse environment to support reporting and analysis needs. Write complex SQL queries to extract, transform, and manipulate data from various sources. Develop interactive and visually compelling dashboards and reports using Power BI to communicate insights to stakeholders. Perform ad-hoc data analysis to address business questions and support strategic initiatives. Collaborate with IT teams to ensure data quality, integrity, and security standards are maintained. Stay updated on industry best practices and emerging trends in data analytics and visualization. Qualifications: Bachelor's degree in Computer Science, Statistics, Mathematics, Economics, or related field. 2-5 years of proven experience as a Data Engineer or similar role. Strong proficiency in SQL and experience with relational databases (e.g., SQL Server, MySQL, PostgreSQL). Solid understanding of data warehousing concepts and methodologies. Hands-on experience with data visualization tools, particularly Power BI (Tableau or similar tools will also be considered). Excellent analytical and problem-solving skills with a keen attention to detail. Ability to communicate complex findings and technical concepts effectively to non-technical stakeholders. Strong organizational and time-management skills with the ability to manage multiple priorities in a fast-paced environment. Experience with programming languages (e.g., Python, R) and ETL tools (e.g., SSIS, Pentaho) is a plus. Benefits: Competitive salary commensurate with experience. Comprehensive benefits package including health, dental, and vision insurance. Retirement savings plan with company match. Flexible work arrangements and opportunities for professional development. Dynamic and collaborative work environment with opportunities for growth and advancement. Equal Employment Opportunity and Employment Eligibility Mayne Pharma provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. Mayne Pharma also complies with all applicable national, state and local laws governing nondiscrimination in employment as well as work authorization and employment eligibility verification requirements of the Immigration and Nationality Act and IRCA. Mayne Pharma is an E-Verify employer. Any communication you receive from Mayne Pharma will never come from a personal email domain or chat service such as \"gmail\" or \"Google Chat\". All official communication from Mayne Pharma will be received from our company email domain - @maynepharma.com.",
        "url": "https://www.linkedin.com/jobs/view/3925159194"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3963955968,
        "company": "ClifyX",
        "title": "Data Engineer",
        "created_on": 1720638643.0559819,
        "description": "- Client Flex Full Time - salary range - Job Title: Data Engineer : with Python and SQL Experience. Full Time Visa Status- Apart H1B, anything is fine. Location Remote: Flexible until the pandemic The end Client is Facebook 1st Round: 1 HourHands-on-technical Coding Interview 2nd Round Discussion for 30 Mins. Responsibilities:- Utilize strong SQL & Python expertise to engineer sound data pipelines and conduct routine and ad hoc analysis to assess the performance of legacy products and the saliency of new features. Build reporting dashboards and visualizations to design, create and track campaign/program KPIs Perform analyses on large data sets to understand drivers of marketing engagement and provide recommendations on campaign and product optimization Project manage end-to-end process of analytics tooling feature development, including request intake, requirements evaluation, cross-functional team alignment, feature execution, QA testing, and stakeholder communications Interface and consult with marketers, analysts, and cross-functional partners to understand their reporting and data needs, serving as the point of contact for requests, inquiries, and action items Interface with other data engineering, product, and data science teams to implement client needs and initiatives. TLDR: Strong SQL and Python skills are essential to be successful at this role. So if you see people who have worked as Data Engineer - Ask them to rate themselves on Python and SQL. Above is for your reference If the rating is 3 or above, then we are good. I will send you the document which has the detail on how the rating should be done.",
        "url": "https://www.linkedin.com/jobs/view/3963955968"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "South Burlington, VT",
        "job_id": 3932047680,
        "company": "OnLogic",
        "title": "Data Engineer",
        "created_on": 1720638644.7093446,
        "description": "We are seeking a talented and motivated Data Engineer that will play a critical role in designing, building, and maintaining the data infrastructure supporting our business intelligence, reporting and AI initiatives. You will work closely with data scientists, machine learning engineers, and other stakeholders to ensure seamless data flow and availability while enabling cutting-edge AI solutions. Additionally, you are responsible for fully understanding the data contained in our Enterprise IT systems and how it can be effectively used to drive business value. As OnLogic continues to grow quickly, your ownership of the data infrastructure will be integral to current and future decision making for all departments across the globe. On an average day, you'll... Design, develop, and maintain scalable data pipelines (ETL) to process large volumes of structured and unstructured data from diverse sources. Build and manage data warehouses, optimizing for performance and scalability. Work with data scientists, analysts, and subject matter experts to understand data requirements and develop appropriate data models and schemas. Implement data governance and security protocols to ensure data privacy and compliance with regulatory requirements. Collaborate with cross-functional teams to understand business requirements and translate them into technical specifications. Monitor and optimize data systems for performance, reliability, and cost-efficiency. Automate repetitive data processing tasks to improve efficiency and reduce manual effort. Document data architecture, processes, and systems to ensure maintainability and knowledge sharing within the team. The team you will be joining: Our IT team touches every aspect of our business and each of our day to day lives here at OnLogic. Made up of software developers, data scientists, architects, infrastructure and security experts, systems and network engineers and business technology professionals, our IT team is constantly working to improve our internal systems and enable cutting-edge solutions to make our lives easier, while optimizing our website to do the same for our customers. From the technology we use to do our jobs, to the infrastructure that supports the way we build, test and ship our products, our IT team keeps our business running smoothly. Learn more about Life at OnLogic. Requirements You have a Bachelor's or Master's degree in Computer Science, Information Systems, Business Analytics, or a similar technical or business field or equivalent experience. Five (5) years of general technical background working closely with or part of an Information Technology team. You have three (3) years professional experience regarding data models, database design development, data mining and segmentation techniques. Strong knowledge of and experience with databases, ETL, maintaining data pipelines,  and SQL. Proficiency in one or more programming languages such as Python or Typescript. Experience with Google Cloud Platform (particularly BigQuery) or other cloud services is highly desirable. Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy. Ability to work in the U.S. without visa sponsorship. Satisfactory completion of a background check. Who we're looking for: An experienced Data Engineer that enjoys the challenges and opportunities offered by technology. A self starter with the ability to work in a fast paced environment, thinking independently and acting proactively. Effective communicator, delivering critical feedback in a constructive way while working collaboratively cross-functionally. Technically adept and embraces innovation and automation with a mind toward continuous improvement. Who we are: OnLogic is growing, and we want to give you the same opportunity to grow in your career! We design and manufacture specialized computers and hardware solutions for companies all over the world, helping them to make the seemingly impossible possible. Our computers are designed to work where others would fail, and it's our vision to be the first choice in industrial computing. To make that vision a reality, we've built a team of ambitious problem solvers, guided by the company's core values of Open, Fair, Innovative and Independent. We have an open office, open salaries and strive to be fair and transparent in our decision making. We encourage input and feedback from every member of our team and look to improve ourselves and our business every single day. Diversity is an essential element of our core values. Not just respecting, but actively embracing a variety of backgrounds, life experiences, and opinions, helps us foster innovation, enhances our problem-solving capabilities, and promotes learning and engagement among the members of our team. We strongly encourage those with diverse backgrounds to apply. We are committed to providing a safe, inclusive, and harassment-free workplace for all employees. We do not tolerate any form of harassment, discrimination, or bias based on race, ethnicity, gender, sexual orientation, religion, disability, age, or any other protected characteristic. To learn more about our values, our mission and what it's like to work at OnLogic, visit www.onlogic.com/careers. Benefits The salary range for this role is $80,000 to $120,000. We determine final compensation based on discussions with applicants and their experience in similar roles. A competitive Salary based upon your experience and the requirements of the role A comprehensive Benefits package 401k Plan with 3% Employer Contribution An Annual Profit Share Bonus Paid Maternity & Paternity Leave, and Short & Long Term Disability Opportunity to Participate in our Employee Stock Purchase Plan A personal development plan created to help you (and us) grow",
        "url": "https://www.linkedin.com/jobs/view/3932047680"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Mooresville, NC",
        "job_id": 3961461426,
        "company": "Kforce Inc",
        "title": "Data Engineer",
        "created_on": 1720638646.4297166,
        "description": "Responsibilities Kforce has a client that is seeking a Data Engineer in Mooresville, NC. Essential Functions: Translates complex cross-functional business requirements and functional specifications into logical program designs, code modules, stable application systems, and data solutions; Partners with Product Team to understand business needs and functional specifications Collaborates with cross-functional teams to ensure specifications are converted into flexible, scalable, and maintainable solution designs; Evaluates project deliverables to ensure they meet specifications and architectural standards Coordinates, executes, and participates in component integration (CIT) scenarios, systems integration testing (SIT), and user acceptance testing (UAT) to identify application errors and to ensure quality software deployment Data Engineering Responsibilities Executes the development, maintenance, and enhancements of data ingestion solutions of varying complexity levels across various data sources like DBMS, File systems (structured and unstructured), APIs and Streaming on on-prem and cloud infrastructure; Demonstrates strong acumen in Data Ingestion toolsets and nurtures and grows junior members in this capability Builds, tests, and enhances data curation pipelines integration data from wide variety of sources like DBMS, File systems, APIs, and streaming systems for various KPIs and metrics development with high data quality and integrity BI Engineering Responsibilities Responsible for the development, maintenance, and enhancements of BI solutions of varying complexity levels across different data sources like DBMS, File systems (structured and unstructured) on-prem and cloud infrastructure; Creates level metrics and other complex metrics; Use custom groups, consolidations, drilling, and complex filters Demonstrates database skill (Teradata/Oracle/Db2/Hadoop) by writing views for business requirements Requirements Bachelor's degree in Engineering/Computer Science, CIS, or related field (or equivalent work experience in a related field) 5 years of experience in Data or BI Engineering, Data Warehousing/ETL, or Software Engineering 4 years of experience working on project(s) involving the implementation of solutions applying development life cycles (SDLC) Technical Competencies Agile Development Big Data Management and Analytics Cloud Computing Database Design (Physical) Release Management Additional Skills Strong knowledge of data processing in Scala or Python Experience with data modeling and query optimization Deep understanding of BigQuery architecture, best practices, and performance optimization Proficiency in LookML for building data models and metrics Experience with DataProc for running Hadoop/ Spark jobs on GCP Knowledge of configuring and optimizing DataProc clusters The pay range is the lowest to highest compensation we reasonably in good faith believe we would pay at posting for this role. We may ultimately pay more or less than this range. Employee pay is based on factors like relevant education, qualifications, certifications, experience, skills, seniority, location, performance, union contract and business needs. This range may be modified in the future. We offer comprehensive benefits including medical/dental/vision insurance, HSA, FSA, 401(k), and life, disability & ADD insurance to eligible employees. Salaried personnel receive paid time off. Hourly employees are not eligible for paid time off unless required by law. Hourly employees on a Service Contract Act project are eligible for paid sick leave. Note: Pay is not considered compensation until it is earned, vested and determinable. The amount and availability of any compensation remains in Kforce's sole discretion unless and until paid and may be modified in its discretion consistent with the law. This job is not eligible for bonuses, incentives or commissions. Kforce is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.",
        "url": "https://www.linkedin.com/jobs/view/3961461426"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Independence, OH",
        "job_id": 3969678550,
        "company": "Flynn Group",
        "title": "Data Engineer",
        "created_on": 1720638648.311854,
        "description": "Job Description Position Description The Data Engineer will play a crucial role in designing, implementing, and maintaining our organization's data infrastructure. This role will contribute to the efficient storage, retrieval, and analysis of data, enabling informed decision-making across our organization. Responsibilities Collaborate with cross-functional teams to understand data requirements to design efficient data warehouse solutions Develop and maintain a robust data warehouse architecture that ensures scalability, performance, and data integrity Implement pipelines to move raw data in Azure Synapse using Azure Data Factory, SQL, Python, C# in line with well-established architectural standard related to Data Lakehouse and Data Warehouse modeling standards Data Quality Assurance - ensure data accuracy, consistency, and integrity throughout all processes, and implement data governance best practices. Monitor and tune the data warehouse performance to ensure optimal query execution and data retrieval times Identify and resolve bottlenecks in the ETL pipelines and data warehouse infrastructure Develop and maintain data models, including dimensional and star schemas, to support efficient querying and reporting Collaborate with reporting and analytics teams to understand requirements and translate them into effective data structures Implement security measures to protect sensitive data within the data warehouse Work closely with developers, analysts, and other stakeholders to understand their data needs and provide necessary support Document data warehouse processes, data dictionaries, and ETL workflows for knowledge sharing and future reference Design, implement, and document data architecture and data modeling solutions Participate with Data Analyst(s) as needed to define minimal viable data assets in support of the visual needs. Review and provide architectural guidance for analytic solutions Triage and troubleshoot data anomalies submitted by the user community Knowledge, Skills, & Abilities Experience with ETL tools and techniques for data integration Strong understanding of data modeling concepts, including dimensional modeling and normalization Strong communication skills to collaborate with technical and non-technical stakeholders Experience with version control systems (e.g., Git) and agile development methodologies is a plus Experience in Python/PySpark notebooks Experience in Azure Data Factory or SSIS, Azure DW, Azure Data Lake, and Power BI Experience with Microsoft Fabric a plus Requirements Proven experience as a Data Warehouse Engineer or in a similar role Practical experience in Python related to data engineering (spark, pandas, etc) Practical experience in SQL related to data engineering Proficiency in designing and implementing data warehouse solutions using technologies such as MS SQL databases and Azure Synapse or other cloud-based data warehousing services Ability and initiative to learn and research new concepts, ideas, and technologies quickly Ability to work independently as well as function as an integral part of a team, take initiative and contribute in a fast-paced environment Why Work for Flynn? Flynn Group offers a variety of benefits and perks to encourage and empower our employees. We are committed to helping each employee work and live to his or her fullest potential. We offer a variety of benefits and perks while working for us: Medical/Dental/Vision Retirement and Savings Plan Short- and Long-Term Disability Basic Life Insurance Voluntary Life Insurance Tuition Reimbursement Paid Time Off Flexible/Hybrid Work Schedules Company Outings Dining Discounts PC/Laptop Purchase Assistance On-Site Fitness Center On-Site Daycare On-Site Café FUN Work Environment! The Flynn Group is an Equal Opportunity Employer",
        "url": "https://www.linkedin.com/jobs/view/3969678550"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Provo, UT",
        "job_id": 3967155312,
        "company": "PrincePerelson and Associates",
        "title": "Data Engineer",
        "created_on": 1720638650.0793545,
        "description": "Data Engineer Hybrid / Provo, UT Senior Data/Analytics Role open. Great company that is profitable and growing. Looking for someone that has experience building data warehouses and working with ETL processes for moving data. SQL Skills and ideally you will have predictive modeling and built UI Interfaces to interact with the data. Python, R, and AWS. Qualifications That Set You Apart: Mastery in Data Engineering and Analytics: Armed with a BS Degree and over 5 years of hands-on experience, you're a seasoned navigator of data realms, adept at sculpting insights from raw information. Command of Data Languages: Fluent in the languages of data—SQL, Python, and R—you wield these tools with precision to orchestrate intricate data symphonies. Architect of Data Excellence: A maestro in database management, you sculpt data architectures with finesse, ensuring robust structures that stand the test of time. Visual Virtuoso: Your canvas extends beyond code—excel in data visualization, transforming complex insights into captivating stories that captivate and enlighten. The Science of Prediction: Seasoned in machine learning, statistical, and predictive modeling, you predict tomorrow's trends with today's data, guiding strategic decisions with foresight and confidence. Embark on a Data Adventure: Unleash Insights: Dive deep into intricate business challenges, uncovering hidden truths using cutting-edge statistical, algorithmic, and visualization techniques. Master Data Mastery: Gather and dissect data from diverse sources, turning chaos into clarity with expert finesse. Illuminate Findings: Skillfully translate complex results into actionable insights, empowering diverse audiences with crystal-clear revelations. Safeguard Data Integrity: Pave the way for flawless data integrity, triumphantly overcoming imperfections and ensuring unwavering reliability. Forge Data Frontiers: Lead the charge in sourcing new data streams, elevating value and bridging gaps with automated precision. Shape Predictive Horizons: Craft ingenious predictive models, offering innovative solutions and strategies to conquer business puzzles. Deploy with Precision: Collaborate seamlessly with engineering and BI wizards to deploy models, ensuring they soar to new heights. Catalyze Change: Rally allies across IT and beyond, revolutionizing analytics with persuasive influence and visionary tools. Statistical Sorcery: Apply advanced statistical techniques with finesse, unraveling data mysteries and revealing insights in a language everyone understands. Visualize Impact: Create mesmerizing data visualizations that captivate and clarify, ensuring impactful communication tailored to every audience. Code Craftsmanship: Collaborate effortlessly, delivering top-notch code with flawless precision, while pioneering iterative advancements. PrincePerelson & Associates is an Equal Opportunity Employer and we do not discriminate against applicants due to race, color, religion, sex, national origin, age, disability, genetics, veteran status, or on the basis of disability or any other federal, state or local protected class. All applicants applying for U.S. job openings must be authorized to work in the United States.",
        "url": "https://www.linkedin.com/jobs/view/3967155312"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Austin, TX",
        "job_id": 3960671336,
        "company": "Augment Jobs",
        "title": "Data Engineer",
        "created_on": 1720638651.9026182,
        "description": "Position Overview: We are seeking a talented and experienced Data Engineer to join our dynamic tech team. As a Data Engineer, you will be responsible for designing, constructing, and maintaining our data architecture and infrastructure. You will work closely with data scientists, analysts, and other stakeholders to understand data requirements and implement solutions that support our business goals. This role is crucial in ensuring our data pipelines are robust, scalable, and efficient, enabling seamless data extraction, transformation, and loading (ETL) processes. Roles And Responsibilities Data Architecture Design: Design and implement scalable and optimized data models and schemas. Data Pipeline Development: Develop and maintain robust ETL pipelines to process and transform data from various sources into formats suitable for analysis and reporting. Data Integration: Integrate new data sources and APIs into existing data pipelines. Data Quality Assurance: Implement data quality checks and monitoring to ensure data integrity and reliability. Performance Optimization: Optimize data infrastructure and processes for improved efficiency and performance. Data Security: Ensure data security and compliance with data protection regulations. Collaboration: Collaborate with data scientists, analysts, and business stakeholders to understand data requirements and deliver solutions that meet business needs. Documentation: Document data architecture, data flows, and processes for knowledge sharing and future reference. Continuous Improvement: Stay updated with emerging technologies and best practices in data engineering and contribute to continuous improvement initiatives. Skills And Qualifications Bachelor’s degree in Computer Science, Information Technology, or a related field; Master’s degree preferred. Proven experience (X years) as a Data Engineer or similar role in data management and analytics. Strong programming skills in languages such as Python, Java, Scala, or similar languages used in data processing. Experience with data warehousing solutions (e.g., Snowflake, Redshift, BigQuery) and database technologies (e.g., SQL, NoSQL). Proficiency in building and optimizing ETL pipelines and data integration workflows. Knowledge of cloud platforms such as AWS, Azure, or Google Cloud Platform for data storage and processing. Familiarity with data modeling, schema design, and metadata management. Understanding of data governance, data security, and compliance considerations. Excellent problem-solving skills and attention to detail. Strong communication and collaboration skills to work effectively in a cross-functional team environment. Compensation The compensation package for this full-time position includes a competitive salary commensurate with experience and skills. Additional benefits may include health insurance, retirement plans, paid time off, and professional development opportunities. Company Culture: Our company fosters a collaborative and innovative environment where team members are encouraged to explore new ideas and technologies. We value diversity and inclusion, and we strive for excellence in everything we do. Join us in shaping the future of our data infrastructure and making an impact through data-driven insights. Application Process: To apply for the position of Data Engineer, please submit your resume and a cover letter detailing your relevant experience and why you are interested in joining our team. We look forward to reviewing your application and discussing how your skills align with our needs. This job description outlines the key responsibilities, required skills, and potential compensation for a Data Engineer role in a technology company. Adjustments can be made based on specific company needs and industry standards.",
        "url": "https://www.linkedin.com/jobs/view/3960671336"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "New Hampshire, United States",
        "job_id": 3945735342,
        "company": "Alexander Technology Group",
        "title": "Data Engineer",
        "created_on": 1720638653.6277597,
        "description": "Alexander Technology Group is currently working with a Military Defense Client who is looking to hire a Data Engineer to join their team remotely. This is a 6-month contract-to-hire position. No Third Party or C2C candidates will be considered. Responsibilities: Build data pipelines from CRM, EPR, and other business applications for the data warehouse (on-prem). Ability to create data and standardize data pipelines across the organization with Python. Work with business units to automate manual processes. Clean and stage data from the data warehouse for the Business intelligence/Reporting team. Requirements: 5+ Years of Data Engineering Experience. Experience with SQL, PL/SQL, Data Warehousing, and Python is required. Experience with Apache Airflow or Prefect is a plus. Must be a US Citizen and be able to pass a background check and drug screen, no exceptions. Ability to work EST hours.",
        "url": "https://www.linkedin.com/jobs/view/3945735342"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Dallas, TX",
        "job_id": 3944474224,
        "company": "INSPYR Solutions",
        "title": "Data Engineer",
        "created_on": 1720638655.3928201,
        "description": "Title: Lead Data Engineer Location: Dallas, TX on site daily. No remote. Compensation: $150K to $175K plus bonus. Work Requirements: US Citizen, GC Holders or Authorized to Work in the US Job Description: The Lead Data Engineer is the senior software engineer in the Business Intelligence and Data Warehousing team. This role develops data ingestion to enable high-value business intelligence products. This is a senior individual contributor role with a focus on hands on development, collaborative problem solving, and mentorship of junior engineers. Successful candidates will directly influence solution designs to create a robust data platform to deliver a wide range of products. This role requires 12+ years of engineering experience including work on Azure Data Factory and Informatica Power Center / IICS. Responsibilities Design, develop, and maintain data pipelines. Deliver solutions using the MS Azure platform. Scope the data engineering roadmap and deliverables to assist project and sprint planning. Automate tasks and deploy production standard code with unit testing, continuous integration, versioning, etc. Work in an Agile framework and collaborate with team members to solve complex problems. Assist junior engineers with Production Support when needed to resolve issues. Skillset / Experience: Strong recent experience working with cloud data preferably Azure. Experience in Informatica IICS or Power Center Experience around Cloud Application Integration and Cloud Data Integration. Experience in developing application integration using SOAP/REST APIs and big data technologies such as Apache Spark. Strong experience in DevOps platforms such as GitHub with hands-on experience in branching, merge strategy, rebasing and reviewing history. Good knowledge of Rest V2, Swagger file generation and Python scripting. Preferred skills Working knowledge of SQL and data analysis. Experience in Kubernetes, Docker, Terraform, and/or Snowflake is a plus. Benefits & Perks: Time Off: 25 days of PTO for full-time employees and 12 company holidays. Company Paid Benefits: Life insurance, Short-term disability, Long-term disability, Paid parental leave, Employee Assistance Program, and medical insurance in our high deductible health plan. Optional Employee Paid Benefits: Medical insurance in our EPO plan, Dental benefits, and Vision benefits. We also offer Health Savings Accounts, Flexible Spending Accounts, Supplemental Life insurance, and more. 401(k): Eligible after 60 days. Discretionary company match of 50% up to the first 6% of contributions. About INSPYR Solutions: Technology is our focus and quality is our commitment. As a national expert in delivering flexible technology and talent solutions, we strategically align industry and technical expertise with our clients’ business objectives and cultural needs. Our solutions are tailored to each client and include a wide variety of professional services, project, and talent solutions. By always striving for excellence and focusing on the human aspect of our business, we work seamlessly with our talent and clients to match the right solutions to the right opportunities. Learn more about us at inspyrsolutions.com. INSPYR Solutions provides Equal Employment Opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability, or genetics. In addition to federal law requirements, INSPYR Solutions complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities.",
        "url": "https://www.linkedin.com/jobs/view/3944474224"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Scottsdale, AZ",
        "job_id": 3943043800,
        "company": "ONE (Open Network Exchange)",
        "title": "Data Engineer (On-site Scottsdale AZ)",
        "created_on": 1720638657.0683136,
        "description": "Job Type Full-time Description At Open Network Exchange (ONE), we are building integrated technology to modernize and revolutionize the e-commerce, loyalty, and technology space. ONE has built a full-stack, end-to-end e-commerce solution that drives enterprise level digital transformation for our partners across the travel, hospitality, retail, financial and other global industries. At ONE, we are a passionate and scrappy team coming together from a wide range of backgrounds to create innovative solutions for our business partners and build cool stuff. You will work with a world-class team of creative, supportive, entrepreneurial, and results-driven co-workers. You'll work hard both as an individual contributor and a team member, but at the end of the day you and your work will matter. You also will have the opportunity shape what and how products look like in our company and directly impact the future of our business. Job Description: We are looking to hire a Data Engineer to join the ONE team. Our ideal candidate is an overachiever who has a strong grasp of data fundamentals and back end requirements of data systems. The ideal candidate will be self motivated, finding what is needed and seeking solutions. You will be tasked with working within the call and sales center, establishing and supporting our data infrastructure. You will be working within the team to suggest improvements and work to build out our infrastructure to support the growing team. The ideal candidate will work to improve our state, cleaning up existing data and trying to move us toward a solid steady state. Duties/Responsibilities: Develop and support SQL back end reporting. Work hand in hand with the team in developing dbt semantic layer. Design, develop, and maintain scalable data pipelines and ETL processes. Collaborate with data architects and analysts to understand data requirements and implement data models. Optimize data storage and retrieval for performance and scalability. Ensure data quality and integrity through data validation and cleansing processes. Monitor and troubleshoot data pipelines to identify and resolve issues. Implement and maintain data security and privacy measures. Stay up-to-date with emerging technologies and industry trends in data engineering. Requirements Bachelor's degree in Computer Science, Engineering, or a related field. Proven experience as a Data Engineer or similar role. Strong proficiency in SQL and experience with relational databases. Proficiency in programming languages such as Python, Java, or Scala. Experience with data modeling, ETL processes, and data warehousing concepts. Familiarity with cloud-based data platforms (e.g., AWS, Azure, GCP). Knowledge of big data technologies (e.g., Hadoop, Spark) is a plus. Excellent problem-solving and analytical skills. Strong communication and collaboration abilities. Physical Requirements Prolonged periods of sitting at a desk and working on a computer, typically in a cubicle environment (constant noise, fluorescent overhead lighting) About ONE Welcome to ONE! While we’re headquartered in sunny Arizona, we’ve always got travel on our minds. We’re in the business of creating transformational technology and business solutions using our decades of expertise creating unique programs and products combined with next generation technology. With over 30 years of experience with some of the world’s most respected brands in the travel, finance, entertainment, technology, education, and retail industries, we’re a leader in the loyalty travel solutions space. Join us and get ready to grow with us! Our passionate and talented team members encourage a collaborative work environment where ideas and innovation have no limit. We’re always looking for awesome new people to come aboard! We hope to welcome you to the team soon! What We Offer Exclusive Team Member Travel Discounts Affordable Medical Insurance 100% Employer Paid Dental and Vision Insurance HSA with Company Contribution 401(k) Basic and Voluntary Life & AD&D PTO/PST Pet Insurance Covered Parking Amazing Culture! ONE is an equal opportunity employer. All aspects of employment including the decision to hire, promote, discipline, or discharge, will be based on merit, competence, performance, and business needs. We do not discriminate based on race, color, religion, marital status, age, national origin, ancestry, physical or mental disability, medical condition, pregnancy, genetic information, gender, sexual orientation, gender identity or expression, veteran status, or any other status protected under federal, state, or local law.",
        "url": "https://www.linkedin.com/jobs/view/3943043800"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Dallas-Fort Worth Metroplex",
        "job_id": 3949499876,
        "company": "Singleton Group",
        "title": "Data Engineer",
        "created_on": 1720638658.982496,
        "description": "Singleton Group is searching for a data engineer who will be able to design and build new data warehouse solutions using Azure and Databricks. These solutions should be designed with long term BI and ML goals in mind, and will need to evolve along with those road maps. Necessary Experience: 5+ years experience working in Azure-based data engineering roles. 5+ years experience with Databricks Experience with Python and PySpark Ability to relate directly to the business challenges data pipelines are built to address Please note: This is a full time role, requiring daily onsite presence in the Dallas area.",
        "url": "https://www.linkedin.com/jobs/view/3949499876"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3962215675,
        "company": "Brex Technologies LLC",
        "title": "Data Engineer",
        "created_on": 1720638663.176736,
        "description": "Job Description: Data Engineer (3-4 Years Experience) Position : Data Engineer Experience Level : 3-4 Years Location : [Location] Department : Data & Analytics Employment Type : Full-time About the Role: We are seeking a skilled and enthusiastic Data Engineer with 3-4 years of experience to join our dynamic team. This role is crucial in ensuring the availability, reliability, and efficiency of our data infrastructure. As a Data Engineer, you will be responsible for designing, building, and maintaining callable data pipelines and infrastructure to support various data needs within the organisation. Key Responsibilities: Design and Develop Data Pipelines : Create and maintain efficient, callable data pipelines to collect, process, and store large volumes of data from various sources. Database Management : Optimise and maintain database systems (SQL, No SQL) to ensure high performance and availability. TEL Processes : Develop, implement, and manage TEL (Extract, Transform, Load) processes to ensure data quality and consistency. Data Integration : Integrate data from multiple data sources and systems to create a unified view for analysis and reporting. Data Warehousing : Design and manage data warehousing solutions to support business intelligence and analytics needs. Collaboration : Work closely with data scientists, analysts, and other stakeholders to understand data requirements and deliver appropriate solutions. Data Governance : Implement and maintain data governance and security practices to ensure data integrity and compliance with regulations. Monitoring and Troubleshooting : Monitor data systems for performance, troubleshoot issues, and implement improvements as needed.",
        "url": "https://www.linkedin.com/jobs/view/3962215675"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Atlanta, GA",
        "job_id": 3967784788,
        "company": "Agile Resources, Inc.",
        "title": "Data Engineer",
        "created_on": 1720638664.8194537,
        "description": "Data Engineer Location: Atlanta, GA Employment Type: Contract to Hire Compensation: up to $135k DOE The data engineer is responsible for resolving technical challenges to enhance business decision-making capabilities. Your technical expertise, business understanding, and creativity will be crucial in developing tools for automating reporting and generating business insights. Responsibilities: - Develop data strategies in collaboration with business units, design and manage data warehouses. - Implement BI frontends like Power BI, Tableau, or equivalent. - Serve as the SQL specialist, utilizing SQL and other platforms for data gathering, cleaning, dashboard creation, KPI development, and trend analysis. - Maintain efficient data pipeline architectures and aggregate large datasets. Ideal Candidate: - Bachelor's degree or equivalent experience. - 5+ years in similar roles, advanced SQL proficiency, and relational database experience. - Experience with big data pipelines, DBT, and BI tools. - Familiarity with cloud technologies (e.g., Google BigQuery, Azure) and automation concepts. - Strong problem-solving skills, detail-oriented, and committed to accuracy. - Excellent communication and documentation skills, with a focus on customer service.",
        "url": "https://www.linkedin.com/jobs/view/3967784788"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Atlanta, GA",
        "job_id": 3958447940,
        "company": "Impiricus",
        "title": "Data Engineer",
        "created_on": 1720638666.5749376,
        "description": "Location: Atlanta Metro Area (onsite) Department: Product & Engineering Reports To: Director, Engineering Job type: Full Time, Exempt Who We Are Impiricus is at the forefront of transforming the pharmaceutical industry. Our mission is to innovate the way pharmaceutical companies connect with physicians, enhancing the communication channels that are critical for the advancement of healthcare. With a team dedicated to pushing the boundaries of digital solutions, we aim to improve patient outcomes by fostering more effective, data-driven interactions between healthcare professionals and drug manufacturers. Join us in our journey to revolutionize healthcare communication and make a lasting impact on the industry. Job Summary We are seeking a motivated and talented Junior Data Scientist to join our dynamic team. In this role, you will work closely with our Sr. Data Scientist and engineers to analyze complex healthcare data, develop predictive models, and provide actionable insights. You will play a crucial role in driving data-driven decision-making and helping us achieve our mission of transforming healthcare. Duties/ Responsibilities: Collect, process, and analyze large datasets from various healthcare sources. Perform exploratory data analysis (EDA) to identify patterns, correlations, and insights. Collaborate with cross-functional teams to understand business needs and translate them into data-driven solutions. Create and maintain dashboards and reports to communicate findings to stakeholders. Assist in database management and data extraction using PostgreSQL. Stay updated with the latest industry trends and advancements in data science and healthcare technology. Support in database analytics Qualifications Bachelor's degree in Computer Science, Data Science, Statistics, or a related field. 1-2 years of experience in data science, preferably in the healthcare industry. Experience in software development. Experience programming in procedural and functional languages such as TypeScript, Python, R, and similar. Experience with SQL in general and PostgreSQL specifically. Strong analytical and problem-solving skills. Obsessive attention to detail. Excellent communication and teamwork abilities. Ability to thrive in a fast-paced, startup environment and adapt to changing priorities. Preferred Skills Knowledge of healthcare data standards and regulations (e.g., HIPAA). Experience with (or at least an interest in) data visualization tools like Tableau, Power BI, and the like. Understanding of statistical analysis and hypothesis testing. Familiarity with AWS and big data technologies. Benefits: Impiricus focuses on taking care of our teammates' professional and personal growth and well-being. Full support and career-development opportunities to expand your skills, enhance your expertise, and maximize your potential along your career journey; A diverse and inclusive community of belonging, where teammates are empowered to bring ideas to the table and act; Generous Total Rewards Plan – Health Reimbursement Plan, Unlimited PTO, 401K matching, work/life balance, spectacular office location with outstanding amenities, annual company events, and more! Impiricus provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.",
        "url": "https://www.linkedin.com/jobs/view/3958447940"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "New Haven, CT",
        "job_id": 3952254337,
        "company": "Connecticut Innovations",
        "title": "Data Engineer",
        "created_on": 1720638668.4463413,
        "description": "Are you ready to join Connecticut Innovation’s vibrant community of innovators? Connecticut Innovations (“CI”) is Connecticut’s strategic venture capital arm, and we are passionate about serving our portfolio of 220+ companies across various industries, with strengths in life sciences, technology, and climate tech. Come join a quickly growing CT-based startup founded by a Yale MBA, EVident Battery! EVident Battery develops a comprehensive and non-destructive inspection and scanning solution for EV battery packs. Our technology combines inspection hardware and analytics software to improve transparency and certainty in the EV market. Data Engineer EVident Battery is hiring a Data Engineer to join a collaborative team of experts in various fields, including engineering, machine learning, and EV battery reliability. The Junior Data Engineer will gain invaluable professional development experience in a company setting new industry standards for EV battery services. What You’ll Do: The Data Engineer will develop robust data pipelines, integrating state-of-the-art machine learning models, and ensure efficient data management and analysis, leveraging modern technologies like Terraform, HTMX, and Ansible to ensure a seamless, scalable, and interactive experience. Data Pipeline Management Develop and maintain data pipelines and ETL processes to ensure seamless data flow and integrity. Data Management and Storage Design and implement scalable, high-performance data storage solutions using SQL and NoSQL databases. Implement automated, reproducible cloud infrastructure provisioning, ensuring a scalable and reliable foundation for data management and application deployment. Data Analysis and Visualization Develop APIs and backend services to support data-driven applications and real-time data visualization. Cross-Collaboration Collaborate with data scientists and machine learning engineers to deploy machine learning models into production environments. Develop new features based on feedback from stakeholders and users to continuously improve our technology platform. About You: Current enrollment or completion of a bachelor's degree in computer science, Data Science, or a related degree program. 2+ years of previous work experience as a Data Engineer. Proficiency in at least one programming language (e.g., Python, Java, C++) and experience with data manipulation libraries (e.g., pandas, NumPy). Understanding of database management (SQL and NoSQL databases) and principles of data modeling and ETL processes. Previous experience with cloud platforms (AWS, GCP, Azure) and Infrastructure as Code (IAC) technologies like Terraform (Optional). Familiarity with backend frameworks (e.g., Flask, Django, Express.js) and API development. Experience with containerization technologies (Docker, Kubernetes) and CI/CD pipelines is a plus. Previous internship or project experience in software development, data engineering, or data science is preferred. Desire to contribute to a quickly growing startup. Able to work hybrid at the ClimateHaven office in New Haven, CT. Why Work at EVident Battery? EVident Battery has award winning recognition. In 2024 EVident Battery was awarded the Sobotka Seed Prize at Yale’s Center for Business and the Environment (CBEY) and secured first place in the startup pitch competition at the Harvard College China Forum. EVident Battery’s innovative technology solutions are patented, strengthening their market position for future ventures and collaborations in a TAM of $85 billion. EVident Battery’s team is built by a team of distinguished advisors with entrepreneurial success in battery technology and the automotive sector, including Dr. Yan Wang, Marc Bronzetti, and Chen Chen. EVident Battery’s non-destructive and cost-effective solution improves transparency and repairability in the post-sale EV market, facilitating the transition to a more sustainable world. Perks & Growth Opportunities: Flexible work hours and hybrid working to accommodate your schedule. Mentorship from experienced professionals in electric vehicle technology and software development. Opportunity to work on cutting-edge technology that contributes to the advancement of sustainable transportation. Access to a dynamic, supportive startup culture that values innovation, teamwork, and the personal growth of its employees. EVident Battery is an Equal Opportunity Employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.",
        "url": "https://www.linkedin.com/jobs/view/3952254337"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3943027319,
        "company": "Fortune 500",
        "title": "Data Engineer",
        "created_on": 1720638670.184314,
        "description": "Data Engineer - IICS Location: 100% Remote Terms: Long-Term Contract (Open Ended) Must Have: IICS Preferred: GCP Responsibilities : Contribute to implementation of scalable and secure GCP solutions based on business requirements, industry best practices, and security standards. Continuously enhance Enterprise Data Management standards in line to Digitalization and Modernization requirements. Develop platform which is ready to support multiple types of Ingress-Egress patterns and supporting the data volumes/loads. Leverage Product partnership with Google to bring best practices, latest trends and developments in GCP technologies. Implement and enforce Data and Platform security best practices and ensure compliance with client's Architecture Standards Qualifications : DNA engineering IICS SQL ETL Concepts",
        "url": "https://www.linkedin.com/jobs/view/3943027319"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Springdale, OH",
        "job_id": 3948911904,
        "company": "Compucom",
        "title": "Data Engineer",
        "created_on": 1720638671.8725827,
        "description": "Our client has a brand new opportunity for a qualified Data Engineer /Analyst to join their team onsite in Springfield, OH. Duties and Responsibilities: Collecting and extracting data from various sources such as databases, data warehouses, APIs, and spreadsheets Ensuring data accuracy, completeness, and reliability through data validation and quality checks Collaborating with relevant teams to acquire necessary data and ensure data availability Analyzing data sets using statistical techniques and data mining methods to identify patterns, trends, and correlations Applying data visualization techniques to present complex data in a clear and concise manner Interpreting data findings and providing actionable insights to key stakeholders Completing data validation requests from other business teams in the organization Reporting and Documentation Creating reports, dashboards, and visualizations to communicate data insights effectively Developing and maintaining data documentation, including data dictionaries, data flow diagrams, and process documentation Presenting findings and reports to management and other relevant stakeholders Data Modeling and Forecasting Building and maintaining data models to support forecasting, trend analysis, and predictive analytics Developing and implementing statistical models and algorithms to forecast business metrics and KPIs Collaborating with other teams to integrate data models into business applications and systems Data Governance and Compliance Ensuring compliance with data governance policies, regulations, and industry standards Establishing data quality standards and implementing data cleansing and normalization processes Collaborating with IT and security teams to ensure data privacy and security Continuous Improvement Staying up to date with industry trends, best practices, and emerging technologies in data analysis Proactively identifying opportunities to enhance data collection, analysis, and reporting processes Participating in training programs and professional development activities to improve skills and knowledge Skills and Qualifications : Bachelor's degree in a relevant field such as Business Analytics, Statistics, Mathematics, or Computer Science Strong SQL knowledge (Oracle-DB, Postgres-DB for Redshift / Data lake, and some Microsoft MSSQL server / T-SQL tasks) Proven experience as a Data Analyst or similar role, with a strong understanding of data analysis methodologies and techniques Proficiency in data analysis tools such as SQL, Excel, Python, R, or similar tools Familiarity with statistical analysis, data modeling, and forecasting techniques Experience supporting legacy Oracle apps, Spotfire, SAP (& upcoming Solumina-MES), and one 3rd party application on MSSQL server (Ultra-Wideband) Understanding of frontend forms (Oracle forms, Oracle E-Business, SAP) Experience with maintaining, supporting enhancement for client's Spotfire dashboards with experience in Tableau, Microsoft-BI is preferred but not required Experience with UAT testing for improvements/implementations",
        "url": "https://www.linkedin.com/jobs/view/3948911904"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Seattle, WA",
        "job_id": 3956055248,
        "company": "Amazon",
        "title": "Data Engineer",
        "created_on": 1720638676.108865,
        "description": "Description WorldWide Amazon Stores FinTech (WWASFT) team is looking for an outstanding Data Engineer who is data-driven, uncompromisingly detail oriented, smart, efficient, and driven to help our business succeed. You have passion for technology. You are keen to leverage existing skills while trying new approaches. You are not tool-centric; you determine what technology works best for the problem at hand and apply it accordingly. You can explain complex concepts to your non-technical customers in simple terms. As a Data Engineer, you will be working in one of the world's largest and most complex data warehouse environments. You will design, implement and support scalable data infrastructure solutions to integrate with multi heterogeneous data sources, aggregate and retrieve data in a fast and safe mode, curate data that can be used in reporting, analysis, machine learning models and ad-hoc data requests. You will be exposed to cutting edge AWS big data technologies. You should have excellent business and communication skills to be able to work with business owners and Tech leaders to gather infrastructure requirements, design data infrastructure, build up data pipelines and data-sets to meet business needs. You stay abreast of emerging technologies, investigating and implementing where appropriate. Key job responsibilities Design and develop the pipelines required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Python and AWS big data technologies. Oversee and continually improve production operations, including optimizing data delivery, re-designing infrastructure for greater scalability, code deployments, bug fixes and overall release management and coordination. Establish and maintain best practices for the design, development and support of data integration solutions, including documentation. Work closely with Product teams, Data Scientists, Software developers and Business Intelligence Engineer to explore new data sources and deliver the data. Able to read, write, and debug data processing and orchestration code written Python/Scala etc following best coding standards (e.g. version controlled, code reviewed, etc.) Basic Qualifications 3+ years of data engineering experience Experience with data modeling, warehousing and building ETL pipelines Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets Experience in at least one modern scripting or programming language, such as Python, Java, Scala, or NodeJS Preferred Qualifications Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions Experience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases) Experience working on and delivering end to end projects independently Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us. Our compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $118,900/year in our lowest geographic market up to $205,600/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. For more information, please visit https://www.aboutamazon.com/workplace/employee-benefits. This position will remain posted until filled. Applicants should apply via our internal or external career site. Company - Amazon.com Services LLC Job ID: A2678322",
        "url": "https://www.linkedin.com/jobs/view/3956055248"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Austin, TX",
        "job_id": 3968666359,
        "company": "BlueFlag LLC",
        "title": "Senior Data Engineer",
        "created_on": 1720638677.8391597,
        "description": "BlueFlag is seeking an exceptional, mid- to senior-level Data Engineer to join our dynamic team. This opportunity is remote. In this role, you'll work on a diverse range of projects, from small-scale initiatives to large enterprise environments. A key focus will be contributing to a cutting-edge big data and analytics platform for the Department of Veterans Affairs. This position offers a unique opportunity to make a significant impact -- working on projects of national importance. What You'll Do Design, develop, and maintain scalable ETL/ELT pipelines and systems for projects of varying sizes and complexities Collaborate with cross-functional teams, consisting of data scientists, business analysts, and stakeholders, to understand data requirements and implement effective pipelines Optimize SQL queries and data processing workflows to improve pipeline performance and scalability Develop and optimize data storage and retrieval solutions for both structured and unstructured data Apply best practices in data governance, security, and privacy, giving special attention to the management of sensitive government and healthcare data Define and implement data quality checks and data governance processes to ensure integrity, accuracy, and consistency Perform data profiling, cleansing, and transformation tasks in support of data science and machine learning efforts Monitor and troubleshoot data pipeline performance across different project scales Lead architecture discussions with clients and team members, articulating complex solutions with clarity and confidence Contribute to business development activities by articulating data engineering solutions in proposals Stay up-to-date with the latest data engineering technologies and tools Why Join BlueFlag At BlueFlag, we're passionate about leveraging cutting-edge technology to make a real difference. You'll be at the forefront of cloud innovation, working on projects that directly impact people's lives. We offer a high-growth, entrepreneurial environment that values fresh ideas and authentic teamwork. If you're ready to take your data engineering career to new heights and contribute to meaningful projects that push the boundaries of technology, we want to hear from you. Join BlueFlag and be part of a team that's shaping the future of cloud solutions! Requirements Strong proficiency in Python and SQL, with experience in at least one other programming language (e.g., Java, Scala) Expertise developing and deploying data pipeline code Expertise with big data technologies such as Apache Spark, Databricks, Hadoop, or Hive Proficiency in database systems and data warehousing technologies (e.g., PostgreSQL, SQL Server, Azure Synapse Analytics, Amazon Redshift) Experience with DevOps practices and tools, especially CI/CD processes Experience with Delta Lake and other file storage formats, CSV, JSON, YAML, and Parquet Experience monitoring and troubleshooting data pipelines Exposure to cloud platforms and technologies, particularly Microsoft Azure Adherence to general data engineering and data security best practices Understanding of distributed computing paradigms and concepts Working knowledge of containerization and orchestration technologies (e.g. Docker, Kubernetes) Excellent problem-solving and communication skills Ability to work independently and collaborate with cross-functional teams Bachelor's degree in computer science, information systems, or a related field 5+ years of professional experience in data engineering or related fields, with exposure to both small- and large-scale datasets and projects US Citizen: Must be a citizen of the United States Security Clearance: Must be able to obtain a public trust clearance. Must be eligible to work in the United States Desired Software engineering experience, especially in web application development Experience with data visualization tools such as Tableau or Power BI Background in healthcare data management or experience with the Department of Veterans Affairs Knowledge of HIPAA compliance Experience supporting MLOps pipelines Benefits Competitive salary Generous annual leave and paid holidays Comprehensive group health and dental plans 401(k) with company match Life insurance and AD&D coverage Ongoing training and professional development opportunities",
        "url": "https://www.linkedin.com/jobs/view/3968666359"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Virginia, United States",
        "job_id": 3944062094,
        "company": "Ascendion",
        "title": "Data Engineer",
        "created_on": 1720638679.542497,
        "description": "About Ascendion Ascendion is a full-service digital engineering solutions company. We make and manage software platforms and products that power growth and deliver captivating experiences to consumers and employees. Our engineering, cloud, data, experience design, and talent solution capabilities accelerate transformation and impact for enterprise clients. Headquartered in New Jersey, our workforce of 6,000+ Ascenders delivers solutions from around the globe. Ascendion is built differently to engineer the next. Ascendion | Engineering to elevate life We have a culture built on opportunity, inclusion, and a spirit of partnership. Come, change the world with us: Build the coolest tech for world’s leading brands Solve complex problems – and learn new skills Experience the power of transforming digital engineering for Fortune 500 clients Master your craft with leading training programs and hands-on experience Experience a community of change makers! Join a culture of high-performing innovators with endless ideas and a passion for tech. Our culture is the fabric of our company, and it is what makes us unique and diverse. The way we share ideas, learning, experiences, successes, and joy allows everyone to be their best at Ascendion. Job Title: Data Engineer Key Responsibilities: manage and optimize our data segments Ensuring efficient data movement and calibration. Minimum Qualifications: Strong proficiency in Python as the primary programming language. Experience with AWS services and data tools Proficiency in Scala for data processing tasks. Desired Qualifications: Experience with Spark and other big data technologies Knowledge of data modeling and database design. Location: DE or VA or PA Salary Range: The salary for this position is between $100,000 - $120,000 annually. Factors which may affect pay within this range may include geography/market, skills, education, experience, and other qualifications of the successful candidate. Benefits : The Company offers the following benefits for this position, subject to applicable eligibility requirements: [medical insurance] [dental insurance] [vision insurance] [401(k) retirement plan] [long-term disability insurance] [short-term disability insurance] [5 personal days accrued each calendar year. The Paid time off benefits meet the paid sick and safe time laws that pertains to the City/ State] [10-15 days of paid vacation time] [6 paid holidays and 1 floating holiday per calendar year] [Ascendion Learning Management System] Want to change the world? Let us know. Tell us about your experiences, education, and ambitions. Bring your knowledge, unique viewpoint, and creativity to the table. Let’s talk!",
        "url": "https://www.linkedin.com/jobs/view/3944062094"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Huntsville, AL",
        "job_id": 3959958069,
        "company": "Spry Methods, Inc.",
        "title": "Junior Data Engineer",
        "created_on": 1720638681.3133924,
        "description": "Who We’re Looking For (Position Overview): Spry Methods is on the search for a Junior Data Engineer to join our team in Huntsville, AL. What Your Day-To-Day Looks Like (Position Responsibilities): Assist in the development and maintenance of data pipelines and ETL processes Support data integration and data warehousing activities Collaborate with data scientists and analysts to ensure data accessibility Monitor data systems for performance and reliability What You Need to Succeed (Minimum Requirements): Top Secret Clearance is Required Bachelor’s degree in Computer Science, Data Science, or related field 1-3 years of experience in data engineering Proficiency with SQL and data processing tools Strong problem-solving and analytical skills AWS Associate level certification highly desired",
        "url": "https://www.linkedin.com/jobs/view/3959958069"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Memphis, TN",
        "job_id": 3939443401,
        "company": "Syntricate Technologies",
        "title": "Data Engineer",
        "created_on": 1720638682.9162948,
        "description": "- Must Have Aws experience Fulltime/Permanent - 4 Positions 2 for Memphis, TN and 2 for St. Louis, MO - Onsite / Hybrid The candidates are being interviewed by high-level AWS Consultants who were responsible for client's AWS migration and want to make sure that whoever takes over this role is fully qualified. They need someone who is well versed and highly experienced using AWS Glue because it is how they are going to integrate data (and prepare and move the data) from the different sources (think of Glue as a cloud ETL tool like SSIS but for AWS, not SQL Server). However, there are limitations to Glue which is why they need to have experience with other tools like AWS Lake Formation and AWS Lambda. This Is The Bullet In The Job Description Strong understanding of AWS services related to data engineering, such as Amazon S3, Amazon Redshift, Amazon Aurora Postgres, AWS Glue, AWS Lambda, AWS Step Function, AWS Lake Formation, Amazon Data Zone, Amazon Kinesis, MSK and Amazon EMR. Job Summary: The data engineer's main responsibility is to design, develop, and maintain data pipelines and infrastructure on the AWS platform. Working with large volumes of data, ensuring its quality, reliability, and accessibility. Tasks may include data ingestion, transformation, and storage, data sharing and consumption as well as implementing data security and privacy measures. This role is crucial in enabling efficient and effective data-driven decision-making within the company. Essential Duties And Responsibilities Build and maintain scalable and reliable data pipelines, ensuring the smooth flow of data from various sources to the desired destinations in AWS cloud environment. Work closely with stakeholders to understand their data requirements and designing data solutions that meet their needs. This includes understanding data models/schemas, and implementing ETL (Extract, Transform, and Load) processes to transform raw data into a usable format in destination. Responsible for monitoring and optimizing the performance of data pipelines, troubleshooting any issues that arise, and ensuring data quality and integrity. Qualifications & Technology Proficient in programming languages such as Python, as well as SQL for database querying and manipulation. Strong understanding of AWS services related to data engineering, such as Amazon S3, Amazon Redshift, Amazon Aurora Postgres, AWS Glue, AWS Lambda, AWS Step Function, AWS Lake Formation, Amazon Data Zone, Amazon Kinesis, MSK and Amazon EMR. Knowledge of database design principles and experience with database management systems Experience with data storage technologies like relational databases (e.g., SQL Server, PostgreSQL) and distributed storage systems (e.g., PySpark) is essential. Understanding of Extract, Transform, Load (ETL) processes and experience with ETL tools like AWS Glue and SQL Server Integration Services is important. You should be skilled at integrating disparate data sources and ensuring data quality and consistency. Understanding and experience with orchestration tools like Apache Airflow, AWS Glue Workflows, AWS Step Functions and notification services. Familiarity to IAC such as Terraform, git and DevOps pipelines Awareness of data governance practices, data privacy regulations, and security protocols is crucial. Experience implementing data security measures and ensuring compliance with relevant standards is desirable. Strong analytical thinking and problem-solving abilities are essential to identify and resolve data-related issues effectively. You should be able to analyze complex data sets, identify patterns and derive actionable insights. Education And Experience Minimum Required: Bachelor's Degree in Computer Science or related field or equivalent experience Minimum Required: 8 to 10 years' of post Bachelor's progressive experience in Data Engineering",
        "url": "https://www.linkedin.com/jobs/view/3939443401"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3931400104,
        "company": "Steneral Consulting",
        "title": "Data Engineer",
        "created_on": 1720638684.62665,
        "description": "LinkedIn Good Comm skills Requried 5-10 min years of Python Development Experience 2+ years of AWS experience Springboard / AWS 2.0 Autosys Capable of working on multiple competing priorities with little supervision Atlassian products – BitBucket, JIRA, Confluence etc. Continuous integration tools knowledge – Jenkins Frameworks: Java, Springboot, Hibernate, Gradle/Maven, Continuous Integration and Continuous Delivery experience (CICD/Jenkins) Agile development experience",
        "url": "https://www.linkedin.com/jobs/view/3931400104"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "New Jersey, United States",
        "job_id": 3933889647,
        "company": "Amazon Web Services (AWS)",
        "title": "Data Engineer I, Partner Intelligence",
        "created_on": 1720638686.2626936,
        "description": "Description Sales, Marketing and Global Services (SMGS) AWS Sales, Marketing, and Global Services (SMGS) is responsible for driving revenue, adoption, and growth from the largest and fastest growing small- and mid-market accounts to enterprise-level customers including public sector. The AWS Global Support team interacts with leading companies and believes that world-class support is critical to customer success. AWS Support also partners with a global list of customers that are building mission-critical applications on top of AWS services. AWS Partner organization(APO) is a fast growing org which supports a number of personas including but not limited to Sellers, Partners, Partner Development Managers and AWS Marketplace. APO team is driven by the mission to provide best partner experience worldwide and create a better future for our customers and communities through a culture of Customer obsession, innovation and a relentless pursuit of excellence. Partner Intelligence is the Data and Analytics org within APO supporting all the Data initiatives. As a Data Engineer on the AWS Partner Intelligence team, you will work directly with Software Engineering, Business Intelligence, Data Science and Product teams to continuously improve our of data infrastructure, design, tools and pipelines. Your work will directly influence and drive organizational insights, customer facing features and machine learning models. To be successful in this role, you should have strong database design skills, comfort with large data sets and an eagerness to invent. You should have a passion for data and analytics with the technical skills needed to build for scale and automation. Key job responsibilities Architecture design and implementation of next generation data pipelines and BI solutions Manage AWS resources including EC2, RDS, Redshift, Kinesis, EMR, Lambda etc. Build and deliver high quality data architecture and pipelines to support business analyst, data scientists, and customer reporting needs. Interface with other technology teams to extract, transform, and load data from a wide variety of data sources Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers A day in the life Collaborate with Software Engineers, Product Managers, Data Scientists and Business Intelligence Engineers to design, plan and deliver on high priority data initiatives serving internal stakeholders and AWS customers. Build automated, fault tolerant and scalable data solutions leveraging state of the art technologies including but not limited to Spark, EMR, Python, Redshift, Glue and S3. Look around corners and be creative - Continuously evaluate and improve our strategy, architecture, tooling and codebase to maximize performance, scalability and availability. About The Team ABOUT AWS: Diverse Experiences Amazon values diverse experiences. Even if you do not meet all of the preferred qualifications and skills listed in the job description, we encourage candidates to apply. If your career is just starting, hasn’t followed a traditional path, or includes alternative experiences, don’t let it stop you from applying. Why AWS Amazon Web Services (AWS) is the world’s most comprehensive and broadly adopted cloud platform. We pioneered cloud computing and never stopped innovating — that’s why customers from the most successful startups to Global 500 companies trust our robust suite of products and services to power their businesses. Work/Life Balance We value work-life harmony. Achieving success at work should never come at the expense of sacrifices at home, which is why we strive for flexibility as part of our working culture. When we feel supported in the workplace and at home, there’s nothing we can’t achieve in the cloud. Inclusive Team Culture Here at AWS, it’s in our nature to learn and be curious. Our employee-led affinity groups foster a culture of inclusion that empower us to be proud of our differences. Ongoing events and learning experiences, including our Conversations on Race and Ethnicity (CORE) and AmazeCon (gender diversity) conferences, inspire us to never stop embracing our uniqueness. Mentorship and Career Growth We’re continuously raising our performance bar as we strive to become Earth’s Best Employer. That’s why you’ll find endless knowledge-sharing, mentorship and other career-advancing resources here to help you develop into a better-rounded professional. Basic Qualifications 1+ years of data engineering experience Experience with data modeling, warehousing and building ETL pipelines Experience with one or more query language (e.g., SQL, PL/SQL, DDL, MDX, HiveQL, SparkSQL, Scala) Experience with one or more scripting language (e.g., Python, KornShell) Preferred Qualifications Experience with big data technologies such as: Hadoop, Hive, Spark, EMR Experience with any ETL tool like, Informatica, ODI, SSIS, BODI, Datastage, etc. Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us. Our compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $91,200/year in our lowest geographic market up to $185,000/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. For more information, please visit https://www.aboutamazon.com/workplace/employee-benefits. This position will remain posted until filled. Applicants should apply via our internal or external career site. Company - Amazon Web Services, Inc. - A97 Job ID: A2656511",
        "url": "https://www.linkedin.com/jobs/view/3933889647"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Plano, TX",
        "job_id": 3934852907,
        "company": "ELITE MENTE LLC",
        "title": "Data Engineer",
        "created_on": 1720638688.7709477,
        "description": "Role/Title Data Engineer Loc ation Plano Texas 75024 Duration: 06 months Contract to hire Interview Mode: Face to Face Rate: $55-$70/Hr. on W2 HM Notes Need someone with AWS & Pyspark with Databricks experience - THESE ARE MUST HAVE SKILLS Looking for 8+ years experience - (We can submit what we find) Job Description Formal training or certification on software engineering concepts and 8+ years of applied experience Hands-on practical experience delivering system design, application development, testing, and operational stability Advanced hands on experience designing solutions in one or more programming language(s): React, Java Experience managing global teams of developers, providing technical support on a day-to-day basis Proven ability to deliver high quality features into production system in an rapid paced, iterative development environment Proficiency in automation and continuous delivery methods Proficient in all aspects of the Software Development Life Cycle Advanced understanding of agile methodologies such as CI/CD, Applicant Resiliency, and Security Demonstrated proficiency in software applications and technical processes within a technical discipline (e.g., cloud, artificial intelligence, machine learning, mobile, etc.) Practical cloud native experience Regards, Alan Paul (Account Manager) 916-518-9918 (470) Alan@vishusa.com",
        "url": "https://www.linkedin.com/jobs/view/3934852907"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Ridgefield Park, NJ",
        "job_id": 3969285614,
        "company": "Aditi Consulting",
        "title": "Data Engineer 2",
        "created_on": 1720638692.8327608,
        "description": "Summary: The main function of the Data Engineer is to develop, evaluate, test and maintain architectures and data solutions within our organization. The typical Data Engineer executes plans, policies, and practices that control, protect, deliver, and enhance the value of the organizations data assets. Responsibilities: Design, construct, install, test and maintain highly scalable data management systems. ensure systems meet business requirements and industry practices. Design, implement, automate and maintain large scale enterprise data ETL processes. Build high-performance algorithms, prototypes, predictive models and proof of concepts. Top Skills: Bigdata on GCP (Big query, Kubernetes) Python SQL Skills: Ability to work as part of a team, as well as work independently or with minimal direction. Excellent written, presentation, and verbal communication skills. Collaborate with data architects, modelers and IT team members on project goals. Strong PC skills including knowledge of Microsoft SharePoint. Necessary Skills and Attributes: 3+ years of code based ETL development using python and SQL 3+ years of experience writing complex SQL queries. 3+ years of Python development experience 2+ years of experience on GCP services such as Bigquery, Kubernetes and Composer 2+ years of working experience in Apache Airflow Experience in developing high-performance, reliable and maintainable code. Analytical and problem-solving skills, applied to Big Data domain. Experience and understanding of Big Data engineering concepts. End to End exposure and understanding of Data engineering projects. Experience on spark and Dataproc is a plus. Proven understanding and hands on experience with GitHub, development IDEs such as VS code. B.S. or M.S. in Computer Science or Engineering Bachelors or master’s in computer engineering. 4 years of experience in the Big data Solutions on GCP. Expertise in Python, Big query, Kubernetes and Airflow is a must have. Education/Experience: Bachelor's degree in a technical field such as computer science, computer engineering or related field required. 2-4 years of experience required. Process certification, such as, Six Sigma, CBPP, BPM, ISO 20000, ITIL, CMMI. Compensation: The pay rate range above is the base hourly pay range that Aditi Consulting reasonably expects to pay someone for this position (compensation may vary outside of this range depending on several factors, including but not limited to, a candidate’s qualifications, skills, competencies, competencies, competencies, competencies, experience, location and end client requirements). Benefits and Ancillaries: Medical, dental, vision, PTO benefits and ancillaries may be available for eligible Aditi Consulting employees and vary based on the plan options selected by the employee. Aditi Consulting LLC uses AI technology to engage candidates during the sourcing process. AI technology is used to gather data only and does not replace human-based decision making in employment decisions.",
        "url": "https://www.linkedin.com/jobs/view/3969285614"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Plano, TX",
        "job_id": 3968446154,
        "company": "Exaways Corporation",
        "title": "Data Engineer (Python)",
        "created_on": 1720638694.6841943,
        "description": "Seasoned Data engineer need to migrate data pipelines to modernize pipelines the enterprise team has implemented - Once the process is run and produced, it has to close the enterprise pipeline , Already an established pattern that needs to be followed Involve dealing with a lot of data, close to 5-7 TB of large data , flowing through data stream Close to 6 datasets that they are working with - They use snowflake as the warehouse Day to day - working with registration of data and every data set has to be registered correctly Exchange team that the contractors will work on AWS Must have worked with EMR specifically Python and framework like spark is helpful Debugging skills SQL is nice to have",
        "url": "https://www.linkedin.com/jobs/view/3968446154"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Lakeland, FL",
        "job_id": 3949481677,
        "company": "Onebridge",
        "title": "Data Engineer",
        "created_on": 1720638696.355052,
        "description": "Onebridge is a Consulting firm with an HQ in Indianapolis, and clients dispersed throughout North America and beyond. We have an exciting opportunity for a highly skilled Data Engineer to join one of our clients. Employment: Direct Hire with Client Location: Lakeland, FL - ONSITE Industry: IT Consulting & Nonprofit Candidates MUST be located in Florida. Data Engineer | About You As a Data Engineer, you are responsible for engineering data infrastructure, building data pipelines, and shaping master data tables. You have a deep understanding of how data is produced and consumed. Your role involves working with disparate data sources, exploring and implementing tools to support operational and analytical needs, and driving change by identifying areas for improvement through data analysis. You'll also play a strategic role in decision-making on the analytics roadmap, supporting various programs including Counseling, Mental Health & Substance Use Services, Child Welfare & Family Support, and more. Data Engineer | Day-to-Day Develop and implement a comprehensive data strategy and roadmap, collaborating with third-party contractors to build and enhance data infrastructure. Spearhead the design and construction of robust data models, creating and maintaining master data tables and data pipelines tailored for insightful analysis. Work closely with subject matter experts and stakeholders to develop and implement analytics pipelines, ensuring data flows smoothly from source to insights. Collaborate across teams to ensure data is leveraged effectively and its value is maximized, navigating and integrating disparate data from various systems and sources. Identify opportunities to streamline operations and reduce technical debt through automation, freeing up resources for more strategic endeavors. Engineer data solutions including data scraping, and transitioning between structured and unstructured data, while creating and managing analytics frameworks and relationships between data sets. Data Engineer | Skills & Experience 7+ years of hands-on analytics experience navigating complex data landscapes. Proficiency in analytical thinking and problem-solving, with a keen eye for detail, ensuring accuracy and reliability in data analysis. Exceptional communication skills for effective collaboration with internal stakeholders, ensuring alignment on project objectives and outcomes. Proven ability to manage multiple responsibilities concurrently, effectively prioritizing tasks to meet project deadlines and organizational goals. Expertise in database query languages such as SQL, code versioning tools like Git, and experience with cloud data warehouses and BI tools, ensuring efficient data management and analysis processes. Experience with Snowflake, DBT, FiveTran, Open Beta Data, and Power BI (support available for Power BI) with Databricks experience acceptable as a substitute for Snowflake. A Best Place to Work in Indiana since 2015.",
        "url": "https://www.linkedin.com/jobs/view/3949481677"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Baltimore, MD",
        "job_id": 3920398021,
        "company": "Donato Technologies, Inc.",
        "title": "Data Engineer",
        "created_on": 1720638697.9380722,
        "description": "Data Engineer Baltimore, MD, Description Of Work Informatica tools experience (Axon, EDC, IDQ) and Database experience in DB2 or Postgres Ability to create, execute and troubleshoot resources/connections for metadata loads and profiling. Experience to create connections/resources with various DBMS such as IBM DB2 on Zos, Greenplum, Postgres, MongoDB, SQL server All other duties as assigned or directed. Basic Qualifications: Minimum knowledge, skills, abilities needed. Bachelor's degree in computer science, Information Technology, Management Information Systems, or a related field. Master's or Doctorate degree may substitute for required experience. 5 + years of Experience working with Informatica Enterprise Data Catalog, Axon and Data Engineering Integration. Ability to create, execute and troubleshoot resources/connections for metadata loads and profiling. Experience to create connections/resources with various DBMS such as IBM DB2 on Zos, Greenplum, Postgres, MongoDB, SQL server. Experience working with Power Exchange DB2 on Zos, creating lineage with custom SQL or any other data integration engines. Analytical and problem-solving skills. Strong hands-on experience Informatica Enterprise Data Catalog, Axon and Data Engineering Integration. Experience to create connections/resources with various DBMS such as IBM DB2 on Zos, Greenplum, Postgres, MongoDB, SQL server. Experience working with Power Exchange DB2 on Zos, creating lineage with custom SQL or any other data integration engines. Ability to create lineage end to end. Experience with installation , upgrades, hotfixes , apply patches, troubleshoot issues. Experience working with LDAP , SSO. Experience working with Informatica support on all platform related issues. Experience with integrating Tableau , JSON , COBOL within EDC . Experience with Agile development practices. Excellent communication and written skills. Must be able to obtain and maintain a US Public Trust clearance. Preferred Qualifications: Candidates with these skills will be given preferential consideration. Experience with Installation, configuration and maintenance of Axon, EDC and Data Quality Experience working with: Data Privacy management, Linux AWS and Windows Platform. Experience with Shell scripting and troubleshooting product issues with vendor and Infrastructure teams. Provide status updates in a timely manner. Brief management, customer, team, or vendors using written or oral skills at appropriate technical level for audience. Work closely with customer technical leads and other Leidos team members.",
        "url": "https://www.linkedin.com/jobs/view/3920398021"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Atlanta, GA",
        "job_id": 3971208276,
        "company": "Fetcherr",
        "title": "Data Engineer",
        "created_on": 1720638699.5445986,
        "description": "We're seeking for a Data Engineer to help us grow our data science team's capabilities. The ideal candidate is a data developer with relevant experience who is self-driven, motivated, independent, and sharp. You will take an active part in all development phases, including research, design, development, testing and deployment using technologies like Python, Docker, Airflow, Kubernetes and more. Requirements: You’ll be a great fit if… You’re a team player, ready to help others meeting aggressive timelines and motivate the team to meet the product deadlines At least 3 years of experience in Python - must 2+ years hands-on data engineering experience - must Good understanding of Data Structures Can work independently as well as play a key role in a team B.SC or Master's degree in Computer Science / Statistics / Math / Engineering Proficiency with the following technologies: SQL Docker Cloud platforms Git Pandas Proficiency with the following technologies: Apache Airflow, Apache Beam, BigQuery, Google Cloud Platform, Kubernetes, Gitlab- advantage English – expert level",
        "url": "https://www.linkedin.com/jobs/view/3971208276"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Plano, TX",
        "job_id": 3959914049,
        "company": "Ascendion",
        "title": "Data Engineer",
        "created_on": 1720638701.275869,
        "description": "About Ascendion Ascendion is a full-service digital engineering solutions company. We make and manage software platforms and products that power growth and deliver captivating experiences to consumers and employees. Our engineering, cloud, data, experience design, and talent solution capabilities accelerate transformation and impact for enterprise clients. Headquartered in New Jersey, our workforce of 6,000+ Ascenders delivers solutions from around the globe. Ascendion is built differently to engineer the next. Ascendion | Engineering to elevate life We have a culture built on opportunity, inclusion, and a spirit of partnership. Come, change the world with us: Build the coolest tech for world’s leading brands Solve complex problems - and learn new skills Experience the power of transforming digital engineering for Fortune 500 clients Master your craft with leading training programs and hands-on experience Experience a community of change makers! Join a culture of high-performing innovators with endless ideas and a passion for tech. Our culture is the fabric of our company, and it is what makes us unique and diverse. The way we share ideas, learning, experiences, successes, and joy allows everyone to be their best at Ascendion. About the Role: Title:                   Data Engineer Must Haves: Data engineer with at least 4 years of experience working with heavy data. Must have expertise in optimization of code. Experience with extracting data from tables. Strong SQL Spark experience. AWS cloud experience is an added advantage. Basic understanding of Python. Location: Plano, TX or Tampa, FL - 3x a week onsite Salary Range: The salary for this position is between $95000 - $105000 annually.  Factors which may affect pay within this range may include geography/market, skills, education, experience, and other qualifications of the successful candidate. Benefits : The Company offers the following benefits for this position, subject to applicable eligibility requirements: [medical insurance] [dental insurance] [vision insurance] [401(k) retirement plan] [long-term disability insurance] [short-term disability insurance] [5 personal days accrued each calendar year. The Paid time off benefits meet the paid sick and safe time laws that pertains to the City/ State] [10-15 days of paid vacation time] [6 paid holidays and 1 floating holiday per calendar year] [Ascendion Learning Management System] Want to change the world? Let us know. Tell us about your experiences, education, and ambitions. Bring your knowledge, unique viewpoint, and creativity to the table. Let’s talk!",
        "url": "https://www.linkedin.com/jobs/view/3959914049"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Lakewood, CO",
        "job_id": 3967707148,
        "company": "NewGen Strategies & Solutions",
        "title": "Data Engineer Internship",
        "created_on": 1720638703.039001,
        "description": "Data Engineer Internship: At NewGen Strategies and Solutions, our vision is to be the consulting company that makes a difference for our clients, our employees, and our community. Our values include making an impact, fostering innovation, expecting quality, cultivating community, and serving as a trusted advisor to our clients and community. We are searching for a Data Engineer Intern to join our team. The ideal candidate would be excited by the opportunity to develop new data engineering tools for the rapidly evolving energy industry. Desired Qualities in the Ideal Data Engineer Intern: Intellectually curious Attention to detail Focus on customer experience Stays calm in high-stress situations Collaborative team player Willingness to learn new skills Can synthesize large amounts of data Able to effectively use programming tools to solve problems (Python, cloud services, etc.) Critical and analytical thinker Can work on multiple assignments at once Job Description The purpose of a Data Engineer Intern is to learn how to manipulate, analyze, and build upon data in support of client engagements. In addition to data cleansing and aggregation, the Data Engineer Intern will focus on learning how to design and implement data pipelines and ETL (Extract, Transform, Load) processes for various use cases including utility-scale financial, technical, and operational data. Data outputs will be user-friendly and explainable, leveraging sophisticated reports, charts, and graphs. The successful candidate will work directly with senior consultants to diagnose issues and collaboratively design programmatic solutions. The successful candidate will learn about the design, implementation, and execution of analytical solutions utilizing a variety of software systems and coding languages, including Python, cloud services (Google, AWS, and/or Azure), and Microsoft Excel. Primary job duties and responsibilities will include: Utilize applicable coding languages to design replicable and unique data solutions. Build and maintain data pipelines and ETL processes. Work independently and with various internal and external project teams. Conduct research on data engineering techniques and successfully integrate results. Learn analytical methods and methodology to benefit external clients. Interpret results of data analytics and present findings clearly and efficiently. Clearly communicate techniques and results with clients and project team members. Support all other project work, as directed. Our intern program serves to give undergraduate and graduate students first-hand experience with consulting at NewGen. Qualifications The successful candidate will have the following qualifications: Pursuing a Master’s or PhD in a quantitative field (e.g., Engineering, Physics, Computer Science). Problem-solving and data analytical capabilities. Ability to work collaboratively within a team and communicate effectively with colleagues and clients. Proficiency in the English language and ability to communicate effectively using verbal, written, and visual graphic skills. Background with financial and/or utility systems and experience with SQL, R, STATA, and/or SAS is a plus. Experience with other coding languages/platforms for advanced analytics is also desirable (Linux, Bash, PowerShell, Java). Experience with APIs and web scraping is considered a plus. Knowledge of Power Query, PowerPivot, Power BI, and working in Excel's Data Model coding in DAX language and writing VBA is considered a plus. Additional Information Location : Lakewood, CO or Austin, TX Employee Type : Part-time We offer a competitive hourly wage based on prior experience. This is a temporary part-time position and is not eligible for benefits. Start and end dates are flexible. Base Compensation : $25 per hour The range provided is NewGen's reasonable estimate of the base compensation for this role. The actual amount may be higher or lower, based on non-discriminatory factors such as location, experience, knowledge, skills, and abilities.",
        "url": "https://www.linkedin.com/jobs/view/3967707148"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3962164157,
        "company": "Venafi",
        "title": "Data Engineer, Snowflake",
        "created_on": 1720638704.9850152,
        "description": "There are 2 actors on a network, people and machines. Just as usernames and passwords are used by people to access machines, machine identities are used by machines to identify and access each other. Venafi is the inventor of the technology that manages and protects machine identities, the most important security initiative in our Global 5000 customers. We are Warriors! Are you passionate about making a positive impact and protecting the world from cybercriminals? If so, you may be a natural Venafi Warrior! How You’ll Be Protecting The World Venafi is looking for a Data Engineer with a Snowflake expertise, who will report to the Sr. Director of Enterprise Analytics to support their Data as a Service mission. The Data Engineer is responsible for designing, and developing high-performance, resilient, automated data pipelines, and data transformations that feed our cloud-based Enterprise Data Platform. You will work with data analysts to design data integrations that meet organizational needs within our Snowflake data ecosystem. You are comfortable with BI work from the requirements phase through ETL, all the way through the presentation layer of BI. You will also implement and govern Data Management best practices and proactively recommend approaches and solutions. Strong problem-solving skills are needed with the ability to work across multiple projects at a time. The Ideal Venafi Warrior Will Be Armed With Expert in programming languages like SQL, Python, Scala etc. Working experience with Snowflake - data modeling, ELT using Snowflake SQL, implementing complex stored Procedures and standard DWH and ETL concepts Expert knowledge of Snowflake concepts like Streams, Tasks, Snowpipes, Zero copy clone, time travel, query profiling, RBAC controls, virtual warehouse sizing and experience using these features Proven track record in designing complex scalable pipelines using Cloud supported ELT Tools. Take initiative; identify key requirements in dynamic environments Ability to communicate effectively and credibly with stakeholders and other team members Experience implementing Data warehouse, Data lakes in the cloud Ability to create & implement data engineering best practices for the full software development life cycle, including coding standards, code reviews, source control management, documentation, build processes, automated testing, and operations. 5+ years of experience While you are busy protecting the world, we’ve got you covered! In addition to fostering a virtual first collaborative environment, Venafi offers a benefits package that is in the top 10%. Venafi pays 90% of the monthly premium for medical insurance and 100% of the monthly premium for dental, vision, life insurance, short and long-term disability, and accident insurance for both team members and their families. We offer an open time off policy and observe 12 holidays each year. We also offer a 401(k) with company matching, company HSA contribution, 2x salary employer-paid life insurance, parental leave, pet insurance, fertility, adoption and surrogacy benefits! More About Venafi Venafi is the undisputed leader in Machine Identity Management. Why? Because we created the category and are light years ahead of anyone that would consider competing! Gartner has recognized Venafi as number one in our space and as it turns out, one is NOT the loneliest number! Venafi is the inventor of the technology that secures and protects machine identities. The Venafi platform provides visibility, intelligence, and automation for SSL/TLS, IoT, mobile, cloud native, Kubernetes, and SSH machine identity types. Many of the largest organizations in the world use Venafi. Billions of dollars have been spent protecting usernames and passwords and almost nothing managing machine identities—organizations are just now realizing that managing and protecting machine identities is as important as managing usernames and passwords. The bad guys know this and are using stolen or forged machine identities in their cyberattacks. In fact, Gartner says 50% of network attacks will use machine identities. Come help us protect the world! The anticipated pay range for this position is $98,000 to $115,000. This is a general estimate for informational purposes only. The actual salary offered will be determined based on the candidate’s relevant qualifications, experience, and skills. Upon review of the candidates and based on the objective factors listed above, this position may be filled at a higher or lower tier.",
        "url": "https://www.linkedin.com/jobs/view/3962164157"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Irving, TX",
        "job_id": 3921636237,
        "company": "Syntricate Technologies",
        "title": "Data Engineer",
        "created_on": 1720638708.2850397,
        "description": "Required Skills Required Skills: 10+ years of experience in solutioning data pipeline for large enterprise data Warehouse applications using AWS, Data Lake, Data Ingestion, Data Transformation, /Computation, Orchestration, Reporting & Data Analytics Good experience with Schema design, ETL setup, Batch jobs setup / custom scripting, data curation and aggregation Postgres, MongoDB Strong knowledge in CICD Pipeline for automatic deployment. Strong knowledge on design and integration patterns Proficient in technical artifacts e.g., Application Architecture, Solution Design Documents, etc Strong at analytical and problem-solving skills, Experience working with multi-vendor, multi-culture, distributed offshore and onshore development teams in dynamic and complex environment. Must have excellent written and verbal communication skills Experience working in Agile delivery",
        "url": "https://www.linkedin.com/jobs/view/3921636237"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Minneapolis, MN",
        "job_id": 3919924442,
        "company": "EA Team Inc",
        "title": "Data Engineer",
        "created_on": 1720638710.2364206,
        "description": "Primary : SQL, Azure, Azure Data Factory, Azure DevOps, Azure DataBricks Snowflake, SQL Server, GitHub Secondary: Python, Pyspark Sr Data Engineer with expertise to design and create Data Pipelines in Azure Cloudusing Azure Data factory, ADLS, Azure Databricks, Log Analytics , Application Insight, Azure MonitorExpertise in writing python and SQL ScriptsFamiliarity with Terraform or other Infrastructure as code Software.Writing Data quality automation tests using test framework like Great ExpectationsTools and Languages.Python. PySpark. SQL Cloud Technologies. Azure. Azure Data Factory. Azure DevOps. Snowflake. Databases. Snowflake. SQL Server. Logging and Documenting. Confluence. GitHubResource Type IT ContractorWhat Segment will this Contractor(s) Support Optum Technology",
        "url": "https://www.linkedin.com/jobs/view/3919924442"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3925502637,
        "company": "Intellisoft Technologies",
        "title": "Data Engineer",
        "created_on": 1720638711.9949706,
        "description": "immediate backfill opening I have at Health Care Company/Optum? The candidate who was previously in the role had a background working on Azure and AWS. His most recent project was not Azure and on AWS, but he was basically gathering batch data and creating jobs/pipelines in Databricks and streaming it. Candidates must be on your W2. Company: Health Care Company/Optum Position: Data Engineer Location: 100% Remote Duration: 6+ month 3-5 years of experience working on Azure. Data Factory pipelines Strong Databricks and Spark experience Setting up jobs in Databricks and using Spark SQL query experience Python development Scala and R development is a plus",
        "url": "https://www.linkedin.com/jobs/view/3925502637"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Austin, TX",
        "job_id": 3931196742,
        "company": "Zortech Solutions",
        "title": "Python Data Engineer-US/ Fulltime",
        "created_on": 1720638713.7263489,
        "description": "Role: Python Data Engineer Location: Austin, TX (Onsite from day 1) Duration: Full-time Minimum 5+ years of experience as a Data Engineer role. Proven experience in designing, building, and deploying data pipelines in the cloud (ideally GCP). Python API development experience Strong understanding of data architecture principles and best practices. Expertise in SQL and experience with BigQuery. Familiarity with data analysis techniques and tools. Excellent communication and collaboration skills. Ability to translate business requirements into technical specifications. Experience working in a customer-facing role is highly desirable. Experience with data warehousing concepts is highly desirable. Proficiency in scripting languages (Python) is a plus. Proficiency in BI tools such as Tableau, Power BI, or equivalent. Design, develop, and maintain BI reports, dashboards, and data visualizations using BI tools (e.g., Tableau, Power BI, etc.). GCP certification (Professional Data Engineer or equivalent) is a plus.",
        "url": "https://www.linkedin.com/jobs/view/3931196742"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Atlanta Metropolitan Area",
        "job_id": 3962975875,
        "company": "Vesta",
        "title": "Data Engineer",
        "created_on": 1720638715.3660874,
        "description": "The Vesta team is seeking a highly motivated individual to take on the role of Data Engineer . The Data Engineer is committed to the support and enhancement of our SQL Server data warehouse environment. This role is responsible for the movement and loading of data and the support of those processes. Your experience with large data sets and the processing of that data through the use of T-SQL is a core requirement for this role. Vesta is a SaaS-based, global leader in fraud protection solutions. We are headquartered in Portland, OR with flagship offices in Atlanta, GA, Dundalk (Ireland), and Mexico City (Mexico). We are looking for energetic new talent to help us execute on our plans for innovation, growth and expansion. Candidates in the Atlanta, GA will be preferred, however, remote office is possible for the right candidate. Core Responsibilities: programming and systems analysis, as well as quality assurance, prototyping, construction, integration, and migration functions and maintain knowledge relevant to consumer behavior, fraudulent activity patterns and payment processing. the creation and maintenance of a diverse set of calculations that support the Data Science team others with your deep knowledge of T-SQL and data processing approaches to tune workloads and integrate data from multiple platforms is a plus Education & Experience Required: selected candidate will possess at least five (5) years of extensive experience working with T-SQL, views, joins, tuning and ETL development will have experience supporting multi-Terabyte databases in a Data Warehouse environment or equivalent Competencies Required: analytical and detail oriented with strong problem-solving skills. skills in Microsoft T-SQL development and working knowledge of data modeling, data warehousing and data mining. verbal and written communication, interpersonal and customer service skills. organizational skills. be able to work independently and display initiative, self-motivation and dedication necessary for timely and successful work completion. have demonstrated history of impeccable integrity, ethics and fair play. of work will be in T-SQL on Microsoft SQL Server and involve the movement of data within and across databases. Other: On occasion this role may require off-hours work in order to address escalations or urgent concerns. About Vesta Vesta is a Software-as-a-Service (SaaS) company that specializes in managing payments in the “card-not-present” (CNP) arena. We use next-level technologies and cutting-edge data science methodologies to bring unparalleled accuracy to fraud detection. We are a company reinventing ourselves for the future in an industry that is growing and changing at the same time. We are looking for talented people to join us who resonate with our Vesta Values: trust, partner, empower, passion and courage -- and who love to tackle new challenges every day. We are also a casual, fun and flexible workplace that also offers highly competitive benefits including a selection of medical, dental and vision care options, paid parental leave, generous paid-time-off and a paid sabbatical program. Vesta is an equal opportunity employer and considers qualified applicants for employment without regard to race, color, religion, creed, sex, sexual orientation, gender identity, marital status, national origin, ancestry, age, veteran status, or disability. If you are an individual with a disability and require a reasonable accommodation to complete any part of the application process or are limited in the ability or unable to access or use this online application process and need an alternative method for applying, you may contact people@vesta.io for assistance.",
        "url": "https://www.linkedin.com/jobs/view/3962975875"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Atlanta, GA",
        "job_id": 3961721085,
        "company": "i3 Verticals",
        "title": "Data Engineer",
        "created_on": 1720638717.4188557,
        "description": "Job Details Description JOB TITLE: Data Engineer FLSA STATUS: Non-Exempt DEPARTMENT: Enterprise Services REPORTS TO: Enterprise Services Turnkey Solutions Practice Associate Director JOB LOCATION: Remote (US Based) TRAVEL: Less than 10% Summary Of Position The transition of onboarding new clientele from other software platforms requires unique individuals with diverse skills. A data conversion engineer performs varied job functions and is a vital member of the Turnkey Solutions Practice team. Essential Duties & Responsibilities Provides data conversion experience to collaborate with and assist team members on company initiatives Performs review of converted data to ensure accuracy Reviews and ensures financial balancing is achieved with converted data Assists in the evaluation and recommendation of conversion tools Corresponds with management, various company personnel, and/or clients to ensure conversion requirements and specifications are identified and understood Compiles and assesses feedback to improve performance for future conversions Executes and delivers tasks for the complete conversion project life cycle from initiation to project closure Develops, designs, and documents data mapping schematics Develops a data conversion plan, strategy, and approach Develops additional or other documentation requirements as needed to guide future conversion projects Develops and deploys scripts for analyzing and ensuring data integrity Develops and deploys applications and scripts for data migration Identifies and resolves risks and issues, managing and escalating appropriately Mentors Associate- Level Data Engineers and Data Engineers Work with Project Manager to ensure successful completion of project on-time, in scope and under budget Minimum Qualifications (education And Experience) Excellent verbal and written communication. Excellent time management skills. Experience in a client-facing role. Experience in stakeholder management. Experience working in a diverse and challenging environment. Ability to determine estimates of effort and impacts on a variety of solutions. Ability to manage rapid change in a fast-paced environment. Ability to manage deliverables and provide support to multiple projects. Data Conversion experience. Knowledge of Microsoft Windows applications. Analytical and problem-solving skills. Meticulous attention to detail. Experience in testing and validation of output data, including reports. Minimum of two years of experience with SQL. Preferred Qualifications (education And Experience) ▪ Bachelor’s degree in Information Technology or related field, or equivalent experience ▪ Experience in County/Municipal Government software operations and processes a plus ▪ Hands on experience developing in SQL, XML, scripting languages, and/or Java Not currently recruiting from California, Colorado, Connecticut, Maryland, Nevada, New Jersey, Ohio, Rhode Island, Washington, or New York",
        "url": "https://www.linkedin.com/jobs/view/3961721085"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Beverly, NJ",
        "job_id": 3961557826,
        "company": "Burlington Stores, Inc.",
        "title": "Data Engineer I",
        "created_on": 1720638719.2424862,
        "description": "LOCATION 4287 Route 130 S Edgewater Park NJ US 08010 Overview Come join our growing team of data practitioners and be on the leading edge of Burlington’s digital transformation! Burlington is seeking a self-driven and highly motivated individual to join a dynamic team with a passion for data, software, and engineering. At Burlington you will have the opportunity to work with the latest technologies in a goal-oriented environment. As a Data Engineer I you will be a member of the Enterprise Data and Analytics team supporting business areas including Merchandising, Allocations, Marketing, IT and Supply Chain Analytics teams with insights gained from analyzing Burlington and external data. To be successful in this position you will have strong experience pulling data from various internal and external data sources and preparing it for advanced analytics, segmentation, and modeling. Additionally, you shoudl have strong interpersonal and relationship building skills as well as written and verbal communication skills. Experience: 3-5 years of experience in designing and implementing large scale data loading, manipulation, processing, analysis, and exploration solutions Experience developing SQL based data processing 3+ years of experience with Data Architecture, Data Warehouse, Data Lake, Data Marts and Data Stores with focus on AI/ML techniques Experience with Snowflake, Oracle Databases, Azure/AWS and ADLS Skills and Abilities: Technical expertise with pulling and massaging data Great understanding of first/third party data Agile Development methodology Database Normalization Advanced SQL skills Understanding of data management principles and processes Passion for data, analytics and pushing business innovation Education: Bachelor’s or master’s degree in Computer Science / Engineering, Informatics, or related areas Come join our team. You’re going to like it here! You will enjoy a competitive wage, flexible hours, and an associate discount. Burlington’s benefits package includes medical, dental and vision coverage including life and disability insurance. Full time associates are also eligible for paid time off, paid holidays and a 401(k) plan. We are a rapidly growing brand and provide a variety of training and development opportunities so our associates can grow with us. Our teams work hard and have fun together! Burlington associates make a difference in the lives of customers, colleagues, and the communities where we live and work every day. Burlington Stores, Inc. is an equal opportunity employer committed to workplace diversity. \\ Posting Number 2024-225572 Location US-NJ-Edgewater Park Address 4287 Route 130 S Zip Code 08010 Workplace Type Hybrid Position Type Regular Full-Time Career Site Category Corporate Position Category Information Technology Evergreen Yes Min USD $65,000.00/Annual Mid USD $83,000.00/Annual",
        "url": "https://www.linkedin.com/jobs/view/3961557826"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Dallas, TX",
        "job_id": 3941711036,
        "company": "Leap Metrics",
        "title": "Data Engineer",
        "created_on": 1720638721.053595,
        "description": "Overview We are accepting applications for the Data Engineer positions to work on a product that helps improve the health of our population. Do you enjoy working with leading-edge technologies? Be part of a fast-moving, fast-growing, innovative technology team. Directly improve the daily lives of tens of thousands of our neighbors? If so, please contact us! Who We Are Leap Metrics is headquartered in Richardson, TX in the Dallas/Fort Worth Metroplex. We are a healthcare analytics and care management software company founded to improve health outcomes, lower the cost of care, and improve regulatory compliance. Our software platform is called Sevida which stands for “In the service of human lives”. We are on a mission to leverage technology to serve populations with chronic healthcare needs. We are a company of experienced technologists; a passionate dedicated group of developers, designers, client advocates, and entrepreneurs. Our team has some of the best enterprise software minds in the technology industry. To learn more about us, please visit our website at http://leapmetrics.io Why Work for Leap Metrics? MEANINGFUL MISSION – Empowering health providers to focus on care FULFILLMENT – Make an impact by building a meaningful product LEADING EDGE TECHNOLOGY – Advance your technical skills by working with the latest and most innovative technology. COMPANY CULTURE: - team-oriented, collaborative, supportive, innovative REMOTE WORK – Work from anywhere in the US. Job Description Work with seasoned technology entrepreneurs Develop effective ETL/ELT pipelines Data Modeling and building advanced queries Analyze, design, develop, and maintain the application Develop and commit organized and well-structured code Automate data pipelines Assist with the management of data infrastructure Conduct rigorous testing and identify and debug issues Test, troubleshoot, and debug pipelines Write technical documentation as required Product support Qualifications Experience with Python programming Knowledge of data visualization tools like Sigma Computing, PowerBI, Tableau, or equivalent Experience in Machine Learning is a plus Strong understanding of development Tools such as Git and Jira Creating appropriately detailed documentation Working as an individual contributor and as a member of a high-performing team Working creatively and analytically in a problem-solving environment Thinking clearly under pressure to solve problems and deliver solutions Experience working with teams distributed globally Excellent communication and teamwork skills Great attention to detail MS or BS in computer science, engineering, or equivalent experience in a related field (or experience gained as a self-taught hacker) Pursuing MS or BS in computer science, engineering, or equivalent experience in a related field (or experience gained as a self-taught hacker) Nice to have Exposure to ETL/ELT pipelines Knowledge of Snowflake or equivalent Big Data technologies Knowledge of Apache Airflow and DBT Experience writing and consuming GraphQL APIs Exposure to Google Cloud Job Type Full-time Location US only - Dallas/Ft. Worth Metroplex US only - Hybrid Benefits Flexible schedule Experienced mentors Professional development assistance Other Applicants for employment in the US must have work authorization that does not require sponsorship of a visa for employment authorization in the United States.",
        "url": "https://www.linkedin.com/jobs/view/3941711036"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Dallas, TX",
        "job_id": 3925056468,
        "company": "Kforce Inc",
        "title": "Data Engineer",
        "created_on": 1720638722.7749481,
        "description": "Responsibilities Kforce's client in Dallas, TX is looking for an experienced Senior Data Engineer to join their Data Warehousing initiative. Summary: The Senior Data Engineer will be working with Cloud native data warehouses that are running and built on technologies like Azure Data Factory, Azure Synapse, and Microsoft SQL Server. Improving analytical capabilities is a key element of their strategy and centralizing data using ETL development is the foundation of that success. Requirements MS/Bachelor's degree in Computer Information Systems or a related field; Or equivalent work experience 3+ years of hands-on ETL/ELT development using SQL Programming is required Understanding of relational databases, data warehousing modeling and architecture best practices Hands-on experience developing and maintaining data warehouses, data lakes with a medium-to-large scope and complexity Experienced with tools like Azure Data Factory, SQL Server Data Tools, Microsoft SSIS, or similar technologies Experience in CI/CD pipelines Previous experience with Azure Synapse is helpful and preferred The pay range is the lowest to highest compensation we reasonably in good faith believe we would pay at posting for this role. We may ultimately pay more or less than this range. Employee pay is based on factors like relevant education, qualifications, certifications, experience, skills, seniority, location, performance, union contract and business needs. This range may be modified in the future. We offer comprehensive benefits including medical/dental/vision insurance, HSA, FSA, 401(k), and life, disability & ADD insurance to eligible employees. Salaried personnel receive paid time off. Hourly employees are not eligible for paid time off unless required by law. Hourly employees on a Service Contract Act project are eligible for paid sick leave. Note: Pay is not considered compensation until it is earned, vested and determinable. The amount and availability of any compensation remains in Kforce's sole discretion unless and until paid and may be modified in its discretion consistent with the law. This job is not eligible for bonuses, incentives or commissions. Kforce is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.",
        "url": "https://www.linkedin.com/jobs/view/3925056468"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Stamford, CT",
        "job_id": 3969202880,
        "company": "Smith Arnold Partners",
        "title": "Data Engineer",
        "created_on": 1720638724.541578,
        "description": "Exciting Data Engineer opportunity! You will work with a talented team of Data Architects and Principle Azure Data Engineers to modernize this corporations Data Warehouse Environment! Chance to expand your Azure Data Engineering expertise, while utilizing your SQL Data Engineering expertise! Competitive compensation, tremendous benefit package and 401k. Cutting edge technology, incredible culture and working environment! Employee testimonials: Fantastic Place to Work. Hope to finish out the rest of my career with this great company! Good salary and package deal Friendly environment Great career advancement Everyone loves working here and you feel it! Great company, culture, and work-life balance! Title: Data Engineer Location: Stamford, CT Salary: $130,000 – $160,000 +Generous Bonus, Incredible benefits and retirement package! Responsibilities: Support the timely migration and operation of an existing on-premises SQL Server Data Warehouse to Microsoft Azure. Gather requirements and build customized data feeds and reports. Learn and promote the use of newer BI tools and participate in their implementation. Performs data analysis and development in support of the data integration of new system implementations and upgrades, business projects, and changes to existing reports You will work with a small team responsible for the design, development, implementation, operation, and ongoing support of a new Azure Data Lake/Data Warehouse. Provide support in designing and overseeing enterprise-grade data pipelines and data stores. Implement automation and streamline processes to optimize the entire data and analytics platform, ensuring efficient throughput and high-performance outcomes Designing, implementing, and managing data extraction, transformation, and loading (ETL) processes Create, maintain, and continuously enhance scalable data pipelines, while also developing new data source integrations to accommodate the growing volume and complexity of data. Creating data processing and integration solutions for both batch and real-time scenarios, proficiently handling structured and unstructured data Requirements: Demonstrated expertise in Microsoft Azure development. 6+ years experience with SQL development and relational database experience, including significant utilization of Microsoft Business Intelligence development tech stack. Advanced data integration skills utilizing Microsoft SSIS or similar ETL toolset 3-5 years of hands-on Data Warehouse development experience within the Microsoft Azure environment Must have experience deploying modern data solutions leveraging components like Azure functions, Azure DataBricks Azure Synapse, Azure Data Factory, Data Flows, Azure Data Lake, Azure SQL. Exhibit an understanding of Data Lake architectures, including raw, enriched, and curated layer concepts, and ETL/ELT operations. Exhibit a solid understanding of database design, data warehousing concepts, big data platforms, and ETL operations.",
        "url": "https://www.linkedin.com/jobs/view/3969202880"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Dallas, TX",
        "job_id": 3971244413,
        "company": "Donato Technologies, Inc.",
        "title": "Data Engineer - Tier 2 Support",
        "created_on": 1720638726.3783424,
        "description": "Job Title: Data Engineer - Tier 2 Support Work location: Los Angeles, CA / Dallas, TX. Job Summary: We are seeking a Data Engineer with expertise in Teradata and Vantage Cloud Lake to join our Tier 2 support team . The ideal candidate will be familiar with Goldengate tools and experienced in data synchronization processes. This role requires excellent problem-solving skills and the ability to analyze and resolve issues within ETL jobs effectively. Key Responsibilities Manage and optimize data synchronization processes using Teradata and Vantage Cloud Lake. Utilize Goldengate tools to ensure data accuracy and consistency across distributed systems. Troubleshoot and resolve issues in ETL jobs, implementing fixes to prevent future occurrences. Collaborate with cross-functional teams to improve data flow and quality. Monitor data systems performance and adjust configurations to enhance efficiency. Document technical procedures and configurations related to data management and recovery. Qualifications Proven experience in data engineering with a strong focus on Teradata and Vantage Cloud Lake. Familiarity with Goldengate or similar data integration tools. Strong problem-solving skills and experience in troubleshooting ETL job failures. Ability to work collaboratively in a fast-paced, team-oriented environment. Excellent communication skills and the ability to document and explain technical details clearly. Bachelor’s degree in computer science, Information Systems, or a related field. Preferred Skills Familiarity with SQL and script writing. Experience in a Tier 2 support role or similar. Knowledge of data warehousing best practices and principles.",
        "url": "https://www.linkedin.com/jobs/view/3971244413"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Columbus, Ohio Metropolitan Area",
        "job_id": 3964407986,
        "company": "Brooksource",
        "title": "Data Engineer",
        "created_on": 1720638728.083541,
        "description": "Data Engineer 12 Month CTH Columbus, OH $28/hr As a Data Engineer, you will be supporting our energy client! This is a large project initiative team that is providing a ton of growth opportunities. You will be working with multiple systems, providing valuable analysis and solutions. If you are looking for a team with exposure to senior leaders, career growth, and an opportunity to learn business operations, this is the role for you! Requirements Degree in Data Analytics, Management Information Systems, Computer Science, Supply Chain, or similar area of study SQL experience Cloud experience such as GCP (Google Cloud Platform), AWS Azure Tableau experience Process mapping experience Agile experience Excellent written and verbal communication Responsibilities Work with the team to support internal projects Participate in Scrum stand up meetings and the Agile process Partner with business product owners on project initiatives Work along side Tableau engineers Work to build machine learning models, analyze large data sets, and work between multiple data sets Work with the data lake and write SQL queries, big queries, and additional data tools Why Should You Apply? Be mentored by senior team members Participate in technical training and professional development through the Elevate Program Eight Eleven Group provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, national origin, age, sex, citizenship, disability, genetic information, gender, sexual orientation, gender identity, marital status, amnesty or status as a covered veteran in accordance with applicable federal, state, and local laws.",
        "url": "https://www.linkedin.com/jobs/view/3964407986"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Las Vegas, NV",
        "job_id": 3930049085,
        "company": "Caesars Entertainment, Inc.",
        "title": "Data Engineer (Corporate) - Las Vegas",
        "created_on": 1720638730.0364017,
        "description": "Job Description The Data Engineer is a data and technology expert supporting the broader Data Engineering team. The primary responsibility of a Data Engineer is to develop, maintain, and support data ingestion pipelines. Candidates that succeed in this role have a passion for technology and innovation, and understand the business value and meaning behind the data they work with. An ideal candidate should have a proven history as a strong independent contributor that designs source to target pipelines in a cloud first environment. Responsibilities include, but may not be limited to the following: Collaborate with business users to gather requirements, write functional and technical specifications and communicate technical requirements. Develop, configure, and support complex SQL and/or ETL solutions within various computing environments and pipelines. Define new capabilities and identify opportunities for continuous improvement. Document and comment code from design to completion to assist in future understanding of solutions. Perform technical reviews of code to maintain consistent technical direction and minimize system impact on production pipeline. Ability to work with stakeholders to identify high priority items and to allocate time effectively to meet expected deadlines. Provide support for production data pipelines and associated technology. Technical Skills Required Expertise in SQL is required; Must be comfortable working with large structured and unstructured datasets, and writing complex SQL logic to achieve the desired output. Background in end-to-end data pipeline development and deployment with at least 1+ year hands-on experience with cloud platforms required. Familiarity with SQL orchestration tools like Airflow and cloud platforms like Snowflake, Google Cloud Platform, or Azure highly preferred. Hands on development experience using ETL tools and code repositories required. Experience with programming languages such as C# and Python. Experience with modern BI Reporting platforms (such as Tableau, Power BI) preferred. Knowledge And Experience Minimum of 3 years of full-time work experience in data field Bachelor’s degree in technical field highly preferred Candidates must have the ability to uphold and demonstrate the highest level of integrity in all situations and recognize standards required by a regulated business. About Us At Caesars Entertainment, Inc., our Team Members create the extraordinary. We are the largest casino-entertainment company in the U.S. and one of the world's most diversified casino-entertainment providers. Since beginning in Reno, Nevada, in 1937, Caesars Entertainment has grown through the development of new resorts, expansions and acquisitions. Our resorts operate primarily under the Caesars®, Harrah's®, Horseshoe® and Eldorado® brand names. We focus on building loyalty and value with our guests through a combination of impeccable service, operational excellence and technological leadership. The company is committed to its Team Members, suppliers, communities and the environment through its PEOPLE PLANET PLAY framework. Our Caesars family is driven by our Mission, Vision and Values. We take great pride in living these values – Together We Win, All In On Service and Blaze the Trail – every day. Our mission, “Create the Extraordinary”. Our vision, “Create spectacular worlds. That immerse, inspire and connect you. We don’t perform magic; we create it with excellence. #WeAreCaesars”. If you are ready to create some magic, we invite you to explore our dynamic, yet unique, career opportunities.",
        "url": "https://www.linkedin.com/jobs/view/3930049085"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Greater Minneapolis-St. Paul Area",
        "job_id": 3943043000,
        "company": "Robert Half",
        "title": "Data Engineer",
        "created_on": 1720638731.9458535,
        "description": "We are currently looking for a highly competent Data Engineer, enthusiastic about mathematics and statistics, to join our team. The successful candidate will have strong skills in Power BI and will have the ability to effectively shape our strategic decisions in response to data analysis findings. Experience in data visualization and presentation is necessary as the successful person will be asked to convey complex quantitative findings to less technical stakeholders. Key Duties and Responsibilities: Designing and maintaining data systems using Power BI, SQL, and Azure. Developing and implementing databases, data collection systems, data analytics, and other strategies that optimize statistical efficiency and quality. Using statistical tools to interpret data sets, paying particular attention to trends and patterns that could be valuable to our company. Preparing reports that effectively communicate trends, patterns, and predictions using relevant data. Liaise with management to understand data needs and devise data-driven solutions. Collaborating with team members to collect, analyze, and interpret results using standard statistical tools and techniques. Pinpointing trends, correlations, and patterns in complex data sets. Driving process improvement through accurate data analysis. Having a strategic approach and accurately analyzing and reporting the data findings to the upper management. Requirements: Proven experience working as a data engineer or in a relevant field. Excellent command on Power BI, SQL, and Azure. A knack for dealing with voluminous data. Solid experience in data analysis and visualization. Excellent analytical skills and the ability to combine data from various sources. Problem-solving aptitude. Strong communication skills to effectively convey complex data in understandable terms to less technical teams and stakeholders.",
        "url": "https://www.linkedin.com/jobs/view/3943043000"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Seattle, WA",
        "job_id": 3964649530,
        "company": "Atimi Software",
        "title": "Data Engineer",
        "created_on": 1720638733.6463323,
        "description": "Atimi seeks an experienced data engineer to fill a contracting position in Seattle, U.S.A. This position is on-site at the client offices. Atimi works with some of the leading companies in North America, providing them with high-quality software solutions that integrate both mobile and web experience. If you are a creative, self-motivated individual with vast user experience working with complex problems, Atimi is the place for You. We are looking for an established leader in the domain with solid experience in software development principles, data engineering, hands-on knowledge of the latest cloud technologies, business intelligence, and soft skills. You work well with colleagues, partners and clients and have excellent communication skills. Responsibilities Working closely with stakeholders to understand their requirements, design and implement the right solution Work closely with other teams, analyze source systems, define underlying data sources and transformation requirements, design suitable data models and document the design/specifications Implementing data models and data mining solutions Designing and implementing an ETL and data cleansing solutions Build and maintain the infrastructure to answer questions with data, using software engineering best practices, data management fundamentals, data store principles Delivering fair outcomes for our customers, ensuring conduct maintains a high level of expertise Requirements Basic qualifications Bachelor's degree in Computer Science, Engineering, Math, Finance, Statistics or related discipline from a recognized university 5+ years of relevant experience in a business intelligence role, including data warehousing and business intelligence tools, techniques and technology 3+ years of experience in analytics, business analysis or comparable consumer analytics solutions Expert knowledge of SQL (DQL and DDL) Expertise in very large Data Warehousing and Online Analytical Processing Expertise in AWS Technology stack (Redshift, RDS, S3, EMR) Familiarity with Linux scripting (bash, python, javascript) Solid experience with business intelligence reporting tools (Tableau, OBIEE, Cognos, MicroStrategy, SSAS Cubes) Hands-on experience with recent advances in distributed computing such as MapReduce, MPP architecture and NoSQL databases Experience with Hadoop, Hive, Spark, EMR Expertise in data warehouse design Expertise in relational database design Expertise in data mining algorithms Expertise in data cleansing Experience working directly with clients and other project stakeholders to define and refine requirements Strong English skills (written and verbal.) The position is open to anyone, but you must be located in Seattle, WA, U.S.A. Relocation support is not provided. Please submit your resume and cover letter for review. All applications will be reviewed, but only those who are able to demonstrate the right skills will be contacted for a remote interview. The successful candidate will be required to work from office at least three days every week. Benefits About Atimi Software Hello, we're Atimi. If you've got a smartphone or a computer, you've seen our work. You may not know our name, but you use our software - whether it's on Apple or Android devices, you're already familiar with what we do. You just don't know it yet. We work with high-profile companies that want to extend their brand reach. Our clients hire us to do the flagship work for major brands. We know what it takes to get noticed: over 60% of our apps have been featured by Apple in TV ads, iTunes advertising, and in-store or in-print ads. We work with Fortune 500 companies who want to be recognized for being innovative and want to ensure a true brand experience at every customer touch-point. Fundamentally, Atimi believes in compensating people based on the value they provide. All of us are evaluated on the core skills we are able to demonstrate when doing our job. Once you demonstrate new skills, there's no reason that shouldn't be recognized. We want to provide developers with fast-moving, cutting-edge projects where everybody has a voice, and nobody is concerned with ego or internal politics, so all of us are challenged to improve constantly. About The Interview Process The interview process for this position involves multiple stages that cover: Communication and soft-skills skills evaluation, technical skills evaluation, and live practical exercise Cultural fit with other team members and final approval by the client",
        "url": "https://www.linkedin.com/jobs/view/3964649530"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Minneapolis, MN",
        "job_id": 3940939605,
        "company": "Sibitalent Corp",
        "title": "Data Engineer",
        "created_on": 1720638735.3019261,
        "description": "Hello, I hope you are doing fine. Please find the below Jd and let me know if you are interested. Role- Data Engineer Location – Minneapolis, MN (Onsite) Full Time Role Job Description Primary Skills: PySpark, Azure DataFactory, Databricks, SQL Secondary Skills: DBT, Snowflake Data Engineer with 5-8 years of experience Must have good experience in pyspark programming and SQL scripting Translate business requirement document, functional specification, and technical specification to related coding Must have data migration experience using tools like Azure Datafactory and Databricks Must have data pipeline creation on Azure Databricks data processing engine Must have good experience in complex sql scripts and stored procedures Develop efficient code with unit testing and code documentation Ensuring accuracy and integrity of data and applications through analysis, coding, documenting, testing, and problem solving Setting up the development environment and configuration of the development tools Manage, monitor, and ensure the security and privacy of data to satisfy business needs Contribute to the automation of modules, wherever required To be proficient in written, verbal and presentation communication (English) Co-ordinating with the UAT team Thanks & Regards Krishna Chaudhary Sibitalent Corp. 101 E. Park Blvd., Suite 600, Plano, TX – 75074 Phone: 972-502-9119 Email:krishna@sibitalent.com URL: www.sibitalent.com",
        "url": "https://www.linkedin.com/jobs/view/3940939605"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Overland Park, KS",
        "job_id": 3959387732,
        "company": "NavMD",
        "title": "Data Engineer",
        "created_on": 1720638737.0237422,
        "description": "Job Position : Data Engineer Job Levels : I, II, III FLSA Status : Exempt EEO : Professionals Job Summary : We are seeking a meticulous and detail-oriented Data Engineer to join our team. As a Data Engineer, you will play a crucial role in processing monthly data files, utilizing Excel and SQL for validation and testing of data and systems. The ideal candidate will have a strong background in data manipulation, SQL querying, and a knack for ensuring data accuracy and reliability. Duties/Responsibilities : Process and analyze monthly data files using Excel for initial data assessment and preprocessing. Develop SQL queries to validate, test, and analyze data across various systems and databases. Work with cross-functional teams to understand data requirements and ensure data quality standards are met. Design and implement data validation routines to identify discrepancies and anomalies in datasets. Optimize data processing pipelines for efficiency and scalability. Troubleshoot data-related issues and provide timely resolutions. Document data workflows, processes, and validation procedures for future reference. Required Skills/Abilities : Proven experience as a Data Engineer or similar role, handling large datasets and performing data manipulation tasks. Strong SQL skills with experience in writing complex queries for data validation and testing. Familiarity with data warehousing concepts and ETL processes. Experience with scripting languages (e.g., Python, R) for data manipulation is a plus. Ability to work independently and collaboratively in a team environment. Excellent analytical and problem-solving skills. Strong communication skills to interact with technical and non-technical stakeholders. Education and Experience : Bachelor’s degree in Computer Science, Engineering, Information Systems, or a related field. At least three years of related data experience Physical Requirements : Able to sit and work on a computer for long periods of time Able to lift up to 15 lbs at a time Benefits: Competitive salary and benefits package including 401k. Opportunities for professional development and career growth. Supportive and collaborative work environment. Entertainment perks Other : Job descriptions prepared by the Company serve as an outline only. Job duties and responsibilities will be established and communicated regularly as part of the Company's EOS cadence. Responsibilities and qualifications may differ here depending upon experience and manager/non-manager levels. Due to business needs, you may be required to perform job duties that are not within your written job description. Furthermore, the Company may have to revise, add to, or delete from your job duties per business needs with or without advanced notice to employees. NavMD is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity or expression, pregnancy, age, national origin, disability status, genetic information, protected veteran status, or any other characteristic protected by law.",
        "url": "https://www.linkedin.com/jobs/view/3959387732"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Foxborough, MA",
        "job_id": 3970107547,
        "company": "KAGR (Kraft Analytics Group)",
        "title": "Data Engineer 2",
        "created_on": 1720638738.900106,
        "description": "Job Details Job Location KAGR - Foxborough, MA Position Type Full Time Education Level Bachelor's Travel Percentage No travel Job Shift Day Job Category Technical/Analytics Description SUMMARY: The Data Engineer 2 will join a fun, dynamic team to help solve integration and data problems relating to sports and entertainment. This role will integrate data from various systems and improve and expand the existing data processes as needed. The position will be responsible for performing ETL and ELT from many disparate systems into the data warehouse and will have the opportunity to complete projects from beginning to end. The right candidate will be motivated to learn, contribute to the team and organization, and grow along with the business. Duties And Responsibilities Data Integration Using cloud technology, combine data from various sources, cloud and on-premises, based on requirements Perform data cleansing and standardization Load data into a cloud data warehouse as projects dictate Using the enterprise ETL tool, create modify, and improve integration pipelines Translate business requirements into data warehouse pipelines using ETL/ELT methodologies Extract and load many disparate systems into a centralized data warehouse Business Intelligence & Data Analysis Assist with preparing and loading data for Analysis and BI reports and dashboards Identify opportunities for new data sources Collaborate with analysts to come up with creative solutions to data challenges Ongoing Responsibilities Import and integrate new data sources based on business need Proactively identify potential data problems, but react as needed to unexpected issues Improve existing processes to streamline efforts Handle multiple projects and meet deadlines Monitor, schedule, and maintain existing integrations Special projects and assignment as business dictates Responsible for the maintenance, creation and control of all personally identifiable information or any other information protected by any Confidentiality or Privacy Standards or Company policies that you have access or knowledge of, including but not limited to any state or federal regulations including HIPAA Supervisory Responsibilities This position has no supervisory responsibilities. Skills And Qualifications Bachelor’s degree in information systems, Computer Science, or related field 2-3 years of experience working with data using SQL or similar technology Very high attention to detail Familiarity with a data integration platform, such as Snaplogic, SSIS, or Informatica Familiarity with BI Visualization tools, such as Tableau Ability to work on multiple projects in a fast-paced environment Strong communication skills to all levels of technical expertise Must have attention to detail and focused concentration Must be able to learn new tasks and complete tasks independently Must be able to make timely decisions in the context of the workflow Must possess strong organizational skills, ability to multi-task and responsiveness Must be able to complete tasks that require a particular speed or productivity quota PHYSICAL DEMANDS Sitting for extended periods of time Dexterity of hands and fingers to operate a computer keyboard, mouse, and other computing equipment The employee frequently is required to talk or hear The employee is occasionally required to reach with hands and arms Specific vision abilities required by this job include close vision, distance vision, color vision, peripheral vision, depth perception, and ability to adjust focus Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions WORK ENVIRONMENT The noise level in the work environment is usually moderate Fast paced office environment Ability to work nights and weekends as business dictates CERTIFICATES, LICENSES, REGISTRATIONS None required Other Duties Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice. This company is an equal opportunity employer. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status, and other legally protected characteristics.",
        "url": "https://www.linkedin.com/jobs/view/3970107547"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Houston, TX",
        "job_id": 3962446073,
        "company": "Enzo Tech Group",
        "title": "Data Engineer",
        "created_on": 1720638740.5750725,
        "description": "For a well established player in the energy space Enzo Tech Group is actively seeking an a Senior Data Engineer to join my organisations Data team. The team is focused on working on providing efficient energy solutions to form its Data Centre of Excellence. The company is rapidly expanding this business unit and as such would like to bring on experienced Data Engineers based in Houston Tx. The following skills would be required SQL Data Engineer 5+ Years Google Cloud Python Expertise Docker Kubernetes This is a Contract to Hire Function with the right candidate expected to be onsite 2 Days a Week in Central Houston.",
        "url": "https://www.linkedin.com/jobs/view/3962446073"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Scottsdale, AZ",
        "job_id": 3967789673,
        "company": "CBTS",
        "title": "Data Engineer",
        "created_on": 1720638742.287424,
        "description": "CBTS is currently seeking a Data Engineer for a position located in Scottsdale, AZ. Responsibilities: Collaborate with fellow data engineering and cross-functional data teams to build and deliver custom enterprise data warehousing and database applications. Accurately estimate tasks, bugs, and spikes in your domain. Develop software that meets code quality standards and metrics. Build productive internal/external working relationships. Effectively manage risk, change, and uncertainty with support from your leader and peers. Build a solid understanding of inter-team functional data dependencies and navigate appropriate communication channels for solving issues as they arise. Design and implement new features while continuously improving quality of technical data products and database applications. Act as a technical expert and engineer on an agile squad. Create and execute long term vision of technical data products and database applications. Drive the architectural solutions across data warehousing and database applications. Develop and Lead communities of practice by mentoring engineers, creating, and enforcing standards, reviewing others code, and sharing knowledge and know-hows. Remain current on new cloud data solutions, design patterns, products, and development trends. Assist with problem resolution for customers and end users. Skills and Experience: Strong knowledge of data engineering, database solution design, solid principles of data modeling. Experience 10+ years in data engineering and database technologies Strong in developing reusable common component data artifacts. Experience with Data Engineering and Cloud products Strong in various Data Engineering Methodologies Data security best practices experience Strong in data accessibility best practices SQL and Enterprise Data Warehousing experience Strong in Azure or similar cloud technologies Proficient in building out ELT (Extract Load Transform) solutions Proficient in SQL Server, Azure Synapse or similar. Proficient in writing SQL Queries Preferred Experience: Knowledge of one or more modern database design and Reporting Data frameworks such as ELT, ETL or similar. Git HUB – Enterprise or similar Cloud Technologies – Azure Resources for Data Engineering Cincinnati Bell Technology Solutions provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, disability, genetic information, marital status, amnesty, or status as a protected veteran in accordance with applicable federal, state and local laws. “Notice of Collection for California Employees, Applicants, and Contractors https://www.cbts.com/privacy-policy/california-privacy-policy/”",
        "url": "https://www.linkedin.com/jobs/view/3967789673"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Atlanta, GA",
        "job_id": 3948474414,
        "company": "LanceSoft, Inc.",
        "title": "Data Engineer",
        "created_on": 1720638743.904377,
        "description": "If you are interested in this job and need more details, please send me (Surbhi) your resume on Surbhik@Lancesoft.com and call/ text me on 703-889-6630. Thanks! Job Title: Computer Vision Engineer/Data Scientist Location: Atlanta GA 30313 Duration: 5 Month Contract Hours: Business Hours (Mon – Fri) Qualifications Bachelor's or Master's degree in Computer Science, Electrical Engineering, or a related field, with a focus on computer vision, machine learning, or image processing. Strong programming skills in Python. Experience with Alteryx Experience with machine learning frameworks (e.g., TensorFlow, PyTorch) and computer vision libraries (e.g., OpenCV). Proven ability to develop and implement computer vision algorithms. Excellent problem-solving and analytical skills. Team player with strong communication skills to collaborate effectively with various teams. Exposure to machine learning deployment tools like Docker, Kubernetes, etc. is beneficial. Experience with deploying modules in cloud environment. Experience with version control tools such as Git. Working Conditions 3-month position with flexible hours with possible extension Office environment with options to work remotely as per company policy. Collaborative and innovative team-focused atmosphere.",
        "url": "https://www.linkedin.com/jobs/view/3948474414"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3947707099,
        "company": "Netflix",
        "title": "Software Engineer (L5), Data Analytics & GenAI Tools",
        "created_on": 1720638745.7643566,
        "description": "At Netflix, we want to entertain the world and are constantly innovating on how entertainment is imagined, created, and delivered to a global audience. We currently stream content in more than 30 languages in 190 countries, topping over 260 million paid subscribers, and are expanding into new forms of entertainment such as gaming. Within the Infrastructure Engineering organization, our Data Platform exists to maximize the value of data throughout Netflix by improving the productivity and innovation of Netflix engineers, data scientists, machine learning practitioners, and business users. Our Data Platform enables Netflix to scale effectively and evolve rapidly by providing highly performant, secure, compliant, and flexible data infrastructure that supports the expanding needs of the business. Ultimately, we aim to democratize data by making it more accessible and useful to technical and non-technical employees. The Opportunity The mission of the Data Platform’s GenAI Tools team is to boost the productivity of the data science and engineering community at Netflix. This means enabling users to spend more time solving business problems and less time engineering lower-level systems. We are seeking a senior software engineer to help build a developer platform that will provide the building blocks for Netflix to efficiently create GenAI applications. The platform will enable our teams to leverage LLM models and internally fine-tuned models. It will provide tools to support the LLM development lifecycle, high-level abstraction APIs for RAG, and tools for prompt engineering, management, and LLM operations. What You Will Do Design, build, and deploy APIs to access third-party large language models and internal fine-tuned ML models. Develop frameworks and components for building GenAI applications. Develop and maintain integrations between third-party products and the Netflix data platform solutions. Lead cross-functional initiatives and collaborate with engineers, product managers, and technical program managers across teams. Solve real business needs at scale by applying your software engineering and analytical problem-solving skills. Rapidly iterate with users to improve product experiences while establishing foundational capabilities. Desired Background 8+ years of software engineering experience with a successful track record of delivering quality results Strong interest and experience with the latest GenAI stack (LLMs, RAG, Agents/Tools) Strong experience building SDKs and APIs for internal or external products Strong software design and development fundamentals, with experience in building and operating scalable, observable, fault-tolerant, distributed systems Proficiency in Java, GRPC, Python, Python package management tooling, and GraphQL Experience in large-scale build, release, continuous integration/continuous deployment (CI/CD), and observability methods Experience designing, building, and deploying ML applications At Netflix, we carefully consider a wide range of compensation factors to determine your personal top of market. We rely on market indicators to determine compensation and consider your specific job family, background, skills, and experience to get it right. These considerations can cause your compensation to vary and will also depend on your location. The overall market range for roles in this area of Netflix is typically $100,000 - $700,000 This market range is based on total compensation (vs. only base salary), which is in line with our compensation philosophy. Netflix provides comprehensive benefits including Health Plans, Mental Health support, a 401(k) Retirement Plan with employer match, Stock Option Program, Disability Programs, Health Savings and Flexible Spending Accounts, Family-forming benefits, and Life and Serious Injury Benefits. We also offer paid leave of absence programs. Full-time hourly employees accrue 35 days annually for paid time off to be used for vacation, holidays, and sick paid time off. Full-time salaried employees are immediately entitled to flexible time off. See more detail about our Benefits here. Netflix is a unique culture and environment. Learn more here. We are an equal-opportunity employer and celebrate diversity, recognizing that diversity of thought and background builds stronger teams. We approach diversity and inclusion seriously and thoughtfully. We do not discriminate on the basis of race, religion, color, ancestry, national origin, caste, sex, sexual orientation, gender, gender identity or expression, age, disability, medical condition, pregnancy, genetic makeup, marital status, or military service.",
        "url": "https://www.linkedin.com/jobs/view/3947707099"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Portland, ME",
        "job_id": 3963455854,
        "company": "RxAnte",
        "title": "Data Engineer",
        "created_on": 1720638749.3268645,
        "description": "Description Data Engineer, Data Services Company Overview Over the next ten years, there will be at least 4.6 million hospitalizations from the misuse of prescription drugs in people 65 or older, resulting in $528 billion in annual avoidable costs. RxAnte is on a mission to improve people’s health by helping them get more from medicines. A rapidly growing, tech-enabled healthcare services company with over 30 million lives under management, RxAnte has become a leading provider of value-based pharmacy care management solutions for health plans. RxAnte launched Mosaic Pharmacy Service in 2019, a wholly owned subsidiary designed to offer pharmacy and chronic care management services for our clients’ most medically complex and vulnerable members. Using data, advanced analytics, specialized software and pharmacy automation, Mosaic is transforming the pharmacy experience for medically complex seniors while also helping payers achieve their quality improvement and cost savings objectives. Job Profile The Data Engineer of Data Services reports directly to the VP, Data Services and is responsible for taking part in the managing, designing, and building of systems required to deliver Mosaic and RxAnte's analytic products in a scalable manner using cloud data warehouse/lake technology. Strong analytic, communication, and AWS cloud experience is required. The Data Engineer will have data architectural and system engineering skills. In addition to taking part in the design and development of the systems, the Data Engineer will contribute to overall future cloud data warehouse/lake vision. The Data Engineer will be responsible for helping to assess and gather project requirements and assess work effort. Additionally, the role will interact with both technical and non-technical internal stakeholders. We are seeking someone who loves to set the vision in a new environment as a trailblazer, educate internal team members, and then work within the environment to apply best practices. This is a remote work position. Specific Responsibilities Include Work with Data Services leadership to architect, develop, and maintain processes and programs Establish and maintain project-deliverable processes with an eye toward full scalability and automation Support and establish cloud data environment design and development Engineer within the AWS cloud data environment using Glue, EMR Serverless, Databricks, Snowflake, or similar technologies Collaborate with internal IT team to establish best practices Collaborate with product, analytical, business intelligence, and data teams to establish best practices utilizing the cloud data environment Gather business requirements and work on system design frameworks documentation using Atlassian tools A strong desire to be able to lead projects, set vision of data governance, establish CI/CD pipelines, and contribute to ongoing development Ability to successful manage individual projects through the entire project lifecycle Other activities as needed Requirements 5+ years of relevant/related experience in similar role Experience with SQL and/or NoSQL databases including coding and system design Experience with clouds (ideally AWS) Experience with Java, Spark, and/or Python Experience with ETL/ELT tool set Experience with CI/CD pipeline Experience designing, constructing, and using data databases/lakes for product delivery Experience working with administrative health care data (e.g., commercial medical, hospital, and pharmacy claims, and Medicare or Medicaid data), healthcare informatics, or health care claims processing a plus Experience with SAS programming a plus Strong communication, analytical, and data quality skills Ability to document and work through requirements gathering Experience working in an environment which utilizes project management tools such as Atlassian Willingness to travel as needed We strongly encourage candidates from all backgrounds and every walk of life to apply. We are committed to creating an inclusive and diverse workforce. Every person on our team brings their own unique perspective, and it’s what makes our products better and our work more rewarding.",
        "url": "https://www.linkedin.com/jobs/view/3963455854"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Home, KS",
        "job_id": 3945056012,
        "company": "LanceSoft, Inc.",
        "title": "Data Engineer",
        "created_on": 1720638751.1888192,
        "description": "Gather, analyze, and interpret data to solve specific use cases. Develop and participate in the review of requirements, data mappings, database specifications documentation and other artifacts and clearly communicate to both the business and technical team members the interpretation of the data. Demonstrate the ability to access, trace, analyze, and remediate data issues leveraging SQL and applicable AWS tools such as Athena and Redshift. Good analytical skills and experience in handling large sets of data Expertise to understand complex queries, proof test, and tweak for the intended use. Critical analysis skills to extract data and analyze the data for inconsistencies and issues. Design and manage information systems, conduct analysis, and generate accurate and comprehensive reports on operational data. Very strong in documentation of data artifacts, design and reports details. Support Architects, Leads and product owners to ensure that all aspects of the information analysis and requirements gathering process are completed with the highest degree of accuracy and quality, including the development and socialization of key project artifacts. Support ad hoc data requests as required as part of DevOps team. Review, extract, and analyze data and information on system processes and procedures. Experience developing testing strategies and identifying comprehensive test scenarios based on business requirements. Demonstrated experience with AWS Lambda, NodeJS, SNS/SQS, S3, IAM, CloudWatch, RDS, DynamoDB, Redshift and Athena. Willingness and ability to perform in multiple roles on a team, including testing and production support in addition to data analysis. Strong ability to understand and internalize the big picture and broader implications. Communicate and understand business goals and requirements, and work to create data solutions that add value to the business. Support team with functional, regression and end to end testing efforts with a focus on flow of data ensuring that features deliver the expected functionality with high quality. Self-starter with ability to set priorities, work independently and attain goals. Develop data-driven reporting solutions from concept to deployment. Strong SQL skills and ability to work with different data sources. Focus on designing and developing effective user-friendly reporting assets including interfaces and outputs. Cultivate and maintain knowledge of *** products, data systems and information assets. Work with a highly proficient team using Tableau/Tableau Server, SQL, Relational/Columnar Databases, Data Modeling, MS Office Develop scripts to extract data in Athena, Timestream and Redshift. Automate the data extract pull and able to visualize these on charts/graphs/Tableau Exhibit good communication and practical decision-making skills, a believer in good feedback and documentation. About You BA/BS required (major in an analytical field desired) A minimum of 5 to 10&plus; years work-related experience. Experience in hands-on development of reporting applications for the Web in a professional environment The Ethos of continuous improvement and interest in learning new things. More about you Strong analytical thinking and structured problem-solving ability Ability to handle multiple projects and assignments simultaneously and effectively in a cross-functional team environment. Versed on the agile methodology and best practices. Excellent interpersonal and collaboration skill with the ability to work with a diverse set of colleagues, across functions, from different organizations, disciplines, etc. Self-starter, ability to set priorities, work independently and attain goals",
        "url": "https://www.linkedin.com/jobs/view/3945056012"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Overland Park, KS",
        "job_id": 3965435529,
        "company": "Propio Language Services",
        "title": "Data Engineer II",
        "created_on": 1720638752.8693047,
        "description": "Propio Language Services is transforming communication by developing tools and technologies that make it easier and more efficient for clients to engage with the Limited English Proficiency Population to improve access to healthcare and essential services in social services, education, legal and many others. The Data Engineer II will play a key role in designing, developing, and maintaining our data infrastructure. This position requires a blend of technical skills, analytical thinking, and the ability to collaborate with cross-functional teams to support data-driven decision-making processes. Key Responsibilities: Design, implement, and optimize data pipelines for efficient data processing and analysis. Develop and maintain ETL (Extract, Transform, Load) processes to ensure data accuracy and availability. Collaborate with data scientists, analysts, and other stakeholders to understand data requirements and deliver solutions. Ensure the scalability, reliability, and performance of data systems. Implement and maintain data security and privacy measures. Troubleshoot and resolve data-related issues and anomalies. Continuously improve and document data engineering processes and best practices. Act as a subject matter expert and strategic advisor on data-related initiatives, ensuring alignment with organizational objectives Perform ad hoc data analytics and reporting to meet emergent business requirements. Develop and optimize complex SQL queries and stored procedures for data extraction and modeling. Build custom automated paginated reports using SSRS (SQL Server Reporting Services). Qualifications: Bachelor’s Degree in Computer Science, Data Science, Mathematics, Statistics, or a related field; or equivalent work experience. Minimum of 3 years of experience in a comparable data engineering role. Proven experience with designing and building scalable data pipelines and ETL processes. Experience with cloud platforms (AWS, Azure, Google Cloud) and their data services. Proficiency in SQL and experience with relational databases (e.g., MySQL, PostgreSQL). Strong programming skills in languages such as Python or Scala. Experience with big data technologies (e.g., Hadoop, Spark). Familiarity with data warehousing solutions (e.g., Databricks). Knowledge of data modeling, data warehousing, and data lake concepts.",
        "url": "https://www.linkedin.com/jobs/view/3965435529"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Gilbert, AZ",
        "job_id": 3966155212,
        "company": "Champions Funding LLC",
        "title": "Data Engineer",
        "created_on": 1720638754.549193,
        "description": "Description The Data Engineer is responsible for developing and maintaining data solutions, including databases and BI pipelines. Their duties include collaborating with Data & Analytics leadership and IT colleagues on creating data infrastructure, assuring data dependability and uptime, and updating systems to accommodate changes in company needs. Who We Are Champions Funding is a nationwide wholesale lender that provides non-QM loan options for consumers and investors. As a community development financial institution (CDFI), Champions is uniquely poised to serve underserved borrowers. As designated by the U.S. Treasury, our flagship Ally program and other robust loan programs give our broker partners the tools to fulfill the dreams of homeownership for diverse homebuyers. Requirements 5+ years of experience working as a Data Engineer, BI Developer, or similar. Strong expertise in SQL, ETL development, and database optimization (SQL Server preferred). Knowledge of data architecture and associated best practices. Familiarity with BI system integration is preferred. Experience working in a financial services environment is a plus but not required. Sponsorship for work authorization is not available for this position.",
        "url": "https://www.linkedin.com/jobs/view/3966155212"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "New York City Metropolitan Area",
        "job_id": 3927963097,
        "company": "Trepp, Inc.",
        "title": "Data Engineer II",
        "created_on": 1720638758.1100907,
        "description": "(This position is based in Trepp's NYC office which operates in a hybrid work schedule) The Data Operations Group at Trepp is responsible for creating and developing our suite of software products responsible for our core applications. Being a member of the Data Operations Group allows you the flexibility to learn and hone your skills on various tools and technologies. You will have the opportunity of working with and creating services and pipelines used by teams including Structured Finance Modelers, Research, and Data Platform. Our products focus on data and analytics in structured finance (CMBS), collateralized loan obligations (CLO), commercial real estate (CRE), and banking & lending. This team works closely with our Product Groups to always refine & improve our product and feature sets, data capabilities and delivery, along with creating new Product Workflows. About this role: As a Data Engineer-II, you are responsible for building quality data products for Trepp’s data organization. The position requires technical skills and must be able to work effectively with a group of high performing individuals. The role involves modelling best practices and provides optimal ways to build reliable data pipelines with low latency. Address the needs of stakeholders by utilizing expertise in data design, data architecture, and data modeling. Responsibilities: Demonstrate data engineering expertise in building reliable data products Ability to quickly learn, understand, and work with new emerging technologies, and methodologies A drive to work on financial data systems & pipelines including experience working with large datasets Work effectively with other engineers, product managers in a collaborative environment Involve in identifying data quality issues and apply techniques to improve the same Identify critical infrastructure dependency/updates and work to plan those tasks Understand the requirements of stakeholders to build optimal solutions for various data challenges Qualifications: Bachelor's or Master's degree in computer science or in a related field 3+ years of experience building backend software systems 2+ years of experience with Python or Java, or Scala 2+ years of experience with SQL Experience with cloud services like AWS and/or GCP Experience building data pipelines using Spark/Airflow Experience with Agile and SDLC, Git workflows, and CI/CD Strong understanding of data engineering concepts Preferred Skills: Analyze, design, and implement interactive RESTful services and APIs Exposure to pub-sub messaging systems like Kafka Exposure to IaaS like Terraform Understanding of serverless architectures, and containers Experience working in Financial Services or related industry. Salary Range: Base salary range: Starting from $130k plus bonus eligible Benefits and Perks Base + target bonus compensation structure Medical, Dental, Vision insurance 401K (with employer match) Life insurance, long term disability, short term disability all covered by the company Flexible paid time off (PTO) Sixteen (16) weeks paid primary caregiver leave (Biological, adoptive, and foster parents are all eligible) Four (4) weeks paid parental leave Pet insurance Laptop + WFH equipment Career progression plan Pre-tax commuter benefit with company subsidy (For NYC-office based employees only) Involvement in Diversity and Inclusion programs Fun company events and volunteering opportunities Workplace Policy: NYC, Dallas, and London office-based positions: Trepp’s offices follow a 3-2 hybrid-working policy with the expectation of in-office work on Tuesday-Thursday and the option to work from home on Monday and Friday. Trepp maintains a drug-free workplace.",
        "url": "https://www.linkedin.com/jobs/view/3927963097"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Atlanta, GA",
        "job_id": 3958452564,
        "company": "Impiricus",
        "title": "Junior Software Engineer",
        "created_on": 1720638759.7838006,
        "description": "Location: Atlanta Metro Area (onsite) Department: Product & Engineering Reports To: Director, Engineering Job type: Full Time, Exempt Who We Are Impiricus is at the forefront of transforming the pharmaceutical industry. Our mission is to innovate the way pharmaceutical companies connect with physicians, enhancing the communication channels that are critical for the advancement of healthcare. With a team dedicated to pushing the boundaries of digital solutions, we aim to improve patient outcomes by fostering more effective, data-driven interactions between healthcare professionals and drug manufacturers. Join us in our journey to revolutionize healthcare communication and make a lasting impact on the industry. Job Summary We are seeking an energetic, technically literate individual to join our team as a Junior Software Engineer. In this role, you will collaborate in design, development, and deployment of various software products. You will report to the Director of Software Engineering. This detail-oriented and technical facing individual will have some experience or a strong track record in software engineering and working in health technology. The ideal candidate has a strong passion in health technologies, start-ups and an understanding of REST APIs and is willing to learn and collaborate on new product development. Duties/ Responsibilities: Work in collaboration under the current development team to produce the highest quality software products. Show interest in development and work on real-world problems HCPs face. Work with our client services team to configure, set-up, deploy, monitor and report on new campaigns. Be willing to learn and participate in all team meeting and not be afraid to make mistakes quickly and bounce back. Develop scalable software products under mentorship of the development team. Experience: Bachelors in computer science or relevant field. Min of 0-2 years' experience in software engineering. Good understanding and experience working in a Scrum team. A strong interest in building health tech solutions. Proven history of design, development, and deployment of scalable software OR a strong academic track record. Min 1-2 years of development in Python, JavaScript, or any other OOP Language. Experience with PostgreSQL or other RDBMS. Willing to learn and pick up new technologies quickly. A strong commitment/attitude of working in a collaborative start-up environment. Benefits: Impiricus focuses on taking care of our teammates' professional and personal growth and well-being. Full support and career-development opportunities to expand your skills, enhance your expertise, and maximize your potential along your career journey; A diverse and inclusive community of belonging, where teammates are empowered to bring ideas to the table and act; Generous Total Rewards Plan – Health Reimbursement Plan, Unlimited PTO, 401K matching, work/life balance, spectacular office location with outstanding amenities, annual company events, and more! Impiricus provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.",
        "url": "https://www.linkedin.com/jobs/view/3958452564"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Hartford, CT",
        "job_id": 3956720288,
        "company": "Travelers",
        "title": "Data Engineer I",
        "created_on": 1720638761.7870355,
        "description": "Who Are We? Taking care of our customers, our communities and each other. That’s the Travelers Promise. By honoring this commitment, we have maintained our reputation as one of the best property casualty insurers in the industry for over 160 years. Join us to discover a culture that is rooted in innovation and thrives on collaboration. Imagine loving what you do and where you do it. Compensation Overview The annual base salary range provided for this position is a nationwide market range and represents a broad range of salaries for this role across the country. The actual salary for this position will be determined by a number of factors, including the scope, complexity and location of the role; the skills, education, training, credentials and experience of the candidate; and other conditions of employment. As part of our comprehensive compensation and benefits program, employees are also eligible for performance-based cash incentive awards. Salary Range $105,100.00 - $173,400.00 Target Openings 1 What Is the Opportunity? Travelers Data Engineering team constructs pipelines that contextualize and provide easy access to data by the entire enterprise. As a Data Engineer, you will play a key role in growing and transforming our analytics landscape. In addition to your strong analytical mind, you will bring your inquisitive attitude and ability to translate the stories found in data. You will leverage your ability to design, build and deploy data solutions that capture, explore, transform, and utilize data to support Artificial Intelligence, Machine Learning and business intelligence/insights. What Will You Do? Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions. Design data solutions. Analyze sources to determine value and recommend data to include in analytical processes. Incorporate core data management competencies including data governance, data security and data quality. Collaborate within and across teams to support delivery and educate end users on data products/analytic environment. Perform data and system analysis, assessment and resolution for defects and incidents of moderate complexity and correct as appropriate. Test data movement, transformation code, and data components. Perform other duties as assigned. What Will Our Ideal Candidate Have? Bachelor’s Degree in STEM related field or equivalent Six years of related experience Proficient use of tools, techniques, and manipulation including Cloud platforms, programming languages, and an understanding of software engineering practices. The ability to deliver work at a steady, predictable pace to achieve commitments, decompose work assignments into small batch releases, and contribute to tradeoff and negotiation discussions. Demonstrated track record of domain expertise including the ability to understand technical concepts and possess in-depth knowledge of immediate systems worked on. Proven problem solving skills including debugging skills, allowing you to determine source of issues in unfamiliar code or systems and the ability to recognize and solve repetitive problems. Strong verbal and written communication skills with the ability to interact with team members and business partners. Leadership - Intermediate leadership skills with a proven track record of self-motivation in identifying personal growth opportunities. What is a Must Have? Bachelor’s degree or equivalent training with data tools, techniques, and manipulation. Four years of data engineering or equivalent experience. What Is in It for You? Health Insurance: Employees and their eligible family members – including spouses, domestic partners, and children – are eligible for coverage from the first day of employment. Retirement: Travelers matches your 401(k) contributions dollar-for-dollar up to your first 5% of eligible pay, subject to an annual maximum. If you have student loan debt, you can enroll in the Paying it Forward Savings Program. When you make a payment toward your student loan, Travelers will make an annual contribution into your 401(k) account. You are also eligible for a Pension Plan that is 100% funded by Travelers. Paid Time Off: Start your career at Travelers with a minimum of 20 days Paid Time Off annually, plus nine paid company Holidays. Wellness Program: The Travelers wellness program is comprised of tools and resources that empower you to achieve your wellness goals. In addition, our Life Balance program provides access to professional counseling services, life coaching and other resources to support your daily life needs. Through Life Balance, you’re eligible for five free counseling sessions with a licensed therapist. Volunteer Encouragement: We have a deep commitment to the communities we serve and encourage our employees to get involved. Travelers has a Matching Gift and Volunteer Rewards program that enables you to give back to the charity of your choice. What Is in It for You? Health Insurance: Employees and their eligible family members – including spouses, domestic partners, and children – are eligible for coverage from the first day of employment. Retirement: Travelers matches your 401(k) contributions dollar-for-dollar up to your first 5% of eligible pay, subject to an annual maximum. If you have student loan debt, you can enroll in the Paying it Forward Savings Program. When you make a payment toward your student loan, Travelers will make an annual contribution into your 401(k) account. You are also eligible for a Pension Plan that is 100% funded by Travelers. Paid Time Off: Start your career at Travelers with a minimum of 20 days Paid Time Off annually, plus nine paid company Holidays. Wellness Program: The Travelers wellness program is comprised of tools, discounts and resources that empower you to achieve your wellness goals and caregiving needs. In addition, our mental health program provides access to free professional counseling services, health coaching and other resources to support your daily life needs. Volunteer Encouragement: We have a deep commitment to the communities we serve and encourage our employees to get involved. Travelers has a Matching Gift and Volunteer Rewards program that enables you to give back to the charity of your choice. Employment Practices Travelers is an equal opportunity employer. We believe that we can deliver the very best products and services when our workforce reflects the diverse customers and communities we serve. We are committed to recruiting, retaining and developing the diverse talent of all of our employees and fostering an inclusive workplace, where we celebrate differences, promote belonging, and work together to deliver extraordinary results. In accordance with local law, candidates seeking employment in Colorado are not required to disclose dates of attendance at or graduation from educational institutions. If you are a candidate and have specific questions regarding the physical requirements of this role, please send us an email so we may assist you. Travelers reserves the right to fill this position at a level above or below the level included in this posting. To learn more about our comprehensive benefit programs please visit http://careers.travelers.com/life-at-travelers/benefits/.",
        "url": "https://www.linkedin.com/jobs/view/3956720288"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Virginia, United States",
        "job_id": 3952834354,
        "company": "TALENT Software Services",
        "title": "Data Engineer",
        "created_on": 1720638763.499645,
        "description": "Responsibilities JOB DESCRIPTION Expertise to understand complex queries, proof test, and tweak for the intended use. Critical analysis skills to extract data and analyze the data for inconsistencies and issues. Good hands-on experience of working with python. Must have led implementation of any market popular AI tools for analyzing the data and creating trend reports. Good analytical skills and experience in handling large sets of data Demonstrate the ability to access, trace, analyze, and remediate data issues leveraging SQL and applicable AWS tools such as Athena and Redshift. Strong SQL skills and ability to work with different data sources. Automate the data extract pull and able to visualize these on charts/graphs/Tableau Gather, analyze, and interpret data to solve specific use cases. Develop and participate in the review of requirements, data mappings, database specifications documentation and other artifacts and clearly communicate to both the business and technical team members the interpretation of the data. Design and manage information systems, conduct analysis, and generate accurate and comprehensive reports on operational data. Very strong in documentation of data artifacts, design and reports details. Support Architects, Leads and product owners to ensure that all aspects of the information analysis and requirements gathering process are completed with the highest degree of accuracy and quality, including the development and socialization of key project artifacts. Support ad hoc data requests as required as part of DevOps team. Review, extract, and analyze data and information on system processes and procedures. Experience developing testing strategies and identifying comprehensive test scenarios based on business requirements. Demonstrated experience with AWS Lambda, NodeJS, SNS/SQS, S3, IAM, CloudWatch, RDS, DynamoDB, Redshift and Athena. Willingness and ability to perform in multiple roles on a team, including testing and production support in addition to data analysis. Strong ability to understand and internalize the big picture and broader implications. Communicate and understand business goals and requirements, and work to create data solutions that add value to the business. Support team with functional, regression and end to end testing efforts with a focus on flow of data ensuring that features deliver the expected functionality with high quality. Self-starter with ability to set priorities, work independently and attain goals. Develop data-driven reporting solutions from concept to deployment. Focus on designing and developing effective user-friendly reporting assets including interfaces and outputs. Cultivate and maintain knowledge of Client products, data systems and information assets. Work with a highly proficient team using Tableau/Tableau Server, SQL, Relational/Columnar Databases, Data Modeling, MS Office Develop scripts to extract data in Athena, Timestream and Redshift. Exhibit good communication and practical decision-making skills, a believer in good feedback and documentation. About You BA/BS required (major in an analytical field desired) A minimum of 5 to 10+ years work-related experience. Experience in hands-on development of reporting applications for the Web in a professional environment Passion for new technology and adapting to the latest tools in data analysis and reporting. The Ethos of continuous improvement and interest in learning new things. More about you Strong analytical thinking and structured problem-solving ability Ability to handle multiple projects and assignments simultaneously and effectively in a cross-functional team environment. Versed on the agile methodology and best practices. Excellent interpersonal and collaboration skill with the ability to work with a diverse set of colleagues, across functions, from different organizations, disciplines, etc. Self-starter, ability to set priorities, work independently and attain goals",
        "url": "https://www.linkedin.com/jobs/view/3952834354"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "St Louis Park, MN",
        "job_id": 3963335869,
        "company": "Bridgewater Bank",
        "title": "Data Engineer",
        "created_on": 1720638765.204797,
        "description": "We are looking for a Data Engineer to join our Technology Team at Bridgewater Bank in St Louis Park, MN. In this role, this individual will work with our Data Architect to establish and define ETL processes, data integrations, and automation workflows to support a centralized data warehouse. This is an excellent opportunity for someone with experience in data science and analytics to build and expand their application of engineering tooling for driving our data strategy. ***Candidate must be local to Minnesota*** Responsibilities Data modeling and building meaningful optimized, reusable data sets leveraging SQL Building, maintaining, and optimizing ETL pipelines that power insightful data visualizations Ensure data quality and integrity across multiple data sources and systems Work with cross-functional teams to identify and solve business problems through data-driven insights Continuously improve data quality and accuracy through data cleansing and validation Clearly, sharply articulate and define necessary data architecture to meet the requirements of the organization Develop and document best practices for governance of data sets Qualifications Proven experience with SQL for database querying Familiarity with data modeling concepts Excellent problem-solving skills and attention to detail Effective communication, requirements gathering and collaboration with stakeholders Ability to work independently and as part of a team Bachelor’s degree in Computer Science, Information Systems, related field or 1-2 years of equivalent experience Preferred Skills Proficiency in Snowflake or Microsoft SQL Server Proficiency in Power BI administration Experience in the banking or financial industry Experience in data tooling implementations About Bridgewater Bank It all started with a vision in 2005. This vision was to create a full-service, entrepreneurial bank where clients would notice a difference, team members would be challenged to grow, and the culture would be optimistic. Over a decade later, this unconventional attitude laid the foundation of Bridgewater Bank, a nationwide top-performing bank with an award-winning culture. We’re on a mission to become the finest entrepreneurial bank in the Twin Cities. And it’s working. Join our team and you will be surrounded by remarkable people who want to challenge the status quo and redefine what it means to work in this industry. This journey began in 2005, and it’s just getting started. Will you join us? Please Note The above is intended to describe the general content of and requirements for this position. It is not to be construed as an exhaustive list of duties, responsibilities, or requirements. It is Bridgewater Bank’s policy to promote equal employment opportunities. All personnel decisions, including, but not limited to, recruiting, hiring, training, promotion, compensation, benefits and termination, are made without regard to race, creed, color, religion, national origin, sex, age, marital status, sexual orientation, gender identity, citizenship status, veteran status, disability or any other characteristic protected by applicable federal, state or local law. STATUS: Exempt",
        "url": "https://www.linkedin.com/jobs/view/3963335869"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Minneapolis, MN",
        "job_id": 3919921610,
        "company": "EA Team Inc",
        "title": "Data Engineer",
        "created_on": 1720638766.9507537,
        "description": "Mandatory Skills - Azure Cloud, Databricks(pyspark), ADF, ADLS, GIT, DWH Job Details Design, implement, and manage Azure cloud solutions for various projects. \" Develop and maintain data pipelines using Databricks (PySpark) for data ingestion, processing, and analysis. \" Configure and manage Azure Data Factory (ADF) to orchestrate data workflows and ETL processes. \" Implement and optimize Azure Data Lake Storage (ADLS) for efficient data storage and retrieval. \" Collaborate with cross-functional teams to design and implement data warehouse solutions. \" Utilize Git for version control and collaboration on codebase. \" Monitor, troubleshoot, and optimize data processes for performance and reliability. \" Implement security best practices and manage access controls using Azure Active Directory (AAD). \" Document technical designs, processes, and procedures. \" Stay updated with the latest Azure cloud technologies and best practices. Requirements \" Bachelor's degree in Computer Science, Engineering, or a related field. \" Proven experience working with Azure cloud services, including Azure Data Lake Storage, Azure Data Factory, and Azure Active Directory. \" Strong proficiency in PySpark and experience with Databricks for data engineering and analytics. \" Hands-on experience with Git for version control and collaboration. \" Familiarity with data warehousing concepts and technologies. \" Experience with SQL and relational databases. \" Strong analytical and problem-solving skills. \" Excellent communication and collaboration skills. \" Ability to work effectively in a fast-paced, dynamic environment. \" Azure certifications (e.g., Azure Administrator, Azure Data Engineer) are a plus",
        "url": "https://www.linkedin.com/jobs/view/3919921610"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Plano, TX",
        "job_id": 3953532654,
        "company": "Lorven Technologies Inc.",
        "title": "Data Engineer -- Full-Time (Permanent) -- Plano, TX (Hybrid)",
        "created_on": 1720638768.854369,
        "description": "Job Title: Data Engineer Location: Plano, TX (Hybrid) Duration: Full-Time (Permanent) Job Description Minimum Requirements: Bachelor's or Master's degree in Computer Science, Electrical Engineering, Computer Engineering 3+ years of data engineering experience or experience working as a data architect with cloud technologies such as Azure, AWS, Google, etc. 3+ years of experience with Python 3+ years of experience with SQL and querying large data sets 3+ years of experience designing and managing relational databases (RDMS) 3+ years of experience with data transformation (ETL) 3+ years of experience with Databricks / Spark Preferred Qualifications Hands-on experience with AI engineering and AI/ML projects Hands-on experience analyzing and exploring large data sets utilizing Python or Databricks/Spark Hands-on experience with notebook tools such as Jupyter Notebooks or Databricks Experience operating in an Agile/SCRUM environment",
        "url": "https://www.linkedin.com/jobs/view/3953532654"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3945523128,
        "company": "Vernovis",
        "title": "Data Engineer",
        "created_on": 1720638770.7886102,
        "description": "Job Title: Data Engineer Location/ Work Structure : Remote Who we are: Vernovis is a Total Talent Solutions company that specializes in Technology, Cybersecurity, Finance & Accounting functions. At Vernovis, we help these professionals achieve their career goals, matching them with innovative projects and dynamic direct-hire opportunities in Ohio and across the Midwest. Client Overview: Vernovis is currently partnering with a local construction company to bring in a Data Engineer to add to the team. What You'll Do: Design, develop, and maintain scalable and reliable data pipelines that integrate data from multiple sources (CRM, ERP, etc.) into a cohesive data ecosystem. Collaborate with stakeholders to understand data requirements and deliver comprehensive data models that support business needs. Contribute to developing and documenting internal and external standards for pipeline configurations, naming conventions, partitioning strategies, and more. Analyze and improve existing data architectures to enhance performance and scalability. Develop and enforce data governance policies and procedures to ensure data integrity and security. Stay abreast of industry trends and technologies to drive innovation within the data management space. Ensure high operational efficiency and quality of the data platform datasets to ensure project reliability and accuracy to all our clients. Implement and manage Master Data Management (MDM) strategies and solutions to ensure data accuracy, completeness, and consistency across the organization. What Experience You'll Have: Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL), and working familiarity with various databases (MS SQL, PostgreSQL, MySQL, Oracle, etc.). Experience building and optimizing big data pipelines, architectures, and data sets. Strong analytic skills related to working with unstructured datasets. Build processes supporting data transformation, data structures, metadata, dependency, and workload management. Ability to quickly understand complex systems and data flows. Excellent problem-solving, analytical, and communication skills. Experience with CRM and ERP systems integration is highly desirable. Skills in one or more languages such as SQL (Required), Python, C#, Java, Kotlin, Scala, R, and JavaScript 5+ years of experience in a data engineer role, with a graduate degree in computer science, statistics, informatics, information systems, or another quantitative field. Proven experience in data engineering, data integration, and data architecture. Strong proficiency in SQL and experience with relational and NoSQL databases. A successful history of manipulating, processing, and extracting value from large, disconnected datasets.",
        "url": "https://www.linkedin.com/jobs/view/3945523128"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Jersey City, NJ",
        "job_id": 3938227249,
        "company": "ICONMA",
        "title": "Database Developer/Data Engineer",
        "created_on": 1720638774.8164096,
        "description": "Description At least 7-10 years of experience in Data, SQL & Python, data frames & APIs Experience in modern CI/CD pipelines and tools including Bitbucket, Jenkins and Ansible. Substantial programming experience: Real world use of Python is necessary with an interest in programming languages to maintain, extend and support an in-house graph computation language which is embedded in Python. Strong data analysis skills Hands-on experience in integration development, testing, and deployment experience Exposure to disparate databases like Teradata, Hadoop, SQL server, data lakes, Data virtualization concepts, SQL query optimization, performance tuning, materialized views and advanced ANSI SQL Solid understanding of end-to-end data life cycle and good to have exposure to container platforms like OpenShift and Kubernetes. Knowledge of API development, management and micro-services Contribute to architecture & design discussions, understanding technology components, aware of latest industry standards, tools and best practices Excellent communication skills, proficiency in JIRA. Confluence tools and having prior experience working with developers and product owners Ability to work in a fast-paced environment with strict SLAs, multiple high-priority deliverables, initiatives and releases Desired Skills And Experience We Are Looking For Experience with REST interfaces/APIs. Analytical skills to perform technical and functional analysis with strong communication skills. As an equal opportunity employer, ICONMA provides an employment environment that supports and encourages the abilities of all persons without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state, or local laws.",
        "url": "https://www.linkedin.com/jobs/view/3938227249"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Jersey City, NJ",
        "job_id": 3958863703,
        "company": "Smart Gauge Staffing LLC",
        "title": "Data Engineer (with Strong Python Coding)",
        "created_on": 1720638778.8023186,
        "description": "Title: Data Engineer (with Strong Python Coding) Client Location: JC, NJ Work Authorization: US Citizen,Greencard Holder,EAD Job Description Must have: Python, Pyspark (3-4 years) AWS Data Transformation exp. Nice To Have Certifications - AWS Solutions architect Financial Domain Summary \" Executes creative software solutions, design, development, and technical troubleshooting with ability to think beyond routine or conventional approaches to build solutions or break down technical problems \" Develops secure and high-quality production code, and reviews and debugs code written by others \" Identifies opportunities to eliminate or automate remediation of recurring issues to improve overall operational stability of software applications and systems",
        "url": "https://www.linkedin.com/jobs/view/3958863703"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3950390499,
        "company": "Tegria",
        "title": "Data Engineer",
        "created_on": 1720638783.1153202,
        "description": "As a Data Engineer, your work at Tegria will center on strategic opportunities and implementation, process improvement, and growing Tegria as a company. This role will focus on developing, constructing, testing, and maintaining data structures, data pipelines, and architecture. You will recommend and implement ways to improve the readability, efficiency, and quality of data. A Data Engineer at Tegria will also be focused on delivering data sets for data modeling, data mining, and production reporting. The role you play An effective Data Engineer will help the organization on a whole achieve success through: Create and maintain optimal data pipeline architecture while working within time and budget constraints (i.e. SSIS, Azure Data Factory, Apache Spark, Databricks, etc.) Work with stakeholders to assist with data-related issues and support their data infrastructure needs Consult, assess and provide recommendations for improvement in client’s current data architecture Assemble large, complex, and disparate datasets to meet functional and non-functional business needs Utilize multiple programming languages to develop the best solution for the client (i.e. SQL, R, Python, C++, etc.) Research and develop processes for utilizing cloud-based services (i.e. Snowflake, Google Cloud Services, Amazon Web Services, and Microsoft Azure) Create data tools for analytics and data science team members that will aid in the development and optimization of our current and future services into an industry leader in healthcare analytics Client Engagement Delivery Working independently or as part of a project team on a client engagement. Could be full-time on a single customer engagement or part-time across customers Serving as a liaison between diverse IT and operations groups Facilitating meetings and owning meeting scheduling and coordination, preparation, documentation, and follow-up Utilizing, reviewing, and creating project tools and templates for assigned projects Creating and maintaining project plans Evaluating and documenting current-state processes through discovery and analysis. Presenting recommendations for improvements based on industry experience and best-practices Facilitating future-state workflow, policy, and process design and planning Building, testing, training, converting and/or deploying new infrastructure, workflows, policies, and processes Participating in major milestone reviews and decision gates Presenting to a wide variety of audiences Documenting measurable outcomes resulting from initiatives through KPI analysis and impact tracking Effectively utilizing communication, decision-making, and escalation pathways Executing effective project wrap-up through outcomes documentation, lessons-learned, and leave-behind materials allowing customers to sustain ongoing operations Mentoring Associate(s) on project activities and deliverables and collaborating with others on the same Mentoring customer counterparts for successful, long-term ownership and growth Internal Team Development Contributing to personal and team development by participating in training activities and team events, while sharing your experience and expertise to help your team grow Participating in internal projects for Tegria’s strategic growth Planning and executing team and company-wide gatherings, such as retreats, inter-team meetings, etc. Modeling and holding fellow team members accountable to company values Referring new talent to Tegria’s Talent team to continue growing Tegria’s knowledge and capabilities Tegria Business Development Developing marketing materials and/or blog posts to promote Tegria services and outcomes Leveraging industry connections and knowledge to identify potential business development opportunities What we’re looking for We expect: 5+ years of professional experience working as a Data Architect, Data Engineer, ETL Developer, Database Administrator, or similar positions Experience working with SSIS and other data integration tools and a desire to stay current as the data engineering field advances A deep conceptual knowledge and demonstrated practical understanding of data modeling techniques and best practices Hands-on experience with cloud automation frameworks (Terraform, Ansible, Chef, Puppet) and automation tools (Azure Resource Manager templates) In-depth understanding of data warehouse design with advanced knowledge in querying languages such as SQL Experience working with unstructured and semi structured data (JSON, free-text entries, etc.) Experience with Caboodle Dev, Workday, Azure Experience migrating from prem to the cloud Demonstrated ability in project management (waterfall and/or agile), and other organizational management such as risk management, or change management Capable of and comfortable with working remotely Capable of and comfortable with traveling to client sites as needed W2 project-based consultants will be expected to provide a computing device that adheres to industry standards and security best practices to use in satisfying the Project Services on behalf of Tegria. We’d love to see: Prior consulting experience Experience with interface engines such as Epic Bridges, HL7 or FHIR Some experience implementing, supporting, optimizing, and upgrading Epic Certification in one or more Epic data model and/or application(s) Formal project management certification – either PMP or CSM Formal process improvement certification – ex: Lean Six Sigma or ITIL Need a few more details? Compensation: 97k-130k/yr. Status: Exempt | Full-time Eligibility: Must be legally authorized to work in the United States without sponsorship. Work Location: This position is remote. Must work in a location within the United States. Travel: Up to 25% Benefits Eligibility: Eligible Now, a little about us ... At Tegria, we bring bold ideas and breakthroughs to improve care, technology, revenue, and operations in ways that move healthcare organizations from patient-centered to human-centered. We are helping healthcare put people first—both patients and those who dedicate their lives to delivering care. And at the very core of this vital work is our incredibly talented people. People with different backgrounds who welcome challenge and change. People who listen first, ask hard questions, and make decisions to cultivate a culture of equity and inclusion. People who chase after goals, growth, and generosity. ________________ Perks and benefits Top talent deserves top rewards. We’ve carefully curated a best-in-class benefits package, meant to meet you wherever you are in your life and career. Your health, holistically. We offer a choice of multiple health and dental plans with nationally recognized networks, as well as vision benefits, a total wellness program, and an employee assistance program for you and your family. Your financial well-being. We offer competitive wages, retirement savings plans, company-paid disability and life insurance, pre-tax savings opportunities (HSA and/or FSA), and more. And everything in between. Our lifestyle benefits are unrivaled, including professional development offerings, opportunities for remote work, and our favorite: a generous paid-time-off program, giving you the flexibility to plan a vacation, take time away for illness (or life’s important events), and shift your schedule to accommodate those unexpected curve balls thrown your way. ________________ Tegria is an equal employment opportunity employer and provides equal employment opportunities (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. All qualified candidates are encouraged to apply.",
        "url": "https://www.linkedin.com/jobs/view/3950390499"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3938137281,
        "company": "Flexware Innovation",
        "title": "Data Engineer",
        "created_on": 1720638786.5593944,
        "description": "What Flexware Does Flexware Innovation is a leading technology integrator that helps forward thinkers in manufacturing and related industries build comprehensive and long-lasting solutions with ease. Founded in 1996 and based in Indiana, our teams of talented engineers leverage technology to solve real business problems with teams of engineers focused on industrial controls, manufacturing systems integration, software development, business intelligence, and Internet of Things (IoT) devices. Our passion is helping our customers build solutions that stand the test of time by creating solid architecture and helping customers avoid costly design mistakes. But we’re not just invested in technology – we’re also invested in people. Our internal promise to our Flexdogs is to have a positive and lasting impact on our families by providing a healthy and engaging work environment. Our environment is fun, family-friendly, energetic, and was nominated for TechPoint’s Mira Award for “Company Culture of the Year” in 2017, four Powderkeg awards in 2019, and 5 Powderkeg awards in 2022. What You Will Do Flexware Innovation is seeking a Data Engineer to join our team! You will be responsible for all aspects of enterprise data warehousing solutions for Flexware’s clients and helping them build long-lasting solutions. This role will assist in the design and architecture of new solutions for clients while still having a hands-on approach to development. You will be responsible for developing batch processing or real-time ETL/ETL solutions that ingest data from a variety of software systems, integrate the data together into a common data model, and load star schemas that you design. The solutions you develop will allow our customers to make enterprise-wide data-driven decisions. You will be challenged to come up with innovative solutions that will free our customers to focus on what they do best. This position will: Assist in the architecture and development of solutions using tools and technologies such as Azure Data Factory, Azure Synapse, Microsoft Fabric, DevOps, DataBricks, Azure Data Lake, T-SQL, Python, Azure Analysis Services, and Power BI. Ingest Data from a variety of sources (i.e. RestAPI, Oracle, SQL Server) into the enterprise data warehouse and/or lakehouse, pending customer needs Optimize data structures and queries for performance and efficiency to enable real-time or near-real-time analytics and reporting capabilities Develop and debug stored procedures and merges for data transformation and incremental loading Analyze and integrate data from external software systems such as CRM, Historian, ERP, and WMS systems into a common data model Work alongside solutions architects to design appropriate deliverables Work directly with the customer to analyze, derive, and document solution requirements Design Power BI reports and write DAX measures as defined by customer needs Flexware is looking for a candidate that is servant minded and has a willingness to see a problem and find a solution. Please note that we are not interviewing candidates that require sponsorship now or in the future** What You Might Have Done Before Flexdogs are a rare breed . They come from varied backgrounds, but typically have some of the following traits: Received a technical Bachelor’s degree such as Computer Science, Computer Engineering, Computer Technology, Computer Information Technology, or similar technical concentration Have several years of experience (4+) working in a professional environment with familiarity with modern database warehousing concepts, methodologies, and technologies Experienced SQL database developer Demonstrated ETL/ELT skills and methodologies (SSIS, ADF, Python, incremental loading, real-time processing) Experience with Azure cloud platforms and services Knowledge of cloud-based data storage and processing solutions Knowledge of SDLC and development best practices If you’re interested in this opportunity, we’re excited to start a conversation with you! Please reach out to our recruiting team at resumes@flexwareinnovation.com Your inquiry and conversation will be treated with confidentiality, and we will not share your information with others.",
        "url": "https://www.linkedin.com/jobs/view/3938137281"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Nevada, United States",
        "job_id": 3967189457,
        "company": "Clear Capital",
        "title": "Sr. Data Engineer",
        "created_on": 1720638788.3284605,
        "description": "Clear Capital is building the future of real estate data, and we need your help! We are seeking experienced product builders: with your talent as a Senior Data Engineer, help us reach our goals of knowing more about a property than anybody else and in the process making home ownership valuations more fair and equitable for millions of people. Become part of an innovative team supporting and developing the data and machine learning products that will shape Clear Capital’s future. The Senior Data Engineer role at Clear Capital will work closely with product teams to build next generation data products. Working alongside Software Engineers, Data Quality Analysts, ML Engineers, Data Scientists, and ML Ops Engineers who, like you, are dedicated to build exceptional software. We are looking for a Senior Data Engineer to assist with the development and implementation of systems leveraging structured and unstructured data to deliver data and data science solutions at scale. As Senior Data Engineer at Clear Capital, you are committed to enabling the best work of others on the team. You help yourself and your team to consistently “level up.” You think ahead to anticipate the needs of others and provide concise information for decision-making. About Us Clear Capital is a national real estate valuation technology company with a simple purpose: build confidence in real estate decisions to strengthen communities and improve lives. Our goal is to provide customers with a complete understanding of every U.S. property through our field valuation services and analytics tools, and improve their workflows with our platform technologies. Our commitment to excellence — wherever it leads, whatever it takes® — is embodied by team members. Clear Capital is an equal opportunity employer. To all recruitment agencies: Clear Capital does not accept agency resumes. Please do not forward resumes to our jobs alias, Clear Capital employees, or any other company location. Clear Capital is not responsible for any fees related to unsolicited resumes.",
        "url": "https://www.linkedin.com/jobs/view/3967189457"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "New York, United States",
        "job_id": 3959661225,
        "company": "RiVi Consulting Group L.L.C",
        "title": "Data Engineer",
        "created_on": 1720638790.044907,
        "description": "This role sits within the Data and Analytics Office which directs all aspects of data management within the Bank, and coordinates policies that will get promoted across the Federal Reserve System. Serves as a coordinating and centralizing body for data and analytics, organizing data access, establishing data foundations, while also driving innovation through advanced analytics. The ideal candidate will be a Senior with experience with - 7+ years Python - 7+ years Data Engineering - 5+ years AWS (ideally migration work) - Native technologies such as lambda, glue, s3, flyway - 5+ years Databricks - experience with Gitlab - ETL experience The environment is set up with Databricks and Starburst, and the ideal candidate will understand how AWS can relate Additional Skills & Qualifications Cloud Infrastructure Design: Collaborate with cross-functional teams to plan and design cloud infrastructure solutions using AWS services. Apply best practices for scalability, reliability, and security. Data Engineering Integration: Leverage your data engineering expertise to integrate data pipelines, ETL processes, and analytics workflows within the AWS ecosystem. Optimize data storage, retrieval, and processing using services like Amazon S3, Redshift, Glue, and Athena. Employee Value Proposition (EVP) Contribute to the critical infrastructure by leveraging cutting edge cloud technologies to enhance the efficiency of the global financial system. Work Environment Collaborative and fast paced atmosphere, where team members work diligently to uphold the highest standard of reliability and security while fostering innovation and continuous improvement to meet the needs of the FS industry. The team is set up in Scrum ceremonies and will report directly to the product manager Business Drivers/Customer Impact New users are trying to onboard into the data platform and the platform needs to modernize and shift into the cloud Why is the position open(provide details) new headcount as this AWS migration is heavily funded External Communities Job Description Our financial services client is seeing an AWS Engineer to join the team. The ideal candidate will have experience with Python, Data Engineering, AWS, and databricks",
        "url": "https://www.linkedin.com/jobs/view/3959661225"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Houston, TX",
        "job_id": 3931609498,
        "company": "Steneral Consulting",
        "title": "Data Engineer – Real-time Operations-7 openings",
        "created_on": 1720638791.6678498,
        "description": "MAx 1-2 pages resumes Job Title: Data Engineer – Real-time Operations Location: Houston, TX ON-SITE REQUIRED M-F This position and program will require the candidate to be on-site at our client's office in downtown Houston, TX. Responsibilities Data Management & Integration : Design and build pipelines to ingest and transport data Own data quality and reliability for app-related data pipelines. Collaborate with data and product teams to deliver robust, scalable data solutions. API Development & Management Design and develop secure, robust, user-friendly APIs for applications and users. Ensure API performance, fault tolerance, and comprehensive documentation. Data Storage & Accessibility Build and support databases specific to app usage. Work with enterprise database teams to make relevant app data available to stakeholders. Analytics & Monitoring Generate business rule-based alerts and notifications for stakeholders. Develop and support app health monitoring infrastructure to ensure uptime and reliability. Enhance and maintain the performance of real-time streaming data processes. Additional Responsibilities Understand the oil and gas business domain to translate needs into technical solutions. Participate in code review processes to ensure code quality and security. Provide data quality support and maintenance for assigned applications. Requirements Minimum 5 years of experience as a Data Engineer or in a similar role. Direct exposure to developing scientific or engineering use cases; or degree in a STEM field. Strong proficiency in Python Demonstrated experience in stream processing and real-time data calculations, ideally with IoT data. Proven ability in working with data caching, relational databases (RDBMS), and API creation/integration. Strong problem-solving skills and a proven track record of learning and adapting to new technologies. Excellent collaboration and communication skills, with a keen ability to work effectively within dynamic team environments. Client’s Approach To Technology We prioritize finding the right cultural fit and strong technical aptitude, with a willingness to learn and adapt. While we've listed technologies based on past implementations, we are flexible to similar experience and value candidates who can demonstrate their ability to excel in a dynamic environment. Understanding of Relational Database Management Systems (RDBMS) Oracle, SingleStore, MySQL, or Postgres Experience with NoSQL Databases Redis, MongoDB, and Elasticsearch Exposure to Event Log/Message Queuing Systems RabbitMQ, Pulsar, or Kafka API Development and Integration API creation concepts and experience with Python and FastAPI is a benefit Stream Processing Expertise Experience with large-scale IoT stream processing using Python for real-time data calculations and transformations (on data volumes exceeding MBs per second) is desirable. Infrastructure and Deployment Skills GitHub Actions, Kubernetes, Docker, Terraform, and Azure for deployment and infrastructure management on Linux (CentOS)",
        "url": "https://www.linkedin.com/jobs/view/3931609498"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Houston, TX",
        "job_id": 3943487427,
        "company": "Steneral Consulting",
        "title": "Data Engineer – Real-time Operations-7 Openings",
        "created_on": 1720638793.404634,
        "description": "Be a good culture fit. If any have upstream oil and gas experience , specifically if it involving some domain concepts . If the candidate is creative and has solid critical thinking skills. They need candidates who are equipped to handle the level of ambiguity that developers encounter here. Not looking for candidates that have been in mostly siloed roles, as a selected candidate MUST be able to handle the open-ended nature of their projects. They seem to reject candidates who have been strictly in very data pipeline focused jobs. As during the Technical interview EOG will expect candidates to have lots of API and server-side knowledge. Candidates MUST be able to see the bigger picture behind what they work on. They need to be comfortable explaining the end purpose of what they have built or the real needs of the end user/ customer. MAx 1-2 pages resumes Job Title: Data Engineer – Real-time Operations Location: Houston, TX ON-SITE REQUIRED M-F This position and program will require the candidate to be on-site at our client's office in downtown Houston, TX. Responsibilities Data Management & Integration : Design and build pipelines to ingest and transport data Own data quality and reliability for app-related data pipelines. Collaborate with data and product teams to deliver robust, scalable data solutions. API Development & Management Design and develop secure, robust, user-friendly APIs for applications and users. Ensure API performance, fault tolerance, and comprehensive documentation. Data Storage & Accessibility Build and support databases specific to app usage. Work with enterprise database teams to make relevant app data available to stakeholders. Analytics & Monitoring Generate business rule-based alerts and notifications for stakeholders. Develop and support app health monitoring infrastructure to ensure uptime and reliability. Enhance and maintain the performance of real-time streaming data processes. Additional Responsibilities Understand the oil and gas business domain to translate needs into technical solutions. Participate in code review processes to ensure code quality and security. Provide data quality support and maintenance for assigned applications. Requirements Minimum 5 years of experience as a Data Engineer or in a similar role. Direct exposure to developing scientific or engineering use cases; or degree in a STEM field. Strong proficiency in Python Demonstrated experience in stream processing and real-time data calculations, ideally with IoT data. Proven ability in working with data caching, relational databases (RDBMS), and API creation/integration. Strong problem-solving skills and a proven track record of learning and adapting to new technologies. Excellent collaboration and communication skills, with a keen ability to work effectively within dynamic team environments. Client’s Approach To Technology We prioritize finding the right cultural fit and strong technical aptitude, with a willingness to learn and adapt. While we've listed technologies based on past implementations, we are flexible to similar experience and value candidates who can demonstrate their ability to excel in a dynamic environment. Understanding of Relational Database Management Systems (RDBMS) Oracle, SingleStore, MySQL, or Postgres Experience with NoSQL Databases Redis, MongoDB, and Elasticsearch Exposure to Event Log/Message Queuing Systems RabbitMQ, Pulsar, or Kafka API Development and Integration API creation concepts and experience with Python and FastAPI is a benefit Stream Processing Expertise Experience with large-scale IoT stream processing using Python for real-time data calculations and transformations (on data volumes exceeding MBs per second) is desirable. Infrastructure and Deployment Skills GitHub Actions, Kubernetes, Docker, Terraform, and Azure for deployment and infrastructure management on Linux (CentOS)",
        "url": "https://www.linkedin.com/jobs/view/3943487427"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Columbus, OH",
        "job_id": 3933806017,
        "company": "Turnberry Solutions",
        "title": "Python Data Engineer",
        "created_on": 1720638795.2303004,
        "description": "Duration: 6+ months Location: OH-Columbus, 100% Remote Overview: Every Turnberry consultant belongs to a practice, an internal group of consultants and leaders with shared experience and expertise. Each of these practices aligns to one of the core services Turnberry offers to clients. As a Python Data Engineer, you will join Turnberry's Data Strategy and Intelligence practice and service. This service provides insights into company data, advanced analytics solutions, robust data governance, and tailored solutions for specific data challenges. Responsibilities Assist the IT Data Engineering Team in developing the next generation data and analytics infrastructure Write high quality SQL code to retrieve and analyze data from database tables (primarily Databricks) Develop high quality SQL models for ad-hoc requests, as well as ongoing reporting/dashboarding Work directly with business stakeholders to translate between data and business needs Continually improve SQL models through automating or simplifying self-service support for datasets Qualifications Bachelor's degree in Computer Science, Information Systems, Engineering, Data Science, or other similarly technical related field Mathematics, or specialized training/certification or equivalent work experience 2-3-years minimum professional experience in cloud data engineering and science and associated technologies utilizing cloud data platforms such as Databricks, AWS RedShift, and Snowflake 2-3-years minimum experience with advanced SQL data modeling and query optimization and Python Proficiency in using visualization tools such as Tableau, Domo, or Power BI Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement Strong verbal, written & presentation skills with the ability to effectively communicate complex technical information to personnel at all levels of the organization Experienced in Python (requests library) Proficient in extracting data from APIs and landing it in a data lake) Preferred Qualifications Specific experience with Data Warehouse/Data Lake configuration and development using Databricks platform Experience with Tableau/Sigma Computing Experience operating in an Agile development environment Familiarity with usage of Agile tools (JIRA/Confluence) Understanding of CI/CD deployment models and release strategy as well as SCM tools (Git preferred) and code management best practices Experience in AWS environment Experience with cloud ELT platforms such as AWS Glue, Talend Stitch, or FiveTran The salary range for this role is $80,000 to $160,000 or the hourly equivalent. Pay is based on several factors including but not limited to education, work experience, certifications, etc. In addition to your salary, Turnberry Solutions offers benefits such as a comprehensive healthcare package (medical, dental, vision), disability and group term life insurance, health and flexible spending accounts, a utilization bonus, 401(k) with match, flexible time off for salaried employees, parental leave for salaried employees, and flexible work arrangements (all benefits are subject to eligibility requirements). No matter where or when you begin a career with Turnberry, you'll find a far-reaching choice of benefits and incentives. At Turnberry, inclusion is one of our core values. We are fully invested in and focused on hiring and growing a diverse team of high performers. We're committed to creating a positive and connected work environment for all. We believe that uniqueness in ideas, experiences, and backgrounds make us a better Turnberry: Turnberry is an Equal Employment Opportunity/Affirmative Action employer, and recruits, employs, trains, compensates, and promotes regardless of age, ancestry, family medical or genetic information, gender identity and expression, marital, military, or veteran status; national and ethnic origin; physical or mental disability; political affiliation; pregnancy; race; religion; sex; sexual orientation; and any other protected characteristics.",
        "url": "https://www.linkedin.com/jobs/view/3933806017"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Houston, TX",
        "job_id": 3899572560,
        "company": "MHK TECH INC",
        "title": "Junior Data Engineer",
        "created_on": 1720638796.9973109,
        "description": "Responsibilities • Develop, maintain, and optimize data pipelines to extract, transform, and load large datasets from diverse sources into our data ecosystem • Design and implement efficient and scalable data models that align with business requirements, ensuring data integrity and performance • Collaborate with cross-functional teams to understand data needs and deliver solutions that meet those requirements • Work closely with data scientists, analysts, and software engineers to ensure seamless integration of data solutions into larger systems • Identify and resolve data quality issues, ensuring accuracy, reliability, and consistency of the data infrastructure • Continuously monitor and improve data pipelines and processes, identifying opportunities for automation and optimization Qualifications • Bachelor's or Master's degree in Computer Science, Engineering, or a related field • 5+years of hands-on experience as a Data Engineer, working on complex data projects and implementing data modeling solutions • Solid understanding of SQL and expertise in working with relational databases (e.g., PostgreSQL, MySQL) • In-depth knowledge of data modeling techniques and experience with data modeling tools • Working knowledge on Data warehousing • Familiarity with cloud-based data platforms and services (e.g., Snowflake, AWS, Google Cloud, Azure) • Experience with version control systems (e.g., Git) and agile software development methodologies • Strong communication skills to effectively convey technical concepts to both technical and non-technical stakeholders • Primary Skillset: Data Engineering",
        "url": "https://www.linkedin.com/jobs/view/3899572560"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Portland, Oregon Metropolitan Area",
        "job_id": 3970007646,
        "company": "VBeyond Corporation",
        "title": "Data Engineer",
        "created_on": 1720638798.6241462,
        "description": "Data Engineer with Data Modeling Location: Portland, OR / Vancouver, WA ( Day One Onsite) Mode : Full Time Experience: 10+ yrs. Skills: Healthcare Payer, Data Modeling, Data Engineering, Data Profiling, DBT, SQL, Python, Snowflake Required: • DBT: Prefer candidates with real-world DBT experience. • Healthcare Industry Experience: Some experience is required, preferably Payer experience. • Data modeling. Must be highly skilled in data modeling, including 3rd normal form entity relationship modeling and dimensional data modeling. • SQL: Highly Skilled. Must be familiar with common table elements (CTEs), complex joins, and analytic & windowing functions. • Python: Some experience is required. We prefer Intermediate or higher skill. • Snowflake: Prefer candidates with real-world experience using Snowflake. • Data Profiling: Must be able to identify unique and primary keys and determine cardinality, data type, precision, and scale, and skew. • Effective communication skills to communicate internally within our team and externally across other teams, notably Product and QA • Understands the scope and context of the work and takes initiative to achieve the goals, rather than waiting to take specific direction. Responsibilities: • Data modeling, including 3rd normal form entity relationship models, dimensional data models, and denormalized models. • Data profiling to identify primary keys and issues with the data. • ETL to bring data onto the Cambia Data Platform, de-duplicate data, create or update dimensional data structures, and produce use case-specific output. • Unit testing, functional testing, and performance testing and tuning. • Interacting with the Product team to understand and refine requirements. • Interacting with QA to address reported findings. • Working individually and as a team to achieve our goals. • Taking initiative to take on additional work if the present work stream slows down • Other similar or related activities. Thanks & Regards, Harinath M Vbeyond Corporation Direct : 908-633-2603 Email ID: HarinathM@vbeyond.com",
        "url": "https://www.linkedin.com/jobs/view/3970007646"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Raleigh, NC",
        "job_id": 3925326213,
        "company": "Home Solutions",
        "title": "Data Engineer",
        "created_on": 1720638802.5752625,
        "description": "About the Role Home Solutions is seeking a hands-on, self-motivated Data Engineer to own our data analytics pipeline for one of our fastest growing businesses. In this role, you will be responsible for collecting and modeling data through the deployment of modern cloud tools, and turning that data into insights through the building of dashboards and reports. This role is for the data engineer that seeks to get closer to the business, or the business analyst that seeks to dive deeper into understanding where their data comes from. Your Day to Day Create and maintain scalable data pipeline through combination of modern SaaS applications and custom-built solutions Create and maintain business intelligence roadmap to help enable business goals Partner with business users to gather and understand data requirements Extract data from sources through custom-built data integrations (typically Python) with discipline towards automation Write SQL-based transformations to turn raw data into production-ready business models Develop and manage business and executive team dashboards Create and maintain data storage systems Develop data discoverability tools and data monitoring systems About You Proficiency in SQL and Python (and/or proven ability to pick up a new language) Proven ability to operate autonomously to achieve results General understanding of the Marketing Technology stack, components, and best-practices. An ideal candidate will have some familiarity with common marketing tools (Google Analytics, Tag Manager, Segment (CDP), Facebook Ads, etc.) Experience with common enterprise analytics tools with proven ability to translate data insights into action (Tableau, PowerBI, Looker Studio, Superset, etc.) Working knowledge of software development best practices as they apply to data engineering, including: Version Control, Unit Testing, and Continuous Integration/Continuous Delivery (CI/CD) About Us Launched Fall 2017, Home Solutions targets the rapidly digitizing home services vertical and matches homeowners with the right service provider to meet their needs. Through our websites, Home Solutions has a proprietary audience of 47M+ homeowners and prime real estate in search engines. We are on a mission to make homeownership easier by creating high quality content that pairs consumers with providers in a range of related categories. Home Solutions was incubated within Three Ships, a growth equity firm that launches and invests in digital companies, builds great leadership teams, and helps them rapidly scale. The Three Ships portfolio currently includes several businesses - Home Solutions, Pillar 4, Stacksphere , and 3S UK - and over 50+ websites that help consumers navigate the overwhelming choices through online marketplaces and the most informative content online. Headquartered in Raleigh with offices in Charlotte and London, we are always looking to find the right people to help us continue to grow this business and place a high value on teammates with a growth mindset and a \"get after it\" mentality. Why You Should Join Us Results : At Three Ships we have eleven consecutive years of profitability and a track record of successful growth in the digital marketing space. Stability : We are privately owned, have a holding period of \"forever,\" have no debt, and have significant cash to invest – we're \"rock-solid\" financially. Exposure : You will have a front-row seat in growing a business. Your teammates have built and sold companies, managed 100s of employees, and run campaigns with Fortune 500 brands. Market Landscape: The digital home services marketing landscape is transforming. There's no better time than now to be building a business in this space. Career Growth: There is no cap on growth, promotions or the opportunity to own and put your stamp on important projects. Prove your value and you will be rewarded accordingly. Learning : We are a small and mighty team but also have the advantage of tapping into the Three Ships ecosystem and working with subject matter experts in paid media, creative, branding, content strategy, SEO, and more. The opportunity to grow new skills and learn from smart people is endless. Fun : The team has a high bar for excellence, but also a real interest in each other and making work fun. The Package As a full-time employee of Home Solutions, you'll have access to competitive benefits, including flexible time off, health/dental/vision, 401k match, an annual Relax & Recharge Bonus, an annual Learning & Development stipend to enroll in class(es) of your choosing, and up to $75 mobile reimbursement. If you join us in person in our Raleigh or Charlotte locations, we have an office stocked with snacks, coffee, and just about every other beverage you can imagine. EEOC Statement All applicants are considered without regard to race, color, religion, sex, national origin, age, disability, veteran status, gender identity, or any other discriminatory factors. Please note that we do not provide immigration sponsorship for this role. All offers are subject to a background check.",
        "url": "https://www.linkedin.com/jobs/view/3925326213"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Charlotte, NC",
        "job_id": 3942714826,
        "company": "Moody's Corporation",
        "title": "Associate Data Engineer",
        "created_on": 1720638804.2453039,
        "description": "Experience Level: Experienced Hire Categories: Engineering & Technology Location(s): 1414 S Tryon Street, 7th Floor, The Railyard Floors 5-8, Charlotte, North Carolina, 28203, US At Moody's, we unite the brightest minds to turn today’s risks into tomorrow’s opportunities. We do this by striving to create an inclusive environment where everyone feels welcome to be who they are-with the freedom to exchange ideas, think innovatively, and listen to each other and customers in meaningful ways. If you are excited about this opportunity but do not meet every single requirement, please apply! You still may be a great fit for this role or other open roles. We are seeking candidates who model our values: invest in every relationship, lead with curiosity, champion diverse perspectives, turn inputs into actions, and uphold trust through integrity. About the Department: Technology Services Group - Digital Workplace Services - Endpoint Engineering About the Role: We are seeking a motivated and detail-oriented Associate Data Engineer to join our Digital Workplace Services Endpoint Engineering team. The ideal candidate will have foundational knowledge in SQL, Python, and Power BI, with a strong interest in data modeling, processing, and analysis. You will support the team in architecting and deploying data models and processes, ensuring data quality and accuracy, and interfacing with cross-functional teams. Key Responsibilities: Assist in architecting, implementing, and deploying data models and processes in production environments. Perform data analysis to generate business insights. Interface with Engineers, Product Managers, and Product Analysts to gather requirements and provide support. Help manage Data Warehouse plans and process large datasets with strong data modeling skills. Translate data requirements into data pipeline development with guidance from senior team members. Gain proficiency in SQL, Data Model Design, ETL/ELT, and data analysis. Prepare data and create data models for use in Power BI under supervision. Work with varied data infrastructures including relational databases, data lakes, Spark, and column stores. Understand Data Engineering tools, frameworks, and standards. Adopt and apply Infrastructure as Code (IaC) practices. Design, develop and maintain pipelines using Azure tools such as Synapse, Azure Machine Learning, ADF, and Microsoft Fabric. Ensure data accuracy and quality while querying from relational databases, data lakes, and data warehouses with oversight. Apply basic knowledge of Python libraries (Numpy, Pandas) for data processing and visualization. Utilize Agile methodologies (Jira/Confluence) under supervision. Use Git/Azure DevOps for CI/CD activities- Assist in creating and maintaining technical and process documentation. Experience working with Generative AI tools (OpenAI, Llama2). Develop strong troubleshooting, analytical, and problem-solving skills. Behavioral / Professional Skills: Be curious and eager to learn modern technologies. Demonstrate tech-savviness with an ability to quickly implement and utilize new tools. Show fearlessness by not shying away from big ideas. Exhibit digital dexterity to utilize digital workplace technologies effectively. Take a pragmatic and detail-oriented approach to work. Influence others to gain agreement and support for ideas and initiatives. Have a business-results orientation, understanding business needs, and anticipating, identifying, and meeting end-user needs. Qualifications: Bachelor's or master's degree in related field. Advanced proficiency in SQL. 1+ year experience working with Python and Power BI. Experience with the Azure cloud platform; Azure certification is a plus. Intermediate knowledge of scripting with PowerShell or Bash. Knowledge of programming languages are a plus. Basic understanding of Azure Synapse Analytics and Azure Data Factory (ADF) / Microsoft Fabric Experience working in an agile environment. For US-based roles only: the anticipated hiring base salary range for this position is $78,000 - $113,100, depending on factors such as experience, education, level, skills, and location. This range is based on a full-time position. In addition to base salary, this role is eligible for incentive compensation. Moody’s also offers a competitive benefits package, including not but limited to medical, dental, vision, parental leave, paid time off, a 401(k) plan with employee and company contribution opportunities, life, disability, and accident insurance, a discounted employee stock purchase plan, and tuition reimbursement. Moody’s is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, sex, gender, age, religion, national origin, citizen status, marital status, physical or mental disability, military or veteran status, sexual orientation, gender identity, gender expression, genetic information, or any other characteristic protected by law. Moody’s also provides reasonable accommodation to qualified individuals with disabilities or based on a sincerely held religious belief in accordance with applicable laws. If you need to inquire about a reasonable accommodation, or need assistance with completing the application process, please email accommodations@moodys.com. This contact information is for accommodation requests only, and cannot be used to inquire about the status of applications. For San Francisco positions, qualified applicants with criminal histories will be considered for employment consistent with the requirements of the San Francisco Fair Chance Ordinance. This position may be considered a promotional opportunity, pursuant to the Colorado Equal Pay for Equal Work Act. Click here to view our full EEO policy statement. Click here for more information on your EEO rights under the law. Click here to view our Pay Transparency Nondiscrimination statement. Click here to view our Notice to New York City Applicants. Candidates for Moody's Corporation may be asked to disclose securities holdings pursuant to Moody’s Policy for Securities Trading and the requirements of the position. Employment is contingent upon compliance with the Policy, including remediation of positions in those holdings as necessary. For more information on the Securities Trading Program, please refer to the STP Quick Reference guide on ComplianceNet Please note: STP categories are assigned by the hiring teams and are subject to change over the course of an employee’s tenure with Moody’s.",
        "url": "https://www.linkedin.com/jobs/view/3942714826"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Paoli, PA",
        "job_id": 3955698188,
        "company": "Zeus Fire and Security",
        "title": "Data Engineer",
        "created_on": 1720638805.793474,
        "description": "Role Overview The ideal candidate will have expert-level skills in T-SQL and a robust understanding of data warehousing technologies to help us build and maintain a modern enterprise data warehouse on Azure. This role will require deep technical knowledge in Azure Data Factory, SQL Server, CDC (Change Data Capture), and Azure Synapse, as well as proficiency in developing PowerBI reports. The ideal candidate will have experience with data migrations and a background in financial data, ensuring data governance throughout the lifecycle of the project. The Data Engineer will collaborate closely with the product owner to align the data architecture with business needs, driving data initiatives that support strategic business goals. Status Full Time, Exempt Key Responsibilities Design, construct, and manage the enterprise data warehouse and data architecture on the Azure platform. Develop and optimize data pipelines using Azure Data Factory, implementing CDC mechanisms for real-time data processing. Manage and enhance database solutions with SQL Server, leveraging T-SQL for complex data manipulation and extraction. Utilize Azure Synapse for large scale data analytics solutions, optimizing data storage and performance. Create insightful, actionable PowerBI reports and dashboards to meet business analysis needs. Engage in data migration projects, ensuring accuracy and integrity of data transfers. Work directly with the product owner to understand and fulfill data requirements, ensuring that the data architecture supports and enhances business processes. Implement and maintain strict data governance practices to ensure data security, quality, and compliance with industry standards, particularly in financial environments. Document all systems, processes, and metadata to maintain transparency and sustainability of the data architecture. Qualifications Bachelor’s or Master’s degree in Computer Science, Information Systems, Data Science, or a related field. 3-5 Years experience with T-SQL and a proven track record in developing and managing large scale data warehouses. Strong experience with Microsoft Azure services, including Azure Data Factory, SQL Server, CDC, and Synapse. Proficiency in creating and managing PowerBI reports. Demonstrable experience with data migrations and handling sensitive financial data. Deep understanding of data governance principles and practices. Preferred Qualifications Experience with dbt (data build tool) is highly desirable. Prior experience in a financial services environment, understanding complex data needs and regulatory requirements. Certifications in Azure data technologies, data engineering, or related fields.",
        "url": "https://www.linkedin.com/jobs/view/3955698188"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Seattle, WA",
        "job_id": 3934414641,
        "company": "Amazon",
        "title": "Data Engineer, PXT PeopleInsight",
        "created_on": 1720638807.6372619,
        "description": "Description Are you driven by the excitement of unraveling intricate data puzzles? Does your DNA resonate with collaborative problem-solving? Do you thrive on witnessing the ripple effects of your contributions in the larger scheme of things? If you've nodded along to any of these questions, welcome to our PeopleInsights team! We're not just a team; we're a dynamic cohort of engineers dedicated to leveraging cutting-edge innovations for Amazon's PXT Business Analytics tools and software. Together, we confront tangible challenges head-on, poised to revolutionize customer experiences. We are looking for experienced, self-driven Data Engineer. In this role, you will be building complex data engineering and business intelligence applications using AWS stack. You should have deep expertise and passion in working with large data sets, data visualization, building complex data processes, performance tuning, bringing data from disparate data stores and programmatically identifying patterns. You should have excellent business acumen and communication skills to be able to work with business owners to develop and define key business questions and requirements. You will provide guidance and support for other engineers with industry best practices and direction. PeopleInsight (PI) has culture of data-driven decision-making, and demands timely, accurate, and actionable business insights. Key job responsibilities In This Role You Will Design, implement, and support business critical data warehouse / data lake infrastructure using AWS big data stack, Python, Redshift, Glue/lake formation, EMR/Spark/Scala, Athena etc in a stable, low cost model. Collaborate with Business Intelligence Engineers to recognize and help adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation Internalize our customer challenges and derive creative solutions which apply new and innovative technology. Work directly with customers to integrate new data types, equipment, and incorporate feedback. Empower technical and non-technical, internal customers to drive their own analytics and reporting (self-serve reporting) and support ad-hoc reporting when needed. Help continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers. Be comfortable with a degree of ambiguity and willing to develop quick proof of concepts, iterate and improve Apply software best practices including coding standards, code reviews, source control management, agile development, build processes, and testing. Actively support and foster a culture of inclusion. A day in the life Successful candidate would have experience working with big data, building data warehouses and data processing services. They are effective at seeing data patterns and building generic data solutions to improve user experience. About The Team Our mission is to harness the power of people data to empower PXT and people leaders to make decisions that help Amazon become the Earth’s Best Employer. We are open to hiring candidates to work out of one of the following locations: Seattle, WA, USA Basic Qualifications 1+ years of data engineering experience 1+ years of analyzing and interpreting data with Redshift, Oracle, NoSQL etc. experience Experience with data modeling, warehousing and building ETL pipelines Knowledge of writing and optimizing SQL queries in a business environment with large-scale, complex datasets Experience with one or more scripting language (e.g., Python, KornShell, Scala) Preferred Qualifications Experience with big data processing technology (e.g., Hadoop or ApacheSpark), data warehouse technical architecture, infrastructure components, ETL, and reporting/analytic tools and environments Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us. Our compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $91,200/year in our lowest geographic market up to $185,000/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. For more information, please visit https://www.aboutamazon.com/workplace/employee-benefits. This position will remain posted until filled. Applicants should apply via our internal or external career site. Company - Amazon.com Services LLC Job ID: A2656852",
        "url": "https://www.linkedin.com/jobs/view/3934414641"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3929087956,
        "company": "DivIHN Integration Inc",
        "title": "Data Engineer 3 (REMOTE)",
        "created_on": 1720638809.2931292,
        "description": "DivIHN (pronounced “divine”) is a CMMI ML3-certified Technology and Talent solutions firm. Driven by a unique Purpose, Culture, and Value Delivery Model, we enable meaningful connections between talented professionals and forward-thinking organizations. Since our formation in 2002, organizations across commercial and public sectors have been trusting us to help build their teams with exceptional temporary and permanent talent. Visit us at https://divihn.com/find-a-job/ to learn more and view our open positions. Please apply or call one of us to learn more For further inquiries regarding the following opportunity, please contact one of our Talent Specialists Sivanesan at 224 369 0756 Title: Data Engineer 3 (REMOTE) Location: Remote Duration: 8 Months Description Maintain, build, and optimize data warehouse, developing ETL solutions, automated data flow, and analytics “as-a-service” environment. Design and develop scalable, high-performance data processing solutions on AWS using tools like AWS Lambda, Step Functions, Amazon Redshift, and AWS Glue. Collaborate with stakeholders across the organization to understand business requirements and design data solutions that meet their needs Monitor actively the health of the environment in terms of data integrity, data pipeline performance, and overall security, to ensure resolutions are developed, implemented, and communicated efficiently Implement and maintain data governance policies and procedures to ensure data quality, security, and compliance Develop and maintain data architecture and infrastructure to support the growth and expansion of the data platform Design and develop tools to constantly monitor the health of the environment and ensure incidents communicated efficiently and resolved quickly. Mentor and guide junior data engineers in the team Drive and implement automation to empower the organization to scale efficiently and with flexibility Keep up to date with new technologies and methodologies related to data engineering and apply them to improve the data platform Summary: The main function of a data engineer is to coordinate changes to computer databases, test, and implement the database applying knowledge of database management systems. A typical data engineer is responsible for planning, coordinating and implementing security measures to safeguard the computer database. Job Responsibilities: Test programs or databases, correct errors and make necessary modifications. Modify existing databases and database management systems or direct programmers and analysts to make changes. Write and code logical and physical database descriptions and specify identifiers of database to management system or direct others in coding descriptions. Skills: Verbal and written communication skills, problem solving skills, customer service and interpersonal skills. Basic ability to work independently and manage one�s time. Basic knowledge of database management software. Education/Experience: Associate's degree in computer programming or equivalent training required. 5-7 years experience required. About Us DivIHN , the 'IT Asset Performance Services' organization, provides Professional Consulting, Custom Projects, and Professional Resource Augmentation services to clients in the Mid-West and beyond. The strategic characteristics of the organization are Standardization, Specialization, and Collaboration. DivIHN is an equal opportunity employer. DivIHN does not and shall not discriminate against any employee or qualified applicant on the basis of race, color, religion (creed), gender, gender expression, age, national origin (ancestry), disability, marital status, sexual orientation, or military status.",
        "url": "https://www.linkedin.com/jobs/view/3929087956"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Phoenix, AZ",
        "job_id": 3949807405,
        "company": "Virtuous",
        "title": "Data Engineer",
        "created_on": 1720638810.9641576,
        "description": "About Us Virtuous is on a mission to inspire global generosity by helping nonprofits build better relationships with their donors. We offer a modern software platform that provides mid-sized charities with elegant tools for fundraising, marketing, volunteerism, and online giving. Our talented team is driven to disrupt the status quo in the nonprofit sector. We are hungry, humble, and committed to delivering best-in-class software solutions, customer success interactions, and sales experiences to the world’s leading nonprofits We also recognize the importance of giving back and making a difference in the communities where we live and work. That's why we practice radical generosity by volunteering at nonprofits or going the extra mile for our team and the customers we serve. We take our work seriously, but we don’t take ourselves too seriously. We believe that life is too short not to love what you do. The ideal candidate for Virtuous embodies our values by: Asking questions with a spirit of curiosity Giving feedback freely with candor & grace, welcoming it in return Displaying a passion for philanthropy and technology Serving with joy. Everyone is willing to make the coffee! Celebrating the wins & milestones of others Assuming good intent & demonstrating trust in others Pursuing relationships with people different from themselves & creates space to be human Find our core values & more here . Position Summary We are seeking a highly skilled and detail-oriented Data Engineer to join our product engineering team. In this role, you will be responsible for developing, managing, and optimizing data infrastructure that drive strategic decision-making. You will be responsible for designing and implementing scalable data pipelines that enable efficient data collection, transformation, and storage across cloud environments. Utilizing technologies such as SQL and Python to ensure seamless data flow and accessibility for analytics, reporting, and machine learning applications. Your expertise in ETL processes, data warehousing and tools like Snowflake and DBT and a familiarity with .NET will be critical for maintaining data integrity and availability. You will collaborate closely with a cross-functional team, including product management, engineering, and data operations, to ensure data accuracy and efficiency. This position offers a unique opportunity to contribute to meaningful projects and support our mission through data-driven insights. The ideal candidate is someone who thrives in a fast-paced environment, is passionate about data, and possesses a strong technical background. Responsibilities Write and optimize SQL queries to extract and manipulate data. Develop and maintain comprehensive reports and dashboards using Sigma Reporting. Create detailed user stories and collaborate with engineering teams to ensure accurate implementation. Manage and optimize data models in Snowflake to support cost management. Collaborate with cross-functional teams to identify data requirements and deliver insights. Ensure data quality and integrity through rigorous testing and validation processes. Conduct exploratory data analysis to uncover trends and insights. Provide training and support to team members on data tools and best practices. Participate in the development and refinement of the overall data strategy. Must Haves Ability to create ETL tools and optimize big data sets Expert-level experience with SQL server and data modeling in Snowflake. Proficiency in using DBT, fiveTran or data bricks for data transformation and modeling Hands-on experience with Sigma Reporting or similar BI tools. Strong analytical and problem-solving skills. Excellent communication and collaboration abilities. Detail-oriented and able to manage multiple tasks and projects simultaneously. Experience working with nonprofits or a strong desire to support nonprofit initiatives. A curious and analytically-minded approach to data analysis. Knowledge of python and/or the ability to easily pull data from APIs Nice to Haves Ability to think strategically and develop tools to enable self-service. Experience with AI or predictive models Knowledge of advanced statistical methods and data science techniques. Experience with cloud platforms such as AWS, Azure, or GCP. Prior experience in a senior data role or leading data projects. Strong understanding of data governance and best practices. Ability to mentor and guide junior team members. Advanced degree in a related field (e.g., Data Science, Computer Science, Statistics). What We Offer Market competitive pay leveraging Carta data Employee recognition through Bonusly (birthdays, anniversaries, achievements, etc.) 401(k) retirement plan with company matching- 50% match up to 6% of compensation after 90 days We value our employee’s work-life balance and encourage taking advantage of Unlimited PTO Supportive time off including paid volunteer days and company holidays Employer-contributed healthcare benefits, encompassing medical, dental, and vision coverage, with plans available for dependents and choices for Health Savings Accounts (HSA) and Flexible Spending Accounts (FSA). 12 weeks primary parent leave, 4 weeks secondary parent leave - full pay (adoption as well) We pride ourselves on Community and host exciting company outings and events.",
        "url": "https://www.linkedin.com/jobs/view/3949807405"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3960915446,
        "company": "Relanto",
        "title": "Data Engineer",
        "created_on": 1720638815.164553,
        "description": "Role: Data Engineer Location: 100% REMOTE Duration: Long Term Job Description: Responsibilities: Collaborate in designing and developing the next generation of our AWS data infrastructure Develop integrations with external resources (sFTP, API’s, etc.) to bring assets into the data warehouse Create data pipeline monitoring automation Produce custom datasets to enable Data Scientists and Data Analysts Write and test production level code that can be deployed within our existing cloud framework Explore new data technologies Skills: BS/BA degree in Computer Science, Math, Physics, or a related field, or equivalent years of experience in a relevant field 3+ years of experience in a Data Engineering role Extensive coding and design skills in SQL. Good experience with Python Experience in designing and deploying cloud data infrastructure in AWS Experience working in a Linux environment Working knowledge of Git & Github",
        "url": "https://www.linkedin.com/jobs/view/3960915446"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Draper, UT",
        "job_id": 3964672742,
        "company": "BambooHR",
        "title": "Data Engineer (Analytics)",
        "created_on": 1720638816.8853233,
        "description": "Please Note: This is a Utah-based hybrid position which will require some regular in-office days each week. Essential Job Duties As a Data Engineer focused on analytics engineering, we'll rely on your expertise to develop, automate, operate, and maintain data warehouses and data marts. Crafting performant and reliable data pipelines, effective data structures, and thoughtful data models will be key to your success. You will: Collaborate with data analysts, data scientists, AI software engineers, and stakeholders to make effective use of core data assets Meet with stakeholders to define and document modeling requirements Design ERDs and data flow diagrams mapping business processes to data Build and maintain data marts, facts, and dimensions for business analytics Support key performance indicators and other business or operational metrics Leverage a combination of tools and SQL code to automate and orchestrate data modeling Monitor and improve the health, quality, and performance of data pipelines and modeled objects Develop SQL based data quality control tests Participate in peer SQL code reviews, code approval, and pull requests Provide expertise in operation, troubleshooting, and tuning of tools, code, SQL, and systems Contribute to the evolution of tools, systems, and methods for harnessing data Help ensure appropriate data privacy and security You'll be part of a team applying data to analyze and optimize key business processes such as finance, marketing, sales, product, customer experience, and more. What You Need to Get the Job Done Knowledge of data warehouses and data marts Experience modeling data across a wide range of business systems and use cases An extroverted personality ready to engage with business leaders and stakeholders An excitement around and desire to evangelize data modeling methodology Ability to quickly gain data domain knowledge in multiple areas Demonstrated SQL skills UDF programming abilities Exposure to various data modeling methodologies and use cases Strong understanding of Kimball data modeling methodology Experience with Databricks, Snowflake, Redshift, Vertica, Greenplum, or other similar engines Ability to automate data modeling pipelines with tools like dbt Git-based team coding workflows Cloud-based computing environments like AWS and GCP What Will Make Us REALLY Love You Python programming Strong experience with Databricks and Snowflake Experience with streaming data What You'll Love About Us Great Company Culture. We've been recognized by multiple organizations like Inc, Salt Lake Tribune, Glassdoor, & Comparably for our great workplace culture. Make an Impact . We care about your individuality by giving you freedom to grow and create within the company, regardless of your position. Rest and Relaxation . 4 weeks paid time off, 11 paid holidays, and we pay you to go on vacation (ask us about this)! Health Benefits. Medical with HSA and FSA options, dental, and vision. Prepare for the Future . 401(k) with a generous company match, access to a personal financial planner, and both legal and life insurance. Financial Peace University . We pay for a one year subscription and you walk away with financial savvy and a bonus. Give back . Get paid to give your time to the community: ask us about this! Educational Benefits . Whether you are a previous student, or currently enrolled in higher education, we can help cover some of those expenses. Amazing Office Amenities . We've got incredible benefits at our Draper headquarters including a full size gym, pickleball courts, a great office cafe, and free fountain drinks! Ask us more about our office! In-Person Onboarding! All new hires get to experience our in-person onboarding class, Bamboo Beginnings, at our Draper, UT headquarters! Ask us more about Bamboo Beginnings! Flexible Work Models . In-office, work-from-home, or hybrid, depending on position and location. About Us Our mission is simple: we want to set people free to do meaningful work. People love our software—and it turns out that people love working here too. We've been recognized as a \"Best Company to Work For\" and we're proud of our team for creating software that makes an impact in the lives of HR pros and employees all over the world. BambooHR is committed to the full inclusion of all qualified individuals and will ensure that persons with disabilities are provided reasonable accommodations throughout the hiring process. If you would like to request accommodations, please let your recruiter know. BambooHR is An Equal Opportunity Employer--M/F/D/V Because our team members are trusted to handle sensitive information, we require all candidates that receive and accept employment offers to complete a background check before being hired. For information on California Privacy Policy, click here.",
        "url": "https://www.linkedin.com/jobs/view/3964672742"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Brentwood, TN",
        "job_id": 3958102359,
        "company": "Conexess Group",
        "title": "Data Engineer",
        "created_on": 1720638819.7201977,
        "description": "Remote | Product and Development | Full-Time Conexess Group is aiding a large financial client in their search for a senior Data Engineer. This is an immediate, full time job opening and we are only able to accept candidates that are eligible to work on W2. This is an individual contributor role, working closely with other data team members to complete the building out of ETL pipelines and Operational Data Store (ODS). Your day-to-day tasks will include extracting data from company applications, transforming it according to business logic, and efficiently loading it into our Operational Data Store (ODS). You will work with members from the cross-functional team to help them with the consumption of the data. You will be expected to use your expertise to optimize the process for performance and accuracy. Your commitment to continuous improvement will drive enhancements in our data architecture and ensure that our data system can scale. Responsibilities Daily Responsibilities Contribute to building out ETL pipeline to include data from all company products Write performant and maintainable code Ensure the quality of ETL process Communicate well with other members of the data team and across the functional area Advocate for best practices and continuous improvement Participate in architecting and building a scalable data system Mentor team members on their area of expertise Required Skills: 3&plus; Years of experience as Data Engineer designing and implementing data warehouse and data lake solutions on Microsoft Azure. Experience with Azure Data Factory is required Azure Data Lake is a plus Exposure to Microsoft Fabric is a plus Very strong background in MS SQL Server is required Expertise in data architecture using different database types and data formats Expertise in building data pipelines to clean, enrich, and transform data Experience with Python is a plus and highly desirable Expertise in database design and tuning techniques Strong understanding of ETL process and tooling Experience with version control systems (Azure DevOps, Git) Familiar with CI/CD concept Preferred Skills: Experience working in horizontally scaling systems Exposure to Azure SQL/warehouse/data lake products such as Azure SQL, Synapse, Databricks, ADLS Exposure to Microsoft Fabric is a plus Experience with other cloud data stacks (Google, AWS) is a plus Familiarity with message/event driven architecture patterns and distributed systems architecture Familiarity with systems integration An automation mindset Expectations: Intentional mentorship: We are dedicated to teaching and growing talent and expects everyone to help those less experienced. Honesty: Whether reviewing someone's code, participating in retrospectives, or working with your team on what direction to take a project we expect openness and honesty. Honesty creates trust, and we believe that all great teams are built on trust. Low Ego: Have confidence in your skills and experience but be willing to alter your opinions and ideas when another, better one comes along. Have strong opinions, but loosely held. Deep Curiosity: You will be expected to research new and exciting technologies, perfect the use of existing technologies, and Client new libraries and tools that can affect change across the organization. Motivation: You are a natural self-starter, and you enjoy solving problems. You can solve the problem with minimal instruction and figuring out what should be done. Benefits Information: A fun, fast-paced work environment Responsible PTO Plan that meets or exceeds state and local medical and family leave laws 11 paid holidays Community and social events to keep you connected and engaged Mental Health Benefits Medical, Dental and Vision insurance Company-paid Group Life Insurance, Short- and Long-Term Disability Flexible Spending Account & Health Savings Account Aflac Benefits – Critical Illness, Cancer Protection, & Hospital Choice Pet Insurance 401 (k) with company match with eligibility on Day 1 of employment 2 Paid Volunteer Time Off Days And much more!",
        "url": "https://www.linkedin.com/jobs/view/3958102359"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3944431589,
        "company": "Atika Technologies & BPM",
        "title": "Data Engineer",
        "created_on": 1720638823.2946885,
        "description": "We are looking for a Senior Data Engineer with hands-on experience in Streaming/Batch infrastructure. It is essential that they have strong programming skills in Java/Python and must have experience with Google Cloud Platform. Ideally, look for candidates who have worked with large data sets (1 Petabyte). Experience in Messaging/Stream Processing systems on Cloud such as Pub/Sub, Kafka, Kinesis, Dataflow, Flink etc., and/or Experience in Batch Processing systems such as Hadoop, Pig, Hive, Spark. Experience with Dataproc is a strong plus. This will be a remote role, but candidate need to go onsite quarterly once for a week on their own expenses. Preference will be given to candidates who are local to the PST or MST time zone. professional experience in Stream/Batch Processing systems at scale. Strong Programming skills in Java, Python. Experience in Public Cloud is a must. Experience with GCP and GCP managed services is a strong plus. Experience in Messaging/Stream Processing systems on Cloud such as Pub/Sub, Kafka, Kinesis, Dataflow, Flink etc., and/or Experience in Batch Processing systems such as Hadoop, Pig, Hive, Spark. Experience with Dataproc is a strong plus. Knowledge of DevOps principles and tools (e.g. CI/CD, IaC/Terraform). Strong understanding of Containerization technologies (e.g., Docker, Kubernetes). Strong problem solving and critical thinking skills. Strong written/verbal communication skills with the ability to thrive in a remote work environment. (For Senior leads/architects) Ability to explore new areas/problems as well as design and architect scalable solutions in Stream/Batch Processing at scale. Ability to technically lead a team of engineers on a project/component.",
        "url": "https://www.linkedin.com/jobs/view/3944431589"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Houston, TX",
        "job_id": 3941727132,
        "company": "Proven Recruiting",
        "title": "Data Engineer",
        "created_on": 1720638824.8638604,
        "description": "Python Data Engineer Who you are: 7 years experience with Python Programming as a Data Engineer Ability to develop and maintain data pipelines Experience with MLOps Bachelors Degree Required What you’ll do: Lead Python Data Engineer in Data Engineering team Collaborate with Business domain experts and Data Scientists Solve oil and gas midstream problems using analytics, ML, AI Location: Downtown Houston, 5 x week What’s Next: Email jfarmer@provenrecruiting.com if you are interested. What does this position pay? Compensation is determined by several factors which may include skillset, experience level, and geographic location. The expected range for this role is $80 to $100 hourly. Please note this range is an estimate and actual pay may vary based on qualifications and experience. Diversity Note: We actively support and promote people of various background from race, religion, gender to geographical area, university, lifestyle, and personality type. Proven recruiting is minority-owned, majority women and is a strong advocate for diversity and inclusion in the community. Apply Today! #IND3",
        "url": "https://www.linkedin.com/jobs/view/3941727132"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "New York City Metropolitan Area",
        "job_id": 3967163964,
        "company": "Robert Half",
        "title": "Data Engineer",
        "created_on": 1720638826.5718331,
        "description": "We're looking for a talented Senior Data Engineer to join our team and play a key role in developing and maintaining the data pipelines that drive our platform delivery. You'll collaborate with a brilliant crew of architects, engineers, and data scientists to transform raw data into actionable insights, enabling us to solve complex business problems and create innovative solutions for our customers. What you'll do: Design, build, test, and maintain high-quality, scalable data pipelines using cutting-edge practices and industry standards. Analyze complex datasets, prepare data for modeling, and translate insights into actionable reports. Integrate data from diverse sources to create a unified view for analysis. Develop and maintain clear and concise documentation for processes and data structures. Proactively monitor data pipelines, identify and troubleshoot issues, and implement solutions. Write unit, integration, and functional automation tests to ensure data quality and pipeline reliability. Apply software development best practices, including modularity, code reusability, CI/CD workflows, and a focus on maintainability.",
        "url": "https://www.linkedin.com/jobs/view/3967163964"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Henrico, VA",
        "job_id": 3959389392,
        "company": "Kforce Inc",
        "title": "Data Engineer",
        "created_on": 1720638828.260171,
        "description": "Responsibilities Kforce has a client in need of a Data Engineer in Richmond, VA. Responsibilities: Data Engineer will design and complete medium sized data task Ensure adherence to data governance and compliance initiatives Perform data profiling and implement technical designs As a Data Engineer, you will drive root cause analysis of production issues Assist in integrating diverse datasets and data consumption patterns Assist with renovating the data management infrastructure Requirements Bachelor's degree in Computer Science or equivalent alternative education 2+ years of experience in data management or software development Experience with Database/Schema Design Experience in Azure Cloud technologies and Snowflake Basic understanding of streaming data ingestion Basic understanding of Big Data concepts The pay range is the lowest to highest compensation we reasonably in good faith believe we would pay at posting for this role. We may ultimately pay more or less than this range. Employee pay is based on factors like relevant education, qualifications, certifications, experience, skills, seniority, location, performance, union contract and business needs. This range may be modified in the future. We offer comprehensive benefits including medical/dental/vision insurance, HSA, FSA, 401(k), and life, disability & ADD insurance to eligible employees. Salaried personnel receive paid time off. Hourly employees are not eligible for paid time off unless required by law. Hourly employees on a Service Contract Act project are eligible for paid sick leave. Note: Pay is not considered compensation until it is earned, vested and determinable. The amount and availability of any compensation remains in Kforce's sole discretion unless and until paid and may be modified in its discretion consistent with the law. This job is not eligible for bonuses, incentives or commissions. Kforce is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.",
        "url": "https://www.linkedin.com/jobs/view/3959389392"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Pennsylvania, United States",
        "job_id": 3959939396,
        "company": "EV.Careers",
        "title": "Data Engineer",
        "created_on": 1720638830.0347996,
        "description": "Please be aware of recruiting scams! All legitimate communication from our recruitment team will come from an official calstart.org email address via email, we will not text you about a role you have not applied to or shown interest in. We will not perform any interviews via text or Zoom chat. CALSTART does not ask for any fees or personal information such as social security numbers or bank details during the recruitment process. About Us CALSTART is a mission-driven industry organization focused on transportation decarbonization and clean air for all. For over 30 years, it’s been CALSTART’s mission to develop, assess, and implement large-scale, zero-emission transportation solutions to mitigate climate change and support economic growth. CALSTART works with businesses, organizations, governments, and communities to create real-life impact toward clean air and equitable access to clean transportation for all. CALSTART provides scientific, technical and policy support for regulatory development and clean technology and infrastructure acceleration. About The Role As a Data Engineer, you'll design and refine advanced data pipelines using Apache Airflow, integrating complex datasets with Python, Pandas, and GeoPandas to support critical decision-making. You will implement Open-metadata for top-tier data governance and enhance data visualizations through Metabase, driving strategic insights. Collaborating with diverse stakeholders, you'll create a unified data ecosystem. Additionally, you'll elevate our AWS security and infrastructure using tools like Docker. Your expertise in data engineering, cloud architectures, and data governance will drive process improvements and compliance. You'll tackle complex challenges, fostering an inclusive, collaborative environment, and advancing our mission for clean transportation through innovative solutions. What You'll Do Architect and refine state-of-the-art data pipelines utilizing Apache Airflow, integrating complex datasets with Python, Pandas, and GeoPandas into a seamless flow that powers decision-making. Champion the implementation of Open-metadata for elite data governance, crafting frameworks that ensure data excellence across the spectrum of our initiatives. Innovate and enhance data visualizations and dashboards through Metabase, enabling transformative insights that drive strategic initiatives and foster a culture of data-driven decision making. Collaborate with a spectrum of stakeholders from technical experts to strategic managers, weaving together diverse perspectives to craft a unified data ecosystem. Elevate our security posture and cloud infrastructure on AWS, deploying cutting-edge tools like Docker to safeguard our pioneering work in clean transportation. What You'll Bring To The Table Deep technical expertise in data engineering, with a rich background in designing and deploying robust data systems using PostgreSQL, PostGIS, and advanced Python libraries. Demonstrable proficiency in cloud architectures, particularly AWS, with a mastery in leveraging cloud services to maximize efficiency and security. Adept at leading data governance and compliance efforts, with a proven track record of architecting data solutions that align with stringent standards and organizational goals. Exceptional problem-solving skills, with the ability to dissect and address complex challenges, turning obstacles into opportunities for growth and innovation. Exemplary communication abilities, capable of articulating complex technical details and engaging diverse audiences to foster an inclusive and collaborative environment. Preferred Qualifications A passion for sustainability and technological innovation in the field of clean transportation. Agile project management prowess, adept at steering projects through agile frameworks to ensure adaptability and milestone achievement. Advanced expertise in security protocols, cloud DevOps practices, and proactive measures to ensure the integrity and privacy of sensitive data. $74,880 - $90,048 a year CALSTART values transparency and strives to provide as much information regarding compensation as possible. The complete salary range for this role is $74,880 - $90,048 . We determine pay based on several factors, including but not limited to job-related skills, qualifications, experience, education, internal equity and other factors relevant to the job. We understand that not everyone will match the above qualifications 100%. If your background isn't perfectly aligned but you feel you would be a great addition to the team, we'd love to hear from you. We're a tight-knit team of world-class innovators, business minds, and change agents who believe passionately in our mission and put our team ahead of self. We are committed to the continued development and growth of our employees and invest in your success! We care about your personal well being as much as your professional success and offer generous benefits to full time employees including: 100% company paid comprehensive health benefits for Medical, Dental, Vision, Short Term Disability, Long Term Disability and Life Insurance, Retirement plan with generous company contributions, FSA for Health and Dependent Care, 3 weeks of vacation time in the first year of employment, 11 paid company holidays, paid sick time, paid family leave, and more! Our inclusive environment focuses on making decisions based on merit without regard to race, color, hair texture, gender, religion, age, nationality, social or ethnic origin, sexual orientation, gender identity, gender expression, LGBTQIA+ status, marital status, pregnancy, disability, genetics, veteran status, or any other characteristic protected by law.",
        "url": "https://www.linkedin.com/jobs/view/3959939396"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Cincinnati, OH",
        "job_id": 3943399155,
        "company": "Ohio's Hospice",
        "title": "Data Engineer 1",
        "created_on": 1720638831.7259328,
        "description": "What you should know about the Data Engineer 1 position: This is a FT position, partially remote, M-F 8am-5pm We provide superior care and superior services to patients at their end of life journey. Only those who have a heart for hospice will succeed Job Summary/Purpose As a Data Engineer I, you will be responsible for supporting the data infrastructure and pipelines within the organization. You will work closely with data scientists and analysts to ensure efficient data flow and storage. Your duties may include data cleaning, transformation, and loading tasks, as well as troubleshooting data pipeline issues. You will also assist in the maintenance and optimization of databases and data warehouses. Key Responsibilities Develop and maintain data pipelines for efficient data extraction, transformation, and loading (ETL). Collaborate with data scientists and analysts to understand data requirements and ensure data quality. Perform data cleaning and transformation tasks to prepare raw data for analysis. Assist in the development and maintenance of databases and data warehouses. Troubleshoot data pipeline issues and perform root cause analysis. Implement data governance and security measures to ensure data integrity and compliance. Stay up to date with emerging technologies and best practices in data engineering. Qualifications Bachelor’s degree in information systems, computer science, engineering or an equivalent field required. Equivalent work experience will be considered. Healthcare industry experience preferred. 2+ years’ professional experience or internship. Knowledge of SQL Server 2016/2019/2022 and strong T-SQL skills is required. Strong discipline in following SQL coding styles which improves code readability and supportability. Experience with ETL tools and techniques. Knowledge of database systems (e.g., SQL, MongoDB, MySQL). Strong analytical and problem-solving skills. Excellent oral and written communication skills and interpersonal skills including the ability to effectively communicate technical information to both technical and non-technical personnel. Ability to work successfully and independently in an atmosphere of multiple projects, shifting priorities, and deadline pressures. Ability to work effectively with both users and technical staff in team-oriented environments. Translation of business questions and requirements into reports, views, and SQL queries. Ability to work an on-call schedule that includes being available nights and weekends. Benefits & Perks Competitive Pay (we actually mean it!) Competitive Health, Dental, and Vision Insurance Short- & Long-Term Disability Life Insurance Paid Time Off Matching Retirement Plans Tuition Reimbursement Preparation for certification and pay incentive on Hospice certification achievement Scrubs provided Mileage reimbursement Organizational preceptor to assist with orientation and ongoing education Educational programs geared toward career advancement Career growth And much, much, more! Ohio’s Hospice offers opportunity, advancement, and a great foundation for growth to energetic people looking to serve our mission. Those who join our RN Extended team are committed to providing superior care and service so our patients and their families can celebrate life. We provide our staff members with the resources and support to contribute and make a difference in the lives of patients and families every day. Come join a group of people that are wildly passionate about taking care of our patients and each other! As a member of our team, you'll have a chance to impact many lives. You may find a deeper meaning in your work or rediscover why you chose your profession in the first place. The passion you may have been missing in previous workplaces can be found at Ohio’s Hospice! Ohio’s Hospice complies with applicable Federal civil rights laws and does not discriminate on the basis of race, color, national origin, age, disability, or sex. Ohio’s Hospice is proud to be platinum certified through SAGECare, which provides training and consulting on LGBT aging issues to service providers. Ohio’s Hospice welcomes those in the LGBT community to join our team.",
        "url": "https://www.linkedin.com/jobs/view/3943399155"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Irving, TX",
        "job_id": 3935083181,
        "company": "Reprise Financial",
        "title": "Data Engineer",
        "created_on": 1720638835.2393773,
        "description": "Remote position in states where Reprise currently has staff (TX, FL, TN), preference for DFW,TX Role Overview Reprise Financial is seeking a passionate and highly motivated Data Engineer to join our Enterprise Data Management team. In this role, you will design, build, and maintain data pipelines and ETL jobs that are essential for allowing the enterprise to access and interpret data effectively. You will be integral in architecting and delivering modern data solutions, working cross-functionally with Analytics, IT, and business teams to ensure reliable data pipelines for performance and scalability. In this role, you will join a team that values and encourages collaboration and peer-to-peer knowledge sharing. We foster a culture where ideas are freely exchanged, and every team member is encouraged to expand their knowledge both technically and within the business context. We celebrate everyone's wins and recognize that each project and business problem we tackle is an opportunity for collective learning. Our guiding principle is to make data easy and accessible for everyone to use, enabling better decision-making across the company. Essential Functions Create and maintain optimal data pipeline architecture. Assemble large, complex data sets that meet functional / non-functional business requirements. Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. Manage and support data engineering platform tools using technologies such as Microsoft SQL Server, SSMS, SSIS, REST APIs, and Azure Cloud Platform to move data from various heterogeneous data sources ranging from flat files, Excel, Oracle, SQL Server, XML, XSL files, and JSON files. Design and develop a robust data engineering infrastructure to support optimal extraction, transformation, and loading (ETL) of data from diverse sources, utilizing SQL, Microsoft Azure, AWS, and advanced big data technologies. Develop and perform unit tests; maintain up-to-date code in source control. Collaborate on code reviews and adhere to change management processes to deploy changes to production systems. Troubleshoot issues reported by users and perform root cause analysis for production issues with minimal guidance. Participate in on-call rotation to provide support and implement fixes as needed. Continuously learn about the business and the data that supports it. Adheres to Company policies and procedures, including but not limited to Compliance (UDAAP, BSA/AML, etc.), Information Technology, employee handbook, etc. Perform other duties as assigned. Education & Experience We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Engineering, Mathematics, Information Systems, or related technical discipline. Proficient in T-SQL, Stored Procedures, Functions, and SSIS with capability to implement REST APIs. Experienced with cloud data platforms like Microsoft Azure and Microsoft Fabric, ensuring scalable data solutions. Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. Experience building and optimizing big data' data pipelines, architectures, and data sets. Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. Strong analytic skills related to working with unstructured and semi-structured datasets. Build processes supporting data transformation, data structures, metadata, dependency and workload management. A successful history of manipulating, processing and extracting value from large, disconnected datasets. Experience supporting and working with cross-functional teams in a dynamic environment. Experience in unit testing, data quality validation, and data analysis. Knowledge of cloud data and platform security, with a preference for AWS and Azure. Competencies Customer Focus: Strong drive to meet data consumer needs and deliver effective solutions. Critical Thinking: Ability to analyze data and solve problems to reach well-reasoned solutions. Team Mentality: Ability to work effectively within a team to achieve common goals. Learning Agility: Openness to learning new technologies and methodologies to maintain a competitive edge. Communication: Ability to communicate effectively orally and in writing. Problem Solving: Ability to work effectively to solve problems independently and with others.",
        "url": "https://www.linkedin.com/jobs/view/3935083181"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Louisville, CO",
        "job_id": 3956544199,
        "company": "TrainingPeaks",
        "title": "Data Engineer - TrainingPeaks",
        "created_on": 1720638836.9061835,
        "description": "Description Are you ready to work on a product impacting millions of people? At TrainingPeaks our user base of athletes and coaches is growing rapidly. To meet their demands TrainingPeaks needs innovators, collaborators, and excellent engineers like you. Together we’re building the world’s best training platform. Join TrainingPeaks today. You may know us as TrainingPeaks, MakeMusic, TrainHeroic and Alfred Music. All these brands are under the Peaksware umbrella. TrainingPeaks develops software for coaches and athletes to track, analyze and plan endurance training. TrainHeroic develops software solutions for the strength and conditioning needs of coaches and athletes. MakeMusic develops software to transform how music is composed, taught, learned and performed. Alfred Music creates and publishes educational music to help teachers, students, professionals and hobbyists experience the joy of making music. We would love to have you join our ever-growing team! All applicants will receive equal consideration for employment regardless of gender, race, national origin, age, sexual orientation, gender identity, physical disability, religion, or length of time spent unemployed. General Summary As a Data Engineer, you will be responsible for creating and maintaining data solutions that help meet business needs. You will work closely with data analysts and engineers to improve the quality, reliability and coverage of our data. You must enjoy challenging problems, be proactive, and engage as a collaborative team player. You are a continuous learner with a hunger for knowledge. You are eager to stay on top of the best data solutions in the industry and think about how they might up-level our data team. You approach challenges as opportunities to improve. You value team members’ input from all levels and you actively seek ways to support your colleagues. You will sit directly with the data team and report to the Senior Manager of Data Analytics and Insights. Core Functions Model, create and maintain end-to-end data pipelines to ensure optimal data flow. Independently identify and resolve issues with existing data pipelines, contributing to the continuous improvement of data reliability. Work closely with data analysts to comprehend end-user requirements and collaborate with software engineers to design code that aligns with the database structure. Mentor and train other team members on data engineering and industry best practices. Stay current with emerging technologies and tools in the data engineering space. Collaborate with the data team to assess their potential impact on our data platform and provide strategic recommendations. The work characteristics described here are representative of those an employee encounters while performing the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. Requirements Required Qualifications: 2+ years of experience in a Data Engineer role or in a related field Strong expertise in SQL and experience with data modeling and database design conventions. Experience with software engineering, preferably with Python, C# or Java. Experience working with cloud-based tools and data warehouses, preferably Fivetran, Snowflake and AWS. Desired Qualifications Experience working collaboratively in a team environment, utilizing version control for effective collaboration. Experience working with APIs and effectively leveraging their results. Familiarity with data pipeline schedulers (e.g., Airflow, Dagster) to enhance visibility and facilitate the debugging of data pipelines. Exposure to data analytics or data science concepts and best practices. Degrees are not required, and we value all forms of continued education including traditional four-year degrees, post-graduate degrees, associate degrees, bootcamps, online training, professional certifications, self-teaching, and more. Don’t meet every single requirement? Don’t worry. We still want to hear from you and encourage you to apply. Benefits Compensation Peaksware/TrainingPeaks is committed to fair and equitable compensation practices. The salary range for this role is $80,317 - $133,861. Final compensation for this role will be determined by various factors such as a candidate’s relevant work experience, skills, and certifications. This role is eligible for variable compensation, including bonus. Benefits And Perks Health 100% company-paid Medical for employees with buy-up options Dental Vision Health Savings Account Flexible Spending Account Dependent Care Flexible Spending Account Paid Parental Leave Teladoc Employee Assistance Program (EAP) Additional coverage options such as accident and critical illness insurance and hospital indemnity Disability and Life Company-paid Short Term Disability Company-paid Long Term Disability Company-paid Basic Life Insurance and AD&D Employee-paid Supplemental Life Insurance for Employee, Spouse, and/or Child Additional 401(K) 401(K) Matching Pet Insurance 9 paid holidays annually and unlimited Flexible Time Off (FTO) Free TrainingPeaks, TrainHeroic, MakeMusic accounts, and Alfred Music product Access to the Performance and Recovery Center (PARC), our on-site fitness facility Employee only access to on-site locker rooms and showers Employee only access to secure, indoor bike storage Access to our onsite Music Studio An assortment of “grab’n go” fruit and snacks as well as on tap cold brew, kombucha, and beer. Beautiful onsite cafe that includes indoor and outdoor seating and lounge areas. Access to e-bikes available exclusively to Peaksware employees Significant investment in resources for employee growth and development Corporate discounts on select gym memberships and top brand gear Flexible work schedule in a culture of trust Please contact careers@peaksware.com if you require a reasonable accommodation to review our website or to apply online. Work Environment This job operates in a professional office environment that is well-lighted, heated, and/or air-conditioned with adequate ventilation and a noise level that is usually moderate. This role routinely uses standard office equipment such as computers, phones, photocopiers and filing cabinets. All employees must comply with all safety policies, practices and procedures. Report all unsafe activities to your manager and/or Human Resources. Physical Demands While performing the duties of this job, the employee is regularly required to sit and move about the facility; use hands to handle, or feel; talk by expressing ideas by means of the spoken word; and hear by perceiving the nature of sounds. The employee is occasionally required to stand, walk, and reach with hands and arms. The employee must occasionally lift and/or move up to 10 pounds. Specific vision abilities required by this job include close vision, distance vision, color vision, peripheral vision, depth perception, and ability to adjust focus. To view the Peaksware Privacy Policy, click here . By submitting an application, you acknowledge and agree to the Peaksware Privacy Policy.",
        "url": "https://www.linkedin.com/jobs/view/3956544199"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Seattle, WA",
        "job_id": 3964363245,
        "company": "Control Risks",
        "title": "Data Engineer",
        "created_on": 1720638842.3180163,
        "description": "The Data Engineer will be working on building and maintaining complex data pipelines, Assemble large and complex datasets to generate business insights and to enable data driven decision making and support the rapidly growing and dynamic business demand for data. As a Data Engineer you will have an opportunity to collaborate and work with various teams of Data Scientists, Database Developers and Managers, Software Dev Engineers, and Program managers to determine how best to design, implement and support solutions. Design,      implement and support analytical data platform solutions for data driven      decisions and insights. Design      data schema and operate internal data warehouses & SQL/NOSQL database      systems. Work on      different data model designs, architecture, implementation, discussions      and optimizations. Interface      with other teams to extract, transform, and load data from a wide variety      of data sources, big data technologies like EMR, RedShift, Elastic Search etc. Work on      SQL technologies on Hadoop such as Spark, Hive, Impala etc. Help      continually improve ongoing analysis processes, optimizing or simplifying      self-service support for customers Must      possess strong verbal and written communication skills, be self-driven,      and deliver high quality results in a fast-paced environment. Recognize      and adopt best practices in reporting and analysis: data integrity, test      design, analysis, validation, and documentation. Enjoy      working closely with your peers in a group of talented engineers and gain      knowledge. Be      enthusiastic about building deep domain knowledge on various GSII AFP      domains. Own the      development and maintenance of metrics, reports, analyses, dashboards,      etc. to drive key business decisions. Requirements Candidates must have a degree in      Computer Science or Engineering or related field, and 3+ years of industry experience      as a Data Engineer or in a similar role. Hands on experience in languages      like Python, Java, Scala, etc. Working knowledge of RDBMS, Big      Data, SQL, NOSQL, ETL and Data-warehousing. Experience working with distributed systems and      big data processing technologies (Hadoop, Hive, Hbase, Spark, EMR,      etc.). Candidate must have good written      and oral communication skills, be a fast learner and have the ability to      adapt quickly to a fast-paced development environment. Knowledge of software engineering      best practices across the development life cycle, including agile      methodologies, coding standards, code reviews, source management, build      processes, testing, and operations. Knowledge of cloud services such as      AWS or equivalent. Knowledge on different      reporting/visualization tools in the industry. Sharp problem-solving skills and      ability to resolve ambiguous requirements . Benefits Control Risks offers a competitively positioned compensation and benefits package that is transparent and summarised in the full job offer. We operate a discretionary bonus scheme that incentivizes, and rewards individuals based on company and individual performance Control Risks supports hybrid working arrangements, wherever possible, that emphasize the value of in-person time together - in the office and with our clients - while continuing to support flexible and remote working.",
        "url": "https://www.linkedin.com/jobs/view/3964363245"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Atlanta, GA",
        "job_id": 3934553147,
        "company": "The Home Depot",
        "title": "Data Engineer II",
        "created_on": 1720638843.9023666,
        "description": "Req117439 Position Purpose Data Engineer II is responsible for supporting our enterprise data warehouse for our Supply Chain Operations. As a Data Engineer II, you will be part of a dynamic team with engineers of all experience levels who help each other build and grow technical and leadership skills while creating, deploying, and supporting production applications. In addition, Data Engineer IIs may be involved in configuration, security, resilience, performance tuning and production monitoring. Key Responsibilities 60% Delivery and Execution - Collaborates and pairs with other product team members (UX, engineering, and product management) to create secure, reliable, scalable software solutions; Documents, reviews and ensures that all quality and change control standards are met; Works with Product Team to ensure user stories that are developer-ready, easy to understand, and testable; Writes custom code or scripts to automate infrastructure, monitoring services, and test cases; Writes custom code or scripts to do destructive testing to ensure adequate resiliency in production; Program configuration/modification and setup activities on large projects using HD approved methodology; Configures commercial off the shelf solutions to align with evolving business needs Creates meaningful dashboards, logging, alerting, and responses to ensure that issues are captured and addressed proactively 20% Learning - Actively seeks ways to grow and be challenged using both formal and informal development channels; Learns through successful and failed experiment when tackling new problems 20% Plans and Aligns - Collaborates with other team members in agile processes; Assists in creating new and better ways for the team to be successful; Relates openly and comfortably with diverse groups of people; Builds partnerships and works collaboratively with others to meet shared objectives Direct Manager/Direct Reports This position typically repots to Software Engineer Manager or Sr. Manager This position has 0 Direct Reports Travel Requirements No travel required. Physical Requirements Most of the time is spent sitting in a comfortable position and there is frequent opportunity to move about. On rare occasions there may be a need to move or lift light articles. Working Conditions Located in a comfortable indoor area. Any unpleasant conditions would be infrequent and not objectionable. Minimum Qualifications Must be eighteen years of age or older. Must be legally permitted to work in the United States. Preferred Qualifications 1-3 years of relevant work experience Extensive ETL experience, preferably with a focus on Google Cloud Platform Experience working in SQL, with the ability to work with most databases Previous experience working with BigQuery as a Data Warehouse service Hands-on experience in big data environments Experience in cloud computing techniques: Google Kubernetes, Google Cloud Composer, Google Big Query, Google Secret Manager, Google Cloud DNS Experience in CI/CD tools Experience in version control systems Familiarity with Data Science principles and practices Knowledge of Dimensional Modeling using relevant tools Additional Qualifications Experience in Angualar,Javascript /Typescript frameworks, Python, Bash Experience in an object-oriented programming language, Java Experience in source code version control Experience in Relational or noSQL database technology Experience in microservice-based architecture Experience with modern debugging and root cause analysis techniques Exposure to security frameworks for user and services authorization and authentication Exposure to creating and executing unit, functional, destructive and performance tests Minimum Education The knowledge, skills and abilities typically acquired through the completion of a bachelor's degree program or equivalent degree in a field of study related to the job. Preferred Education No additional education Minimum Years Of Work Experience 2 Preferred Years Of Work Experience No additional years of experience Minimum Leadership Experience None Preferred Leadership Experience None Certifications None Competencies Global Perspective Manages Ambiguity Nimble Learning Self-Development Collaborates Cultivates Innovation Situational Adaptability Communicates Effectively Drives Results Interpersonal Savvy The application window is anticipated to be closed on July 15th, 2024 See more benefits: livetheorangelife.com Apply End Date: 07/15/2024",
        "url": "https://www.linkedin.com/jobs/view/3934553147"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Greensboro, NC",
        "job_id": 3959778113,
        "company": "Qorvo, Inc.",
        "title": "Systems Data Engineer",
        "created_on": 1720638845.7022836,
        "description": "Qorvo (Nasdaq: QRVO) supplies innovative semiconductor solutions that make a better world possible. We combine product and technology leadership, systems-level expertise and global manufacturing scale to quickly solve our customers' most complex technical challenges. Qorvo serves diverse high-growth segments of large global markets, including consumer electronics, smart home/IoT, automotive, EVs, battery-powered appliances, network infrastructure, healthcare and aerospace/defense. Visit www.qorvo.com to learn how our diverse and innovative team is helping connect, protect and power our planet. Summary As a Senior Systems Data Engineer, you will be part of the EBS Reporting Team under Qorvo’s IT Enterprise Business Applications organization. Your primary focus will be on leading and overseeing the design, development, and management of the data infrastructure on the Databricks platform within an AWS government cloud. You will manage key client technical projects and workstreams, coordinating the work of more junior engineers, and often working alongside them. You will also partner closely with business analysts and project managers to complete projects on time, within budget and scope, and with high customer satisfaction. This position can be based in Greensboro, NC, Richardson, TX, or Hillsboro, OR. Qualifications B.S. in Computer Science/Engineering or relevant field; Masters degree preferred 5+ years of experience in the IT industry 5+ years of hands-on experience in data engineering/ETL using Databricks on AWS/Azure cloud infrastructure and functions Government Cloud experience Expert understanding of data warehousing concepts (Dimensional (star-schema), SCD2, Data Vault, Denormalized) implementing highly performant data ingestion pipelines from multiple sources Expert level skills with Python / PySpark and SQL Demonstrable experience in developing and nurturing a Data Engineering framework to include package/dependency management tools, functional testing (e.g., Pytest, Pytest-Cov, PyLint), and load testing Experience with CI/CD on Databricks using tools such as Unity Catalog, Jenkins, GitHub Actions, and Databricks CLI Integrating the end-to-end Databricks pipeline to take data from source systems to target data repositories ensuring the quality and consistency of data is always maintained Strong understanding of Data Management principles (quality, governance, security, privacy, life cycle management, cataloging) Evaluating the performance and applicability of multiple tools against customer requirements Working within an Agile delivery/DevOps methodology to deliver proof of concept and production implementation in iterative sprints Experience with Delta Lake, Unity Catalog, Delta Sharing, Delta Live Tables (DLT) Hands on experience developing batch and streaming data pipelines Able to work independently Energetic and self-motivated, willingness to learn and openness to change are important Ability to work in a fast-paced, changing environment, and with all levels of the organization and cope with rapidly changing information Power BI/Tableau/QuickSight experience preferred Nice to have: experience with SAP ECC or S/4, AWS Redshift Responsibilities Establish and grow a data engineering framework to ensure the reliability, scalability, quality, and efficiency of data pipelines, storage, processing, and integration Establish data pipelines to ingest and curate data containing SAP business content from S/4 to Databricks Improve, maintain and execute the data strategy at Qorvo including governance, project prioritization, resourcing, and value delivery Follow the Medalion Architecture (Bronze, Silver, Gold) to logically organize data in a lakehouse, with the goal of incrementally and progressively improving the structure and quality of data as it flows Work effectively in an Agile Scrum environment Create technical, functional, and operational documentation for data pipelines and applications Use business requirements to drive the design of data solutions/applications and technical architecture Provide overall project management and administration activities for the data engineering organization Work with other developers, designers, and architects to ensure data applications meet requirements and performance, data security, and analytics goals Anticipate, identify, track, and resolve issues and risks affecting delivery Lead the configuration, build, and testing of data applications and technical architecture Coordinate and participate in structured peer review/ walkthroughs/code reviews Provide application/technical support Maintain and/or update technical and/or industry knowledge and skills through continuous learning activities Mentor and develop junior engineers Adhere to lean principles and standard processes to ensure continuous improvement Communicate clearly and effectively Soft Skills Team Player Excellent communication skills Collaboration Logical thinking Time Management MAKE A DIFFERENCE AT QORVO We are Qorvo. We do more than create innovative RF and Power solutions for the mobile, defense and infrastructure markets – we are a place to innovate and shape the future of wireless communications. It starts with our employees. As a unified global team, we bring a commitment to excellence, growth and a passion for creating what's next. Explore the possibilities with us. We are an Equal Employment Opportunity (EEO) / Affirmative Action employer and welcome all qualified applicants. Applicants will receive fair and impartial consideration without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, military or veteran status, physical or mental disability, genetic information, and/or any other status protected by law.",
        "url": "https://www.linkedin.com/jobs/view/3959778113"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Greater St. Louis",
        "job_id": 3967166947,
        "company": "Brooksource",
        "title": "Data Engineer",
        "created_on": 1720638847.3915932,
        "description": "Data Engineer Hybrid in St. Louis On-going Contract Brooksource is looking for a Data Engineer to join our Federal client's the Data Desk agile team within the Research Division to develop cloud native data operations tools to maintain the data ingestion pipeline processes for the client's website. Responsibilities: Write secure, production-quality Python code Implement cloud native solutions in AWS designed by our solutions and data architects for end-to-end data processing pipelines and data operations tools Perform code review on peers’ code Help document processes Deploy code to production servers using GitLab CI/CD pipelines Participate in agile development team Qualifications: Experience developing and maintaining data processing pipelines, including extraction, transformation, and loading of data (ETL processes) Experience with open-source programming languages, including: Python SQL PHP Combination of the following: Experience with Git or other source control for code management Experience with Amazon Web Services (AWS) such as Lambdas, Step Functions, ECS, SNS, SQS, and EventBridge Experience writing production-ready Terraform code Experience with Docker or containerized application development Experience with Agile methodologies",
        "url": "https://www.linkedin.com/jobs/view/3967166947"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Austin, Texas Metropolitan Area",
        "job_id": 3947882193,
        "company": "Intelliswift Software",
        "title": "Junior Data Reporting Engineer",
        "created_on": 1720638848.9992063,
        "description": "Title: Junior Data Reporting Engineer Location: Austin, TX (Onsite role) Type: Fulltime with Intelliswift Software Inc Minimum Qualifications: · 2+ years of solid hands-on experience with complex SQL scripting and Dashboard development. · Hands-on experience with design, development, and support of data analysis. · Experience with data platform and visualization technologies such as Google plx dashboards, Data Studio, Looker, GoogleSQL, and BigQuery. · Strong design and development skills with meticulous attention to detail. · Familiarity with Agile Software Development practices and working in an agile environment. · Strong analytical, troubleshooting, and organizational skills. Ability to analyze and troubleshoot complex issues, and proficiency in multitasking. · BS degree in Computer Science, Math, Statistics, or equivalent academic credentials.",
        "url": "https://www.linkedin.com/jobs/view/3947882193"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Redmond, WA",
        "job_id": 3940376412,
        "company": "Microsoft",
        "title": "Data Engineer II",
        "created_on": 1720638850.9771013,
        "description": "The Azure Core Organization is responsible for creating the foundation of Microsoft’s Cloud Platform for utility computing. This platform is one of the lowest levels of the services software/hardware stack and includes an efficient, virtualized computational substrate, a fully automated service management system, and a comprehensive set of highly scalable storage services. The Azure Compute Capacity and Efficiency (AC2E) team is the team in Azure Core tasked with managing all aspects of Compute capacity and efficiency management across the fleet. Capacity Management needs to ensure that on the one hand, there is sufficient capacity across all regions, allocation domains, and hardware infrastructure to meet all customer demand, while on the other hand ensuring that capacity is provisioned efficiently thereby avoiding overspending and COGS/CAPEX impact. At the scale of Azure’s business, managing this trade-off across the entire Azure Compute fleet is an enormously complex and challenging task, where improvements can make the difference between customer allocation failures on the one hand, and gargantuan savings on the other. As a Data Engineer II in the team, you will work closely with our software engineers, program managers, and data scientists across different teams within Azure Core. You will also collaborate with a variety of internal partner teams across Azure and Microsoft. You will build reliable, secured, highly scalable, performant data pipelines to enhance the Azure Compute allocation, deliver capacity management and efficiency improvements. The value of your work will be reflected in improvements to the Azure platform, Azure service capacity fulfillment rate, customer satisfaction, and various efficiency metrics, including COGS reduction. You will have opportunities for mentorship, accelerate your career growth, and work on truly high-business impact areas. Microsoft’s mission is to empower every person and every organization on the planet to achieve more. As employees we come together with a growth mindset, innovate to empower others, and collaborate to realize our shared goals. Each day we build on our values of respect, integrity, and accountability to create a culture of inclusion where everyone can thrive at work and beyond. Responsibilities With guidance, you’ll implement basic code to extra raw data, validate its quality, and ensure the correct data is ingested across multiple areas of work. You’ll also apply standard modification techniques to transform raw data into forms compatible with downstream data sources. You will design and maintain assigned data tools that are used to transform, manage, and access data. You’ll also write code to validate the storage and availability of data platforms so that they’re more resilient. You will follow existing documentation to implement performance monitoring protocols across a data pipeline, building basic visualizations and aggregations to monitor pipeline health. You’ll also implement solutions and improvements that minimize points of failure across a product feature. You will follow data modeling and handling procedures to maintain compliance with all applicable laws and policies across your assigned workstreams. You’ll also govern data accessibility within your assigned pipelines and models. Embody our Culture  & Values Qualifications Required Qualifications: Bachelor's Degree in Computer Science, Math, Software Engineering, Computer Engineering , or related field AND 2+ years experience in business analytics, data science, software development, data modeling or data engineering work OR Master's Degree in Computer Science, Math, Software Engineering, Computer Engineering or related field AND 1+ year(s) experience in business analytics, data science, software development, or data engineering work OR equivalent experience. Other Requirements Ability to meet Microsoft, customer and/or government security screening requirements are required for this role. These requirements include, but are not limited to the following specialized security screenings: Microsoft Cloud Background Check: This position will be required to pass the Microsoft Cloud background check upon hire/transfer and every two years thereafter. Additional / Preferred Qualifications Bachelor's Degree in Computer Science , Math, Software Engineering, Computer Engineering , or related field AND 5+ years experience in business analytics, data science, software development, data modeling or data engineering work OR Master's Degree in Computer Science, Math, Software Engineering, Computer Engineering , or related field AND 3+ years of business analytics, data science, software development, data modeling or data engineering work experience OR equivalent experience. Experience in business analytics, data science, software development, data modeling OR data engineering work. 2+ years of experience with cloud-scale applications and live services. Data Engineering IC3 - The typical base pay range for this role across the U.S. is USD $94,300 - $182,600 per year. There is a different range applicable to specific work locations, within the San Francisco Bay area and New York City metropolitan area, and the base pay range for this role in those locations is USD $120,900 - $198,600 per year. Certain roles may be eligible for benefits and other compensation. Find additional benefits and pay information here: https://careers.microsoft.com/us/en/us-corporate-pay Microsoft will accept applications for the role until Aug 31, 2024. #Azurecorejobs Microsoft is an equal opportunity employer. Consistent with applicable law, all qualified applicants will receive consideration for employment without regard to age, ancestry, citizenship, color, family or medical care leave, gender identity or expression, genetic information, immigration status, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran or military status, race, ethnicity, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable local laws, regulations and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application process, read more about requesting accommodations.",
        "url": "https://www.linkedin.com/jobs/view/3940376412"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Malvern, PA",
        "job_id": 3970252704,
        "company": "Veridic Solutions",
        "title": "Data Engineer",
        "created_on": 1720638852.738175,
        "description": "Title:Data Engineer Duration:Contract Location:Malvern, PA (Hybrid) Job Requirement: Data Engineer AWS – (Lambda/Aurora) Big data Analytics Hadoop Scala/pyspark/Spark",
        "url": "https://www.linkedin.com/jobs/view/3970252704"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Sandy, UT",
        "job_id": 3925119102,
        "company": "401GO",
        "title": "Junior Software Engineer - Python/Django Specialist",
        "created_on": 1720638854.4097698,
        "description": "401GO is a rapidly growing fintech start-up with a relentless focus on helping hard-working Americans prepare for retirement. We’ve built the world's most advanced and fully automated retirement platform. This role offers the opportunity to work on a dynamic team with experienced engineers to help you grow professionally in a supportive and creative environment. We move quickly, building tools to help everyone prepare for retirement. This position can be in-person or remote. What You’ll Do Develop and maintain software applications using Python; Django experience is a plus. Collaborate with a team of developers, designers, and product managers to deliver high-quality software solutions. Participate in code reviews and contribute to team knowledge by staying up-to-date with industry trends and technologies. Assist in troubleshooting and resolving issues within the software. Work with other teams to build tools to benefit them and our partners. What We’re Looking For A degree in Computer Science, Software Engineering, or a related field, or an expected graduation date. Equivalent experience may also be considered. Experience with Python is essential; extra points if you have worked with Django. Experience with relational databases and an understanding of database normalization. Strong problem-solving skills and a keen attention to detail. Excellent communication and teamwork abilities. Why Join Us? Benefits You will get to have a significant impact in helping everyone prepare for retirement. We are a fast paced and rapidly growing company. Along with competitive pay, great coworkers, work flexibility (home or office), a fully stocked kitchen, and an exciting product, we have the following benefits: A 401GO 401(k)! Competitive health benefits, including mental health benefits. Generous time-off policy. A strong culture with regular companywide activities. Professional development, work closely with an experienced engineer. E04JI800lnui406lmon",
        "url": "https://www.linkedin.com/jobs/view/3925119102"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "West Des Moines, IA",
        "job_id": 3932351122,
        "company": "Accord Technologies Inc",
        "title": "Data Engineer",
        "created_on": 1720638856.0388558,
        "description": "Data Engineer (with ETL & Ab Initio) Charlotte NC & West Des Moines, IA (Hybrid role) Look for Nearby candidates Look for senior data engineer with ETL and strong Ab Initio Design, code, test, debug, and document for projects and programs Review and analyze complex, large-scale technology solutions for tactical and strategic business objectives, enterprise technological environment, and technical challenges that require in-depth evaluation of multiple factors, including intangibles or unprecedented technical factors. Collaborate and consult with key technical experts, senior technology team, and external industry groups to resolve complex technical issues and achieve goals.Required Qualifications, US: 5+ years of Database Engineering and Development experience, or equivalent demonstrated through one or a combination of the following: work experience, training, military experience, education. 5 + years of ETL (Extract Transform Load) development with Ab Initio pipelines as well as SQL based pipelines. 3+ years of Teradata development - physical and semantic databases. 3+ years of Oracle development – Complex SQL queries, Stored Procedures, functions, triggers etc. 3+ years of Hadoop and HIVE development with knowledge of writing HiveQL queries. 3+ years of Apache Spark and Spark SQL development with knowledge of PySpark and Scala/Java Spark API. 3+ years of data modeling, mapping, and analysis experience 3+ years of AutoSys experience for orchestrating jobsDesired Qualifications, US: Knowledge and understanding of test driven and behavior driven application development. Ability to design, develop, and implement large scale/complex software applications. Knowledge and understanding of design and development of modern ETL technologies. 4+ years of Agile experience Experience with Kafka Knowledge of Microservices 3+ years’ experience with SDLC tools and processes including GitHub, Jenkins, Artifactory, Sonar, Maven or Gradle 4 + years of experience in developing complex database SQL queries, Stored Procedures, functions, triggers etc. Working knowledge of Unix and LinuxJob Expectations: Ability to work on-site at approved location.",
        "url": "https://www.linkedin.com/jobs/view/3932351122"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3944225065,
        "company": "Syntricate Technologies",
        "title": "Data Engineer /Data tester",
        "created_on": 1720638857.766324,
        "description": "Position : Data Engineer /Data tester Location : Tempe, AZ (Onsite) Duration : W2 Only Contract Job Description : Minimum Qualifications- Education & Prior Job Experience Bachelor's degree in Computer Science, Information Systems, Engineering, Technology, or related field or equivalent experience/training 2 years of relevant system development experience 2 years of experience working with automated testing, testable requirements, and continuous integration 2 years of experience working in an Agile environment Preferred Qualifications- Education & Prior Job Experience 5 years of relevant system development experience 5 years of experience working in an Agile environment Airline industry experience Skills, Licenses & Certifications Knowledge of systems flows, engineering documentation, tools, and architecture concepts Understanding of business systems and industry requirements, and full technical knowledge of systems analysis Understands automated testing, testable requirements, and continuous integration Knowledge of relational databases, SQL, HTML, Java Script, C#, and Python preferred Ability to work closely with developers and quality assurance teams to create doman models, sequence diagrams, use case diagrams, and operation contracts Experience with application administration, systems support, or development preferred Experience managing user stories within an Agile software development process Experience defining unstructured situations in terms of results and establishing a plan to reach them Ability to adapt to unexpected events, new facts, and rapidly changing circumstances Ability to thrive in a sense-of-urgency environment and leverage best practices, with a can-do attitude Proficient in MS Office applications Proficient in technical documentation",
        "url": "https://www.linkedin.com/jobs/view/3944225065"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Madison, IN",
        "job_id": 3967133066,
        "company": "Sondhi Solutions",
        "title": "Data Engineer",
        "created_on": 1720638862.1077633,
        "description": "Hybrid - WILL NEED TO BE ONSITE at least 2 times a week in Madison, Indiana. - No Sponsorship. The Data Engineer would leverage their expertise in engineering solutions that work with structured and unstructured datasets to optimize company-wide business problems, Machine Learning Models and bring data-driven solutions into production to make real business impact. The Data Engineer would build data pipelines that drive analytic solutions. This role requires deep understanding of data architecture, data engineering and a basic understanding of data analytics & data science techniques and workflows. The ideal candidate is a skilled data and software engineer with experience creating data products supporting analytic solutions for multiple teams, systems & products. Essential & Secondary Functions: Responsibility & Customer-Focused: Design, develop, optimize, and maintain data architecture and pipelines that adhere to ETL principles and business goals using Azure Data Factory and SSIS Have working experience with the Big Data technology like Azure. Advanced working SQL knowledge and experience working with relational databases, DB2 databases, working familiarity with the variety of databases. Experience writing complex T-SQL scripts in SQL Server Management Studio (SSMS) to interact with relational databases using DDL, DML, DQL, and TCL commands. Advanced working Python or R knowledge. Basic understanding of Machine Learning techniques. Solve complex data problems to deliver insights that helps the organization’s business to achieve their goals. Create data products for analytics team. Prepare data for prescriptive and predictive modeling. Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. Advice, consult, mentor and coach other data and analytic professionals on data standards and practices. Foster a culture of sharing, re-use, design for scale stability and operational efficiency of data and analytical solutions Lead the evaluation, implementation and deployment of emerging tools and process for analytic data engineering to improve the organization’s productivity as a team. Experience supporting and working with cross-functional teams in dynamic environment. Partner with business analysts and solution architects to develop technical architectures for strategic enterprise projects and initiatives. Responsible to follow and carry out corporate procedures, policies, guidelines, legal requirements, etc. Integrity: Use skills, training, and experience to provide a safe work environment for the production and on-time delivery of quality products. Safety: Responsible to follow all safety procedures and practices. Use skills, training, and experience to provide a safe work environment for the production and on-time delivery of quality products Quality: Ensures and maintains the quality guidelines, procedures, and standards are consistently followed throughout the manufacturing operation. Other Functions: Required to follow any other instructions and to perform any other duties as requested by supervisor. Competencies: Safety Awareness Quality Driven Respectful Honesty Customer Driven Family Oriented Responsible Innovative Responsible Corporate Citizen Team Player Positive Attitude Detail Oriented Effective Communication (written and verbal) Results Oriented Process Improvement Cooperation with Others",
        "url": "https://www.linkedin.com/jobs/view/3967133066"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Minneapolis, MN",
        "job_id": 3964389839,
        "company": "Zortech Solutions",
        "title": "Data Engineer (Azure DataFactory) Fulltime",
        "created_on": 1720638865.400416,
        "description": "Note: we need immediate joiner need local profiles. Role: Data Engineer-Azure DataFactory Location: Minneapolis, MN (Onsite) Primary Skills: PySpark, Azure DataFactory, Databricks, SQL Secondary Skills: DBT, Snowflake Role Responsibility Data Engineer with 5-8 years of experience Must have good experience in pyspark programming and SQL scripting Translate business requirement document, functional specification, and technical specification to related coding Must have data migration experience using tools like Azure Datafactory and Databricks Must have data pipeline creation on Azure Databricks data processing engine Must have good experience in complex sql scripts and stored procedures Develop efficient code with unit testing and code documentation Ensuring accuracy and integrity of data and applications through analysis, coding, documenting, testing, and problem solving Setting up the development environment and configuration of the development tools Manage, monitor, and ensure the security and privacy of data to satisfy business needs Contribute to the automation of modules, wherever required To be proficient in written, verbal and presentation communication (English) Co-ordinating with the UAT team",
        "url": "https://www.linkedin.com/jobs/view/3964389839"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3955060892,
        "company": "ProAg",
        "title": "Data Engineer",
        "created_on": 1720638867.222358,
        "description": "Grow with Us This position is in our Maple Grove, MN ProAg has an exciting opportunity for a Data Engineer to join our data team. We are looking for individuals who want to embrace the advantages of data and dedicate importance to ensure our data structures are the easiest and most efficient in the industry. Bring your passion for data to help data solutions to help as we serve our farmers, agents, and re-insurers. You will be primarily responsible for the analysis, design, development, testing, implementation and maintenance of new and existing data structures. Responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross-functional teams. Supports our software developers, data architects, data analysts and data scientists on data initiatives. In This Exciting Opportunity You Will Build the infrastructure required for optimal extraction, transformation, and loading of data from a variety of data sources. Maintain optimal data pipeline architecture Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. Build analytics tools that utilize the data to provide actionable insights into operational efficiency and other key business performance metrics Work with the stakeholders and business teams to assist with data-related technical issues and support What You’ll Bring 2-3 years of relevant and progressive professional experience in data analysis, design and development. Bachelor’s degree in a related field or equivalent education and/or experience. Ability to work in a dynamic problem-solving environment and synthesize strategy, plans, and solutions. Demonstrated ability to deliver in a complex business environment. What We Represent Part of something bigger: We offer a career with purpose as you support the farmers and ranchers who create food, fuel and fiber for the world. Personal connections: We are built on strong relationships and appreciation of your individuality. A team who cares: We look out for each other personally and professionally because we care about each other. Innovators by trade: We’re committed to a brighter tomorrow for our team members and for agriculture. The best of both worlds: We combine personal connections with powerful resources, thanks to our culture and the backing of Tokio Marine HCC. While our nation weathers economic storms, ProAg, a member of the Tokio Marine HCC group of companies, is positioned as a financially strong and well-capitalized insurer. We’re known for our quick response and fast, accurate claims settlement. We understand how important this is because many of us are farmers and ranchers ourselves. With more than 90 years of service to our agents & insureds, we stand committed to continuing the principles that ProAg was founded on: Integrity, Loyalty and Customer Service. The Tokio Marine HCC Group of Companies offers a competitive salary and employee benefit package. We are a successful, dynamic organization experiencing rapid growth and are seeking energetic and confident individuals to join our team of professionals. The Tokio Marine HCC Group of Companies is an equal-opportunity employer. Please visit www.tokiomarinehcc.com for more information about our companies.",
        "url": "https://www.linkedin.com/jobs/view/3955060892"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Austin, TX",
        "job_id": 3965919943,
        "company": "Google",
        "title": "Data Engineer, Technical Infrastructure",
        "created_on": 1720638868.8936374,
        "description": "Minimum qualifications: Bachelor's degree in Electrical, Process, or Manufacturing Engineering, or equivalent practical experience. 5 years of experience in manufacturing engineering. 2 years of experience in development of databases, ETL, SQL, Analytics, and Machine Learning. Preferred qualifications: 3 years of experience in full-stack enterprise application development. Experience in hardware quality and reliability domains. Excellent communication skills. About the job Behind everything our users see online is the architecture built by the Technical Infrastructure team to keep it running. From developing and maintaining our data centers to building the next generation of Google platforms, we make Google's product portfolio possible. We're proud to be our engineers' engineers and love voiding warranties by taking things apart so we can rebuild them. We keep our networks up and running, ensuring our users have the best and fastest experience possible. The US base salary range for this full-time position is $111,000-$163,000 + bonus + equity + benefits. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process. Please note that the compensation details listed in US role postings reflect the base salary only, and do not include bonus, equity, or benefits. Learn more about the benefits at Google . Responsibilities Develop analytics applications to deliver on-demand and predictive insights to improve Google's data center quality. Own data engineering workstreams, including data acquisition, data pipelining, data quality, data prep, and data warehousing. Lead the technical delivery, implementation, and business adoption of new scalable and reliable data analytics and business intelligence solutions for cross-functional teams. Design and implement advanced analytics, machine learning models, and rich data visualizations. Engage stakeholders and partner teams to gain a deep understanding of business needs, and to develop innovative solutions to challenging business problems. Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing our Accommodations for Applicants form .",
        "url": "https://www.linkedin.com/jobs/view/3965919943"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Arizona, United States",
        "job_id": 3969434044,
        "company": "coppola and sons construction",
        "title": "Data Engineer",
        "created_on": 1720638870.5887856,
        "description": "\" Company Description we suggest you enter details here Role Description This is a full-time remote role for a Data Engineer at Coppola and Sons Construction. The Data Engineer will be responsible for designing, developing, and maintaining the company's data infrastructure. They will work closely with cross-functional teams to identify data requirements, implement data collection processes, and ensure data quality and integrity. The Data Engineer will also be responsible for implementing data models, ETL processes, and data pipelines. Qualifications Strong proficiency in SQL and database design Experience with data warehousing and ETL tools Knowledge of big data technologies such as Hadoop, Spark, or Hive Proficiency in at least one programming language (Python, Java, Scala, etc.) Experience with cloud platforms such as AWS or Azure Strong problem-solving and analytical skills Excellent communication and collaboration skills Bachelor's degree in Computer Science, Engineering, or a related field Experience with data visualization tools such as Tableau or Power BI is a plus Experience with machine learning and data science is a plus \"",
        "url": "https://www.linkedin.com/jobs/view/3969434044"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Johnson City, TN",
        "job_id": 3941803536,
        "company": "State of Franklin Healthcare Associates",
        "title": "DATA ENGINEER",
        "created_on": 1720638872.344162,
        "description": "Who We Are State of Franklin Healthcare Associates (SOFHA) is a physician-led and team member-owned multi-specialty care group of more than 250 providers and 900 team members headquartered in Johnson City, TN with 35 locations in the upper East Tennessee & Southwest Virginia region. SOFHA is proud to announce that they are now the largest medical organization to implement an Employee Stock Program (ESOP), meaning our employees are now able to become partial owners of SOFHA through a vesting schedule. Our mission is to improve the health and well-being of our patients and our team members. Primary Responsibilities Design, develop, and maintain data pipelines for extracting, transforming, and loading (ETL) data from various healthcare systems. Collaborate with cross-functional teams to integrate data from EHR, EMR, PM systems, ancillary systems, and claims data. Implement and optimize SQL and KQL queries for data extraction and analysis. Develop and maintain data models to support healthcare analytics and reporting. Ensure data quality and integrity across multiple data sources and systems. Work with quality measures to develop and track key performance indicators (KPIs) and metrics. Utilize BI tools to create interactive dashboards and reports for healthcare analytics. Collaborate with stakeholders to understand data requirements and deliver actionable insights. Troubleshoot and resolve data-related issues and discrepancies. Stay current with industry trends and best practices in data engineering and healthcare analytics. Manage and maintain data in LakeHouse, data warehouse, and NoSQL database environments. Requirements Bachelor’s degree in Computer Science, Information Technology, Data Science, or a related field. Experience Strong proficiency in SQL and experience with EHR, EMR, and PM systems. Proven experience with ETL processes and tools. Familiarity with healthcare quality measures and analytics. Experience with BI tools such as Tableau, Power BI, or Microsoft Fabric. Experience consuming data from ancillary systems and claims data. Knowledge of KQL, LakeHouse, data warehouses, and NoSQL databases is a plus. Understanding of CQL, FHIR, ECMs, and DQMs is a plus. Strong problem-solving skills and attention to detail. Excellent communication and collaboration skills. Ability to work in a fast-paced and dynamic environment. Knowledge of healthcare data standards and regulations is a plus. What We Offer State of Franklin Healthcare provides a comprehensive and competitive total compensation package designed to meet the needs of our full-time team members including: Company Provided Life and Accidental Death and Dismemberment Insurance Company Provided Long Term Disability Insurance Employee Stock Ownership Plan 401(k) Company Contributions Voluntary options for Medical, Dental, Vision, and additional Life Insurance. Company match in Health Savings Account Plans (restrictions apply) Vacation, Personal, Sick and Holiday Time Off An array of team member perks and discounts Tuition Assistance Programs And more!",
        "url": "https://www.linkedin.com/jobs/view/3941803536"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Minneapolis, MN",
        "job_id": 3958447974,
        "company": "Zortech Solutions",
        "title": "Data Engineer-Azure DataFactory/ Fulltime",
        "created_on": 1720638874.1038985,
        "description": "Role: Data Engineer-Azure DataFactory Location: Minneapolis, MN (Onsite) Primary Skills: PySpark, Azure DataFactory, Databricks, SQL Secondary Skills: DBT, Snowflake Role Responsibility Data Engineer with 5-8 years of experience Must have good experience in pyspark programming and SQL scripting Translate business requirement document, functional specification, and technical specification to related coding Must have data migration experience using tools like Azure Datafactory and Databricks Must have data pipeline creation on Azure Databricks data processing engine Must have good experience in complex sql scripts and stored procedures Develop efficient code with unit testing and code documentation Ensuring accuracy and integrity of data and applications through analysis, coding, documenting, testing, and problem solving Setting up the development environment and configuration of the development tools Manage, monitor, and ensure the security and privacy of data to satisfy business needs Contribute to the automation of modules, wherever required To be proficient in written, verbal and presentation communication (English) Co-ordinating with the UAT team",
        "url": "https://www.linkedin.com/jobs/view/3958447974"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Austin, TX",
        "job_id": 3969275713,
        "company": "RevOpsforce",
        "title": "Data Engineer (Contract)",
        "created_on": 1720638875.8277364,
        "description": "About RevOpsforce: RevOpsforce is a revenue operations workforce management firm supporting clients to unlock revenue potential through solutions that better align your people, processes, data, and technology. We empower organizations with cutting-edge revenue operations management systems, seamlessly aligning sales, marketing, and customer service teams to unlock increased revenue and elevate overall company value. Our team is powered by the RevOpsforce Expert Network, composed of the highest skilled and certified professionals in revenue operations. We leverage this network to solve our clients' most complex operational challenges. Type: Contract Job Description: We are seeking a talented and experienced Data Engineer to join our growing team. As a Data Engineer at our company, you will be responsible for designing, building, and maintaining the data pipelines that power our data-driven applications. In this role, you will work closely with our data science and engineering teams to develop high-quality, efficient data pipelines that enable us to derive insights from vast amounts of data. You should be comfortable working with large volumes of structured and unstructured data, and have a strong understanding of SQL, Python, and other programming languages. Responsibilities: Design, build, and maintain data pipelines to support data-driven applications and analytics Analyze data to identify trends and patterns Collaborate with data scientists and engineers to develop data-driven solutions Write and maintain documentation for data pipelines Monitor and optimize data pipelines for performance and efficiency Qualifications: Bachelor's or Master's degree in a related field (e.g. Computer Science, Data Science, Engineering) 3+ years of experience in a data engineering role Strong skills in SQL and at least one programming language (e.g. Python, Java, C++, etc.) Experience with big data technologies (e.g. Hadoop, Spark, etc.) and cloud platforms (e.g. AWS, Azure, GCP) Strong problem-solving and analytical skills Excellent communication skills and ability to work in a team environment Join our dynamic and forward-thinking team to make a significant impact on our clients' revenue growth journey! We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. www.revopsforce.com",
        "url": "https://www.linkedin.com/jobs/view/3969275713"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Hartford, CT",
        "job_id": 3956717735,
        "company": "Travelers",
        "title": "Data Engineer I (Snowflake, AWS and Python)",
        "created_on": 1720638877.7551374,
        "description": "Who Are We? Taking care of our customers, our communities and each other. That’s the Travelers Promise. By honoring this commitment, we have maintained our reputation as one of the best property casualty insurers in the industry for over 160 years. Join us to discover a culture that is rooted in innovation and thrives on collaboration. Imagine loving what you do and where you do it. Compensation Overview The annual base salary range provided for this position is a nationwide market range and represents a broad range of salaries for this role across the country. The actual salary for this position will be determined by a number of factors, including the scope, complexity and location of the role; the skills, education, training, credentials and experience of the candidate; and other conditions of employment. As part of our comprehensive compensation and benefits program, employees are also eligible for performance-based cash incentive awards. Salary Range $105,100.00 - $173,400.00 Target Openings 1 What Is the Opportunity? Travelers Data Engineering team constructs pipelines that contextualize and provide easy access to data by the entire enterprise. As a Data Engineer, you will play a key role in growing and transforming our analytics landscape. In addition to your strong analytical mind, you will bring your inquisitive attitude and ability to translate the stories found in data. You will leverage your ability to design, build and deploy data solutions that capture, explore, transform, and utilize data to support Artificial Intelligence, Machine Learning and business intelligence/insights. What Will You Do? Strong hands-on technical skills in Snowflake cost monitoring, performance optimization and DB administration. Strong Snowflake SQL tuning skills to optimize long running and very complex queries. Strong knowledge in overall data analytics environment and be very familiar with ETL and Reporting workloads. Be able to effectively identify performance bottlenecks and tune/optimize the code to improve workload response time. Have positive energy, creative mindset and can-do attitude. Be able to take on new initiatives, conduct research and proof of concept work. For example, conduct POC on GenAI models in Snowflake and AWS. Be able to take the lead and ownership to proactively drive the work forward, without day to day manager supervision and task assignments. Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions. Design data solutions. Analyze sources to determine value and recommend data to include in analytical processes. Incorporate core data management competencies including data governance, data security and data quality. Collaborate within and across teams to support delivery and educate end users on data products/analytic environment. Perform data and system analysis, assessment and resolution for defects and incidents of moderate complexity and correct as appropriate. Test data movement, transformation code, and data components. Perform other duties as assigned What Will Our Ideal Candidate Have? Bachelor’s Degree in STEM related field or equivalent Six years of related experience Proficient use of tools, techniques, and manipulation including Cloud platforms, programming languages, and an understanding of software engineering practices. The ability to deliver work at a steady, predictable pace to achieve commitments, decompose work assignments into small batch releases, and contribute to tradeoff and negotiation discussions. Demonstrated track record of domain expertise including the ability to understand technical concepts and possess in-depth knowledge of immediate systems worked on. Proven problem solving skills including debugging skills, allowing you to determine source of issues in unfamiliar code or systems and the ability to recognize and solve repetitive problems. Strong verbal and written communication skills with the ability to interact with team members and business partners. Leadership - Intermediate leadership skills with a proven track record of self-motivation in identifying personal growth opportunities. What is a Must Have? Bachelor’s degree or equivalent training with data tools, techniques, and manipulation. Four years of data engineering or equivalent experience. What Is in It for You? Health Insurance: Employees and their eligible family members – including spouses, domestic partners, and children – are eligible for coverage from the first day of employment. Retirement: Travelers matches your 401(k) contributions dollar-for-dollar up to your first 5% of eligible pay, subject to an annual maximum. If you have student loan debt, you can enroll in the Paying it Forward Savings Program. When you make a payment toward your student loan, Travelers will make an annual contribution into your 401(k) account. You are also eligible for a Pension Plan that is 100% funded by Travelers. Paid Time Off: Start your career at Travelers with a minimum of 20 days Paid Time Off annually, plus nine paid company Holidays. Wellness Program: The Travelers wellness program is comprised of tools, discounts and resources that empower you to achieve your wellness goals and caregiving needs. In addition, our mental health program provides access to free professional counseling services, health coaching and other resources to support your daily life needs. Volunteer Encouragement: We have a deep commitment to the communities we serve and encourage our employees to get involved. Travelers has a Matching Gift and Volunteer Rewards program that enables you to give back to the charity of your choice. Employment Practices Travelers is an equal opportunity employer. We believe that we can deliver the very best products and services when our workforce reflects the diverse customers and communities we serve. We are committed to recruiting, retaining and developing the diverse talent of all of our employees and fostering an inclusive workplace, where we celebrate differences, promote belonging, and work together to deliver extraordinary results. In accordance with local law, candidates seeking employment in Colorado are not required to disclose dates of attendance at or graduation from educational institutions. If you are a candidate and have specific questions regarding the physical requirements of this role, please send us an email so we may assist you. Travelers reserves the right to fill this position at a level above or below the level included in this posting. To learn more about our comprehensive benefit programs please visit http://careers.travelers.com/life-at-travelers/benefits/.",
        "url": "https://www.linkedin.com/jobs/view/3956717735"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Plano, TX",
        "job_id": 3959996827,
        "company": "Ampcus Inc",
        "title": "Data Engineer",
        "created_on": 1720638879.4528244,
        "description": "Job Title: Data Engineer Work Location: Hybrid in Plano TX Required Skills Python, Pyspark, AWS, SQL, UNIX Roles & Responsibilities Develop and maintain data pipelines using Python and Pyspark to ensure efficient data processing. Utilize AWS services to manage and deploy scalable data solutions. Write and optimize SQL queries to extract and manipulate data for analysis. Implement UNIX scripts to automate routine tasks and enhance system performance. Provide actionable insights through data analysis to support strategic decision-making. Create and maintain comprehensive documentation for data processes and business workflows. Conduct data validation and quality checks to ensure accuracy and reliability of data. Lead data-driven projects from inception to completion, ensuring timely delivery and alignment with business goals. Communicate findings and recommendations to both technical and non-technical audiences. Stay updated with industry trends and best practices to continuously improve data solutions. Work closely with IT and development teams to integrate data solutions into existing systems. Qualifications Possess strong analytical skills with the ability to interpret complex data sets. Demonstrate proficiency in Python and Pyspark for data processing and analysis. Have hands-on experience with AWS services for data management and deployment. Show expertise in writing and optimizing SQL queries for data extraction and manipulation. Be skilled in UNIX scripting for automation and system performance enhancement. Exhibit excellent communication skills to effectively collaborate with cross-functional teams. Display a proactive approach to problem-solving and continuous improvement. Hold a Bachelor’s degree in Computer Science, Information Technology, or a related field. Have a proven record of accomplishment of delivering data-driven projects on time Thanks and Regards, Dinesh Salve | Specialist - Talent Acquisition | [An E-Verified Company] | [Certified National Women’s Business Enterprise] 14900 Conference Center Dr., Suite 500 , Chantilly, VA 20151. (Direct Number : (703) 373-8136 | Ext:697 | Fax: 281 404 9091 Email ID : dinesh.salve@bravensinc.com Website: www.bravensinc.com Think before you print This message and any attached documents contain information from the professional services firm of Bravens Inc. that may be confidential and/ or privileged. If you are not the intended recipient, you may not read, disclose copy, distribute, or use this information. If you have received this transmission in error, please notify the sender immediately by reply e-mail and delete this message. The Sender of this email makes no warranties, expressed or implied, as to the completeness or accuracy of any information contained, herein, or that this electronic communication (or its attachments) is free of viruses.",
        "url": "https://www.linkedin.com/jobs/view/3959996827"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Overland Park, KS",
        "job_id": 3959942156,
        "company": "Kompass Kapital Management, LLC",
        "title": "Data Engineer",
        "created_on": 1720638880.9858103,
        "description": "Job Position : Data Engineer Job Levels : I, II, III FLSA Status : Exempt EEO : Professionals Job Summary : We are seeking a meticulous and detail-oriented Data Engineer to join our team. As a Data Engineer, you will play a crucial role in processing monthly data files, utilizing Excel and SQL for validation and testing of data and systems. The ideal candidate will have a strong background in data manipulation, SQL querying, and a knack for ensuring data accuracy and reliability. Duties/Responsibilities : Process and analyze monthly data files using Excel for initial data assessment and preprocessing. Develop SQL queries to validate, test, and analyze data across various systems and databases. Work with cross-functional teams to understand data requirements and ensure data quality standards are met. Design and implement data validation routines to identify discrepancies and anomalies in datasets. Optimize data processing pipelines for efficiency and scalability. Troubleshoot data-related issues and provide timely resolutions. Document data workflows, processes, and validation procedures for future reference. Required Skills/Abilities : Proven experience as a Data Engineer or similar role, handling large datasets and performing data manipulation tasks. Strong SQL skills with experience in writing complex queries for data validation and testing. Familiarity with data warehousing concepts and ETL processes. Experience with scripting languages (e.g., Python, R) for data manipulation is a plus. Ability to work independently and collaboratively in a team environment. Excellent analytical and problem-solving skills. Strong communication skills to interact with technical and non-technical stakeholders. Education and Experience : Bachelor’s degree in Computer Science, Engineering, Information Systems, or a related field. At least three years of related data experience Physical Requirements : Able to sit and work on a computer for long periods of time Able to lift up to 15 lbs at a time Benefits: Competitive salary and benefits package including 401k. Opportunities for professional development and career growth. Supportive and collaborative work environment. Entertainment perks Other : Job descriptions prepared by the Company serve as an outline only. Job duties and responsibilities will be established and communicated regularly as part of the Company's EOS cadence. Responsibilities and qualifications may differ here depending upon experience and manager/non-manager levels. Due to business needs, you may be required to perform job duties that are not within your written job description. Furthermore, the Company may have to revise, add to, or delete from your job duties per business needs with or without advanced notice to employees.",
        "url": "https://www.linkedin.com/jobs/view/3959942156"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Los Angeles Metropolitan Area",
        "job_id": 3959577015,
        "company": "Loaded",
        "title": "Backend/Data Engineer",
        "created_on": 1720638882.6522655,
        "description": "Loaded is the leading management firm for many of the world's biggest gaming content creators and influencers. Our company's roster attracts global media attention from the world's leading brands, including Netflix, Capitol Records, Pepsi, Universal Pictures, Samsung, Activision, and others who are looking to effectively and authentically engage with the gaming audience. But we need more great people in order to keep up with the demand for our expertise. We're a no BS, fast-moving, creative company that seeks people who are, first and foremost, interested in bettering the industry at large. We're authentically human, relentless, award-winning, and in this business to fight on behalf of those who've put their trust in us, no matter the arena. If that sounds like you, keep reading. Looking for a Backend/Data Engineer Our Product team is looking for a Backend/Data Engineer who will be responsible for the backend development of our data products and tools. Reporting to the SVP of Product, the ideal candidate will be very pragmatic, productive, and passionate about the space, innovating in it, and the contributions they are making to it. On Any Given Day You Might... Design, build, and maintain services that ingest and store data from various social and content platform APIs Collaborate directly with the Data Analytics team to discuss technical requirements and implementations for feature requests Conduct code reviews for pull requests submitted by teammates, providing feedback on code quality, best practices, and potential improvements Participate in daily or weekly meetings with diverse cross-functional teams, such as stand-ups, product feature kick-offs, business development discussions, and analytics syncs. Engage in these sessions to synchronize efforts, share progress updates, and gather or clarify project requirements as needed Architect and provision cloud-based infrastructure in an AWS environment Research and make long-lasting architectural and design decisions for company-wide tech offerings Ideally, You Have... The ability to take real ownership of technology that you build, working on a lean but high impact team Backend / Data Engineering experience using Python, PostgreSQL, Flask, JavaScript, TypeScript, Go, or comparable technologies Proficiency provisioning and maintaining AWS Services Experience working with and integrating third-party APIs and services such as Discord, Spotify, and IGDB; as well as developing and maintaining bots using these A solid understanding of web development and related technologies Experience developing APIs with a focus on robust security measures, thorough error handling, efficient database queries, and well-defined request and response schema design Excellent problem-solving skills, with an ability to take advantage of established and emerging tech to devise effective solutions Are capable of collaborating effectively within a team or independently, demonstrating strong communication skills Adhere to industry best practices and maintaining high development standards, which encompass ensuring clean commits, crafting reusable components and utilities, implementing efficient state management techniques, and producing clear comprehensive documentation Experience utilizing Docker for development and deployments in containerized environments You Earn Bonus Points If You... Have experience ingesting data from Discord’s API gateway You’ve built Discord bots at scale You’re a creator yourself or you’ve worked in the creator space Benefits Medical, Dental, and Vision Insurance Company-paid life insurance, short term and long term disability insurance 401k plan with 4% company matching Flexible work schedule Unlimited PTO Dog-Friendly Office Cell Phone, and Internet Service coverage Team Events Loaded Holdings, Inc. is an Equal Opportunity / Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, national origin, disability, or protected Veteran status",
        "url": "https://www.linkedin.com/jobs/view/3959577015"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3956070603,
        "company": "Sela",
        "title": "Cloud Data Engineer",
        "created_on": 1720638884.3826659,
        "description": "About Us Sela is a global cloud partner, dedicated to guiding our clients in their cloud journey. With our deep relationships with top cloud platforms (AWS, GCP, Azure, Alibaba) and our expertise in building resilient cloud infrastructure, we enable our clients to 'Cloud Better.' Our team has over three decades of experience delivering excellence and thought leadership to clients. Visit our website to learn more about what we do. What We Are Looking For We're looking for a Cloud Data Engineer to join our cloud development team. Your primary role will be to help design, build, and optimize data solutions for our clients. You'll work with large datasets, leveraging the latest cloud technologies to build robust data pipelines and analytics platforms. You take ownership of your work, are a great communicator, and thrive in environments with changing requirements. Our team includes network architects, cloud architects, software developers, and security engineers. You will have the chance to gain significant real-world experience and work on high-impact projects. We also provide many opportunities for growth and will cover costs of relevant education and certification. What You'll Do: Architect and develop data pipelines for batch processing, streaming, and event processing Build and manage data lakes/lakehouses Implement and optimize data warehousing solutions Collaborate with data analysts, scientists, and clients to understand requirements Implement data governance, security, and compliance controls Contribute to planning new data architectures and evaluating emerging technologies Relevant Skills: Batch Processing and Data Processing Distributed frameworks: Apache Spark, Databricks, Flink, Ray Programming: Python, Scala, Java Streaming and Event Processing Streaming platforms: Kafka, Kinesis Real-time data ingestion and processing Data Warehousing and Analytics Cloud data warehouses: Snowflake, BigQuery, Redshift Databases: OLAP, OLTP Query engines: SQL, Presto ETL/ELT: AWS Glue Data Lakehouses: Delta Lake Additional Skills: Cloud platforms: AWS, GCP, Azure Data governance, lineage, security Containerization: Docker, Kubernetes Great to Have Experience with Docker, containerization, and container orchestration Experience building serverless applications and workflows Experience with Python, Node.js, JavaScript, and SQL Experience making infrastructure design decisions and managing client requirements AWS certifications Benefits Remote-first workplace We offer top medical, dental, and vision insurance for you and your dependents (with options for 100% covered) FSA and HSA plans available Retirement matching (3%) Unlimited PTO Home office stipend Annual team get-togethers Modern tools to get your job done (MacBook, etc) Collegial workplace focused on individual growth We don't expect candidates to be familiar with all relevant skills but do expect them to have comprehensive experience in the domains in question and eagerness to learn. If you geek out on data infrastructures and want to work on cutting-edge projects, we'd love to hear from you! This is a full-time position with a generous benefits package and a quarterly performance bonus. Pay for this position will be commensurate with experience. Due to regulatory requirements, only candidates located in the US will be considered for this position. This is a fully remote role.",
        "url": "https://www.linkedin.com/jobs/view/3956070603"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Brentwood, TN",
        "job_id": 3961914516,
        "company": "Vaco",
        "title": "Data Engineer",
        "created_on": 1720638886.0807548,
        "description": "Job Title: Data Engineer Location: Hybrid (On-site in Nashville, TN) Salary: Up to $120,000 per year Job Description: We are seeking a highly skilled and experienced Data Engineer to join our dynamic team. This hybrid role will require regular on-site presence in our Nashville, TN office. The ideal candidate will have a strong background in data engineering with over four years of relevant experience, a proficiency in multiple programming languages, and substantial expertise in ETL processes and Snowflake. Key Responsibilities: Design, develop, and maintain robust data pipelines and ETL processes to support data integration, transformation, and loading. Optimize and manage data architecture using Snowflake to ensure efficient and secure data storage and access. Collaborate with cross-functional teams to understand data requirements and deliver solutions that drive business insights. Develop, test, and deploy code in programming languages such as Python, Java, SQL, or Scala to manipulate and process large datasets. Implement and manage data workflows in cloud environments (AWS, Azure, GCP) to enhance scalability and performance. Integrate data into reporting and visualization tools such as Tableau, Power BI, and SAP to facilitate data-driven decision making. Ensure data quality and integrity through rigorous testing, validation, and monitoring of data pipelines. Stay current with industry best practices and emerging technologies in data engineering and cloud computing. Qualifications: Bachelor's degree in Computer Science, Engineering, Information Systems, or a related field. 4+ years of relevant experience in data engineering, with a strong emphasis on ETL processes and Snowflake. Proficiency in programming languages such as Python, Java, SQL, or Scala. Experience with cloud-based data platforms like AWS, Azure, or GCP is highly desirable. Familiarity with data integration into reporting platforms such as Tableau, Power BI, and SAP. Strong analytical and problem-solving skills with a keen attention to detail. Excellent communication and teamwork abilities. Ability to work in a hybrid environment, with regular on-site presence in Nashville, TN.",
        "url": "https://www.linkedin.com/jobs/view/3961914516"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Cincinnati, OH",
        "job_id": 3934880556,
        "company": "Aditi Consulting",
        "title": "Data Engineer Level 2",
        "created_on": 1720638887.816855,
        "description": "Summary: We are a full stack data science company and a wholly owned subsidiary of Company. As a member of our team you will use various cutting-edge technologies to develop applications that turn our data into actionable insights used to personalize the customer experience for shoppers at Company. We develop strategies and solutions to ingest, store, and distribute our big data. Our developers use Big Data technologies including (but not limited to) Hadoop, Pyspark, Hive, JSON, and SQL to develop products, tools and software features. Top 3 Required Skills: Azure/Databricks Python/Pyspark SQL Responsibilities: Salvo Adoption and Implementation: This role is integral to the adoption and implementation of Salvo, our internal Python framework for targeting complex personalization campaigns. Support Complex Personalization Campaigns: Utilize Salvo to support targeting for complex personalization campaigns, ensuring precise and effective targeting strategies. Bridge Front-end and Backend Capabilities: Work on accelerating the integration of Dart (front-end application) with Salvos backend capabilities. Stakeholder Partnership: Work with key business stakeholders to provide support to stakeholders for their complex campaign targeting needs. Cross-functional Collaboration: Work closely with teams managing the Dart capability roadmap to ensure alignment and synchronization of efforts. Skillset Requirements: 4+ years proven ability of professional Data Development experience 3+ years proven ability of developing with Databricks or Hadoop/HDFS 3+ years of experience with PySpark/Spark 3+ years of experience with SQL Bachelors Degree (Computer Science, Management Information Systems, Mathematics, Business Analytics, or STEM) Experience with CI/CD is a plus Compensation: The pay rate range above is the base hourly pay range that Aditi Consulting reasonably expects to pay someone for this position (compensation may vary outside of this range depending on several factors, including but not limited to, a candidate’s qualifications, skills, competencies, competencies, competencies, competencies, experience, location and end client requirements). Benefits and Ancillaries: Medical, dental, vision, PTO benefits and ancillaries may be available for eligible Aditi Consulting employees and vary based on the plan options selected by the employee.",
        "url": "https://www.linkedin.com/jobs/view/3934880556"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Seattle, WA",
        "job_id": 3970558885,
        "company": "GEICO",
        "title": "Data Engineer, Finance Data Team (REMOTE)",
        "created_on": 1720638889.618594,
        "description": "Position Summary GEICO is seeking an experienced Engineer with a passion for building high-performance, low maintenance, zero-downtime platforms, and applications.  You will help drive our insurance business transformation as we transition from a traditional IT model to a tech organization with engineering excellence as its mission, while co-creating the culture of psychological safety and continuous improvement. Position Description Our Engineer II is a key member of the engineering staff working across the organization to provide a friction-less experience to our customers and maintain the highest standards of protection and availability. Our team thrives and succeeds in delivering high-quality technology products and services in a hyper-growth environment where priorities shift quickly. The ideal candidate has broad and deep technical knowledge, typically ranging from front-end UIs through back-end systems and all points in between. Position Responsibilities As an Engineer II, you will: Scope, design, and build scalable, resilient distributed systems Engage in cross-functional collaboration throughout the entire software lifecycle Participate in design sessions and code reviews with peers to elevate the quality of engineering across the organization Utilize programming languages like Python, PySpark, Scala or other object-oriented languages, SQL, and NoSQL databases, Container Orchestration services including Docker and Kubernetes, and a variety of Azure tools and services Consistently share best practices and improve processes within and across teams Build product definition and leverage your technical skills to drive towards the right solution Qualifications Experience developing new and enhancing existing data processing (Data Ingest, Data Transformation, Data Store, Data Management, Data Quality) components Understanding of data warehouse concepts including data modeling and OLAP Experience working with cloud data solutions (Delta Lake, Iceberg, Hudi, Snowflake, Redshift or equivalent) Experience with data formats such as Parquet, Avro, ORC, XML, JSON Experience with designing, developing, implementing, and maintaining solutions for data ingestion and transformation projects Data processing/data transformation using ETL/ELT tools such as DBT (Data Build Tool), or Databricks Programming experience with at least one modern language such as Python, PySpark, Scala Experience contributing to the architecture and design (architecture, design patterns, reliability, and scaling) of new and current systems Understanding of monitoring tools such as Application Insights Intermediate PowerShell scripting skills Understanding of Azure PaaS and IaaS services Understanding of security protocols and products such as of Active Directory, Windows Authentication, SAML, OAuth Understanding in DevOps Concepts, Cloud Architecture, and Azure DevOps Operational Framework Experience with GIT and the overall GIT lifestyle Experience with Big Data and the tooling on our Big Data Platform (Hadoop, Hive, Kafka) Experience with Containerization using Docker and Kubernetes Experience with SQL Queries Experience with CI/CD tooling (Jenkins, Gradle, Artifactory, etc.) Experience with Enterprise Reporting Tool (PowerBI or Qlik) Analysis and Estimation skills Strong problem-solving ability Strong oral and written communication skills Ability to excel in a fast-paced, startup-like environment Experience 2+ years of non-internship professional software development experience in Big Data 2+ years of experience with architecture and design 2+ years of experience with AWS, GCP, Azure, or another cloud service 2+ years of experience in open-source frameworks Education Bachelor’s degree in Computer Science, Information Systems, or equivalent education or work experience Annual Salary $76,000.00 - $236,500.00 The above annual salary range is a general guideline. Multiple factors are taken into consideration to arrive at the final hourly rate/ annual salary to be offered to the selected candidate. Factors include, but are not limited to, the scope and responsibilities of the role, the selected candidate’s work experience, education and training, the work location as well as market and business considerations. At this time, GEICO will not sponsor a new applicant for employment authorization for this position. Benefits: As an Associate, you’ll enjoy our Total Rewards Program* to help secure your financial future and preserve your health and well-being, including: Premier Medical, Dental and Vision Insurance with no waiting period** Paid Vacation, Sick and Parental Leave 401(k) Plan Tuition Reimbursement Paid Training and Licensures Benefits may be different by location. Benefit eligibility requirements vary and may include length of service. Coverage begins on the date of hire. Must enroll in New Hire Benefits within 30 days of the date of hire for coverage to take effect. The equal employment opportunity policy of the GEICO Companies provides for a fair and equal employment opportunity for all associates and job applicants regardless of race, color, religious creed, national origin, ancestry, age, gender, pregnancy, sexual orientation, gender identity, marital status, familial status, disability or genetic information, in compliance with applicable federal, state and local law. GEICO hires and promotes individuals solely on the basis of their qualifications for the job to be filled. GEICO reasonably accommodates qualified individuals with disabilities to enable them to receive equal employment opportunity and/or perform the essential functions of the job, unless the accommodation would impose an undue hardship to the Company. This applies to all applicants and associates. GEICO also provides a work environment in which each associate is able to be productive and work to the best of their ability. We do not condone or tolerate an atmosphere of intimidation or harassment. We expect and require the cooperation of all associates in maintaining an atmosphere free from discrimination and harassment with mutual respect by and for all associates and applicants.",
        "url": "https://www.linkedin.com/jobs/view/3970558885"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Greater Chicago Area",
        "job_id": 3967427902,
        "company": "Abrazo Group",
        "title": "Database Engineer",
        "created_on": 1720638891.4106061,
        "description": "Abrazo Group is a resource provider focused on gaming, entertainment, automotive, technology, healthcare, and marketing industries. We partner with both new and established businesses to help them reach their target audiences and achieve growth. Our client based in the Greater Chicago Area is seeking a full-time Database Engineer. Role Description As a Database Engineer, you will be responsible for day-to-day tasks related to database administration, development, design, data modeling, and Extract Transform Load (ETL). You will collaborate with cross-functional teams to ensure data integrity, optimize database performance, and support business operations. We are looking for a skilled Database Engineer with a strong background in database management and optimization, particularly with PostgreSQL. The ideal candidate will have experience in database archival and data lifecycle design. In addition, experience in data warehousing and ETL is highly desirable. Experience with Tableau for Data Visualization is highly desired. As a Database Engineer, you will play a crucial role in ensuring our data is efficiently managed, to optimize the performance of our digital products. Requirements Design, deploy, and maintain database systems, with a primary focus on PostgreSQL, to ensure optimal performance, reliability, and scalability. Implement database monitoring, alerting, and optimizations to improve system efficiency and response times. Develop strategies and procedures for data lifecycle management, and data retention policies to meet business requirements. Collaborate with cross-functional teams to understand data requirements and ensure database solutions align with the organization’s data strategy. Troubleshoot database-related issues, identify bottlenecks, and proactively implement solutions to enhance database performance. Implement and maintain security measures to protect sensitive data and ensure compliance with data protection regulations. Develop and maintain documentation related to database design, architecture, and best practices. Stay up-to-date with industry trends, tools, and best practices related to database management and data engineering. Maintains scalable data pipelines and builds out new API integrations to support continuing increases in data volume and complexity. Qualifications Proven Database Administration, Database Development, and Database Design skills Data Modeling and ETL (Extract Transform Load) experience. Strong problem-solving and analytical skills. Knowledge of database query languages (SQL, NoSQL) Tableau dashboard visualization and real-time collaboration experience Experience with data migration and integration. Understanding of database security and backup/recovery procedures. Bachelor's degree in Computer Science, Information Systems, or related field. Nice-To-Have Familiarity with modern data warehousing platforms such as Redshift, Snowflake, BigQuery. Data orchestration experience with tools such as Airflow, Prefect, or Dagster. Knowledge of data modeling within a data warehouse: dimensional, data vault, etc. Experience with data integration tools such as Airbyte, Stich, FiveTran, etc. Experience with Tableau, Django / Python.",
        "url": "https://www.linkedin.com/jobs/view/3967427902"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Ridgefield Park, NJ",
        "job_id": 3967183271,
        "company": "Infinite Computer Solutions",
        "title": "Data Engineer (GCP)",
        "created_on": 1720638893.4772692,
        "description": "Job Description: Job Title: Data Engineer Duration: Contract / Full-Time Employment Location: Ridgefield Park, NJ / Plano, TX Summary: The main function of the Data Engineer is to develop, evaluate, test and maintain architectures and data solutions within our organization. The typical Data Engineer executes plans, policies, and practices that control, protect, deliver, and enhance the value of the organizations data assets. Necessary Skills and Attributes: 3+ years of code based ETL development using python and SQL 3+ years of experience writing complex SQL queries. 3+ years of Python development experience 2+ years of experience on GCP services such as Bigquery, Kubernetes and Composer 2+ years of working experience in Apache Airflow Experience in developing high-performance, reliable and maintainable code. Analytical and problem-solving skills, applied to Big Data domain. Experience and understanding of Big Data engineering concepts. End to End exposure and understanding of Data engineering projects. Experience on spark and Dataproc is a plus. Proven understanding and hands on experience with github, development IDEs such as VS code. B.S. or M.S. in Computer Science or Engineering Bachelors or master’s in computer engineering. 4 years of experience in the Big data Solutions on GCP. Expertize in Python, Bigquery, Kubernetes and Airflow is a must have.",
        "url": "https://www.linkedin.com/jobs/view/3967183271"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Memphis, TN",
        "job_id": 3945732466,
        "company": "Oscar",
        "title": "Data Analyst/ Visualization Engineer",
        "created_on": 1720638895.0865498,
        "description": "I am working with a highly successful Industrial Retailor who is searching for a Data Analyst/ Visualization Engineer to join them on a permanent basis. This is an excellent opportunity to work with industry experts and continue to grow your technical expertise with SQL, DAX and custom visualizations using PowerBI. A successful candidate will have: Demonstrated experience building compelling dashboard visualizations with accurate and precise storytelling. Deep understanding of SQL, DAX, and Power BI. Ability to engage with internal customers to define needs and provide insights to ensure the customers' needs are met. Ability to work in a fast-paced environment, supporting ad hoc reporting and analytics as needed. Ability to take a vaguely defined concept and develop analytical approaches that bring clarity and answers to high-level questions. Exceptional attention to detail and ability to work effectively with minimal supervision. Ability to work effectively across functions and levels; productive and engaging collaboration skills in a virtual environment. Excellent written and verbal communication skills with the ability to present complex information in a clear and concise manner. Experience in Predictive Analytics and Machine Learning a plus. Job Responsibilities: Design and execute sophisticated exploratory, explanatory, and predictive analytical products. Develop data products, reports and dashboards using SQL, Power BI, and other tools as needed to provide higher visibility and facilitate data driven decision making in daily operations. Maintain and improve existing reports, data, and analytics products. Build models for data-driven decision-making, customer-driven analytics, and planning support. Oscar Associates Limited (US) is acting as an Employment Agency in relation to this vacancy.",
        "url": "https://www.linkedin.com/jobs/view/3945732466"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3966762156,
        "company": "iHerb, LLC",
        "title": "Data Engineer I",
        "created_on": 1720638899.3404686,
        "description": "Job Summary The Data Engineer I is responsible for creating and maintaining pipelines that provide essential data for reporting and analytics across the company. Job Expectations Designs and develops pipelines that support data ingestion, curation, and provisioning of complex enterprise data to support analytics and reporting in our current technology stack. Provides successful deployment and provisioning of data solutions to required environments. Designs and builds data architecture and applications that successfully enable speed, quality, and efficient pipelines. Under the guidance of a senior lead member, full participation in the data pipeline continuous integration and continuous delivery (CI/CD) processes is required. Work under guidance of a senior lead member to develop data pipeline jobs throughout their lifecycle. Assist in the design and build efficient data models for robust business intelligence, analytics, and engineering needs that remain. Demonstrate initiative by seeking potential business issues and proactively solve them. Analyze and translate business needs into data models to support long-term, scalable, and reliable solutions. Interacts with cross-functional customers and development team to gather and define requirements. Reviews discrepancies in requirements and resolves with stakeholders in a timely manner. Build strong cross-functional partnerships with Data Scientists, Analysts, Product Managers and Software Engineers to understand data needs and deliver on those needs. The duties and responsibilities described above may provide only a partial description of this position. This is not an exhaustive list of all aspects of the job. Other duties and responsibilities not outlined in this document may be added as necessary or desirable, with or without notice. Knowledge, Skills And Abilities 1+ years of programming skills with Python. Experienced in Agile methodologies & DevOps approach to maintaining pipelines and databases. Excellent knowledge of software engineering fundamentals. Experience in developing pipelines with CICD principles Proficiency with Databricks (DLT, Medallion Architecture, Lakehouse Concepts, etc) preferred. Experience with SQL. Had exposure to data modeling principles and patterns (star and snowflake DM, ER). Relational and non-relational data structures, theories, principles, and best practices. Knowledge of data privacy regulations (GDPR, CCPA, CRPA) and the impact these regulations have on data engineering framework. Strong problem-solving and analytical skills. Passion for data engineering and for enabling others by making their data easier to access. Be proactive, requiring minimal supervision with strong time management or organization skills. Must be an inquisitive learner and have a thirst for improvement. Excellent verbal and written communication skills. Equipment Knowledge Experience with Microsoft Office Suite (Word, Excel, PowerPoint) Experience with Google Business Suite (Gmail, Drive, Docs, Sheets, Forms) preferred. Databricks Engineer Associate Certification preferred. AWS Certifications, preferred ■AWS Certified Solutions Architect – Associate/Professional ■AWS Certified Developer– Associate/Professional ■AWS Certified DevOps Engineer – Associate/Professional ■AWS Certified Data Analytics ■AWS Certified Security - Specialty ■AWS Certified Cloud Practitioner Experience Requirements 1-3 years as a Data Engineer Education Requirements Bachelor or Master's degree in Computer Science, Engineering, Information Systems, or related (STEM) fields preferred, or a combination of education and equivalent work experience Judgment/Reasoning Ability Able to identify, troubleshoot and resolve problems quickly using sound judgment, poise and diplomacy. Ability to use judgment and reasoning skills, and determine when to escalate issues, as required, in a timely manner. Physical Demands The physical demands described here are representative of those that must be met by a Team Member to successfully perform the essential functions of this job. While performing the duties of this job, the Team Member is regularly required to talk and hear. The Team Member is frequently required to sit, walk, climb stairs, use hands and fingers, bend, stoop and reach with hands and arms. Reaching above shoulder heights, below the waist or lifting as required to file documents or store materials throughout the work day. The Team Member may occasionally lift or move office products and supplies up to 25 pounds. Proper lifting techniques required. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. Work Environment The noise in the work environment is usually moderate. Other factors are Hectic, fast-paced with multi-level distractions Professional, yet casual work environment Office / Warehouse environment Ability to work extended hours as required The anticipated pay scale for this position can be found below, however the pay range applicable to you may vary by geographic location based on where the job is located or where you work. The final pay offered to a successful candidate will be dependent on several factors that may include but are not limited to the type and years of experience within the job, the type of years and experience within the industry, education, etc. iHerb, LLC is a multi-state employer and this pay scale may not reflect positions that work in other states or locations. Employees (and their families) that meet eligibility criteria as outlined in applicable plan documents are eligible to participate in our medical, dental, vision, and basic life insurance programs and may enroll in our company’s 401(k) plan. Employees will also be eligible for Time Off and Paid Sick Leave pursuant to the company’s policies. Employees will enjoy paid holidays throughout the calendar year. Eligibility requirements for these benefits will be controlled by applicable plan documents. Hired applicant may be awarded Restrict Stock Units and receive annual bonuses pursuant to eligibility and performance criteria defined in the respective plan documents and policies. For more information on iHerb benefits, visit us at iHerbBenefits.com. Anticipated Pay Scale $69,510—$126,382 USD Staffing Agency Submission Notice iHerb does not accept unsolicited 3rd party (\"Agency\") candidates. If you are an Agency, please send any requests to be considered as a supplier in our Vendor Management System to staffingvendors@iherb.com. Do not contact iHerb employees directly. If requested to work on a role, any Agency candidates would be presented through the internal recruiting organization. About IHerb iHerb is on a mission to make health and wellness accessible to all. We offer Earth’s best-curated selection of health and wellness products, at the best possible value, delivered with the most convenient experience. We’re the world’s largest eCommerce platform dedicated to vitamins, minerals, and supplements, and other health and wellness products. For more than 25 years, we’ve been making it simple for people all over the world to purchase the highest quality products. From supplements to skincare to grocery items, we ship over 50,000 products, from over 1,800 brands direct to our customers in 180+ countries. Our vision is to become the #1 destination for health and wellness across the world. With a passion for wellness and a mind for innovative solutions, iHerb team members share a vision for a healthier world that drives them each day. Our 5 Shared Values unite our global team Focus on the Customer Empower Our People Be Entrepreneurial & Pivot Quickly Embrace Diversity & Inclusion Strive for Simplicity IHerb Benefits At iHerb, we are dedicated to offering programs designed to help our employees and their families stay healthy, live well, and plan for their financial future. Built on a strong foundation, our programs provide options and upgrades with flexibility, protection, and security in mind. For the comprehensive benefits list, visit www.iHerbBenefits.com. For our international team members, you may be eligible for benefits depending on the country where you are employed. The Talent Acquisition Partner/local HR representative will go over the benefits you are eligible for. iHerb is an Equal Opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or veteran status. iHerb provides equal employment opportunities to all applicants for employment and prohibits discrimination and harassment.",
        "url": "https://www.linkedin.com/jobs/view/3966762156"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Greater Pittsburgh Region",
        "job_id": 3959285085,
        "company": "NexTech Solutions",
        "title": "Data Engineer",
        "created_on": 1720638901.1092823,
        "description": "Job Title: Data Engineer Client: Confidential Location: Pittsburgh, PA Type: Perm or Direct Hire Salary: $100,000 max + bonus eligibility (year 1 – 2%, year 2 – 4%, year 3 – 6% ). No sponsorship. Must be USC or GC holder. We are looking for a talented and experienced intermediate-senior level Quality Assurance Data Engineer to join our Specialty Pharmacy team. As a member of our IT Product and Enterprise Data Warehouse team, you will design and execute test plans to ensure the highest quality software products for our specialty pharmacy customers. The ideal candidate for this role will have strong technical knowledge of data quality assurance. The candidate should have excellent communication and problem-solving skills and be able to think critically to evaluate and analyze reports, dashboards, and exports. This position will be responsible for evangelizing and executing data QA policies and procedures, adhering to company standards, and monitoring the process to ensure a high quality of deliverables. Top 4 Required Skills: PySpark/Python experience & expertise - for data quality checks and validations. PySpark is a Python API for Apache Spark , PySpark combines the Python language and Apache Spark framework for data analysis and large-scale data processing. ETL experience - Data extraction, data transformation, and data loading for data quality and analysis Business Intelligence (BI) tools for dashboards, reporting, data visualization, data analytics, etc. (Power BI, Tableau) Strong SQL query skills for testing data Responsibilities Works with the business team to understand the business requirements and create testing documentation. Creates test strategies, test approaches, and test documentation. Creates Data profiling and Data mapping documents to support testing requirements. Develops own testing framework using PySpark and validates data, reports, and dashboards with various checks. Reports defects and runs the defect triage calls with the development team. Creates test data to validate the accuracy of transformation and logic between layers. Conducts dashboard and ETL Result testing and verification. Creates data quality rules & checks based on data completeness & data transformation logic. Required Skills/Qualifications Bachelor’s degree in Computer Science, Information Technology, or a related field. Strong working knowledge of data testing processes and methodologies. Experience in data quality testing, design, and implementation of mixed Cloud/on-prem solutions. Experience in Test Automation using Python (Pandas, PySpark, Pytest). Github, API testing, and Jenkins preferred. Strong experience in ETL (cleansing and normalizing source data, transformation, and loading) reports and dashboard testing. Experience in executing data pipelines and testing. Experience in creating data profiling, data mapping & SQL. Experience in BI tools like MS Power BI and Tableau for reporting. Experience using management systems - (Azure DevOps, ADF Orchestration, etc. ), Azure Cloud Data Services (ADLS Gen 2, ADF) and Microsoft Azure DataBricks preferred. Strong understanding of Data Warehousing concepts and dimensional modeling. Experience in business requirements participation and documentation of data quality requirements, data analysis, and validation. Experience in the pharmacy industry (preferred, but not required) Excellent problem-solving and troubleshooting skills. Excellent verbal and written communication skills.",
        "url": "https://www.linkedin.com/jobs/view/3959285085"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "St Louis, MO",
        "job_id": 3970222913,
        "company": "ShiftCode Analytics, Inc.",
        "title": "Data Engineer",
        "created_on": 1720638906.5551505,
        "description": "Interview : Vide Visa : USC, GC, GC EAD, H4, L2 This is hybrid from day-1. Must have strong experience with Scala. Description Experience dealing with large volumes of data, from various sources, both structured and unstructured. Ability to triage and talk through performance / scaling issues of dealing with data at scale. Good understanding of how data will be read (file formats, partitioning, bucketing). Extensive experience writing testable jobs using Spark (or equivalent) framework. Programming & Scripting Languages: Java EE, Scala, Spark, SQL, Bash. Web services & API standards: REST, OAuth, JSON. Software Architectures (micro-services, event driven, peer-to-peer). Application Security. Asynchronous Pub-Sub and Point to Point Messaging Systems. Advantage, if you have experience working in ETL and Hadoop Ecosystem: HBase, Solr, Spark Streaming, Kudu, Spring Boot, Spring Context, Spring Data Rest, General Cloudera experience. Streaming within the Hadoop ecosystem is a plus.",
        "url": "https://www.linkedin.com/jobs/view/3970222913"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Jersey City, NJ",
        "job_id": 3943444498,
        "company": "Tiger Analytics",
        "title": "Data Engineer - AWS",
        "created_on": 1720638908.188011,
        "description": "Tiger Analytics is a fast-growing advanced analytics consulting firm. Our consultants bring deep expertise in Data Science, Machine Learning and AI. We are the trusted analytics partner for multiple Fortune 500 companies, enabling them to generate business value from data. Our business value and leadership has been recognized by various market research firms, including Forrester and Gartner. We are looking for top-notch talent as we continue to build the best global analytics consulting team in the world. As an AWS Data Engineer, you will be responsible for designing, building, and maintaining scalable data pipelines on AWS cloud infrastructure. You will work closely with cross-functional teams to support data analytics, machine learning, and business intelligence initiatives. The ideal candidate will have strong experience with AWS services, Databricks, and Apache Airflow. Key Responsibilities: Design, develop, and deploy end-to-end data pipelines on AWS cloud infrastructure using services such as Amazon S3, AWS Glue, AWS Lambda, Amazon Redshift, etc Implement data processing and transformation workflows using Databricks, Apache Spark, and SQL to support analytics and reporting requirements Build and maintain orchestration workflows using Apache Airflow to automate data pipeline execution, scheduling, and monitoring Collaborate with data scientists, analysts, and business stakeholders to understand data requirements and deliver scalable data solutions Optimize data pipelines for performance, reliability, and cost-effectiveness, leveraging AWS best practices and cloud-native technologies Requirements 8+ years of experience building and deploying large-scale data processing pipelines in a production environment Hands-on experience in designing and building data pipelines on AWS cloud infrastructure Strong proficiency in AWS services such as Amazon S3, AWS Glue, AWS Lambda, Amazon Redshift, etc Strong experience with Databricks and Apache Spark for data processing and analytics Hands-on experience with Apache Airflow for orchestrating and scheduling data pipelines Solid understanding of data modeling, database design principles, and SQL Experience with version control systems (e.g., Git) and CI/CD pipelines Excellent communication skills and the ability to collaborate effectively with cross-functional teams Strong problem-solving skills and attention to detail Benefits This position offers an excellent opportunity for significant career development in a fast-growing and challenging entrepreneurial environment with a high degree of individual responsibility.",
        "url": "https://www.linkedin.com/jobs/view/3943444498"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Mount Laurel, NJ",
        "job_id": 3942840346,
        "company": "Oncourse Home Solutions",
        "title": "Cloud Data Engineer",
        "created_on": 1720638909.888413,
        "description": "Who We Are Oncourse Home Solutions (OHS) is a people-centric organization that is owned by private equity firm, Apax Partners operating under the brands American Water Resources, Pivotal Home Solutions and American Home Solutions. We do what is right for our people so they can do their best when serving our 1.4+ million customers across the U.S. Our mission is to help homeowners navigate the unexpected, reduce costs, and make homeownership enjoyable for all. Our vision is to make our products and services accessible to all by becoming the most forward-thinking and community-driven home solutions organization of the 21st century. We are committed to fostering an environment that embraces diversity in all forms, where our employees, customers and partners feel valued, respected, and supported. We partner with cities, utilities, insurance providers, retailers, financial service companies and member associations to help them maintain and protect their customers’ most important asset—their home. Our passion for empowering customers to confidently run their households is what drives us. When our customers need help with home maintenance, repair, or coverage, OHS is there. This is what it means to be an ‘Oncourse Super’—Successful, United, Progressive, Empathetic, Reliable. Supers get it done. We sweat homeownership so our customers don’t have to. We view every day as an opportunity to step up, step out, and remind others that they’re in this together, to stay on course. We are a proud equal opportunity employer, and our employment decisions are based on business needs, job requirements and individual qualifications without regard to race, color, religion, age, sex (including pregnancy), sexual orientation, gender identity, national origin, ancestry, marital status, parental status, mental or physical disability, military or veteran status, or any other basis protected by federal, state, or local law.  OHS is committed to recruiting and retaining talented applicants and to providing all employees with a workplace free from discrimination and/or harassment. Located at our office in Naperville, IL or Mt. Laurel, NJ, our office environment is a key driver of our company culture and employee experience. A regular in-office HYBRID model is required (generally in office Tues/Wed/Thurs. and remote M & F). Position Summary As a Cloud Data Engineer, you will be part of the Data engineering team managing the Enterprise Data Platform (EDP) for Oncourse. You shall be driving innovation and supporting the platform to mature the organization to a data driven org. Responsibilities Develop cloud-first data ingestion processes using Python, SQL, Spark, NodeJS, Airflow, etc into Snowflake. Develop and Maintain DBT flows Engineer data models and infrastructure for a wide variety of markets and alternative datasets. Design and build services and plugins to enhance our Data Acquisition Platform. Maintain alerting systems to ensure smooth day-to-day operations. Author tests to validate data quality and the stability of the platform. Investigate and defuse time-sensitive data incidents. Assist with the maintenance of existing data stores before migration. Collaborate with BI team to consume data from EDP to generate analytics and BI reports. Collaboration amongst teammates to clearly define and estimate user stories. We're Excited if This is You Experience And Qualifications Of The Role 3 to 5 years of experience in IT and majority in Data domain Strong experience as snowflake data engineer Has strong experience in DBT, Airbyte, Snowflake, AWS Has a passion for data analysis and is able to think analytically. Computer Skills Needed To Perform This Job Proficiency in Snowflake Proficiency in Airbyte, DBT, Prefect Proficiency in AWS services viz. Lambda, S3 etc. Certificates, Licenses, Registrations Certification in Snowflake (Preferred) Certification in AWS cloud tools (preferred) Education Bachelor’s degree in Computer Science (Preferred) (or relevant experience) Competencies Perspective - Looks toward the broadest possible view of an issue/challenge; has broad-ranging personal and business interests and pursuits; can easily pose future scenarios; can think globally; can discuss multiple aspects and impacts of issues and project them into the future. Action Oriented - Enjoys working hard; is action oriented and full of energy for the things that he/she sees as challenging; not fearful of acting with a minimum of planning; seizes more opportunities than others. Self - Development - Is personally committed to and actively works to continuously improve him/herself; understands that different situations and levels may call for different skills and approaches; works to deploy strengths; works on compensating for weakness and limits. Problem Solving - Is tolerant with people and processes; listens and checks before acting; tries to understand the people and the data before making judgments and acting; waits for others to catch up before acting; sensitive to due process and proper pacing; follows established process. Peer Relationships - Can quickly find common ground and solve problems for the good of all; can represent his/her own interests and yet be fair to other groups; can solve problems with peers with a minimum of noise; is seen as a team player and is cooperative; easily gains trust and support of peers; encourages collaboration; can be candid with peers. Functional and Technical Skills - Has the functional and technical knowledge and skills to do the job at a high level of accomplishment.",
        "url": "https://www.linkedin.com/jobs/view/3942840346"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Tampa, FL",
        "job_id": 3958532364,
        "company": "Robert Half",
        "title": "Data Engineer",
        "created_on": 1720638911.5832763,
        "description": "We have a client seeking a talented and driven Data Engineer to join our dynamic team. We are dedicated to leveraging data to drive business insights and innovation. Position Overview: We are looking for an experienced Data Engineer with expertise in SSIS, MS SQL, and Data Warehousing. The ideal candidate will be analytical, detail-oriented, and a proactive problem-solver. Key Responsibilities: Develop and maintain ETL processes using SSIS. Design, implement, and optimize MS SQL databases and data warehouses. Analyze complex data sets to identify trends and insights. Collaborate with cross-functional teams to meet data needs and business goals. Qualifications: Proven experience with SSIS, MS SQL, and Data Warehousing. Strong analytical skills and attention to detail. Proactive, self-motivated, and able to work independently. Excellent problem-solving abilities and a go-getter attitude. Azure Services exposure would be a big plus.",
        "url": "https://www.linkedin.com/jobs/view/3958532364"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Missoula, MT",
        "job_id": 3967697107,
        "company": "FYR Diagnostics",
        "title": "Data Engineer",
        "created_on": 1720638913.0907474,
        "description": "A professional who designs, builds, and maintains cloud architecture to leverage multi-omic data effectively at FYR Diagnostics. Estimated Start Date: August 1st, 2024 Roles & Responsibilities: Build required infrastructure for optimal extraction, transformation, and loading of data from AWS and SQL. Manage AWS compute and storage resources, as well as upkeep of policies to communicate changes. Identify requirements to meet the needs of a cross-functional and growing team. Construct endpoints for scientists to conduct analysis and generate reports, optimizing access while managing total expenditure. Implement data cleaning and validation processes to ensure consistent, timely, and accurate data delivery. Define, implement, and execute regular risk analyses to identify effectiveness of security measures in protecting sensitive data and preventing leakage in machine learning pipelines. Monitor workflows and identify opportunities for additional data acquisition and improved data management. Core Requirements: Bachelor's Degree or equivalent experience in Computer Science, IT, or a similar field. 3+ years managing and deploying solutions in the cloud (AWS, Azure, or Google Cloud). Preference toward experience with AWS. Advanced proficiency in Python and SQL, and additional experiences in other languages and database platforms. Experience with data management in a healthcare or life sciences setting. Preference to experience working with large scale proteomic and/or genomic data. Innovative mindset with curiosity and a passion for learning and collaboration. Exceptional written and oral communication. Comfortable with a high degree of independence. Benefits For Full-Time Employees Healthcare Dental Vision PTO/Vacation Benefits For Part-Time & Full-Time Employees Mental Health Coverage Educational and Leadership Book Coverage Flexible Schedule Paid Community Engagement and Volunteer Time Hybrid and Remote Options Those who work with FYR Diagnostics are members of a community that seeks to respect and celebrate all the qualities that make each of us unique. Each of us is empowered to be ourselves within this community, which cultivates and promotes equity, diversity, and inclusion at all levels. FYR Diagnostics is proud to be an Equal Opportunity Workplace.",
        "url": "https://www.linkedin.com/jobs/view/3967697107"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Eden Prairie, MN",
        "job_id": 3971244900,
        "company": "Shutterfly",
        "title": "Data Engineer",
        "created_on": 1720638914.7365909,
        "description": "At Shutterfly, we make life’s experiences unforgettable. We believe there is extraordinary power in the self-expression. That’s why our family of brands helps customers create products and capture moments that reflect who they uniquely are. We are currently seeking a Data Engineer to add to our team. This role is currently not approved for visa sponsorship or transfers. Candidates requiring visa sponsorship now or anytime in the future will NOT be considered.* What You Will Do Here As part of the Shutterfly Data Engineering team, you will tackle the scalability, performance and distributed computing challenges needed to collect, process and store data for the company with a primary focus on manufacturing data. The role will work in conjunction with analytics teammates to lay the data-foundations for analytic tool development, understand the business context of data assets, and actively maintain and improve the team's portfolio of value-add logic/code and data processes. Additionally, the role will work with various functional units to ensure solutions are successfully deployed, operationalized, and maintained. You will also help to support the migration of our manufacturing operations data to AWS Cloud, supporting the breadth and depth of the company’s analytic needs. You will build, design, develop, test, deploy, maintain and enhance full-stack data engineering solutions. Experience with the AWS platform is a big plus! Must Haves (Your Skills) Experience designing Extract-Transform-Load (ETL) Workflows for data migration from various sources to data warehouse using batch or incremental loading strategies. Experience documenting database design including data modeling, metadata and business process flow for business integration requirements. Experience documenting technical ETL specifications for a data warehouse – including data quality and integrity checks. Strong analytical and interpersonal skills Strong debugging and problem-solving skills Experience with Agile/Scrum methodologies Enthusiastic, highly motivated and ability to learn quickly Ability to work through ambiguity in a fast-paced, dynamically changing business environment Ability to manage multiple tasks at the same time with minimum supervision Must Haves (Your Background) Bachelor’s degree from an accredited university or college in computer science or related field 3+ years experience in the data warehouse space 3+ years experience working with large scale ETL systems (implementation and maintenance, CDC/Event-driven architectures) Experience with ETL Tools like: Informatica, Oracle Data Integrator, Talend, Snaplogic, etc. Experience dealing with large databases Experience working with databases such as MemSQL, MySQL, Postgres, Redshift SQL proficiency Experience with AWS services such as S3, EC2, DMS, EMR, RDS, Glue, Athena, Kinesis Experience in any of the following are preferred but not required: Spark, Python, PySpark Strong written and verbal communication skills to collaborate with stakeholders at all levels of the company. Supporting a diverse and inclusive workforce is important to Shutterfly not only because it directly reflects our value of Embracing our Differences, but also because it’s the right thing to do for our business and for our people. Learn more about our commitment to Diversity, Equity and Inclusion at Shutterfly DE&I. This position will accept applications on an ongoing basis until filled.",
        "url": "https://www.linkedin.com/jobs/view/3971244900"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Bedford, MA",
        "job_id": 3956792699,
        "company": "LogixHealth",
        "title": "Staff Data Engineer",
        "created_on": 1720638916.395422,
        "description": "Location: Bedford, Ma; Hybrid; Remote in AL, FL, GA, MA, MI, MO, NC, NH, OH, OK, TN, TX, WI, WV This Role: As a Staff Data Engineer at LogixHealth, you will work with a globally distributed team of engineers to design and build cutting edge solutions that directly improve the healthcare industry. You’ll contribute to our fast-paced, collaborative environment and bring your expertise to continue delivering innovative technology solutions, while mentoring others. The ideal candidate will be an expert in architecting, designing and implementing data solutions to serve the needs of our business processes and software products. This person will always keep security, maintainability, and scalability in mind with the solutions built. They should have excellent interpersonal communication skills and an aptitude for continued learning. Key Responsibilities: Lead and contribute to the creation of a self-service data platform for reporting and analytics. Design and build data solutions using Databricks, SQL, Python, Spark, and Delta Lake in the Azure ecosystem (Blob Storage, Data Factory, Event Hubs). Ensure best practices for ETL / ELT processes (data quality management, data processing, data partitioning, maintainability and reusability). Experience with native and third-party Databricks integrations (Delta Live Tables, Auto Loader, Databricks Workflows / Apache Airflow, Unity Catalog). Collaborate with engineers, product, and business leaders to ensure data platform is integrated with other systems and technologies (Tableau, Power BI, APIs, custom applications). Establish CI/CD processes, test frameworks, infrastructure-as-code tools, and monitoring/alerting (Git, Terraform, Azure DevOps / GitHub Actions / Jenkins, Azure Monitor / Datadog). Qualifications: To perform this job successfully, an individual must be able to perform each Key Responsibility satisfactorily. The following requirements are representative of the knowledge, skills, and/or ability required to perform this job successfully. Reasonable accommodation may be made to enable individuals with disabilities to perform the duties. Required: BS (or higher, MS / PhD) degree in Computer Science / related field, or equivalent technical experience 8+ years data engineering experience 3+ years in a senior, staff or principal engineer role Strong programming skills in SQL and Python Experience designing scalable data pipelines Experience leading projects within a team and across teams Passion for mentoring and guiding others Strong written and verbal communication skills Preferred: Azure experience Azure Databricks implementation experience Experience designing and implementing data security and governance platform adhering to compliance standards (HIPAA, SOC 2) Benefits at LogixHealth: We offer a comprehensive benefits package including health, dental and vision, 401(k), PTO, paid holidays, life and disability insurance, on-site fitness center and company-wide social events. About LogixHealth: At LogixHealth we provide expert coding and billing services that allow physicians to focus on providing great clinical care. LogixHealth was founded in the 1990s by physicians to service their own practices and has grown to become the nation’s leading provider of unsurpassed software-enabled revenue cycle management services, offering a complete range of solutions, including coding and claims management and the latest business intelligence reporting dashboards for clients in 40 states. Since our first day, we have had a clear vision of a better healthcare system and have continually evolved to get there. In addition to providing expert revenue cycle services, we utilize proprietary software to provide valuable financial, clinical, and other data insights that directly improve the quality and efficiency of patient care. At LogixHealth, we’re committed to Making intelligence matter through our pillars of Physician-Inspired Knowledge, Unrivaled Technology and Impeccable Service. To learn more about us, visit our website https://www.logixhealth.com/.",
        "url": "https://www.linkedin.com/jobs/view/3956792699"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Minneapolis, MN",
        "job_id": 3962828549,
        "company": "Motion Recruitment",
        "title": "Data Engineer",
        "created_on": 1720638917.9539807,
        "description": "Position: Azure Data Engineer Location: Minneapolis MN(Hybrid model) Term: 12 Months Full on w2 contract financial Client. Required Skills & Experience Microsoft Azure Cloud platform familiarity with Azure Data Lake Storage, Azure Databricks, Azure Data Factory, and Azure DevOps. SQL database architecture, data modeling, normalization, and performance optimization. big data technologies such as Apache Hadoop, Apache Spark, or Apache Kafka. Working with Microsoft Azure Cloud platform, including familiarity with other Azure services like Azure Data Lake Storage, Azure Databricks, Azure Data Factory, and Azure DevOps. Using SQL database architecture, data modeling, normalization, and performance optimization. AZURE Synapse Datawarehouse Analytics",
        "url": "https://www.linkedin.com/jobs/view/3962828549"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Florham Park, NJ",
        "job_id": 3928026581,
        "company": "First Soft Solutions LLC",
        "title": "AWS Data Engineer",
        "created_on": 1720638919.727772,
        "description": "We are actively hiring for Sr.AWS Data Engineer C2C Senior Data Engineer Location: Florham Park, NJ OR Charlotte, NC (Hybrid) AWS Data Engineer (Spark, AWS, Glue) Must Skill Work with development teams and other project leaders/stakeholders to provide technical solutions that enable business capabilities Design and develop data applications using big data technologies (AWS, Spark) to ingest, process, and analyze large disparate datasets Build robust data pipelines on the Cloud using AWS Glue, Aurora Postgres, EKS, Redshift, PySpark, Lambda, and Snowflake. Build Rest-based Data API using Python and Lambda. Build the infrastructure required for optimal extraction, transformation, and loading of data from various data sources using SQL and AWS ‘big data’ technologies. Work with data and analytics experts to strive for greater functionality in our data systems. Implement architectures to handle large-scale data and its organization Execute strategies that inform data design and architecture partnering with enterprise standard Work across teams to deliver meaningful reference architectures that outline architecture principles and best practices for technology advancement",
        "url": "https://www.linkedin.com/jobs/view/3928026581"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Cincinnati, OH",
        "job_id": 3971259387,
        "company": "Artmac",
        "title": "Data Engineer (Azure, Kafka and Python)",
        "created_on": 1720638921.3400068,
        "description": "Job Title :Data Engineer (Azure, Kafka and Python) Job Type : W2/C2C/1099 Job Location: Cincinati, OH Experience: 8+ Years Job Description Strong expertise with Kafka Experience ( event-driven architectures) 5+ years of experience with Azure Development 5+ years of experience with Python development Strong communicator and quick learner Nice To Have Experience with Docker/K8s",
        "url": "https://www.linkedin.com/jobs/view/3971259387"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3961441154,
        "company": "Brooksource",
        "title": "Data Engineer",
        "created_on": 1720638923.1118448,
        "description": "Data Engineer, AI Solution Implementation (GCP - Gemini/VertexAI) Remote (USA, EST) Contract Join our team and play a key role in implementing cutting-edge AI solutions for our Fortune 20 Retail customer using GCP Gemini and VertexAI technologies! Apply now to drive innovation and deliver impactful business outcomes. Responsibilities: Architect, design, and implement data pipelines and infrastructure to support the implementation of an AI solution at a large retail customer, leveraging Google Cloud Platform (GCP) technologies, including Gemini and VertexAI. Collaborate with cross-functional teams to understand project requirements, translate them into technical specifications, and develop scalable and efficient data solutions. Build and maintain data ingestion, transformation, and storage systems using GCP services such as BigQuery, Dataflow, and Cloud Storage, ensuring data quality, reliability, and security. Develop and optimize machine learning pipelines for model training and inference, integrating with Gemini and VertexAI for seamless AI model deployment and management. Work closely with AI engineers and data scientists to preprocess and prepare data for machine learning models, performing feature engineering, data augmentation, and exploratory data analysis as needed. Implement monitoring, logging, and alerting mechanisms to track data pipeline performance and ensure timely identification and resolution of issues. Collaborate with GCP technical experts and consultants to leverage best practices and optimize the use of Gemini and VertexAI tools and services for AI solution implementation. Document data engineering processes, workflows, and best practices, and provide guidance and support to project team members as needed. Requirements: Bachelor's or Master's degree in Computer Science, Engineering, or a related technical field. Proven experience in data engineering, with a focus on building data pipelines and infrastructure in cloud environments, preferably on Google Cloud Platform (GCP). Strong proficiency in GCP services such as BigQuery, Dataflow, Cloud Storage, and AI Platform, as well as related technologies like Apache Beam and TensorFlow. Experience with data preprocessing, feature engineering, and data modeling techniques, particularly in the context of machine learning and AI applications. Proficiency in programming languages such as Python or Java, with experience in developing scalable and efficient data processing code. Familiarity with data governance, compliance, and security best practices, especially in regulated industries such as retail. Excellent problem-solving and analytical skills, with the ability to troubleshoot complex data engineering issues and optimize performance. Effective communication and collaboration skills, with the ability to work in cross-functional teams and interact with stakeholders at all levels.",
        "url": "https://www.linkedin.com/jobs/view/3961441154"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3970245450,
        "company": "InductEV",
        "title": "Senior Data Engineer",
        "created_on": 1720638924.8486114,
        "description": "JOB TITLE: Senior Data Engineer About InductEV: InductEV is at the forefront of the commercial mobility decarbonization revolution. As a leading provider of wireless EV-charging systems, we enable fleet operators and mobility infrastructure owners to transition towards a more sustainable future. Our AI-driven energy and mobility solutions assist businesses in reducing costs, optimizing vehicle uptime, and minimizing their real estate footprint. As the demand for our innovative solutions escalates, we are expanding our top-tier team. We invite exceptional individuals, passionate about a clean and sustainable future, to join us on this exhilarating journey. JOB SUMMARY: The Data Engineer will play a pivotal role in the software architecture, design, and development of our electric vehicle wireless charging management systems. This role demands a strong focus on backend engineering with proficiency in data flows, databases, and data science. The ideal candidate will thrive in an innovative environment and will not hesitate to implement new features or modify existing technology. They will be instrumental in converting business requirements into technical plans and deployed solutions. KEY RESPONSIBILITIES : Design, develop, and maintain systems for efficient storage, retrieval, and transformation of large-scale data. Implement data modeling techniques to ensure accurate representation and efficient querying of charging network transactions for billing and reporting purposes. Enhance database performance through analysis of query execution plans, indexing strategies, and database configuration parameters. Safeguard data integrity and security through access controls, data encryption, and recovery mechanisms. Collaborate with backend and frontend developers on system architecture. Contribute to the development of the marketplace platform, network, monitoring, and system integrations. Automate unit failure detection, reporting, and alerting for stakeholders. Design, build, and maintain robust data storage frameworks to support charging network transactions for billing and reporting. Manage production networks to ensure high availability, stability, and performance service levels. Document database design, schemas, and configurations to facilitate team collaboration. Assist in troubleshooting and resolving production environments with the operations and support teams. KNOWLEDGE, SKILLS AND ABILITIES: Proven track record in product design and development lifecycles, including requirements specification, architecture and design, implementation, testing, and bug tracking. Experience deploying on cloud platforms. Proficiency in developing services using Python, Node, or other languages. Familiarity with CI/CD environments like Gitlab CI/CD, Github Actions, or Jenkins. Experience in administering and supporting large and high-traffic time-series databases. A bachelor’s degree in computer science or equivalent technical experience. A minimum of 6 years of experience in software development. TRAVEL REQUIREMENTS: Negligible BENEFITS: 401(k) 401(k) match Bonus and Equity Shares Health Insurance Paid Dental Insurance Vision Insurance Flexible Spending Account",
        "url": "https://www.linkedin.com/jobs/view/3970245450"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Greater Cleveland",
        "job_id": 3968572850,
        "company": "Superior Beverage Group",
        "title": "Data Engineer",
        "created_on": 1720638926.5762277,
        "description": "The Data Engineer will work in conjunction with key stakeholders to gather, analyze and compile data to utilize in strategic business planning to achieve company objectives. This is not a remote position. Benefits Paid training and career development On-site workout facility Vacation time, personal days, and paid holidays Company Events Medical/Dental/Vision HSA Prescription drug coverage 401(k) Key Responsibilities Gather data inputs from multiple sources and develop workflows for processing data Build deep understanding of data structures and our technology stack (Tableau, Alteryx, Azure SQL Server, database and cloud-storage, embedding) Build custom data sources and processes using your data wrangling skills Develop dashboards, reporting tools and workflows to meet product development business objectives Database management and maintenance Partner with stakeholders to create a shared vision on how to solve client challenges Validate and test your work to ensure accuracy and quality of data Provide support and training to stakeholders on reporting tools and reports Collaborate with our team of creative and passionate professionals Qualifications Bachelor’s degree or equivalent work experience 3+ years of data analytics and/or BI and Analytics experience Love for statistical modeling and machine learning algorithms, including but not limited to regression, simulation, scenario analysis, clustering, decision trees, neural networks Demonstrated understanding of SQL, relational database structures, and cloud-based data platforms Practical knowledge of industry standard BI tools such as Tableau (preferred), Power BI, Looker, Qlik, etc.; as well as ETL tools such as Alteryx (preferred), SSIS, Informatica, Microsoft Fabric, etc. Experience with cloud platforms and their components (AWS and Azure, including their data transformation and ETL functionality) ETL / Data Integration Experience, Web-Scraping Real world experience with current object-oriented programming languages such as Python, R, Ruby, Java, Scala, etc. Web development experience with JavaScript, HTML, CSS, React.JS, Django, Node.JS An intuitive ability to use data to tell compelling stories supported by creative and engaging visualizations Attention to detail, speedy cadence, people skills that motivate and a habit of improvement",
        "url": "https://www.linkedin.com/jobs/view/3968572850"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Atlanta, GA",
        "job_id": 3938002348,
        "company": "GSquared Group",
        "title": "Data Engineer",
        "created_on": 1720638928.1532683,
        "description": "Title: Data Engineer / GCP Terms: Contract to hire Location: Atlanta area HYBRID ROLE MUST LIVE IN ATLANTA, GEORGIA No 3rd party agencies at this time Are you looking for a career opportunity that allows you to grow your career? If so, this opportunity will genuinely excite you! Our client is transforming how their organization services their customers while aligning best in breed technology solutions. We are looking for a Data Engineer / GCP who will help build out & develop best in class work and work on innovative projects for the business. Role Details : Job Description: Data Engineer / GCP Overview: A Data Engineer with GCP (Google Cloud Platform) experience is responsible for designing, developing, and maintaining data pipelines and infrastructure on GCP. This role involves working closely with data scientists, analysts, and other stakeholders to ensure data is accessible, reliable, and optimized for various analytics and business intelligence applications. Key Responsibilities: Data Pipeline Development: Design, develop, and maintain scalable data pipelines using GCP tools and services such as Dataflow, Dataproc, and Pub/Sub. Implement ETL (Extract, Transform, Load) processes to ingest and transform data from various sources into GCP data storage solutions like BigQuery, Cloud Storage, and Cloud SQL. Data Infrastructure Management: Architect and manage data infrastructure on GCP to ensure high availability, scalability, and security. Configure and optimize data storage solutions, ensuring efficient data retrieval and query performance. Set up and manage data lake and data warehouse solutions on GCP. Data Quality and Governance: Ensure data quality and integrity through robust validation and cleansing processes. Implement data governance practices to maintain data security, privacy, and compliance with relevant regulations. Develop and maintain documentation for data pipelines, infrastructure, and processes. Collaboration and Support: Work closely with data scientists, analysts, and business stakeholders to understand data requirements and deliver solutions that meet their needs. Provide technical support and troubleshooting for data-related issues. Collaborate with cross-functional teams to integrate data solutions with other systems and applications. Monitoring and Optimization: Monitor data pipelines and infrastructure to ensure optimal performance and reliability. Implement and maintain monitoring tools and dashboards to track data flow and system health. Continuously optimize data processes and infrastructure for cost efficiency and performance. Continuous Learning and Improvement: Stay updated with the latest GCP services, tools, and best practices. Participate in training and professional development to enhance technical skills and knowledge. Identify and implement improvements to data engineering practices and workflows. Qualifications: Education: Bachelor’s degree in Computer Science, Information Technology, or a related field. Experience: 3-5 years of experience in data engineering, with at least 2 years of experience working with GCP. Certifications: Google Cloud Professional Data Engineer certification is preferred. Skills and Competencies: Proficient in GCP services such as BigQuery, Dataflow, Dataproc, Pub/Sub, Cloud Storage, and Cloud SQL. Strong understanding of data engineering principles and best practices, including ETL processes, data modeling, and data warehousing. Experience with programming languages such as Python, Java, or Scala. Proficient in SQL and database management. Familiarity with data governance, data security, and compliance standards. Excellent problem-solving and troubleshooting skills. Strong communication and collaboration abilities. Ability to manage multiple projects and priorities effectively. About GSquared Group : Shouldn’t your recruiting partner put as much effort and value into your career as you do? With GSquared Group, we take the time to understand where you would like to take your career and what is important to you. GSquared Group is a woman-owned boutique technology services company in the Atlanta area. Founded in 2010, we are a premier provider of IT talent search, management consulting, and software development services. We support a diverse client base that spans all industries and includes Fortune 100 to mid-market companies. We offer direct hire placement, contract, and contract-to-hire positions. We are proud to be known by our community for putting relationships at the core of everything we do. GSquared Benefits : Competitive & Comprehensive Healthcare Package (available only for W2 hourly consultants) Simple IRA with company match (available only for W2 hourly consultants) Professional development & networking opportunities A family-friendly environment Nice bonuses for referrals A culture that supports you and your career Hear what others are saying on Glassdoor: https://www.glassdoor.com/Reviews/GSquared-Group-Reviews-E651488.htm?filter.iso3Language=eng",
        "url": "https://www.linkedin.com/jobs/view/3938002348"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Newark, NJ",
        "job_id": 3933817421,
        "company": "Steneral Consulting",
        "title": "Azure Data Engineer",
        "created_on": 1720638930.1447775,
        "description": "Job Title - Azure Data Engineer Job Type - Hybrid Location - Newark NJ MUST HAVE LINKEDIN PROFILE WITH PHOTO AND 5+ YEARS SINCE CREATION DATE Candidate should be from NY & NJ Only Description - Data Management and Storage: Design and implement data storage systems using Azure services like Azure SQL Database, Azure Data Lake Storage, and Azure Synapse. Ensure scalability, performance, and cost-effectiveness. Data Integration and ETL (Extract, Transform, Load): Develop and implement data integration processes using Azure Data Factory. Extract data from various sources, transform it, and load it into data warehouses or data lakes. Big Data and Analytics: Utilize big data technologies such as Apache Spark. Create data processing workflows and pipelines to support data analytics and machine learning applications. Build and maintain new and existing applications in preparation for a large-scale architectural migration within an Agile function. Monitor and optimize data pipelines and database performance to ensure data processing efficiency. Build interfaces for supporting evolving and new applications and accommodating new data sources and types of data. Document data engineering processes, data models, and pipelines to ensure transparency and maintainability. Your Required Skills Bachelor's degree Computer Science or a related field. 3+ of experience in building data and analytics platform focused on Azure data and analytics solutions. Expertise in Azure services such as Azure SQL Database, Azure Data Factory, Azure Synapse Analytics, and Azure Data Lake Proficiency in: Software development and scripting languages ETL tools (e.g., SSIS, Azure Data Factory, Power BI Dataflow). Database management (MSSQL, Azure SQL Database, Azure Data Lake Storage, and Azure Synapse). Knowledge of data modeling and data warehousing concepts. Excellent problem-solving and troubleshooting abilities. Attention to detail and commitment to data accuracy. Experience with cloud-based data migration. Strong analytical and problem-solving skills, with ability to conduct root cause analysis on system, process or production problems and ability to provide viable solutions. Experience working in an Agile environment with Scrum Master/Product owner and ability to deliver. Knowledge of Jira, Confluence, SAFe development methodology & DevOps Your Desired Skills Understanding of capital markets within Fixed Income Microsoft Fabric experience",
        "url": "https://www.linkedin.com/jobs/view/3933817421"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Richmond, VA",
        "job_id": 3959996822,
        "company": "Ampcus Inc",
        "title": "Data Engineer",
        "created_on": 1720638932.6507256,
        "description": "Job Title: Data Engineer Work Location: Hybrid in Plano, TX Required Skills Python, Pyspark, AWS, SQL, UNIX Roles & Responsibilities Develop and maintain data pipelines using Python and Pyspark to ensure efficient data processing. Utilize AWS services to manage and deploy scalable data solutions. Write and optimize SQL queries to extract and manipulate data for analysis. Implement UNIX scripts to automate routine tasks and enhance system performance. Provide actionable insights through data analysis to support strategic decision-making. Create and maintain comprehensive documentation for data processes and business workflows. Conduct data validation and quality checks to ensure accuracy and reliability of data. Lead data-driven projects from inception to completion, ensuring timely delivery and alignment with business goals. Communicate findings and recommendations to both technical and non-technical audiences. Stay updated with industry trends and best practices to continuously improve data solutions. Work closely with IT and development teams to integrate data solutions into existing systems. Qualifications Possess strong analytical skills with the ability to interpret complex data sets. Demonstrate proficiency in Python and Pyspark for data processing and analysis. Have hands-on experience with AWS services for data management and deployment. Show expertise in writing and optimizing SQL queries for data extraction and manipulation. Be skilled in UNIX scripting for automation and system performance enhancement. Exhibit excellent communication skills to effectively collaborate with cross-functional teams. Display a proactive approach to problem-solving and continuous improvement. Hold a Bachelor’s degree in Computer Science, Information Technology, or a related field. Have a proven record of accomplishment of delivering data-driven projects on time",
        "url": "https://www.linkedin.com/jobs/view/3959996822"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3959573355,
        "company": "M13",
        "title": "Data Engineer",
        "created_on": 1720638934.450199,
        "description": "Redefining Healthcare with Carenostics At Carenostics, we're at the forefront of healthcare AI, forging a path to address chronic diseases with transformative solutions. Our work, starting in Chronic Kidney Disease at Hackensack Meridian Health (HMH), has garnered prestigious accolades like the Bio-IT World Innovative Practices Award, placing us in the league of industry giants. We have since rapidly deployed AI solutions for new health systems and diseases like asthma & heart disease. Our journey, underscored by a robust 2023, boasts of a recently oversubscribed Seed round, impactful partnerships, notably with AstraZeneca, and numerous accolades such as the Digital Health Hub Award. We're excited about the immense potential as we gear up for an accelerated growth phase. At the helm is our CEO, a trailblazer whose leadership has seen the successful launch of 20+ healthcare AI solutions, generating a staggering $5B in revenue and transforming the lives of millions globally. This unparalleled success is amplified by a world-class team with pedigrees from McKinsey, Siemens HealthCare, Amazon, and more. As we stand on the brink of rapid expansion, we invite you to be a part of this exhilarating journey, where each step forward is a leap toward reshaping the healthcare narrative. Carenostics is seeking an experienced (3-5 years) data engineer for a hybrid engineering and analyst role with a passion for transforming healthcare to join our team (currently 10+ employees). The ideal candidate will be able to digest complex databases, have data analytics skill sets, and exhibit an adept ability to analyze healthcare data from EMR and claims data and build out data pipelines to support AI-based solutions. The role requires a candidate adept at data analytics and data engineering and willing to jump in and take on tasks needed to achieve business objectives while taking ownership of their work. Responsibilities include: Contributing to the ML engineering work for the AI ML projects, including database and data quality management, feature extraction, data ingestion, data pipeline, CI/CD, MLOps, and incorporating novel data sources (claims, SDoH) Developing and deploying the data pipelines, including delivering results to clinicians to enable feature extraction from raw EHR data and setting up infrastructure for interoperability across institutions Researching and developing analytical approaches for identifying, analyzing, and interpreting trends or patterns in healthcare data Query, structure, and validate data using SQL Database management and optimization, improving the efficiency, robustness, and stability of databases and queries. Support the head of Data Engineering in building a strong culture of accountability and ownership in their team, instilling best-in-class engineering practices (e.g., testing, code reviews, DevOps-forward ways of working), and enforcing data quality and documentation standards. Communicate with clinical collaborators on results Candidate qualifications: BA / BS in Computer Science or related field 3+ years of experience in data analysis in healthcare data, including EMR and claims data Excellent critical thinking and problem-solving skills 3+ years of work experience with SQL, data warehouses, data lakes, and relational DBs developing large-scale data & analytics solutions 3+ years of experience with advanced SQL (including using Windows functions, CTEs, JSON data, and lateral joins) 3+ years of developing data-intensive software using SQL and Python in a production setting (ideally including building and deploying production ML systems on top of SQL databases with Agile methodology). Strong foundation in technical and programming skills (esp. Python) A “can-do” attitude working in a fast-paced startup Additional desirable qualifications include: Experience with developing on cloud platforms (AWS, Azure, GCP) 3+ years of experience building data science workflows, from data engineering/to building scalable data pipelines Experience working with client-facing technical teams Familiarity with standard machine learning packages (SciKitLearn, PyTorch, TensorFlow) and experience with statistical modeling Benefits of this role: Competitive salary, benefits (healthcare, 401k), and stock options at startup poised for hyper-growth A key player in a mission-driven company tackling one of society’s most significant challenges Access to rich healthcare data from leading institutions for developing solutions with immediate real-world impact on tens of millions of patients Direct mentorship from proven leaders in healthcare, databases, machine learning, and software engineering No relocation required As an early hire, you will have opportunities for rapid upward mobility: Director roles, empowered to scale the data engineering and software development teams, will be needed as Carenostics grows. Additional opportunities include working directly with clinicians on the interface/workflow and engaging with the startup team on conversations beyond data science (e.g., commercialization, growth strategy, leading new client deployments). Learn more about us, our partnerships, and our journey this past year at www.carenostics.com Carenostics provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state, or local laws.",
        "url": "https://www.linkedin.com/jobs/view/3959573355"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Columbia, TN",
        "job_id": 3949573223,
        "company": "Saransh Inc",
        "title": "Data Engineer",
        "created_on": 1720638936.0755794,
        "description": "Data Engineer Columbia, TN Contract - Hire Roles and Responsibilities: Design and develop database solutions - Creating and maintaining databases in Microsoft SQL Server. ETL Development – Create and maintain appropriate packages and stored procedures in SSIS to pull data from various source systems and load into destination databases on SQL Server MDM Master Data Management Development - Create and maintain scripts in MDM Tool Semarchy xDM Optimize database performance - Ensuring that the database performs optimally by tuning queries, indexes, and other database objects. Monitor and troubleshoot database issues - Monitoring the database for performance issues, errors, and other issues, and troubleshooting those issues as they arise. Maintaining database security – Work with DBAs and team to ensuring that the databases are secure by implementing and maintaining appropriate security measures, such as access controls, encryption, and auditing/logging. Develop DevOps for Data – Work with the DevOps group to help set up appropriate procedures and pipelines for data packages. Qualifications: Bachelor's or master's degree in technical or business discipline or equivalent experience. 8+ years of professional experience as a Data Engineer. Hands on Experience with any MDM Tool preferably Semarchy Xdm. Proven experience as a SQL Database Developer. Expert in writing complex SQL queries and stored procedures using Microsoft SQL Server. Experience with database design, optimization, and performance tuning. Designing, developing, and maintaining ETL process using SQL server Integration services (SSIS). Extracting data from various data sources, transforming, and loading it into target systems",
        "url": "https://www.linkedin.com/jobs/view/3949573223"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Milwaukee, WI",
        "job_id": 3959664055,
        "company": "Dexian",
        "title": "Data & Analytics Engineer",
        "created_on": 1720638937.9283514,
        "description": "Job Summary: Dexian is seeking a Data & Analytics Engineer for an opportunity with a client located in Milwaukee, WI. Responsibilities: Collaborating with cross-functional teams and other data professionals to define business goals and set parameters to measure analytics outcomes Working with data scientists and analytics to collect and clean data for analysis Designing data models, as well as, developing and maintaining data pipelines Creating and maintaining data storage systems Developing documentation for data pipelines and storage systems Developing software to implement data reporting Troubleshooting and debugging data system errors Embracing new technology and staying up to date with industry best practices to continuously improve data analysis skills Requirements: Open to relocation BI Tools: SSIS, SSMS, SSRS, SSAS, power pivot, Power Query, Power BI, Alteryx (don't need all, more the better) Strong SQL Server Strong data modeling - 3nf, Erwin, dimensional modeling Data warehousing Data management / data quality Desired Skills and Experience Candidate Profile: * 7+ years overall IT experience * Minimum of 3-5 years of experience in Data Solution delivery in a complex environment working collaboratively in a team setting * Proficient in Data Solution tools and concepts such as: o Business Intelligence tools: Microsoft tools (SQL Server Management Studio, SSRS, SSAS, Power Pivot, Power Query, PowerBI), Alteryx o Database: SQL Server o Data Query tools: SQL, T-SQL o Data Management and Quality: data mapping, data profiling, metadata repository, relational data modeling, master data management o Data Modeling: ER/Studio Data Architect, 3NF and dimensional modeling o Data Warehousing concepts: Inmon, Kimball, Data Lake o Data Integration concepts and strategies: EII, ETL, EL-T and EAI Dexian is a leading provider of staffing, IT, and workforce solutions with over 12,000 employees and 70 locations worldwide. As one of the largest IT staffing companies and the 2nd largest minority-owned staffing company in the U.S., Dexian was formed in 2023 through the merger of DISYS and Signature Consultants. Combining the best elements of its core companies, Dexian's platform connects talent, technology, and organizations to produce game-changing results that help everyone achieve their ambitions and goals. Dexian's brands include Dexian DISYS, Dexian Signature Consultants, Dexian Government Solutions, Dexian Talent Development and Dexian IT Solutions. Visit https://dexian.com/ to learn more. Dexian is an Equal Opportunity Employer that recruits and hires qualified candidates without regard to race, religion, sex, sexual orientation, gender identity, age, national origin, ancestry, citizenship, disability, or veteran status.",
        "url": "https://www.linkedin.com/jobs/view/3959664055"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3951628920,
        "company": "Brokkr Labs LLC",
        "title": "Data Engineer",
        "created_on": 1720638939.5837674,
        "description": "About Us Brokkr Labs is a technology consulting firm specializing in cloud solutions and DevOps. Our mission is to empower businesses with the transformative capabilities of cloud technology. We provide top-tier professional services and innovative consulting solutions tailored to our clients' unique needs. Our services include AWS Security Architecture Design, Managed DevOps Services, Cloud Migration and Modernization, and more. At Brokkr Labs, we are dedicated to delivering exceptional value and results, helping organizations thrive in the digital age. We are committed to forging digital excellence in everything we do. Job Description The Data Engineer provides consulting services focused on designing, building, and maintaining robust data pipelines and data warehousing solutions for clients. This role involves evaluating various data warehousing solutions and establishing ETL processes to ensure data integrity, availability, and performance. The Data Engineer works closely with client teams to understand their data needs and to ensure that data infrastructure supports business intelligence and data-driven decision-making. Key Responsibilities Engage with clients to understand their data needs and objectives, providing strategic advice on data warehousing solutions and data management best practices. Design, build, and maintain data pipelines tailored to client requirements. Implement and manage data warehousing solutions to ensure data integrity and availability. Establish and maintain ETL processes for efficient data extraction, transformation, and loading. Collaborate with client teams to ensure data infrastructure supports business intelligence and analytics needs. Optimize data workflows for performance and scalability. Ensure compliance with data governance policies and standards. Conduct workshops and training sessions on data management best practices for client teams. The Pay Range For This Role Is 80,000 - 130,000 USD per year(Remote - United States)",
        "url": "https://www.linkedin.com/jobs/view/3951628920"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Irving, TX",
        "job_id": 3934481274,
        "company": "Steneral Consulting",
        "title": "Azure Data Engineer",
        "created_on": 1720638941.3250058,
        "description": "Need 2 managerial references upfront with submission and references will be checked before submission Must have information during submission Candidates Full Name, Mobile Numbers(NO VOIP), Location, Work Authorization, Rate and An Active LinkedIn profile I can reach out to candidates faster if I don't have to go back and forth with you for details. Title- Azure Data Engineer Visa- USC/GC Self Corp or 1099 or W2 Client: VIZIENT HEALTH Client Location - Irving, TX - local candidate only - 3-4 days onsite Duration - 6+ months Interview Process - In-person interview only - no video interviews for this role Must Have - Expert Azure Data Engineer Must be expert in Azure data services Strong SQL And SSIS Skills Domain - Healthcare",
        "url": "https://www.linkedin.com/jobs/view/3934481274"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Bellevue, WA",
        "job_id": 3927882151,
        "company": "ADITI",
        "title": "Data Engineer",
        "created_on": 1720638942.8438325,
        "description": "JOB DESCRIPTION Are you interested in guiding key business decisions around one of Client’s most significant customer facing teams? Do you want to build a cutting-edge highly scalable analytics big data platform using AWS technologies such as Redshift, RDS, S3, EMR, Glue, ADP, Hive, Kinesis, SNS/SQS, and NAWS supported streaming services? Do you want to collaborate with Business Intelligence Engineers (BIE) and Data Scientists (DS) to build ML/LLM models to support our major customer facing features and initiatives to drive team efficiency? We are seeking an experienced, self-driven, analytical, and strategic Sr. Data Engineer. In this role, you will be working in one of the world's largest and most complex data warehouse environments. You should be passionate about working with huge datasets and be someone who loves to bring data together to answer business questions. You should have deep expertise in creation and management of data lake and the proven ability to guide continuous enhancement of data architecture by identifying efficiency gaining solutions and leveraging evolving new technologies. In this role, you will have ownership of end-to-end development of data engineering solutions to complex questions and you’ll play an integral role in strategic decision-making. The right candidate will possess excellent business and communication skills, be able to work with business owners to tackle ambiguous business questions with creative data/science solution designs, and be able to collaborate with BIEs and DSs to build those solutions or answer business questions. You should have a solid understanding of how to build efficient and scalable data infrastructure and data models, and have the capability or the desire to learn and implement Elastic MapReduce (EMR)-based solutions where appropriate. KEY JOB RESPONSIBILITIES In this role, you will have the opportunity to display your skills in the following areas: Design, implement, and support an analytical data infrastructure providing ad hoc access to large datasets and computing power. Managing AWS resources including EC2, RDS, Redshift, etc. Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using SQL and AWS big data technologies. Explore and learn the latest AWS technologies to provide new capabilities and increase efficiency. Collaborate with BIEs to recognize and help adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation. Collaborate with DS to implement advanced analytics algorithms that exploit our rich data sets for statistical analysis, prediction, clustering, and machine learning. Help continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers.",
        "url": "https://www.linkedin.com/jobs/view/3927882151"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Texas, United States",
        "job_id": 3953411811,
        "company": "Caliber Collision",
        "title": "Data Engineer",
        "created_on": 1720638944.5189776,
        "description": "Job Summary The Data engineer will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. This role is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. This position will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be comfortable supporting the data needs of multiple teams and systems. Overall, they must strive for efficiency by aligning data systems with business goals. Duties and responsibilities Create and maintain optimal data pipeline architecture, Assemble large, complex data sets that meet functional / non-functional business requirements. Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and various cloud technologies. Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics. Work with the business to assist with data-related technical issues and support their data infrastructure needs. Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader. Work with data and analytics experts to strive for greater functionality in our data systems. Create complex SQL queries and database objects (stored procs, views, etc.) to pull and manage data. Developing complex data pipelines using SSIS packages, Azure Data Factory, or other related ETL/ELT tools to move and translate data Create and maintain documentation on data pipelines Requirements and Qualifications Bachelor’s degree in Computer Science, Statistics, Informatics, Information Systems, or another quantitative field; or equivalent combination of education experience and training that provides the required knowledge and skills. 4 – 6 years of experience in a Data Engineer role Advanced working knowledge of SQL and T-SQL programming Experience with ETL and Data Migration Experience with relational SQL and NoSQL databases Experience with object-oriented/object function scripting languages: Python etc. Experience building and optimizing data pipelines, architectures, and data sets. Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. Experience supporting and working with cross-functional teams in a dynamic environment. Strong analytic skills related to working with unstructured datasets. Build processes supporting data transformation, data structures, metadata, dependency, and workload management. A successful history of manipulating, processing, and extracting value from large, disconnected datasets. Experience with Machine Learning is a great plus Experience with Snowflake Cloud Data Warehouse is a must Experience with transformation tool Coalesce is a must Experience with Azure Data Factory is a must Experience with message queuing and stream processing such as Pub-Sub, Azure Event Grid, Kafka etc. Experience with data pipeline and workflow management tools such as Airflow, etc.",
        "url": "https://www.linkedin.com/jobs/view/3953411811"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Omaha, NE",
        "job_id": 3964400864,
        "company": "Lutz",
        "title": "Data Engineer",
        "created_on": 1720638946.2965384,
        "description": "Lutz Tech is an information technology firm that partners with local businesses to simplify the complex world of tech. Our team believes in saying it straight, pushing ourselves to be the best, and serving our teammates as well as our clients. Role Summary: We are seeking a highly skilled and versatile Data Engineer to join our team. This individual will be instrumental in connecting to various data sources, understanding and interpreting data structures from a business perspective, and developing efficient data pipelines that feed into our Power BI reports. The ideal candidate will have a strong background in relational databases, experience with cloud and API data integration, and the ability to manage multiple projects across different industries. Responsibilities Include: Design and implement data integration solutions to connect with various source systems, including custom applications and off-the-shelf software. Understand and interpret data structures, mapping source data to common data tables and developing reporting models to support Power BI report development. Work with relational databases (SQL Server, Azure SQL), familiarity with NoSQL environments a plus. Utilize Microsoft Fabric pipelines or individual components such as Synapse and Data Factory for data processing and management. Optionally, administer Power BI environments, including setup, security, and maintenance. Monitor capacity metrics and determine potential efficiency gains on the Power BI data flows or pipelines. Collaborate with teams to understand business requirements and translate them into technical specifications. Manage multiple data engineering projects simultaneously, effectively context-shifting between different industries and client needs. Requirements: Minimum of 3 years of experience in data engineering or a closely related role. Bachelor’s degree in Computer Science, Information Technology, Business Analytics, Engineering, or a related field. Experience in managing multiple projects with the ability to shift contexts effectively between different industries and client needs. Familiarity with Power BI or similar business intelligence tools, including report development and, ideally, administration. Experience in working with NoSQL databases is desirable but not mandatory. Proven experience with SQL Server, Azure SQL, and other relational databases. Demonstrated ability in developing and implementing data integration solutions with various source systems. Hands-on experience with Microsoft Azure Data Services (e.g., Data Factory, Azure Synapse Analytics) or similar cloud data services. Strong understanding of data structures and the ability to map source data to common data tables for analytics purposes. Excellent problem-solving skills and the ability to understand complex business requirements and translate them into technical solutions. Strong communication and collaboration abilities.",
        "url": "https://www.linkedin.com/jobs/view/3964400864"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Atlanta, GA",
        "job_id": 3953810139,
        "company": "Rheem Manufacturing",
        "title": "Azure Data Engineer",
        "created_on": 1720638948.3218954,
        "description": "Job Description Rheem is looking to hire Azure Data Engineer. We are looking for experienced Azure Data engineer to be part of our data & analytics team. You will develop solutions leveraging Azure cloud platform to create a Modern Enterprise data platform. You will work with cross-functional teams to ensure data is collected, stored, processed, and analyzed in a timely, efficient, and accurate manner. You will support digital transformation journey of data and analytics team at Rheem. This exciting transformation will enable new cloud technologies that will transform the way we derive value from our data assets. This position will serve our Enterprise Division located in Atlanta, Georgia. This is a hybrid role and you will be required to come into the office roughly 3 days/week. You must reside in a commutable distance to our Atlanta, Georgia office. Responsibilities Design, develop, and deploy data solutions using Microsoft Azure cloud platform Develop and maintain automated data ingestion, transformation, and validation processes to ensure data accuracy and consistency Data Ingestion: Ingesting data from various sources, such as on-premises systems, cloud-based systems, and third-party services into Data Transformation: Transforming data into the appropriate format, schema, and structure to meet business requirements Data Loading: Loading transformed data into Azure Synapse SQL Warehouse/ Azure Data Lake Storage Error Handling and Monitoring: Implementing error handling and monitoring processes to ensure data integration solutions operate smoothly and provide reliable data. Performance Optimization: Optimizing data integration processes for performance and cost, by tuning queries, caching data, and managing resources. Collaboration: Collaborating with other teams, such as, architects, developers, and business analysts, to ensure data solutions meet their requirements Qualifications Degree in Computer Science or Data Analytics/engineering related streams Strong Understanding of Data Warehouse/Lakehouse, Data Mesh, Dimensional data Modelling, ETL/ELT, CDC concepts. Knowledge of Cloud Infrastructure and services including Azure Data Factory(ADF), Azure Synapse, Azure Synapse Pipeline, Spark Notebooks, Azure Synapse Dedicated SQL Pool Warehouse ,Azure Databricks, Azure Functions, Power BI and Azure Data Lake storage Understanding of with various data formats like relational, json, parquet, delta, streaming and others Proficiency in SQL,T-SQL and Python/PySpark Excellent problem-solving and analytical skills. Aptitude to adapt to new technologies Excellent business facing and internal communication skills How To Stand Out Microsoft Azure Data Engineer Associate Certificate Certification on Data Science/ML. About Us At Rheem, we are dedicated to bringing comfort to people’s lives. As a leading global manufacturer of heating, cooling and water heating equipment, we are innovating all-new ways to deliver just the right temperature while saving energy, water and supporting a more sustainable future. It is an exciting challenge that requires a team of talented, passionate people with a diverse set of skills. From engineers to accountants, sales professionals to support experts, Rheem depends on people to power our innovations. Join Rheem, and help shape the future of products that impact lives—every day. Rheem is an Equal Opportunity Employer. Rheem encourages all qualified candidates to apply, including those of any race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. The job description above has been designed to indicate the general nature and level of work performed by employees within this classification. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications required of employees assigned to this job. Equivalent combination of education, experience, and skills may supplement above minimum job requirements. For U.S. Based jobs, please note that Rheem is unable to hire candidates to be employed in the following states: Alaska, Hawaii, Idaho, Louisiana, Mississippi, Montana, New Mexico, North Dakota, South Dakota, Vermont, West Virginia, or Wyoming. Rheem and its subsidiaries do not accept unsolicited resumes from recruiters or employment agencies. In the absence of an executed Recruitment Services Agreement, there will be no obligation to any referral compensation or recruiter fee.",
        "url": "https://www.linkedin.com/jobs/view/3953810139"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Bellevue, WA",
        "job_id": 3970288048,
        "company": "Akraya, Inc.",
        "title": "Data Engineer II: 24-01871",
        "created_on": 1720638949.9938545,
        "description": "Primary Skills: Python, AWS, SQL Contract Type: W2 Duration: 11 Months Location: Bellevue, WA Onsite Pay Range: $60.00-$67 per hour #LP Job Responsibilities Transform complex data in AWS, developing and improving accessibility for customer data. Design and implement metrics, tackle metadata modeling, reports, and dashboards development. Collaborate with business customers to grasp business requirements and implement solutions to support analytical and reporting needs. Ensure solutions are scalable and compliant with security and privacy standards. Handle the complete data model development cycle in large-scale systems. JOB REQUIREMENTS: Proficiency in one modern scripting or programming language (e.g., Python, Java, Scala, NodeJS). Extensive experience with AWS services (e.g., Lambda, Gateway, SNS, Firehose). Strong foundation in SQL and familiarity with reporting tools (e.g., Quicksight, Tableau). Prior e-commerce experience ABOUT AKRAYA Akraya is an award-winning IT staffing firm consistently recognized for our commitment to excellence and a positive work environment. Voted the #1 Best Place to Work in Silicon Valley (2023) and a Glassdoor Best Places to Work (2023 & 2022), Akraya prioritizes a culture of inclusivity and fosters a sense of belonging for all team members. We are staffing solutions providers for Fortune 100 companies, and our industry recognitions solidify our leadership position in the IT staffing space. Let us lead you to your dream career, join Akraya today!",
        "url": "https://www.linkedin.com/jobs/view/3970288048"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Irving, TX",
        "job_id": 3956381913,
        "company": "CVS Health",
        "title": "Data Engineer",
        "created_on": 1720638951.9648893,
        "description": "Bring your heart to CVS Health. Every one of us at CVS Health shares a single, clear purpose: Bringing our heart to every moment of your health. This purpose guides our commitment to deliver enhanced human-centric health care for a rapidly changing world. Anchored in our brand — with heart at its center — our purpose sends a personal message that how we deliver our services is just as important as what we deliver. Our Heart At Work Behaviors™ support this purpose. We want everyone who works at CVS Health to feel empowered by the role they play in transforming our culture and accelerating our ability to innovate and deliver solutions to make health care more personal, convenient and affordable. Position Summary Aetna Resources, LLC, a CVS Health company, is hiring for the following role in Irving, TX: Data Engineer to Develop, build and manage large-scale data structures, pipelines and efficient Extract/Load/Transform (ETL) workflows to address complex problems and support business applications. Duties include: develop large scale data structures and pipelines to organize, collect and standardize data to generate insights and addresses reporting needs; write ETL (Extract/Transform/Load) processes, design database systems, and develop tools for real-time and offline analytic processing that improve existing systems and expand capabilities; collaborate with Data Science team to transform data and integrate algorithms and models into automated processes; test and maintain systems and troubleshoot malfunctions; leverage knowledge of Hadoop architecture, HDFS commands, and designing and optimizing queries to build data pipelines; utilize programming skills in Python, Java, or similar languages to build robust data pipelines and dynamic systems; build data marts and data models to support Data Science and other internal customers; integrate data from a variety of sources and ensure adherence to data quality and accessibility standards; analyze current information technology environments to identify and assess critical capabilities and recommend solutions to complex business problems; and experiment with available tools and advise on new tools to provide optimal solutions that meet the requirements dictated by the model/use case. Multiple Positions. - Requirements: Master’s degree (or foreign equivalent) in Computer Science, Information Systems, Data Science, Statistics, Mathematics, Analytics, Civil Engineering or a related field and two (2) years of experience in the job offered or related occupation. Requires two (2) years of experience in each of the following: Hadoop and Hive; Cloud technologies: Azure, Amazon Web Services (AWS), or Google Cloud Platform (GCP); Agile methodologies or SAFe Software Development Principles; Unit and automation testing; Analyzing large data sets from multiple data sources; SAS or SQL programming languages; Spark, PySpark, or Scala; and Hadoop architecture or HDFS commands. Pay Range: $120,890.00/year to $140,000.00/year. This pay range represents the base hourly rate or base annual full-time salary for all positions in the job grade within which this position falls. The actual base salary offer will depend on a variety of factors including experience, education, geography and other relevant factors. In addition to your compensation, enjoy the rewards of an organization that puts our heart into caring for our colleagues and our communities. The Company offers a full range of medical, dental, and vision benefits. Eligible employees may enroll in the Company’s 401(k) retirement savings plan, and an Employee Stock Purchase Plan is also available for eligible employees. The Company provides a fully-paid term life insurance plan to eligible employees, and short-term and long term disability benefits. CVS Health also offers numerous well-being programs, education assistance, free development courses, a CVS store discount, and discount programs with participating partners. As for time off, Company employees enjoy Paid Time Off (“PTO”) or vacation pay, as well as paid holidays throughout the calendar year. Number of paid holidays, sick time and other time off are provided consistent with relevant state law and Company policies. For more detailed information on available benefits, please visit jobs.CVSHealth.com/benefits This job does not have an application deadline, as CVS Health accepts applications on an ongoing basis.",
        "url": "https://www.linkedin.com/jobs/view/3956381913"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Richardson, TX",
        "job_id": 3967967799,
        "company": "Infosys",
        "title": "Azure Data Engineer",
        "created_on": 1720638953.637891,
        "description": "Infosys is seeking an Azure Databricks - Data Engineer. In this role, you will provide technical guidance to teams; anchor Proof of Concept developments and support opportunity identification and pursuit processes and evangelize Infosys brand. You will collaborate with some of the best talent in the industry to create and implement innovative high-quality solutions. You will be part of a learning culture, where teamwork and collaboration are encouraged, excellence is rewarded, and diversity is respected and valued. Required Qualifications: Candidate must be located within commuting distance of Richardson (TX) or Raleigh, NC or be willing to relocate to the area Bachelor’s degree or foreign equivalent required from an accredited institution. Will also consider three years of progressive experience in the specialty in lieu of every year of education. At least 4 years of experience in Information Technology. At least 3 years of hands-on experience in using Azure Data Factory (ADF) to orchestrate data ingestion from various sources into Azure Synapse and Azure Data Lake Storage (ADLS). At least 2 years of hands-on experience with Azure, databricks and Hadoop distributed frameworks while handling large amount of big data using Spark and Hadoop Ecosystems. At least 2 years of experience designing and implementing data pipelines using Azure Databricks for data cleaning, transformation, and loading into Azure Synapse Analytics Hands on experience in end-to-end implementation of data warehouse and data marts Experience in writing SQL queries to analysis Type-2 dimension data Understanding of implementing Medallion Architecture using Azure Services Preferred Qualifications: Experience in software engineering and data engineering roles, with a focus on Azure and Databricks. Strong understanding of databases and big data software technologies, specifically Azure and Databricks. Exposure to Unity Catalogue Experience with Agile methodologies, particularly Scrum. Experience in coordinating with offshore data engineering teams Experience with scripting languages like Python, Java, etc. Strong problem-solving skills related to data, data structures, and algorithms. About Us Infosys is a global leader in next-generation digital services and consulting. We enable clients in 50 countries to navigate their digital transformation. With over three decades of experience in leading the systems and workings of global enterprises, we expertly steer our clients through their digital journey. We do it by enabling the enterprise with an AI-powered core that helps prioritize the execution of change. We also empower the business with agile digital at scale to deliver outstanding levels of performance and customer delight. Our always-on learning agenda drives their continuous improvement through building and transferring digital skills, expertise, and ideas from our innovation ecosystem. Infosys is an equal opportunity employer and all qualified applicants will receive consideration without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, spouse of protected veteran, or disability. To learn more about Infosys and see our perspectives in action please visit us at www.Infosys.com",
        "url": "https://www.linkedin.com/jobs/view/3967967799"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Minneapolis, MN",
        "job_id": 3956366463,
        "company": "Capgemini",
        "title": "Azure Data Engineer",
        "created_on": 1720638955.3047986,
        "description": "Job Title: Azure Data Engineer Location: Minneapolis, Minnesota. Required: 5 years of experience in developing and implementing solutions on the Azure platform Should have experience in Azure Cloud Migration Azure Cloud setup Should have experience in Prior payment industry experience and or certifications CTP AAP ISO20022 Should have experience in NER modelling Corpus Linguistics Neural Network Experience with Azure DevOps Git or similar tools for version control and CI CD pipelines Knowledge of containers and orchestration platforms e g Docker Kubernetes in Azure Familiarity with monitoring tools and practices for Azure infrastructure and applications Excellent problem solving skills and attention to detail Strong communication and collaboration skills Maintain excellent working relationship between internal team and the whole organization and external parties vendors customers Must possess good oral and written communication and presentations skills with the ability to lead meetings effectively. Life at Capgemini: Capgemini supports all aspects of your well-being throughout the changing stages of your life and career. For eligible employees, we offer: Flexible work Healthcare including dental, vision, mental health, and well-being programs Financial well-being programs such as 401(k) and Employee Share Ownership Plan Paid time off and paid holidays Paid parental leave Family building benefits like adoption assistance, surrogacy, and cryopreservation Social well-being benefits like subsidized back-up child/elder care and tutoring Mentoring, coaching and learning programs Employee Resource Groups Disaster Relief\\ About Capgemini: Capgemini is a global business and technology transformation partner, helping organizations to accelerate their dual transition to a digital and sustainable world, while creating tangible impact for enterprises and society. It is a responsible and diverse group of 340,000 team members in more than 50 countries. With its strong over 55-year heritage, Capgemini is trusted by its clients to unlock the value of technology to address the entire breadth of their business needs. It delivers end-to-end services and solutions leveraging strengths from strategy and design to engineering, all fueled by its market leading capabilities in AI, cloud and data, combined with its deep industry expertise and partner ecosystem. The Group reported 2023 global revenues of €22.5 billion. Get The Future You Want | www.capgemini.com Disclaimer: Capgemini is an Equal Opportunity Employer encouraging diversity in the workplace. All qualified applicants will receive consideration for employment without regard to race, national origin, gender identity/expression, age, religion, disability, sexual orientation, genetics, veteran status, marital status or any other characteristic protected by law. This is a general description of the Duties, Responsibilities and Qualifications required for this position. Physical, mental, sensory or environmental demands may be referenced in an attempt to communicate the manner in which this position traditionally is performed. Whenever necessary to provide individuals with disabilities an equal employment opportunity, Capgemini will consider reasonable accommodations that might involve varying job requirements and/or changing the way this job is performed, provided that such accommodations do not pose an undue hardship. Capgemini is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please get in touch with your recruiting contact. Click the following link for more information on your rights as an Applicant http://www.capgemini.com/resources/equal-employment-opportunity-is-the-law",
        "url": "https://www.linkedin.com/jobs/view/3956366463"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Dallas, TX",
        "job_id": 3960813234,
        "company": "Simmons Bank",
        "title": "Data Engineer",
        "created_on": 1720638957.1580155,
        "description": "It's fun to work in a company where people truly BELIEVE in what they're doing! We're committed to bringing passion and customer focus to the business. If you are a Google Cloud Data Engineer, Simmons Bank invites you to collaborate in an agile team of peers developing cloud-based analytics platform integrating data from broad number of systems to enable next-gen analytical products. Senior Data Engineering Google Cloud Platform (GCP) is responsible to develop and deliver effective cloud solutions for different business units. This position requires in-depth knowledge and expertise in GCP services, architecture, and best practices. They will collaborate with cross-functional teams to design, implement, and manage scalable and reliable cloud solutions. This position will also be responsible for driving innovation and staying up-to-date with the latest GCP technologies and trends to provide industry-leading solutions. Our Future Colleague Will Contribute to multi year data analytics modernization roadmap for the bank. You will directly work on the platform based on Google BigQuery and other GCP services to integrate new data sources and model the data up to the serving layer. Contribute to this is unique opportunity as the program is set-up to completely rethink reporting and analytics with Cloud technology. Collaborate with different business groups, users to understand their business requirements and design and deliver GCP architecture, Data Engineering scope of work You will work on a large-scale data transformation program with the goal to establish a scalable, efficient and future-proof data & analytics platform. Develop and implement cloud strategies, best practices, and standards to ensure efficient and effective cloud utilization. Work with cross-functional teams to design, implement, and manage scalable and reliable cloud solutions on GCP. Provide technical guidance and mentorship to the team to develop their skills and expertise in GCP. Stay up-to-date with the latest GCP technologies, trends, and best practices and assess their applicability to client solutions. Qualifications What will help you succeed: Bachelors University degree computer science/IT Masters in Data Analytics/Information Technology/Management Information System (preferred) Strong understanding of data fundamentals, knowledge of data engineering and familiarity with core cloud concepts Must have good implementation experience on various GCP’s Data Storage and Processing services such as BigQuery, Dataflow, Bigtable, Dataform, Data fusion, cloud spanner, Cloud SQL Must have programmatic experience of SQL, Python, Apache Spark Atleast 7-8 years of professional experience in building data engineering capabilities for various analytics portfolio with atleast 5 years in GCP/Cloud based platform. Your expertise in one or more of the following areas is highly valued: Google Cloud Platform, ideally with Google BigQuery, Cloud Composer and Cloud Data Fusion,Cloud spanner, Cloud SQL Experience with legacy data warehouses (on SQL Server or any Relational Datawarehouse platform) Experience with our main tools dbt, terraform/terragrunt, Git (CI/CD) Experience with a testing framework Experience with Business Intelligence tools like PowerBI and/or Looker What Sets You Apart Experience in complex migrations from legacy data warehousing solutions or on-prem datalakes to GCP Experience with building generic, re-usable capabilities and understanding of data governance and quality frameworks Experience in building real-time ingestion and processing frameworks on GCP. Adaptability to learn new technologies and products as the job demands. Multi-cloud & hybrid cloud experience Any cloud certification (Preference to GCP Certifications) Experience working with Financial and Banking Industry Equal Employment Opportunity Information: Simmons First National Corporation and its subsidiaries are committed to a policy of equal employment with respect to a person's race, color, religion, sex, ancestry, sexual orientation, gender identity, national origin, covered veterans, military status, physical or mental disability or any other legally protected classifications. Simmons First National Corporation and its subsidiaries are committed to Affirmative Action Programs consisting of results-oriented procedures to ensure equal employment opportunities. These programs require positive action in lieu of neutral non-discrimination and merit hiring/performance policies.",
        "url": "https://www.linkedin.com/jobs/view/3960813234"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "North Reading, MA",
        "job_id": 3946642416,
        "company": "TekWissen ®",
        "title": "Data Engineer I",
        "created_on": 1720638959.0035253,
        "description": "Job Title: Data Engineer I Duration: 6 Months Location: North Reading, MA 01864 Job Type: Contract Work Type: Hybrid (3 days on site at BOS12- North Reading.) Overview TekWissen Group is a workforce management provider throughout the USA and many other countries in the world. Our client is a company operating a marketplace for consumers, sellers, and content creators. It offers merchandise and content purchased for resale from vendors and those offered by thirdparty sellers. Job Description The Client Robotics (AR) Operations organization oversees the manufacturing of high-quality robotics and gets the right materials to the right place at the right time to enable our automated solutions. It’s a fast-paced operation that works cross-functionally with teams across Client globally and is driven by data and performance indicators. Client Robotics is seeking a talented and motivated Data Engineer to design, develop and test data pipelines and centralized data store for test equipment and manufacturing operations data. Client Robotics software facilitates test workflows, robotic control and data pipelines to create a robust test-driven architecture and user interfaces. You acquire expertise as needed, iterate and adapt your approach as you learn more. You are deeply interested in pragmatic problem solving as well as taking risks in pioneering brand-new areas. You create extendable design and easy to maintain technical solution with the long-term vision in mind. If you can dream it and justify it, we will explore it. Key Projects Building data lake Typical Day In The Role Typical Tasks As a Data Engineer you will contribute to the development of a data lake and data services. You will be developing and supporting the analytic technologies that give our customers timely, flexible and structured access to their data. You will be responsible for designing and implementing a platform using third-party and in-house reporting tools, modeling metadata. You will work with business customers in understanding the business requirements and implementing solutions to support analytical and reporting needs. Compelling Story & Candidate Value Proposition What makes this role interesting Selling Points Working with a full scale Team of data engineers. Opportunity to work across AWS tech stack. Candidate Requirements Qualifications 3+ years of data engineering experience 2+ years of analyzing and interpreting data with Redshift, Oracle, NoSQL etc. experience Knowledge of distributed systems as it pertains to data storage and computing Experience with data modeling, warehousing and building ETL pipelines Experience working on and delivering end to end projects independently Experience programming with at least one modern language such as C++, C#, Java, Python, Golang, PowerShell, Ruby Experience with Redshift, Oracle, NoSQL etc. Preferred Qualifications Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions Experience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases) Years of Exp 3+ Degrees BA comp sci/ stats/ Leadership Principles Deliver results Ownerships Bias reaction Top 3 Must-have Hard Skills Python AWS development SQL TekWissen® Group is an equal opportunity employer supporting workforce diversity. TekWissen is an emerging global human capital, recruitment and IT services organization. Operating since 2009, we draw upon more than a decade of staffing experience to deliver critical talent acquisition solutions and IT engagements for our clients. We’re founded on a culture that is passionate about delivering tailored solutions, that create lasting partnerships. Our global footprint covers six countries: United States, Canada, Australia, India, United Kingdom and the Philippines. This allows us to work in close partnership with organizations and manage everything from global talent needs with demanding resourcing strategies, to single sites with lower recruitment volumes. TekWissen® is an equal opportunity employer supporting workplace diversity.",
        "url": "https://www.linkedin.com/jobs/view/3946642416"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Dallas, TX",
        "job_id": 3944125265,
        "company": "Tata Consultancy Services",
        "title": "Data Engineer",
        "created_on": 1720638960.579653,
        "description": "Build and maintain ETL processes in SSIS and Azure Data Factory Engineer scalable, reliable and performant systems to manage data Develop, implement and optimize stored procedures and functions Research and analyze data issues and provide automated solutions Implement new technologies to enhance the optimization of current practices Provide valuable suggestions regarding new ideas and technologies",
        "url": "https://www.linkedin.com/jobs/view/3944125265"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Atlanta, GA",
        "job_id": 3942750262,
        "company": "Marathon TS",
        "title": "Data Engineer",
        "created_on": 1720638964.7306943,
        "description": "Job Description Data Engineer is responsible for supporting our enterprise data warehouse for our Supply Chain Operations. As a Data Engineer, you will be part of a dynamic team with engineers of all experience levels who help each other build and grow technical and leadership skills while creating, deploying, and supporting production applications. In addition, Data Engineer IIs may be involved in configuration, security, resilience, performance tuning and production monitoring. Key Responsibilities: 60% Delivery and Execution - Collaborates and pairs with other product team members (UX, engineering, and product management) to create secure, reliable, scalable software solutions; Documents, reviews and ensures that all quality and change control standards are met; Works with Product Team to ensure user stories that are developer-ready, easy to understand, and testable; Writes custom code or scripts to automate infrastructure, monitoring services, and test cases; Writes custom code or scripts to do destructive testing to ensure adequate resiliency in production; Program configuration/modification and setup activities on large projects using HD approved methodology; Configures commercial off the shelf solutions to align with evolving business needs Creates meaningful dashboards, logging, alerting, and responses to ensure that issues are captured and addressed proactively 20% Learning - Actively seeks ways to grow and be challenged using both formal and informal development channels; Learns through successful and failed experiment when tackling new problems 20% Plans and Aligns - Collaborates with other team members in agile processes; Assists in creating new and better ways for the team to be successful; Relates openly and comfortably with diverse groups of people; Builds partnerships and works collaboratively with others to meet shared objectives Must have experience with: Python, SQL, and GCP/Big Query Company Description Marathon TS provides a full range of professional services for clients that require support from professionals with specialized skills and experience in a specific technical area or subject matter. Marathon TS also provides IT solutions, including strategy, operations, transformation and mission support. Marathon TS provides a full range of professional services for clients that require support from professionals with specialized skills and experience in a specific technical area or subject matter. Marathon TS also provides IT solutions, including strategy, operations, transformation and mission support.",
        "url": "https://www.linkedin.com/jobs/view/3942750262"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Baltimore, MD",
        "job_id": 3938226205,
        "company": "Keylent Inc",
        "title": "Data Engineer  �324851",
        "created_on": 1720638966.3820949,
        "description": "Data Engineer 324851 Max rate $80 Onsite in Baltimore, MD Data Engineer Job Summary The Data Office team at T. Rowe Price is playing a key role in helping build the future of financial services, working together with business partners to create client experiences that are changing the way people invest. We are seeking a highly skilled and experienced Data Engineer to join our team and play a critical role in building and maintaining our data infrastructure. As a Data Engineer, you will work with smart, talented people across our business and technology teams to ensure that our data is accurate, reliable, and accessible. Our team is responsible for developing and maintaining data pipelines that will create the core data products in our organization. We ingest data from a variety of internal and external sources; this data is mapped onto our data model to form our foundational data products. You will collaborate with the data governance team to create automatic data quality checks, data reconciliation and monitoring tools. The team is also responsible for creating tailored data products to enable critical business decisions. We will expect you to be agile and to think outside the box. In return, we will give you challenging work that makes an impact and brings opportunities to gain experience and grow. We help our clients succeed by providing a collaborative culture that encourages every member of our team to bring their point of view to the table. Responsibilities Extract, transform, and load data from various sources into our data lakehouse. Develop, test, maintain, and document data pipelines using DBT, Spark, AWS Glue and Airflow. Build data marts and data cubes to support various business intelligence initiatives. Automate data quality checks to ensure the accuracy and consistency of our data. Business Knowledge Experience collaborating directly with stakeholders, eliciting business requirements, questioning and challenging where necessary. Can articulate business needs and translate them into technology solutions. An understanding of the asset management business and/or financial markets. Qualifications Bachelor's degree in Computer Science, Data Science or a related field. Strong proficiency in Python and SQL. Expertise in data analysis. Experience building a modern enterprise-wide data and analytics platform. Able to quickly learn new technologies and stay abreast of industry trends. Experience with data stack technologies, such as Apache Airflow, Prefect, Dagster, Apache Iceberg, DBT, Spark, Great Expectations. AWS experience, particularly data services, such as Glue, Lake Formation, EMR, EventBridge, Athena. Experience with data warehousing such as Redshift, Snowflake or Dremio. Excellent communication and collaboration skills. Bonus Points Experience with DevOps practices such as continuous integration and continuous delivery (CI/CD). Experience with data visualization tools such as Tableau or Power BI. Experience with metadata management tools (such as Amundsen, Atlas, DataHub, OpenDataDiscovery, Marquez, etc.).",
        "url": "https://www.linkedin.com/jobs/view/3938226205"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3966695499,
        "company": "Motion Recruitment",
        "title": "Data Engineer",
        "created_on": 1720638968.1665208,
        "description": "One of the country's largest small business platforms is searching for a Data Engineer to join their Banking Data Science team 100% remotely . In this position, you will be responsible for building and maintaining ETL pipelines with SQL and Airflow , deploying data quality checks, and optimizing processes via data streaming and reporting tools like Looker . This exciting opportunity to support critical backend data pipelines at a fintech industry leader begins as a 9-month, W2 contract that offers full benefits, holidays, and the potential to extend or convert to permanent employment! Responsibilities: Build and maintain data pipelines that meet industry regulatory compliance requirements domestically and internationally Deploy data quality checks to ensure that data is scalable and integral while improving validation and monitoring processes Optimize regulatory processes with advanced data streaming and reporting tools (Looker) Communicate cross-functionally with teams in legal, development, and reporting departments Qualifications: 3+ years of data engineering experience Advanced proficiency in SQL to produce summary data reports Past experience designing, implementing, and maintaining ETL pipelines with scheduling tools such as Airflow or Prefect. Strong knowledge of visualization dashboarding tools like Looker Motion Recruitment Partners is an Equal Opportunity Employer, including disability/vets. All applicants must be currently authorized to work on a full-time basis in the country for which they are applying, and no sponsorship is currently available. Employment is subject to the successful completion of a pre-employment screening. Accommodation will be provided in all parts of the hiring process as required under Motion Recruitment Employment Accommodation policy. Applicants need to make their needs known in advance.",
        "url": "https://www.linkedin.com/jobs/view/3966695499"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Fountain Hills, AZ",
        "job_id": 3958072131,
        "company": "Zortech Solutions",
        "title": "Data Engineer (DE) ---US",
        "created_on": 1720638972.309736,
        "description": "Role: Data Engineer (DE) Location: Scottsdale AZ (day 1 onsite) Duration: Fulltime Must have skill set: Java , Scala , S3, Glue, Redshift You have 6-8 years of relevant software development experience. You have hands-on experience in Java/Scala/Python, Spark, S3, Glue, Redshift. This is critical. Highly analytical and data oriented. Experience in SQL, NoSql Database Data masking of on prem PII data. Develop API calls with using secure data transfer. Take standard output data to lower environments for pre prod testing! Enable secured channels for data models and data science activities. Need a solution that can scale to handle millions of data. i.e., masking 10 million records in 15 mins You have experience with development tools and agile methodologies.",
        "url": "https://www.linkedin.com/jobs/view/3958072131"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Copper Canyon, TX",
        "job_id": 3966601754,
        "company": "Verdant Infotech Solutions",
        "title": "Data Engineer",
        "created_on": 1720638975.4942656,
        "description": "Title: Data Engineer Location: Hybrid ST Louis MO Duration: Contract Visa: USC Must have skills in resume We are looking for a Data Engineer to work on the FRED Data Desk agile team within the Research Division to develop cloud native data operations tools to maintain the data ingestion pipeline processes for the FRED website. Responsibilities \" Write secure, production-quality Python code \" Implement cloud native solutions in AWS designed by our solutions and data architects for end-to-end data processing pipelines and data operations tools \" Perform code review on peers' code \" Help document processes \" Deploy code to production servers using GitLab CI/CD pipelines \" Participate in agile development team Qualifications Experience developing and maintaining data processing pipelines, including extraction, transformation, and loading of data (ETL processes) Experience With Open-source Programming Languages, Including Python SQL PHP Combination Of The Following Experience with Git or other source control for code management Experience with Amazon Web Services (AWS) such as Lambdas, Step Functions, ECS, SNS, SQS, and EventBridge Experience writing production-ready Terraform code Experience with Docker or containerized application development Experience with Agile methodologies \" Can meet commitments and deliver quality work in a fast paced environment | , 5208 Windsor Ln, Copper Canyon, Texas, 75077",
        "url": "https://www.linkedin.com/jobs/view/3966601754"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3970227984,
        "company": "Vanna Health",
        "title": "Data Engineer",
        "created_on": 1720638977.2783654,
        "description": "Remote Salary range: $135,000-$180,000 per year About Vanna Health Vanna Health focuses on reimagining care for individuals with serious mental illness. Our goal is to empower these individuals to lead healthy and meaningful lives by providing physical and mental healthcare support and connecting members to community programs. Our technology-enabled approach, developed by our own team of engineers and designers, ensures our staff and members are well supported. Vanna delivers in-person and virtual community-based engagement through an interdisciplinary team of coaches and clinicians. We leverage partnerships with existing community resources to facilitate support networks unique to an individual’s needs and location - connecting them to established psychosocial rehabilitation programs, resources for housing, transportation, etc. and various digital engagement opportunities to address members’ needs. About The Role The Data Engineer role is critical for Vanna’s developing data capabilities. Your work will focus on building out the data platform capabilities that are foundational to Vanna’s success across a number of key domains, including population evaluation and selection, member outreach and engagement, and optimization of clinical and financial outcomes for both individual members and our populations as a whole. You’ll become a subject matter expert on all things data at Vanna, working hand-in-hand with our clinical, operations, product, and engineering teams to develop the technology and services that enable our Vanna Coaches to provide compassionate care to our members living with severe mental illness. This is a unique opportunity to join a small but growing team, where you’ll have the opportunity to develop many of our foundational data capabilities from scratch. You have a growth mindset and are excited about expanding your technical skills and stepping into new responsibilities as the team and company grow. Responsibilities Design and implement cloud-based data warehousing environments, data processing frameworks, and data models that support Vanna’s business Implement cloud-based data infrastructure, optimizing for scale and performance as the size of our data grows Automate the movement of data through our data platform, and layer in monitoring, alerting, and data quality checks to ensure data is accurate and up-to-date Build interfaces in collaboration with our engineering team that allow for exposing data and associated processing functionality directly to end users and other systems Partner with data scientists to take machine learning models into production Contribute to overall data platform architecture design discussions, with a focus on balancing short-term needs with long-term scalability Employ software engineering best practices in support of versioning and quality control of both code and data About You 4-6 years of experience in data or back-end software engineering Advanced Python skills Experience working with analytical data stores like Delta Lake, Snowflake, BigQuery, etc. Experience working with relational databases like Postgres, MySQL, or SQL Server Databricks experience strongly preferred Comfort in cloud computing environments (we use AWS) Intermediate to advanced SQL knowledge Knowledge of common software architecture design patterns and when to employ them Experience with SDLC best practices, including agile, version control, CI/CD, test-driven development, documentation, etc. Obsessive focus on end users and business impact Bonus points if you have: Familiarity with healthcare data Experience with infrastructure as code tools like Terraform or Databricks Asset Bundles, or a desire to learn Experience working in highly-regulated environments Experience at an early-stage startup Familiarity with streaming and event-driven architecture Benefits Full medical, dental, and vision insurance 401K HSA/FSA/Dependant Care FSA options Unlimited PTO Company paid holidays off",
        "url": "https://www.linkedin.com/jobs/view/3970227984"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3968917183,
        "company": "Convergenz",
        "title": "Data Engineer",
        "created_on": 1720638979.2072444,
        "description": "Skills: Data Engineering, Data Analysis & Interpretation, Structured Query Language (SQL) Experienced Data Engineer for an enterprise big data and analytics platform that utilizes emerging technology to ingest data in real-time at extreme volumes and high velocity, Support data engineering, data analysis, and data visualization. Provides business and technical skills across all phases of the program lifecycle. Designs for continuity of functional or technical requirements through a set of designs, test plans, training materials, software or technical deliverables. Assumes a significant role in the development of functional and technical information system designs, development of software designs, programming, system testing, business process redesign or business process and architectures; or the resolution of risks and issues related to these functional or technical requirements or activities. Interfaces with other project teams and interacts with clients at the supervisory level.",
        "url": "https://www.linkedin.com/jobs/view/3968917183"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Fort Worth, TX",
        "job_id": 3950853231,
        "company": "System Soft Technologies",
        "title": "Data Engineer",
        "created_on": 1720638984.4475398,
        "description": "Data Analytics Fort Worth, TX $98 - 125K Range (Depending on experience) Also an 8% Bonus (performance based) 3-5 years experience preferred in the analytical or financial services field. Technical skills / abilities: Strong technical aptitude. Familiarity with ETL processes, SQL, SAS, R highly preferred. Leverage analytical tools to perform routine data collection, analysis and reporting of credit risk KPI’s (i.e., delinquencies, vintage losses rates, application conversion) to monitor and communicate trends of BorrowWorks’ loan portfolios. Key Responsibility 1: Mine, model, analyze large datasets, and utilizes predictive modeling techniques with an emphasis on optimizing credit risk and analyzing customer behavior using the following predictive modeling techniques: linear/logistic regression, factor analysis, decision trees, clustering, segmentation, etc. Key Responsibility 2: Build and validate a variety of statistical models, provides analytic support, and develops new criteria and/or strategies. Key Responsibility 3: Perform product or other segment level analytics to analyze performance of existing products and assist with roll-out of new product policy and pricing strategies for various product offerings. Key Responsibility 4: Implement credit models, rules, scorecards, and strategies into the company’s Decision Engine software that houses the Credit Policy. Work with internal IT resources to test and deploy new implementations. Key Responsibility 5: Collaborate with other departments across the organization to drive improvement in quality, volume, service, and profitability. Key Responsibility 6: Identify process improvement opportunities and involve in developing/executing projects Education: Bachelors degree in a quantitative subject. (Mathematics, Statistics, Economics preferred) Required experience: 3-5 years experience preferred in the analytical or financial services field. Technical skills / abilities: Strong technical aptitude. Familiarity with ETL processes, SQL, SAS, R highly preferred.",
        "url": "https://www.linkedin.com/jobs/view/3950853231"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Hartford, CT",
        "job_id": 3953947849,
        "company": "Travelers",
        "title": "Data Engineer I",
        "created_on": 1720638986.1711843,
        "description": "Who Are We? Taking care of our customers, our communities and each other. That’s the Travelers Promise. By honoring this commitment, we have maintained our reputation as one of the best property casualty insurers in the industry for over 160 years. Join us to discover a culture that is rooted in innovation and thrives on collaboration. Imagine loving what you do and where you do it. Compensation Overview The annual base salary range provided for this position is a nationwide market range and represents a broad range of salaries for this role across the country. The actual salary for this position will be determined by a number of factors, including the scope, complexity and location of the role; the skills, education, training, credentials and experience of the candidate; and other conditions of employment. As part of our comprehensive compensation and benefits program, employees are also eligible for performance-based cash incentive awards. Salary Range $105,100.00 - $173,400.00 Target Openings 1 What Is the Opportunity? Travelers Data Engineering team constructs pipelines that contextualize and provide easy access to data by the entire enterprise. As a Data Engineer, you will play a key role in growing and transforming our analytics landscape. In addition to your strong analytical mind, you will bring your inquisitive attitude and ability to translate the stories found in data. You will leverage your ability to design, build and deploy data solutions that capture, explore, transform, and utilize data to support Artificial Intelligence, Machine Learning and business intelligence/insights. What Will You Do? Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions. Design data solutions. Analyze sources to determine value and recommend data to include in analytical processes. Incorporate core data management competencies including data governance, data security and data quality. Collaborate within and across teams to support delivery and educate end users on data products/analytic environment. Perform data and system analysis, assessment and resolution for defects and incidents of moderate complexity and correct as appropriate. Test data movement, transformation code, and data components. Perform other duties as assigned. What Will Our Ideal Candidate Have? Bachelor’s Degree in STEM related field or equivalent Six years of related experience Proficient use of tools, techniques, and manipulation including Cloud platforms, programming languages, and an understanding of software engineering practices. The ability to deliver work at a steady, predictable pace to achieve commitments, decompose work assignments into small batch releases, and contribute to tradeoff and negotiation discussions. Demonstrated track record of domain expertise including the ability to understand technical concepts and possess in-depth knowledge of immediate systems worked on. Proven problem solving skills including debugging skills, allowing you to determine source of issues in unfamiliar code or systems and the ability to recognize and solve repetitive problems. Strong verbal and written communication skills with the ability to interact with team members and business partners. Leadership - Intermediate leadership skills with a proven track record of self-motivation in identifying personal growth opportunities. What is a Must Have? Bachelor’s degree or equivalent training with data tools, techniques, and manipulation. Four years of data engineering or equivalent experience. What Is in It for You? Health Insurance: Employees and their eligible family members – including spouses, domestic partners, and children – are eligible for coverage from the first day of employment. Retirement: Travelers matches your 401(k) contributions dollar-for-dollar up to your first 5% of eligible pay, subject to an annual maximum. If you have student loan debt, you can enroll in the Paying it Forward Savings Program. When you make a payment toward your student loan, Travelers will make an annual contribution into your 401(k) account. You are also eligible for a Pension Plan that is 100% funded by Travelers. Paid Time Off: Start your career at Travelers with a minimum of 20 days Paid Time Off annually, plus nine paid company Holidays. Wellness Program: The Travelers wellness program is comprised of tools, discounts and resources that empower you to achieve your wellness goals and caregiving needs. In addition, our mental health program provides access to free professional counseling services, health coaching and other resources to support your daily life needs. Volunteer Encouragement: We have a deep commitment to the communities we serve and encourage our employees to get involved. Travelers has a Matching Gift and Volunteer Rewards program that enables you to give back to the charity of your choice. Employment Practices Travelers is an equal opportunity employer. We believe that we can deliver the very best products and services when our workforce reflects the diverse customers and communities we serve. We are committed to recruiting, retaining and developing the diverse talent of all of our employees and fostering an inclusive workplace, where we celebrate differences, promote belonging, and work together to deliver extraordinary results. In accordance with local law, candidates seeking employment in Colorado are not required to disclose dates of attendance at or graduation from educational institutions. If you are a candidate and have specific questions regarding the physical requirements of this role, please send us an email so we may assist you. Travelers reserves the right to fill this position at a level above or below the level included in this posting. To learn more about our comprehensive benefit programs please visit http://careers.travelers.com/life-at-travelers/benefits/.",
        "url": "https://www.linkedin.com/jobs/view/3953947849"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3934970392,
        "company": "Nuna Inc.",
        "title": "Data Engineer II, Health Data",
        "created_on": 1720638990.1308074,
        "description": "At Nuna, our mission is to make high-quality healthcare affordable and accessible for everyone. We are dedicated to tackling one of our nation’s biggest problems with ingenuity, creativity, and a keen moral compass. Nuna is committed to simple principles: a rigorous understanding of data, modern technology, and most importantly, compassion and care for our fellow human. We want to know what really works, what doesn't—and why. YOUR TEAM Nuna’s product offerings enable delivery of value-based care contracts, at scale across the United States. Value-based healthcare aims to shift reimbursement from fee-for-service to fee-for-value where providers are incentivized to deliver higher quality care to their patients at a lower cost. The mission of the Health Data team within Nuna is to harness healthcare data to support and promote value-based care contracts between health systems, providers, health plans, and other third-party risk bearing entities; incentivize quality, utilization, and patient outcomes. Operationalizing value-based arrangements has various complexities due to the variability in both contract configurations and measurement of health outcomes. The Data Engineer is accountable for creating automated and scalable tools for the Health Data Analysts to accurately and efficiently implement complex value-based care programs. The Health Data team is also responsible for the comprehension, ingestion, cleaning, and deploying of client data on schedule - ensuring data gets where it needs to go within the company. We care about high value care health care, which is why we highly value quality and consistency down to the detail. YOUR OPPORTUNITIES In this role, you’ll have the opportunity to form deep partnerships with a cross-functional team of analysts, engineers and product managers to drive continuous improvement of delivery speed and analytic quality through introduction and adoption of software development and data management best practices. You will be responsible for design, maintenance, improvements and deployment of reusable logic in support of accurate and scalable value-based payment solutions using healthcare driven guidance. You will provide thought leadership to product managers and influence Nuna’s product roadmap with a focus on analytic tooling maturity and scalable healthcare solutions. Finally, you will be a force multiplier for Nuna’s analysts by introducing software development and testing practices tuned to value-based care payment models. In This Role, You Will Automate and optimize data pipelines with a focus on logical transformations and statistical models Build, maintain and optimize data pipelines using big data technologies Consult and support health data analysts in design and deployment of scalable products for healthcare plans and providers Participate in the development of industry accepted healthcare data models and rules-based engines Qualifications Required Qualifications 4+ years of technical software development via Python, Spark, SQL, Git Integration, Scala and testing frameworks Experience building production-hardened data pipelines, with consideration for performance, scalability, reliability, and repeatability using orchestration tools like Airflow or Prefect Experience building Spark jobs, as well as profiling and debugging them Experience rapidly prototyping new product concepts, especially for enterprise clients Understanding of data testing concepts and the ability to consistently apply them Knowledge of database fundamentals like indexing and SQL queries Experience with AWS Strong oral and written communication skills, keen attention to detail and deadline driven A genuine interest to connect with, understand, and shape the healthcare industry Preferred Qualifications Domain experience working with healthcare data such as claims data, membership data, provider data and more Familiarity with Value Based Care concepts Experience working at or with health plans or providers We take into account an individual’s qualifications, skillset, and experience in determining final salary. This role is eligible for health insurance, life insurance, retirement benefits, participation in the company’s equity program, paid time off, including vacation and sick leave. The expected salary range for this position is $135,000 to $165,000. The actual offer will be at the company’s sole discretion and determined by relevant business considerations, including the final candidate’s qualifications, years of experience, and skillset. Nuna is an Equal Employment Opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, disability, genetics and/or veteran status.",
        "url": "https://www.linkedin.com/jobs/view/3934970392"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Trenton, NJ",
        "job_id": 3941634264,
        "company": "Software Technology Inc.",
        "title": "AWS Data Engineer-Trenton, NJ-(Immediate position)",
        "created_on": 1720638991.8090646,
        "description": "Greetings for the day!!! I have the below positions with my client on an emergency basis. Job Title: AWS Data Engineer Location: Trenton, NJ Duration: Long Term Description: This senior consultant is for an Cloud data Engineer that will lead in data pipelines and data infrastructure as well as build or assist in any data engineering tasks. This is an ongoing role beyond that of the dates above. The State of NJ is seeking an Cloud Data Engineer that will assist in maintaining, and monitoring infrastructure as well as build or assist in building data transformation pipelines. Candidate must have prior experience with AWS or Azure and extensive knowledge of python for ETL development. Additional Cloud-based tools experience is important (see skills section) Additional Desired Skills Include Experience With The Following: Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. Extensive experience leveraging Python to build data transformation pipelines (ETL) and experience with libraries such as pandas and numpy. Experience with pyspark. Experience using AWS Glue and EMR to construct data pipelines Experience building and optimizing 'big data' data pipelines, architectures, and datasets. Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. Strong analytic skills related to working with unstructured datasets . Build processes supporting data transformation, data structures, metadata, dependency, and workload management. A successful history of manipulating, processing and extracting value from large disconnected datasets. Working knowledge of message queuing, stream processing, and highly scalable 'big data' data stores. Strong project management and organizational skills. Experience supporting and working with cross-functional teams in a dynamic environment. We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They Should Also Have Experience Using The Following Software/tools: Experience with big data tools: Spark, Kafka,etc. Experience with relational SQL and NoSQL databases, including Postgres. Experience with data pipeline and workflow management tools: AWS Glue, Airflow,etc. Experience with AWS cloud services: EC2, EMR, RDS, Redshift Experience with stream-processing systems: Storm, Spark-Streaming, etc. Experience with object-oriented/object function scripting languages: Python, Java, etc. It is a strong possibility for extension beyond the stated date above. This will be onsite immediately. Schedule is hybrid, 3 days onsite and 2 days remote per week. Please do not submit any candidates that are unable to relocate prior to start of the assignment. Required/Desired Skills Skill Required /Desired Amount of Experience Prior experience with writing and debugging python Required 5 Years Prior experience with building data pipelines. Required 5 Years Prior experience Data lakes in an aws environment Required 3 Years Prior experience with Data warehouse technologies in an aws environment Required 3 Years Prior experience with AWS EMR Required 2 Years Prior experince with pyspark Highly desired 2 Years Warm Regards, Ramakrishna Aripalli !! Talent Acquisition Specialist Software Technology Inc (STI) 100 Overlook Center, Suite 200 Princeton, NJ 08540 Email: Ramakrishna.aripalli@stiorg.com Ph no:(609)- 447-3346 https://www.linkedin.com/in/rama-krishna-b79198102/ www.stiorg.com",
        "url": "https://www.linkedin.com/jobs/view/3941634264"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Yorkville, TN",
        "job_id": 3928853268,
        "company": "Syntricate Technologies",
        "title": "Azure Data Engineer",
        "created_on": 1720638993.5232425,
        "description": "Required Skills Required Skills: Azure Databricks ADLS PySpark",
        "url": "https://www.linkedin.com/jobs/view/3928853268"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Fountain Hills, AZ",
        "job_id": 3965205508,
        "company": "Zortech Solutions",
        "title": "Data Engineer (DE) - US",
        "created_on": 1720638995.2444925,
        "description": "Role: Data Engineer (DE) Location: Scottsdale AZ (day 1 onsite) Duration: Fulltime Must have skill set: Java , Scala , Python, Spark, S3, Glue, Redshift You have 6-8 years of relevant software development experience. You have hands-on experience in Java/Scala/Python, Spark, S3, Glue, Redshift. This is critical. Highly analytical and data oriented. Experience in SQL, NoSql Database Data masking of on prem PII data. Develop API calls with using secure data transfer. Take standard output data to lower environments for pre prod testing! Enable secured channels for data models and data science activities. Need a solution that can scale to handle millions of data. i.e., masking 10 million records in 15 mins You have experience with development tools and agile methodologies.",
        "url": "https://www.linkedin.com/jobs/view/3965205508"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Emerald Isle, NC",
        "job_id": 3959512857,
        "company": "TransImpact",
        "title": "Data Engineer",
        "created_on": 1720638996.8527799,
        "description": "Job Description DATA ENGINEER What We Do Matters TransImpact is an industry leader in the development and application of end-to-end technology-based solutions that optimize shipping operations, create dramatic supply chain efficiencies, and transform the business performance of customers. We are an exciting entrepreneurial-minded company that continues to grow nationwide. Our clients include many well-known names, and we excel in growing our client base through referrals from our satisfied customers. TransImpact has been honored to have achieved multiple awards and acknowledges our employees for them. LOCATION AND WORK: This position is located at our Independence, Ohio location. The ability to work in our office, with a hybrid schedule is required. Our Benefits And Perks We are consistently reviewing our benefits and making improvements to stay competitive and provide our teams with the best options and the most positive culture. Below are the highlights: 3 weeks of PTO in your first year of employment and 4 weeks of PTO in your 2nd year. Half-day Fridays during the Summer months if your schedule allows Hybrid work schedule - Mondays and Fridays you can work from home Medical, dental, and vision coverages at very affordable premiums that include family coverage 6% 401k match after your first year of employment At our offices, we provide free snacks, drinks, fancy coffee, a putting green, and other amenities that add to our laid-back, casual atmosphere. Our headquarters is by the beach in Emerald Isle North Carolina, and we have adopted that laidback but serious about our work type of atmosphere at all of our locations. We work hard and play hard! About The Position And Our Team Do you have a passion for logistics and a knack for turning data into actionable insights? We are seeking a data engineer to join our team and play a key role in optimizing our system processes. In this role, you will leverage your analytical skills to uncover trends and inefficiencies in our shipping data. You will then collaborate with our engineering team to develop solutions that transform raw data into actionable insights and improve overall data quality. Responsibilities Analyze large datasets related to shipping operations to identify trends, patterns, and areas for improvement. Develop data-driven recommendations to optimize shipping efficiency, reduce costs, and improve customer satisfaction. Work closely with software engineers and architects to design and implement data pipelines and systems for data collection, processing, and analysis. Ensure data quality by identifying and resolving data inconsistencies and errors. Communicate complex data insights to stakeholders in a clear, concise, and visually compelling manner. Stay up-to-date on the latest trends and technologies in shipping logistics and data analytics. Job responsibilities and any other duties needed to help drive our vision, fulfill our mission, and support our organization’s values Qualifications And Skills Required Bachelor's degree in Data Science, Statistics, Mathematics, or a related field (or equivalent experience). Proven experience with data analysis tools and techniques (e.g., SQL, Python, R). Strong analytical and problem-solving skills. Excellent communication and collaboration skills. Ability to work independently and as part of a team. Experience in the shipping or logistics industry preferred but not required. What We Will Love About You You want to be a part of something bigger than yourself You respect all people You are mature and thoughtful in how you express ideas; a strong verbal and written communicator. You hustle every day and understand the importance of urgency and owning your outcomes. You’re a leader and a teambuilder. You’re goal-oriented and know how to influence others to move in a unified direction. What You Will Love About Us Here at TransImpact, we know there’s a direct link between happy employees and happy customers, which together generate success for all of us. We believe that a supportive atmosphere, an emphasis on giving and humanity, and approachable leaders and managers help our employees reach their greatest potential. We are very proud of our corporate culture and commitment to giving back to the community. Our employees are encouraged to commit to charity work and volunteering and are allotted 20 hours of PTO time to support our communities. We Are Serious About Our Mission Our world is changing. Our values are not. Since the beginning, TransImpact has maintained a steadfast dedication to our core values: Work Ethic, Integrity, Professionalism, Passion, and Perfection . They guide everything we do. Every interaction with a client, every interaction with each other, everything we create. TransImpact is an affirmative action and equal opportunity employer (EOE). Applicants will not be discriminated against because of race, color, creed, sex, sexual orientation, gender identity or expression, age, religion, national origin, citizenship status, disability, ancestry, marital status, veteran status, medical condition, or any protected category prohibited by local, state, or federal laws. TransImpact is committed to providing access, equal opportunity, and reasonable accommodation for people with disabilities. To request a reasonable accommodation, contact Wendy Bott, Human Resource Department, 440-394-2206. Recruiters: By submitting candidates without having been formally assigned on and contracted for a specific job requisition by TransImpact, or by failing to comply with the TransImpact recruitment process, you forfeit any fee on the submitted candidates, regardless of your usual terms and conditions. TransImpact works with a preferred supplier list and will take the initiative to engage with recruitment agencies based on its needs and will not be accepting any form of solicitation.",
        "url": "https://www.linkedin.com/jobs/view/3959512857"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Seattle, WA",
        "job_id": 3964308300,
        "company": "Amazon",
        "title": "Data Engineer II, Amazon Search, Search Data Science",
        "created_on": 1720638998.5262184,
        "description": "Description Amazon is one of the most popular sites in the US. Our product search engine, one of the most heavily used services in the world, indexes billions of products and serves hundreds of millions of customers world-wide. Our team leads the data science and analytics efforts for the search page and we own multiple aspects of understanding how we can measure customer satisfaction with our experiences. This includes building science based insights and novel metrics to define and track customer focused aspects. We manipulate massive amounts of data, terabytes worth a day, coming from multiple different systems with a high degree of complexity in structure, schema and different levels of quality. We are looking for a data engineer to build and maintain large and complex data pipelines, orchestrated for daily updates with a very high level of senior leadership visibility. These pipelines bring together data from multiple sources with complex structure. You will be creative with ETL techniques to optimize these jobs for performance and cost. About The Team The mission of the Search Data Science team is to build a world class shopping experience that delights customers. We focus on the long term and big picture, ensuring that the search page is balancing strategic trade-offs. We bring to this effort expertise in constrained optimization, causal inference, and marketplace equilibrium effects. We build systems, metrics, and mechanisms to ensure that product decisions are scientifically sound. We develop models to estimate the downstream dollar value of the quality of the experience. We spend time on evaluating experiments to develop durable learnings. Basic Qualifications 3+ years of data engineering experience Experience with data modeling, warehousing and building ETL pipelines Preferred Qualifications Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions Experience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases) Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us. Our compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $118,900/year in our lowest geographic market up to $205,600/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. For more information, please visit https://www.aboutamazon.com/workplace/employee-benefits. This position will remain posted until filled. Applicants should apply via our internal or external career site. Company - Amazon.com Services LLC Job ID: A2687687",
        "url": "https://www.linkedin.com/jobs/view/3964308300"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Charlotte, NC",
        "job_id": 3958047913,
        "company": "Technogen, Inc.",
        "title": "AWS Data Engineer On-site Position in Charlotte, NC",
        "created_on": 1720639000.1399698,
        "description": "TECHNOGEN, Inc. is a Proven Leader in providing full IT Services, Software Development and Solutions for 15 years. TECHNOGEN is a Small & Woman Owned Minority Business with GSA Advantage Certification. We have offices in VA; MD & Offshore development centers in India. We have successfully executed 100+ projects for clients ranging from small business and non-profits to Fortune 50 companies and federal, state and local agencies. Hi, Title: AWS Data Engineer Location: Charlotte, NC Job Type: Contract – On-Site Duration: 12+ Months Mandatory Skills: Python with Object oriented concepts , Spark , Problem-Solving ability, Effective communication, and minimum 10+Years of experienced professional required Job Description The client is currently seeking a Senior Data Engineer with hands-on coding experience and a strong background in Python, PySpark, and Object-oriented programming. The ideal candidate will be responsible for designing, developing, and implementing new features to our existing framework using PySpark and Python. This position requires a deep understanding of data transformation and the ability to create standalone scripts based on given business logic. Responsibilities Design, Develop, and Implement new features to our existing framework using PySpark and Python. Write efficient and effective standalone scripts in PySpark with transformations as per the defined business logic. Use your expertise in Python and Object-oriented concepts to solve complex problems and implement robust solutions. Work closely with the team to understand the requirements and develop solutions that align with the company's objectives. Test and debug code to ensure it produces the desired results. Document all programming tasks and procedures for future reference and troubleshooting. Qualifications Proficient in Python, PySpark, and Object-oriented programming concepts. Proven experience as a Senior Data Engineer or similar role. Strong problem-solving techniques with an ability to troubleshoot complex software issues. Experience with AWS is preferred. Excellent communication skills, both written and verbal. Self-motivated and able to work independently with minimal supervision. Thanks & regards Nava Surya Talent Acquisistion specialist navasurya.s@technogeninc.com 571-660-1025",
        "url": "https://www.linkedin.com/jobs/view/3958047913"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Salt Lake City, UT",
        "job_id": 3965474163,
        "company": "JERRY SEINER CHEVROLET",
        "title": "Data Engineer",
        "created_on": 1720639001.8424938,
        "description": "Job Summary Jerry Seiner is looking for an exceptional data engineer with experience in PostgreSQL, Big Query, and Python. As the sole member of the Business Intelligence team, the Data Engineer works closely with multiple departments and vendors throughout the organization to transform data into actionable information. The ideal candidate will have experience crafting visual data representations by using Excel, Vena, Tableau and Looker Studio. This position is in Salt Lake City, Utah and relocation would be required. A hybrid schedule is available after training. Key Responsibilities Gather and document requirements for the data warehouse, cloud database solutions, and technologies. Help maintain and support ETL jobs and coding, pulling data from various source systems and loading data into the data warehouse. Testing new data pipelines or workflow processes. Develop and maintain databases, data systems, dashboards, and reports to facilitate data analysis and visualization. Perform data cleaning, validation, and preprocessing to ensure data accuracy and integrity. Conduct advanced statistical analysis and modeling to support business forecasting and decision-making processes. Participate in developing and implementing data-related projects, including data governance and quality initiatives. Design and implement intuitive and actionable user reporting using data visualization tools such as Tableau, Vena, and Looker Studio. Collaborate with stakeholders to understand and translate business requirements into data analysis solutions Interact with employees as a technical resource to troubleshoot problems with the delivered BI solutions. Ability to explain advanced data concepts to non-technical users and provide customer assistance and ad hoc training. Fulfill ad hoc data requests and analysis on time. Strong workload management skills and abilities, with an emphasis on the ability to prioritize and attention. Other Data Engineering tasks as determined. Required Qualifications Bachelor's degree in Computer Science, Statistics, Mathematics, Economics, or a related field. Two years of data warehouse experience including one year of experience working on projects with multiple deliverables Strong proficiency in data manipulation, analysis, and visualization using Big Query, SQL, Python, R, Excel, or Tableau tools. Experience with data querying, data modeling, and database design. Capable of developing ETL code. Solid understanding of statistical concepts and methods (e.g., hypothesis testing, regression, clustering). Excellent troubleshooting and problem-solving skills, attention to detail, and passion for working with complex datasets. Ability to effectively communicate technical information to both technical and non-technical stakeholders. Ability to quickly learn new tools and technology. Nice To Have One year of experience in data analysis and report design/development using BI software such as Tableau, Looker Studio etc. Applicants must be authorized to work for ANY employer in the U.S. We are unable to sponsor or take over sponsorship of an employment Visa at this time. Family-owned since 1980, The Jerry Seiner Dealerships believes in helping all employees find the career opportunity that is right for them. We encourage the individual growth and development of our employees through our many employee-focused benefits. For more information about our organization, please visit : hYps://seinercareers.com/ Experience \" The Seiner Difference! \" At Jerry Seiner, we are a family-owned business proudly serving the Utah community since 1980 and have recently expanded into Arizona and Nevada. The Seiner culture believes in helping our team members achieve their highest level of success through training, mentoring, and career advancement. We encourage individual growth and development of our employees through our employee-focused benefits. Explore the many career opportunities that come with being a part of the Seiner team. Benefits May Include Paid Time Off (PTO) accumulates from Day 1 Grow with us! Educational reimbursement Health, Dental & Vision Insurance Employer Health Saving Account contribution each month 401k Retirement plan Year-end bonus programs Great discounts on vehicle purchases Discounts on parts and service Referral bonuses for vehicle purchases Closed Sundays! Pre-employment screenings, including but not limited to your background screening, drug test, and motor vehicle record, are required. Job Posted by ApplicantPro",
        "url": "https://www.linkedin.com/jobs/view/3965474163"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3941167428,
        "company": "Merit321, Launching Careers",
        "title": "Data Engineer",
        "created_on": 1720639003.5796576,
        "description": "Position: Data Engineer Location: Columbia, MD Clearance: Current active PLACEMENT MANAGER, SCI eligible Summary We are looking for a talented Data Engineer to support the acquisition of mission critical and mission support data sets. The preferred candidate will have a background in supporting cyber and/or network related missions within military spaces, as either a developer, analyst, or engineer. Essential Job Responsibilities The ideal candidate will have worked with big data systems, complex structured and unstructured data sets, and have supported government data acquisition, analysis, and/or sharing efforts in the past To excel in the position, the candidate shall have strong attention to detail, be able to understand technical complexities and have the willingness to learn and adapt to the situation The candidate will work both independently and as part of a team to accomplish client objectives Minimum Qualifications 9 years total relevant experience BS in Engineering, Computer Science, org technical degree or industry experience equivalent (+4 years relevant experience can substitute for a BS degree) Experience with programming languages such as Java Experience with Java Unit and Integration testing Fluency with data extraction, custom translation development, and loading including data prep and labeling to enable data analytics Familiarity with various log formats such as JSON, XML, and others Experience with data flow, management, and storage solutions (i.e., Kafka, NiFi, and AWS S3 and SQS solutions) Ability to decompose technical problems and troubleshoot system and dataflow issues. Must be certified DoD IAT II or higher Willingness to do a programming challenge during the interview process. Experience with Zoom/Teams/Meet style team meetings Place of Performance: either Columbia, MD, Sterling, VA or San Antonio, TX. All positions will involve Hybrid work Desired Skills (Optional) Experience with Python Experience with NOSQL databases such as Accumulo desired Experience developing with Kubernetes environments Prior Experience supporting cyber and/or network security operations within a large enterprise, as either an analyst, engineer, architect, or developer Familiarity with the Agile Project team and task management environment EEO It is the policy of Merit321 to provide equal opportunity in recruiting, hiring, training, and promoting individuals in all job categories without regard to race, color, religion, national origin, gender, age, disability, genetic information, veteran status, sexual orientation, gender identity, or any other protected class or category as may be defined by federal, state, or local laws or regulations. recruiting, hiring, training, and promoting individuals in all job categories without regard to race, color, religion, national origin, gender, age, disability, genetic information, veteran status, sexual orientation, gender identity, or any other protected class or category as may be defined by federal, state, or local laws or regulations.",
        "url": "https://www.linkedin.com/jobs/view/3941167428"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Atlanta, GA",
        "job_id": 3943023241,
        "company": "Robert Half",
        "title": "Data Engineer",
        "created_on": 1720639005.2517464,
        "description": "We are seeking an experienced Data Strategist to join our dynamic team. In this role, you will be responsible for designing, building, and maintaining the data infrastructure that supports our firm's mission-critical applications and data-driven decision-making processes. You will work closely with stakeholders to ensure the availability, reliability, and quality of data assets across the organization. Key Responsibilities: Design, develop, and maintain data pipelines and ETL processes to extract, transform, and load data from various sources (e.g., financial databases, APIs, flat files) into our data warehouses and data lakes. Implement efficient and scalable data ingestion mechanisms, ensuring data quality, integrity, and consistency, while optimizing performance and resource utilization. Collaborate with key stakeholders to ensure the availability, quality and accuracy of data required for analysis and modeling. Participate in cross-functional projects, working closely with teams such as investment management, product management, client solutions and compliance to understand their analytical needs and provide data-driven insights. Build and optimize data models, schemas, and data warehousing solutions to support advanced analytics and reporting needs. Implement data governance policies, data security measures, and access controls to ensure data privacy and compliance with industry regulations. Automate data processes and workflows using scripting languages (e.g., Python, Bash) and orchestration tools (e.g., Apache Airflow, AWS Data Pipeline). Proactively monitor data pipelines, identify and resolve issues. Contribute to the development of data best practices, tools, and processes within the organization. Stay up to date with the latest data engineering technologies, tools, and best practices, and contribute to the continuous improvement of our data infrastructure. Qualifications and Skills: Proficiency in SQL and experience with database management systems (e.g., PostgreSQL, MySQL, Oracle, SQL Server). Knowledge of cloud platforms like AWS, Azure, GCP or Snowflake and experience with cloud-based data services (e.g., AWS Glue, AWS Athena, Azure Data Factory). Knowledge of data visualization tools (e.g., Tableau, Power BI) and their integration with data pipelines. Strong problem-solving and analytical skills, with the ability to break down complex problems into manageable tasks. Excellent communication and collaboration skills, with the ability to work effectively with cross-functional teams and stakeholders. Preferred Qualifications: 3-5 years of experience in data engineering or a related role, with a strong understanding of data warehousing, ETL processes, and data modeling. Strong programming skills in Python, Scala, or similar languages. Experience with financial data models, risk data, or other data domains relevant to the financial services industry. Knowledge and curiosity about AI and LLM and their integration in data warehouse structures. Experience with containerization technologies (e.g., Docker, Kubernetes) and infrastructure as code (IaC) practices.",
        "url": "https://www.linkedin.com/jobs/view/3943023241"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Englewood Cliffs, NJ",
        "job_id": 3970260514,
        "company": "NBC Sports Next",
        "title": "Sr. Data Engineer - NBC Sports Next",
        "created_on": 1720639007.0734098,
        "description": "Company Description We create world-class content, which we distribute across our portfolio of film, television, and streaming, and bring to life through our theme parks and consumer experiences. We own and operate leading entertainment and news brands, including NBC, NBC News, MSNBC, CNBC, NBC Sports, Telemundo, NBC Local Stations, Bravo, USA Network, and Peacock, our premium ad-supported streaming service. We produce and distribute premier filmed entertainment and programming through Universal Filmed Entertainment Group and Universal Studio Group, and have world-renowned theme parks and attractions through Universal Destinations & Experiences. NBCUniversal is a subsidiary of Comcast Corporation. Here you can be your authentic self. As a company uniquely positioned to educate, entertain and empower through our platforms, Comcast NBCUniversal stands for including everyone. Our Diversity, Equity and Inclusion initiatives, coupled with our Corporate Social Responsibility work, is informed by our employees, audiences, park guests and the communities in which we live. We strive to foster a diverse, equitable and inclusive culture where our employees feel supported, embraced and heard. Together, we’ll continue to create and deliver content that reflects the current and ever-changing face of the world. Job Description NBC Sports Next is where sports and technology intersect. We’re fueled by our mission to innovate, create larger-than-life events and connect with sports fans through technology. We’re a subdivision of NBC Sports and home to leading technology platforms and digital applications for Youth & Recreational Sports; Golf; and Emerging Media. At NBC Sports Next, we equip more than 30MM players, coaches, athletes, sports administrators and fans in 40 countries with more than 25 sports solution products, including SportsEngine, the largest youth sports club, league and team management platform; SportsEngine Play, the first ever streaming service for youth and amateur sports, GolfNow, the leading online tee time marketplace and provider of golf course operations technology; and GolfPass the ultimate golf membership that connects golfers to exclusive content, tee time credits, instructional content and more. Golf fuses the team behind products and services like GolfNow, TeeOff and GolfPass, which better connects golfers and golf facilities around the world through innovative solutions like cloud-based golf course management and SmartPlay contactless technology and services that create optimum golfing experiences. The Data Engineer IIII will be responsible for architecture, design, development, and maintenance of analytical data streams for new and existing projects. This position will join a team of data engineers that manage and administer data pipelines, data warehousing, data marts, and business intelligence environments within our NBC SportsNext – Golf team. This individual will work closely with users to define requirements for development, coordinate acceptance testing, training, and to identify enhancements to existing solutions. Also, they'll help lead our data engineer team members and work closely with other development leads to ensure overall integrity of the application. The candidate will design and develop data pipelines, data marts, and schemas to support our data warehousing environments and analytical requirements Database architecture and design of data warehouses & data marts to support presentation layers elastic cubes, reporting and analytics. Hands on experience with tsql development, query tuning, table development, stored procedures, functions Hands on experience with creating indexes to enhance performance of workloads Hands on experience with development of ETL/ELT processes, share responsibilities for monitoring the data warehouse and data marts Hands on development and support of data pipelines in MS SSIS, Azure Data factory, PowerShell, Python, Golang Hands on experience with cloud environments Azure, AWS, GCP Troubleshoot and resolve data issues in production system Communicate with business users and project managers to resolve business problems and create solutions Use of technical experience to suggest better solutions to new and existing problems Involvement with other IT departments providing systems communication processes Meet with various IT groups (other departments and computer operations' staff) to address issues/concerns. This includes other business unit IT groups, as well as the technical operations group (Server Group and other Infrastructure groups) Mentor other Data Engineers Identify software solutions needed for new projects Weekly reporting to management on project status and issues On call production support Qualifications Minimum Qualifications: 7+ years of data warehousing / OLAP experience 7+ years multidimensional database architecture and design experience 5+ years of SQL Experience 3+ years of Python Experience 2+ years of PowerShell Experience 2+ years of Redshift Experience 2+ years of NoSQL Experience Team and vendor partner management Clear understanding of principles and techniques of Web development Experience with AWS Glue and Airflow Experience with API Dimensional database architecture experience Formal systems life cycle methodology experience Desired Qualifications Cloud Data Warehouse operations experience Tableau, SSRS, big data and Realtime BI experience Experience with data streaming This position has been designated as fully remote , meaning that the position is expected to contribute from a non-NBCUniversal worksite, most commonly an employee’s residence Additional Information NBCUniversal's policy is to provide equal employment opportunities to all applicants and employees without regard to race, color, religion, creed, gender, gender identity or expression, age, national origin or ancestry, citizenship, disability, sexual orientation, marital status, pregnancy, veteran status, membership in the uniformed services, genetic information, or any other basis protected by applicable law. If you are a qualified individual with a disability or a disabled veteran, you have the right to request a reasonable accommodation if you are unable or limited in your ability to use or access nbcunicareers.com as a result of your disability. You can request reasonable accommodations by emailing AccessibilitySupport@nbcuni.com.",
        "url": "https://www.linkedin.com/jobs/view/3970260514"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Houston, TX",
        "job_id": 3944012033,
        "company": "Robert Half",
        "title": "Data Engineer",
        "created_on": 1720639008.7014868,
        "description": "Data Integration and ETL: Develop and maintain Extract, Transform, Load (ETL) processes using Spark, Python, Scala, Databricks, Azure Data Factory and other relevant tools to integrate data from various sources. Azure Service Implementation: Implement and configure Azure services such as Azure Data Factory, Azure Synapse Analytics, Azure Databricks, Azure SQL Database, and Azure Cosmos DB to build and manage data pipelines and data processing workflows. Data Modeling and Optimization: Design and implement data models to optimize data storage, querying, and retrieval. Utilize best practices for data modeling and indexing to ensure optimal performance and scalability. Data Security and Compliance: Oversee the implementation of data security measures and ensure compliance with data privacy regulations and industry standards. This includes implementing encryption, access controls, and monitoring mechanisms to protect sensitive data. Performance Tuning and Monitoring: Monitor and optimize the performance of data solutions by identifying and addressing bottlenecks, fine-tuning queries, and ensuring efficient resource utilization. Troubleshooting and Support: Provide support in troubleshooting data-related issues, identifying root causes, and implementing solutions to maintain data integrity and availability. Provide architectural and design direction to development and AMS teams in alignment with principles defined by Architecture team. Support identifying and resolving critical support issues or technical challenges. Establish software development standards and processes for code reviews. Oversee code standards and technical execution for all development staff including AMS team members. Responsible for high level and detailed technical design and explores solution alternatives in collaboration with technical standards defined by architecture group. Translate business requirements to design specifications. Participate in designing solutions to effectively and efficiently fulfill functional and non-functional requirements. Ensures quality integration between various systems. Design and implement standards, best practices, and accountable for technical delivery of data solutions. Promote collaboration and engagement. Own the technical design and have domain decision rights in alignment with Enterprise Architecture. Acts as technical escalations point of contact in respective Delivery Team for technical coordination across IS Operations, Infrastructure, and integrations teams. Position Requirements: Bachelor’s degree in Math, Computer Science, Engineering, Business Administration, or relevant practical experience preferred. Proven technology experience in a complex enterprise environment Strong communication skills, both written and spoken Proficiency in Azure Synapse Analytics, Azure Data Factory, Azure Databricks, Azure SQL Database Experience in data modeling, data warehousing, and data lake design and implementation. Experience with programming languages such as SQL, Python, or Scala for data manipulation and scripting. Knowledge of data integration techniques, ETL processes, and data pipeline orchestration. Knowledge of software engineering best practices such as code reviews, testing frameworks, maintainability and readability would be an asset. Knowledge of AI and ML is a plus. Experience with agile delivery frameworks",
        "url": "https://www.linkedin.com/jobs/view/3944012033"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "New York City Metropolitan Area",
        "job_id": 3943395558,
        "company": "INSPYR Solutions",
        "title": "AWS Data Engineer",
        "created_on": 1720639012.0985098,
        "description": "SUMMARY The AWS Data Engineer will join a critical central infrastructure team to support the build out and integration of new global data pipelines. You will take ownership of key ETL deliverables and drive their completion from end to end. You will be working with very large data sets, well beyond the scalability limits of conventional relational databases. We're looking for people who innovate, love solving hard problems, and never take \"no\" for an answer. You will have the opportunity to research and work with cutting edge technologies, as well as expand into new projects. This is a 5 month W2 contract opportunity with long-term extensions possible. Hybrid work in NYC (onsite 3x per week) is required. ***Please note: This role is not eligible for C2C/C2H work. REQUIREMENTS A successful candidate will have: 2-5+ years of Data Engineering experience Strong Python programming experience Strong SQL experience - writing queries, functions, joins etc Experience with data modeling and data warehousing Experience building ETL pipelines self sufficiently, end-to-end Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions preferred Experience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases) Experience working with very large data sets in an enterprise environment Bachelors degree in computer science or related field BENEFITS - Company-sponsored Health, Dental, and Vision insurance plans.",
        "url": "https://www.linkedin.com/jobs/view/3943395558"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Atlanta, GA",
        "job_id": 3959813100,
        "company": "Georgia-Pacific LLC",
        "title": "Data Engineer",
        "created_on": 1720639013.7533352,
        "description": "Your Job We are seeking a highly motivated, forward-thinking professional to support the enterprise GP Enterprise Analytics Group and develop custom GP advanced analytic solutions used across 50+ facilities within multiple divisions.  This group creates sustainable value and competitive advantage by leveraging analytics, information, technology, and actionable insights across the enterprise while focusing on futuristic possibilities of analytics. This role will have the opportunity to leverage the latest BI and Analytics technologies working closely with our data scientists and manufacturing capabilities. What You Will Do Own the development and deployment of data engineer, model engineering and application engineering. Collaborate with data scientists, other engineers, and product managers to launch new GP AI products, iterate on existing features, and build a world-class user experience. Implement cutting-edge technologies and will be writing state-of-the-art code to ensure deliveries of highly performant and elegant applications. Foster growth and a collaborative mindset for all members of your team. Proactively identify technical debt and implement solutions to improve product iteration speed. Lead by example and foster a culture of excellence, high execution velocity, and intellectual humility. Establish best practices for high-quality technical design and engineering. Manage own learning and contribute to technical skill building of the team. Who You Are (Basic Qualifications) Python, SQL, and additional languages development experience. Experience working with AWS services. Expertise in crafting code within containerized environments. Bachelor’s degree. Familiarity with version control/SCM is a must (experience with git is a plus). Understanding of DevOps processes. Strong organizational and troubleshooting skills with attention to detail. Strong analytical ability, judgment, and problem-solving techniques. Knowledge of (and a passion for) current trends and best practices in front-end architecture, including performance, accessibility, security, and usability. Knowledge of data structures and algorithms. What Will Put You Ahead Experience working with streaming technologies such as Kafka, NATS Kubernetes experience. Terraform development experience. Working experience with monitoring/alerting tools such as Dynatrace, Splunk, LogicMonitor, Solarwinds Bachelor’s or master’s degree in Computer Science, Engineering, or related fields. Knowledge of deploying and executing applications on edge. At Koch companies, we are entrepreneurs. This means we openly challenge the status quo, find new ways to create value and get rewarded for our individual contributions. Any compensation range provided for a role is an estimate determined by available market data. The actual amount may be higher or lower than the range provided considering each candidate's knowledge, skills, abilities, and geographic location. If you have questions, please speak to your recruiter about the flexibility and detail of our compensation philosophy. Hiring Philosophy All Koch companies value diversity of thought, perspectives, aptitudes, experiences, and backgrounds. We are Military Ready and Second Chance employers. Learn more about our hiring philosophy here. Who We Are At Koch, employees are empowered to do what they do best to make life better. Learn how our business philosophy helps employees unleash their potential while creating value for themselves and the company. Our Benefits Our goal is for each employee, and their families, to live fulfilling and healthy lives. We provide essential resources and support to build and maintain physical, financial, and emotional strength - focusing on overall wellbeing so you can focus on what matters most. Our benefits plan includes - medical, dental, vision, flexible spending and health savings accounts, life insurance, ADD, disability, retirement, paid vacation/time off, educational assistance, and may also include infertility assistance, paid parental leave and adoption assistance. Specific eligibility criteria is set by the applicable Summary Plan Description, policy or guideline and benefits may vary by geographic region. If you have questions on what benefits apply to you, please speak to your recruiter. Additionally, everyone has individual work and personal needs. We seek to enable the best work environment that helps you and the business work together to produce superior results. Equal Opportunities Equal Opportunity Employer, including disability and protected veteran status. Except where prohibited by state law, some offers of employment are conditioned upon successfully passing a drug test. This employer uses E-Verify. Please click here for additional information.",
        "url": "https://www.linkedin.com/jobs/view/3959813100"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3940821122,
        "company": "Syntricate Technologies",
        "title": "Snowflake Data Engineer",
        "created_on": 1720639015.5841444,
        "description": "Required Skills: 7 to 10 years' experience Solid experience in cloud based modern DW stack Snowflake, Databricks, Python, SQL , SSRS, SSIS Familiarity with Java is great Familiarity with Airflow and DevOps Roles & Responsibilities: Snowflake + Databricks + Azure Data Factory with Python+ Java Design, develop & maintain data platform services for varied use cases such as Data Extraction, Data Processing, Data Ingestion, Data Observability and Data Discovery. Good hands-on experience on Snowflake Data Warehouse Should be able to implement Row and Column level security in Snowflake. Should have good hands-on experience in writing stored procedures and complex sql queries in snowflake. Experience in performing ETL using Databricks on Azure Cloud. Excellent command of one or more programming languages, preferably Python or PySpark Strong knowledge & experience of architecture & internals of Apache Spark using Python. Proficient in Spring framework and Spring Boot Proficient in SQL and Query tuning Prior experience in the healthcare domain working directly with Healthcare End Users (customers) Comfortable working in Agile methodology with flexible attitude towards learning new technologies. Knowledge of Rally is desirable.",
        "url": "https://www.linkedin.com/jobs/view/3940821122"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Atlanta, GA",
        "job_id": 3966292285,
        "company": "hackajob",
        "title": "Data Engineer",
        "created_on": 1720639017.3019357,
        "description": "Exciting Opportunity Alert! Join our Team as a Data Engineer! hackajob has partnered with a cutting-edge company seeking a talented Data Engineer with expertise in ETL Processes and SQL/NoSQL Databases to join our innovative team! Role: Data Engineer Location: Hybrid (Atlanta, GA, USA) Salary: $110K - $190K depending on experience + benefits package Required Skills: Strong proficiency in Python and Scala Deep understanding of ETL Processes and experience in SQL and NoSQL databases. Experience in building and maintaining Big Data Pipelines. Experience with CI/CD processes and working with cloud providers (Azure, AWS or GCP). Experience in Databricks. If you're interested in finding out more about this fantastic opportunity please get your application in and we can arrange a call. hackajob is a recruitment platform that will match you with relevant roles based on your preferences and in order to be matched with the roles you need to create an account with us. *This role requires you to be based in the US*",
        "url": "https://www.linkedin.com/jobs/view/3966292285"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Charlotte Metro",
        "job_id": 3965383836,
        "company": "Home Solutions",
        "title": "Data Engineer",
        "created_on": 1720639020.7746503,
        "description": "About the Role Home Solutions is seeking a hands-on, self-motivated Data Engineer to own our data analytics pipeline for one of our fastest growing businesses. In this role, you will be responsible for collecting and modeling data through the deployment of modern cloud tools, and turning that data into insights through the building of dashboards and reports. This role is for the data engineer that seeks to get closer to the business, or the business analyst that seeks to dive deeper into understanding where their data comes from. Your Day to Day Create and maintain scalable data pipeline through combination of modern SaaS applications and custom-built solutions Create and maintain business intelligence roadmap to help enable business goals Partner with business users to gather and understand data requirements Extract data from sources through custom-built data integrations (typically Python) with discipline towards automation Write SQL-based transformations to turn raw data into production-ready business models Develop and manage business and executive team dashboards Create and maintain data storage systems Develop data discoverability tools and data monitoring systems About You Proficiency in SQL and Python (and/or proven ability to pick up a new language) Proven ability to operate autonomously to achieve results. General understanding of the Marketing Technology stack, components, and best-practices. An ideal candidate will have some familiarity with common marketing tools (Google Analytics, Tag Manager, Segment (CDP), Facebook Ads, etc.) Experience with common enterprise analytics tools with proven ability to translate data insights into action (Tableau, PowerBI, Looker Studio, Superset, etc.) Working knowledge of software development best practices as they apply to data engineering, including: Version Control, Unit Testing, and Continuous Integration/Continuous Delivery (CI/CD) About Us Launched Fall 2017, Home Solutions targets the rapidly digitizing home services vertical and matches homeowners with the right service provider to meet their needs. Through our websites, Home Solutions has a proprietary audience of 47M+ homeowners and prime real estate in search engines. We are on a mission to make homeownership easier by creating high quality content that pairs consumers with providers in a range of related categories. Home Solutions was incubated within Three Ships, a growth equity firm that launches and invests in digital companies, builds great leadership teams, and helps them rapidly scale. The Three Ships portfolio currently includes several businesses - Home Solutions, Pillar 4, Stacksphere , and 3S UK - and over 50+ websites that help consumers navigate the overwhelming choices through online marketplaces and the most informative content online. Headquartered in Raleigh with offices in Charlotte and London, we are always looking to find the right people to help us continue to grow this business and place a high value on teammates with a growth mindset and a “get after it” mentality. Why You Should Join Us Results : At Three Ships we have eleven consecutive years of profitability and a track record of successful growth in the digital marketing space. Stability : We are privately owned, have a holding period of “forever,” have no debt, and have significant cash to invest – we’re “rock-solid” financially. Exposure : You will have a front-row seat in growing a business. Your teammates have built and sold companies, managed 100s of employees, and run campaigns with Fortune 500 brands. Market Landscape : The digital home services marketing landscape is transforming. There’s no better time than now to be building a business in this space. Career Growth : There is no cap on growth, promotions or the opportunity to own and put your stamp on important projects. Prove your value and you will be rewarded accordingly. Learning : We are a small and mighty team but also have the advantage of tapping into the Three Ships ecosystem and working with subject matter experts in paid media, creative, branding, content strategy, SEO, and more. The opportunity to grow new skills and learn from smart people is endless. Fun : The team has a high bar for excellence, but also a real interest in each other and making work fun. The Package As a full-time employee of Home Solutions, you’ll have access to competitive benefits, including flexible time off, health/dental/vision, 401k match, an annual Relax & Recharge Bonus, an annual Learning & Development stipend to enroll in class(es) of your choosing, and up to $75 mobile reimbursement. If you join us in person in our Raleigh or Charlotte locations, we have an office stocked with snacks, coffee, and just about every other beverage you can imagine. EEOC Statement All applicants are considered without regard to race, color, religion, sex, national origin, age, disability, veteran status, gender identity, or any other discriminatory factors. Please note that we do not provide immigration sponsorship for this role. All offers are subject to a background check.",
        "url": "https://www.linkedin.com/jobs/view/3965383836"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Bellevue, WA",
        "job_id": 3944073241,
        "company": "TekWissen ®",
        "title": "Data Engineer",
        "created_on": 1720639022.4381385,
        "description": "Overview TekWissen Group is a workforce management provider throughout the USA and many other countries in the world. Our client is a company operating a marketplace for consumers, sellers, and content creators. It offers merchandise and content purchased for resale from vendors and those offered by third party sellers. Job Title: Data Engineer II Location: Bellevue, WA 98004 Duration: 11 Months Job Type: Contract Work Type: Onsite Job Description Are you interested in guiding key business decisions around one of Client’s most significant customers facing teams? Do you want to build a cutting-edge highly scalable analytics big data platform using AWS technologies such as Redshift, RDS, S3, EMR, Glue, ADP, Hive, Kinesis, SNS/SQS, and NAWS supported streaming services? Do you want to collaborate with Business Intelligence Engineers (BIE) and Data Scientists (DS) to build ML/LLM models to support our major customer facing features and initiatives to drive team efficiency? We are seeking an experienced, self-driven, analytical, and strategic Sr. Data Engineer. In this role, you will be working in one of the world's largest and most complex data warehouse environments. You should be passionate about working with huge datasets and be someone who loves to bring data together to answer business questions. You should have deep expertise in creation and management of data lake and the proven ability to guide continuous enhancement of data architecture by identifying efficiency gaining solutions and leveraging evolving ne technologies. In this role, you will have ownership of end-to-end development of data engineering solutions to complex questions and you’ll play an integral role in strategic decision-making. The right candidate will possess excellent business and communication skills, be able to work with business owners to tackle ambiguous business questions with creative data/science solution designs and be able to collaborate with BIEs and DSs to build those solutions or answer business questions. You should have a solid understanding of how to build efficient and scalable data infrastructure and data models and have the capability or the desire to learn and implement Elastic MapReduce (EMR)-based solutions where appropriate. Story Behind the Need – Business Group & Key Projects Business Group Delivery Experience Reason New Temporary Worker Extension Typical Day in the Role: Maybe Conversion To FTE Maybe Interaction With Team About the team We consider ourselves a start-up team with a big vision and “anything is possible” mindset when faced with highly ambiguous and complex problems. We are a highly collaborative and high-energy team that isn’t afraid to swing for the fences or exceed expectations even on tactical projects. Primary Responsibilities Key job responsibilities In this role, you will have the opportunity to display your skills in the following areas: Design, implement, and support an analytical data infrastructure providing ad hoc access to large datasets and computing power Managing AWS resources including EC2, RDS, Redshift, etc Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using SQL and AWS big data technologies Explore and learn the latest AWS technologies to provide new capabilities and increase efficiency Collaborate with BIEs to recognize and help adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation Collaborate with DS to implement advanced analytics algorithms that exploit our rich data sets for statistical analysis, prediction, clustering and machine learning Help continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers AWS technologies such as Redshift, RDS, S3, EMR, Glue, ADP, Hive, Kinesis, SNS/SQS, and NAWS supported streaming services TekWissen® Group is an equal opportunity employer supporting workforce diversity. TekWissen is an emerging global human capital, recruitment and IT services organization. Operating since 2009, we draw upon more than a decade of staffing experience to deliver critical talent acquisition solutions and IT engagements for our clients. We’re founded on a culture that is passionate about delivering tailored solutions, that create lasting partnerships. Our global footprint covers six countries: United States, Canada, Australia, India, United Kingdom and the Philippines. This allows us to work in close partnership with organizations and manage everything from global talent needs with demanding resourcing strategies, to single sites with lower recruitment volumes. TekWissen® is an equal opportunity employer supporting workplace diversity.",
        "url": "https://www.linkedin.com/jobs/view/3944073241"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Bethlehem, PA",
        "job_id": 3955651504,
        "company": "André",
        "title": "Data & Analytics - Data Engineer (Machine Learning)- Hybrid - 3 Days onsite",
        "created_on": 1720639024.1172707,
        "description": "André Global Inc. is a global provider of information technology services. Since 2002, we have helped our clients in the APAC region achieve their business goals by leveraging the power of technology. In 2016, we entered the AMERICAS to re-create similar success here as well. Headquartered in New York City, we are now serving our global clients. We strive to deliver customer satisfaction through our service quality to all our clients! Our global and diverse team helps our customers achieve their project goals at a faster pace with high affordability but without any compromise on quality. We take pride in providing an outstanding quality of service to all our customers starting from startups, small businesses to Fortune 500. With more than a decade of experience servicing customers across geographies, we have stationed ourselves at a unique position in the global market. Our ability to assemble the best workforce in record time has been a great asset to our clients. This is a contract position with our Insurance Client. Location: Bethlehem, PA. Resource will be required to work onsite a minimum of 3 days per week in the Bethlehem, PA office. Local candidates preferred. If you are not local to Bethlehem, you will be required to relocate and work onsite from Day 1. You Will Collaborate with data scientists and analysts to understand data requirements and translate them into scalable, high performant data pipeline solutions. Support data discovery & data preparation for model development. Perform detailed analysis of raw data sources by applying business context and collaborate with cross-functional teams to transform raw data into curated & certified data assets to be used for ML and BI use cases. Collaborate with data science and data engineering team to build scalable and reproducible machine learning pipelines for training and inference. Implement machine learning models into operations and processes via batch, streaming and API methods. Monitor and troubleshoot data pipeline performance, identifying and resolving bottlenecks and issues. Develop, test, and maintain robust tools, frameworks, and libraries that standardize and streamline the data & machine learning lifecycle. Contribute to developing and maintaining end-to-end MLOps lifecycle to automate machine learning solutions development and delivery. Implement robust monitoring framework for model performance. Collaborate with cross-functional teams of Data Science, Data Engineering, business units and various IT teams. Create and maintain effective documentation for project and practices ensuring transparency and effective team communication. You Have Bachelor’s or master’s degree with 5+ years of experience in Computer Science, Data Science, Engineering, or a related field. 4+ years of experience in working with Python, SQL, PySpark and bash scripts. Proficient in software development lifecycle and software engineering practices. 2+ years of hands-on experience in using Databricks platform 3+ years of hands-on experience in operationalizing Machine Learning solutions which are used in live production processes. 2+ years of experience and proficiency in API development using FastAPI frameworks and familiarity with containerization technologies like docker or Kubernetes. 3+ years of experience in developing and maintaining robust data pipelines data to be used by Data Scientists to build ML Models. 3+ years of experience working with Cloud Data Warehousing (Redshift, Snowflake, Databricks SQL or equivalent) platforms and experience in working with distributed framework like Spark. Solid understanding of machine learning life cycle, data mining, and ETL techniques. Experience with machine learning frameworks (like Keras or PyTorch) and libraries (like scikit-learn, xgboost). Hands-on experience in building and maintaining tools and libraries which have been used by multiple teams across organization. Proficient in understanding and incorporating software engineering principles in design & development process. Hands on experience with CI/CD tools (e.g., Jenkins or equivalent), version control (Github, Bitbucket), Orchestration (Airflow, Prefect or equivalent) Excellent communication skills and ability to work and collaborate with cross functional teams across technology and business. Good To Have Familiarity with deep learning frameworks and deploying deep learning models for production use cases. Familiarity in using GPU compute either for model training or inference. Understanding of Large language models (LLM) and MLOps lifecycle for operationalizing LLM models.",
        "url": "https://www.linkedin.com/jobs/view/3955651504"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Lithonia, GA",
        "job_id": 3959292394,
        "company": "Robert Half",
        "title": "Data Engineer",
        "created_on": 1720639025.7744935,
        "description": "Robert Half is actively seeking a Data Engineer in the Lithonia, GA market Must be open to onsite 4 Days Must currently live in Georgia to be considered for this role. Required Skills: Qualitative and Quantitative Analysis: Demonstrated aptitude for both qualitative and quantitative analysis. Analytical and Problem-Solving Skills: Strong analytical and problem-solving abilities. Interpersonal and Communication Skills: Excellent interpersonal and communication skills. Technical Proficiency: Highly proficient in Microsoft Excel. Strong knowledge and experience with SQL query tools. Proficiency in Tableau. Project Management: Proven ability to manage multiple projects simultaneously and efficiently, demonstrating both technical and functional competence. Documentation Experience: Experience in creating systems and/or user documentation. Logistics Experience: Logistics experience is a plus.",
        "url": "https://www.linkedin.com/jobs/view/3959292394"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Santa Clara, CA",
        "job_id": 3940589353,
        "company": "Laksan Technologies",
        "title": "Data Engineer - Hadoop",
        "created_on": 1720639027.5728297,
        "description": "Role : Senior Hadoop developer with Cloudera Requirement Need Senior Hadoop developer with focus on cloudera Hadoop. Should be able to demonstrate work experience in hive/Impala and Spark, pyspark. Strong spark hands on skill a must. Very Strong SQL skills. Working knowledge of Python, shell scripting and bash. Must have work experience in Hadoop as data warehouse/data lake implementations. Must be able to work from onsite santa clara ( no exceptions)",
        "url": "https://www.linkedin.com/jobs/view/3940589353"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Miami, FL",
        "job_id": 3945559459,
        "company": "INSPYR Solutions",
        "title": "Data Engineer",
        "created_on": 1720639029.2912903,
        "description": "Title: Data Engineer Location: Miami, Dallas, TX (1231 Greenway Dr Irving, TX 75038), or Bentonville, AK (2600 NE 11 th St Bentonville, AR 7271) Schedule : 4 days on-site in Miami, Dallas, or Bentonville Duration: Contract to Hire Work Authorization: US Citizen or Green Card holder PRIMARY DUTIES AND RESPONSIBILITIES: Design and optimize data models, warehouses, architectures, schemas, indexing, and partitioning strategies. Collaborate with BI and data analysts to understand data requirements and optimize storage for analytical queries. Modernize databases and data warehouses and prepare them for analysis, managing for optimal performance. Design, build, manage, and optimize enterprise data pipelines ensuring efficient data flow, data integrity, and data quality throughout the process. Automate efficient data acquisition, transformation, and integration from a variety of data sources including databases, APIs, message queues, data streams, etc. Competently performs advanced data tasks with minimal supervision, including architecting advanced data solutions, leading and coaching others, and effectively partnering with stakeholders. Interface with other technical and non-technical departments and outside vendors on assigned projects. Under the direction of the IT Management, will establish standards, policies and procedures pertaining to data governance, database/data warehouse management, metadata management, security, optimization, and utilization. Ensure data security and privacy by implementing access controls, encryption, and anonymization techniques as per data governance and compliance policies. Provide training and consulting to other associates on database structure and standards. Document data pipelines, processes, and architectural designs for future reference and knowledge sharing. Stay informed of latest trends and technologies in the data engineering field, and evaluate and adopt new tools, frameworks, and platforms (like Microsoft Fabric) to enhance data processing and storage capabilities. When necessary, implement and document schema modifications made to legacy production environment. Perform any other function required by IT Management for the successful operation of all IT and data services provided to our clients. Available nights and weekends as needed for system changes and rollouts. EDUCATION AND EXPERIENCE REQUIREMENTS: Bachelor's or Master's degree in computer science, information systems, applied mathematics, or closely related field. Minimum of eight (8) years full time employment experience as a data engineer, data architect, or equivalent required. Knowledge of Mortgage and/or Title Industry is helpful. Our benefits package includes: Comprehensive medical benefits Competitive pay 401(k) retirement plan …and much more! About INSPYR Solutions Technology is our focus and quality is our commitment. As a national expert in delivering flexible technology and talent solutions, we strategically align industry and technical expertise with our clients' business objectives and cultural needs. Our solutions are tailored to each client and include a wide variety of professional services, project, and talent solutions. By always striving for excellence and focusing on the human aspect of our business, we work seamlessly with our talent and clients to match the right solutions to the right opportunities. Learn more about us at inspyrsolutions.com. INSPYR Solutions provides Equal Employment Opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability, or genetics. In addition to federal law requirements, INSPYR Solutions complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities.",
        "url": "https://www.linkedin.com/jobs/view/3945559459"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Rapid City, SD",
        "job_id": 3960910602,
        "company": "Innovative Systems Group",
        "title": "Data Engineer",
        "created_on": 1720639034.3905377,
        "description": "Job Title: Data Engineer 1 Location: Rapid City, SD Duration: 24 months Start date: 07/22/2024 Schedule: Monday to Friday; 8:00 AM to 3:00 PM Work set up: Hybrid Summary: The position will be working with the Caterpillar Product Compliance & Support – Substance Compliance team to preform lower-level data analysis specifically for product chemical management regulatory compliance. The position will also assist in regulatory declaration reporting and provide input for new process tool development. A member of the Caterpillar Product Compliance & Support – Substance Compliance team will provide high level leadership for this position, but the individual must be able to self-directed and motivated. Job Responsibilities: • Participate in receiving and tracking of Caterpillar substance compliance customer requests • Conduct substance analysis process using Caterpillar material specifications (1E material specifications) • Assist in maintenance of metrics on Chemical Management customer requests • Support tool & database development by providing end user needs for system requirement development (Material Content Tool, 1ESpec Libraries, TcSC, CDS, SnowFlake, etc.) Skills: • Creativity, verbal and written communication skills, analytical and problem solving ability. • Excel experience (pivot tables, formula creation & execution) • Team player and detail oriented. • Basic knowledge of design techniques, tools, and principles involved in production of precision technical plans, blueprints, drawings, and models. • Basic knowledge of the practical application of engineering science and technology. • Basic knowledge of relevant computer application, such as CADD (Computer Aided Design and Drafting). • Proficiency with use of Microsoft Office base programs; Education/Experience: • AAS or Bachelors required • 2-4 years experience required. Technical Skills (Required) - Proficiency with use of Microsoft Office base programs (Excel, Word, Powerpoint) - Excel experience (pivot tables, formula creation & execution) (Desired) - Ability to complete detailed data analysis - Mechanical print reading - Manufacturing experience - Compliance experience Soft Skills (Required) - Good verbal and written communication skills - Problem solving and critical thinking skills. - Good organizational and time management skills",
        "url": "https://www.linkedin.com/jobs/view/3960910602"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Fort Collins, CO",
        "job_id": 3941466968,
        "company": "SummitStone Health Partners",
        "title": "Data Engineer",
        "created_on": 1720639037.748459,
        "description": "Overview A Data Engineer is needed to help us in our endeavor to build the healthiest community in the nation … one person at a time. The Mission At SummitStone Health Partners, we strive to foster trust, empower recovery, and inspire hope to strengthen and enrich our Northern Colorado community. We need your help to make this vision a reality. We are building a team who believes in providing crucial services at crucial times, helping the people in our community at their most vulnerable — particularly those who are often overlooked and underserved. We Embrace Our Diversity Come As You Are A core value here at SummitStone. We want you to be your authentic self while at work, so we welcome and actively seek team members of different backgrounds, identities and experiences. SummitStone is committed to fostering a diverse and inclusive environment, honoring each individual's whole-self by embracing differences in race, ethnicity, ability, age, gender, sexual orientation, spiritual beliefs, socioeconomic status, language and the inherent intersections of many different identities. We invite everyone to share our journey and are proudly an equal opportunity employer. Experiences And Backgrounds We Look For If this sounds like your calling, then we want to help you succeed, grow, and thrive at SummitStone. Location: Fort Collins, CO Status: Full-time, 40/hrs. week - hybrid with ability to be in person What You’ll Do The Data Engineer will work as part of the team that is responsible for the loading and organizing data from multiple sources into an efficient and secure data warehouse. They ensure the integrity of all data from source systems into the data warehouse as well as all data provided to our ETL processes. The role also has responsibilities for the ongoing expansion of data assets and reviewing opportunities for platform expansion for the organization. Duties Include, But Are Not Limited To Designs and develops data pipelines for data extraction, transformation, and load (ETL) processes Constructs and maintains audit and other data-tracking assets that create visibility into processes to ensure consistency, accuracy, and timeliness of data Prepares data flow and lineage documentation to support analysts, change procedures, and automated processes Integrates data from various sources, including internal databases, external APIs, and third-party data providers Contributes to activities related to data management including database design, data dictionary management, and troubleshooting Reviews and writes complex stored procedures and other related TSQL code Collaborates with cross-functional teams to understand data requirements and designs scalable solutions Evaluates and improves existing data warehouse and BI infrastructure Reviews and tests new platforms/technology related to data and data management Other duties as needed Required Bachelor’s degree in Computer Science, Information Systems, or related technical field 3-5 years proven experience developing data solutions related to ETL and data engineering Advanced knowledge of TSQL and related ETL tools including SSIS and Azure Data Factory Ability to read, write, and debug complex stored procedures and similar code A passion for automation and operational excellence with the ability proactively identify and resolve issues Ability to work independently and as part of a team Analytical mindset and problem-solving skills Highly organized with excellent attention to detail Preferred Experience working with healthcare data and the related privacy and security standards preferred Bilingual/bi-cultural preferred Start With Yes At SummitStone, we show up not only for our clients, but for our staff as well. You will be joining a team of committed professionals who believe in the power of starting with “yes” — where an unparalleled level of teamwork and support can solve any challenge. We Believe That People Can And Do Recover, But We Also Know That They Can’t Always Do It On Their Own. That’s Where You Come In. If You Have Any Or All Of These Qualities, We Want You To Consider Us For Your Next Career Move You believe in being a voice for behavioral health in Northern Colorado You embrace equity for your clients, teammates, and partners Collaboration and creativity are crucial to helping you solve problems You show up as your genuine self to help guide others through their toughest times Through continuous curiosity, exploration, and reflection, you’re eager to adapt and learn Total Rewards The typical hiring range for this position is $83,269 - $99,916 annually, based on relevant years of experience. Benefits We know that most people aren’t just searching for a job, they’re searching for a career. But more than that, they’re searching for a sense of belonging and purpose. Whether you’re at the start of your journey or you already know where you want to be, you will have access to a level of support and teamwork that is uncommon in the field of behavioral health today. Along with that, positions are eligible* for range of benefits including: Time that you deserve for a balanced life Flextime Scheduling Paid Time Off (PTO) Ten paid holidays, including one floating holiday Extended Leave Bank (ELB) Financial benefits 403(b) Retirement Plan with an employer match Cafeteria 125 Plan Loan forgiveness program eligibility Referral Bonus available for employee who refer new hires Ongoing support for learning and development, including reimbursement and supervision for license and certification Health, Medical, and Wellness Medical Insurance Dental & Vision Insurance Life Insurance and Long-Term Disability (LTD) Malpractice Insurance Option to participate in Employee Resource Groups Based on working 20+ hours per week Reach Out SummitStone will provide persons with disabilities reasonable accommodations. If reasonable accommodation is needed to participate in the job application or selection process, please let your recruiter know. Questions? Please email us at hiring@summitstonehealth.org.",
        "url": "https://www.linkedin.com/jobs/view/3941466968"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3963961065,
        "company": "ClifyX",
        "title": "Data Engineer",
        "created_on": 1720639039.4930048,
        "description": "Create and contribute to frameworks that improve the efficacy of logging data, while working with data infrastructure to triage issues and resolve Collaborate with engineers, product managers, and data scientists to understand data needs, representing key data insights in a meaningful way Define and manage SLA for all data sets in allocated areas of ownership Determine and implement the security model based on privacy requirements, confirm safeguards are followed, address data quality issues, and evolve governance processes within allocated areas of ownership Design, build, and launch collections of sophisticated data models and visualizations that support multiple use cases across different products or domains Solve our most challenging data integration problems, utilizing optimal ETL patterns, frameworks, query techniques, sourcing from structured and unstructured data sources Assist in owning existing processes running in production, optimizing complex code through advanced algorithmic concepts Optimize pipelines, dashboards, frameworks, and systems to facilitate easier development of data artifacts Influence product and cross-functional teams to identify data opportunities to drive impact Mentor team members by giving/receiving actionable feedback Minimum Qualifications Bachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent practical experience. 4+ years of work experience in data engineering Experience with SQL, ETL, data modeling, and at least one programming language (e.g., Python, C++, C#, Scala, etc.) Preferred Qualifications Master's or Ph.D degree in a STEM field Experience with one or more of the following: data processing automation, data quality, data warehousing, data governance, business intelligence, data visualization, data privacy Experience working with terabyte to petabyte scale data",
        "url": "https://www.linkedin.com/jobs/view/3963961065"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Irving, TX",
        "job_id": 3921197551,
        "company": "Syntricate Technologies",
        "title": "AWS Data Engineer",
        "created_on": 1720639041.1049812,
        "description": "Position : AWS Data Engineer Location : Irving, TX (Onsite) Duration : Contract Experience : 10+ Years Job Description 10+ years of experience in solutioning data pipeline for large enterprise data Warehouse applications using AWS, Data Lake, Data Ingestion, Data Transformation, /Computation, Orchestration, Reporting & Data Analytics Good experience with Schema design, ETL setup, Batch jobs setup / custom scripting, data curation and aggregation Postgres, MongoDB Strong knowledge in CICD Pipeline for automatic deployment. Strong knowledge on design and integration patterns Proficient in technical artifacts e.g., Application Architecture, Solution Design Documents, etc Strong at analytical and problem-solving skills, Experience working with multi-vendor, multi-culture, distributed offshore and onshore development teams in dynamic and complex environment. ? Must have excellent written and verbal communication skills Experience working in Agile delivery Regards, Ashutosh Pasbola Assistant Manager | Syntricate Technologies Inc. Direct: (781)-552-4332| Fax: 781-649-0786 Email: ashutosh@syntricatetechnologies.com | Web: www.syntricatetechnologies.com We're hiring! connect with us on LinkedIn and visit our Jobs Portal Minority Business Enterprise (MBE) Certified | E-Verified Corporation | Equal Employment Opportunity (EEO) Employer This e-mail message may contain confidential or legally privileged information and is intended only for the use of the intended recipient(s). Any unauthorized disclosure, dissemination, distribution, copying or the taking of any action in reliance on the information herein is prohibited. Please notify the sender immediately by email if you have received this email by mistake and delete this e-mail from your system. You have received this email as we have your email address shared by you or from one of our data sources or from our member(s) or subscriber(s) list. If you do not want to receive any further emails or updates, please reply and request to unsubscribe .",
        "url": "https://www.linkedin.com/jobs/view/3921197551"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Seattle, WA",
        "job_id": 3965486109,
        "company": "TekWissen ®",
        "title": "Data Engineer",
        "created_on": 1720639042.7355342,
        "description": "Job Title: Data Engineer Location: Seattle, WA 98052 Duration: 6 Months Job Type: Contract Work Type: Onsite Pay rate- $70-$70.00 Per HR on W2 Job Description: This role will design and integrate OT infrastructure for machine shop and factory floor applications, using both physical and cloud-based systems. This is a highly collaborative position that interfaces with Project Kuiper production, engineering, and IT teams. Key job responsibilities Programming: Working to requirements and priorities set by the GSE Automation team, develop, configure and deploy software solutions to provide automated test and manufacturing capabilities on the factory floor. QA: Conduct end-to-end testing of automated manufacturing and test applications to ensure all requirements are being met before hand-off to production. Collaboration: Collaborate with cross-functional teams, including teams in Production Operations, IT and Digital Operations (KET), to ensure successful project execution and completion. Safety and Compliance: Ensure that all integrated systems comply with internal IT and Security standards, with designs and code that is well- documented and version controlled. Contribute to the overall architecture for a high-performing, connected, safe and maintainable factory. They must be detail oriented, have superior verbal and written communication skills, strong organizational skills, are able to work independently and can maintain professionalism under pressure. Basic Requirements: Python programming skills Labview programming skills Experience developing software that directly interacts with hardware Experience developing on the Ignition SCADA platform, or other user interfaces in an operational environment Working knowledge of Ethernet-based industrial protocols (OPC UA, MQTT, Modbus TCP etc) Working knowledge of fieldbus protocols such as Profinet and CAN Knowledge of security best practices (encryption in transit, encryption at rest,establishing chains of trust etc.) Knowledge of using version control systems such as Git Knowledge in different web technologies and data formats (i.e. HTTP/HTTPS, JSON/XML) Cross-organizational communication – comfortable evangelizing ideas and concerns clearly and persuasively, in both verbal and written form. A Computer Science or equivalent technical degree and 4-7 years of relevant experience is required. Story Behind the Need – Business Group & Key Projects Can you give me a little detail about your team culture? Internal Client only Almost exclusively within this team. There is another team of software and automation that are filling a similar need, so may be some collaboration with that team. Such as products, automations, best practices, learning from that team as they go through the process, etc. But actual work will be hardware team owners within team. Executing tests, how to do data processing, all the networking, data infrastructure. Typical Day in the Role: Compelling Story & Candidate Value Proposition Role interesting: Doing some of the coolest engineering/innovation out there with Kuiper. Orbital satellites improving connectivity across the world. Hardware technology and manufacturing testing. Being apart of a global mission is always fun to be apart of. Candidate Requirements Years of Experience: 4-7 years of relevant experience is required. Degree or Certification: A Computer Science or equivalent technical degree Top must-have hard skills Experience developing software that directly interacts with hardware Python Programming background is a MUST Working knowledge of Ethernet-based industrial protocols (OPC UA, MQTT, Modbus TCP etc). Knowledge of using version control systems such as Git TekWissen® Group is an equal opportunity employer supporting workforce diversity.",
        "url": "https://www.linkedin.com/jobs/view/3965486109"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3944429652,
        "company": "Atika Technologies & BPM",
        "title": "Data Engineer",
        "created_on": 1720639044.3791268,
        "description": "Location : Remote Minimum Requirements 10+ years experience building scalable Spark data pipelines (preferably using Scala) 4+ years experience in high level programming languages such as Java, Scala, or Python Proficiency in Spark/MapReduce development and expertise with data processing (ETL) technologies to build and deploy production-quality ETL pipelines Good understanding of distributed storage and compute (S3, Hive, Spark) Experience using ETL framework (ex: Airflow, Flume, Oozie etc.) to build and deploy production-quality ETL pipelines Demonstrated ability to analyze large data sets to identify gaps and inconsistencies, provide data insights, and advance effective product solutions Working knowledge of relational databases and expertise in query authoring (SQL) on large datasets Experienced with big data technologies such as Hadoop, Spark, Hive, etc Experience working with Git and Jira (or other source control and task management tools) Good communication skills that allows smooth collaboration with stakeholders.",
        "url": "https://www.linkedin.com/jobs/view/3944429652"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3944222329,
        "company": "Syntricate Technologies",
        "title": "AWS Data Engineer",
        "created_on": 1720639046.177811,
        "description": "Position : AWS Data Engineer Location : Irving, TX (Onsite) Duration : Contract Experience : 10+ Years Job Description : 10+ years of experience in solutioning data pipeline for large enterprise data Warehouse applications using AWS, Data Lake, Data Ingestion, Data Transformation, /Computation, Orchestration, Reporting & Data Analytics Good experience with Schema design, ETL setup, Batch jobs setup / custom scripting, data curation and aggregation Postgres, MongoDB Strong knowledge in CICD Pipeline for automatic deployment. Strong knowledge on design and integration patterns Proficient in technical artifacts e.g., Application Architecture, Solution Design Documents, etc Strong at analytical and problem-solving skills, Experience working with multi-vendor, multi-culture, distributed offshore and onshore development teams in dynamic and complex environment. ? Must have excellent written and verbal communication skills Experience working in Agile delivery",
        "url": "https://www.linkedin.com/jobs/view/3944222329"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3962986923,
        "company": "Brooksource",
        "title": "Data Engineer",
        "created_on": 1720639049.6139057,
        "description": "Azure Data Engineer Full-Time (40 hrs/week) Remote Note: We are unable to provide sponsorship at this time. Job Summary: Serves as a CI/CD Services Architect with responsibilities to architect and manage deployment pipelines. Recommended Years of Experience: Three (3) to five (5) years of experience in cloud-based software testing and test automation. Experience Requirements: Microsoft Certified: Azure Data Engineer Associate (DP 203) certification required. Experience developing, deploying, and managing data pipelines in cloud environments. Experience using Azure data services (e.g., Azure Data Factory, Azure Data Lake Storage, Azure Synapse, Azure Blob Storage). Experience using Databricks, including data ingestion, transformation, and analytics using Databricks notebooks and clusters. Experience using continuous integration and continuous deployment (CI/CD) practices and tools (e.g., Azure DevOps, Jenkins). Experience using scripting languages (e.g., Python, Bash, PowerShell) for automating data pipeline deployments and management. Experience working in an Agile environment – working as a part of teams to iteratively develop and deliver data products. Preferred: Microsoft Certified: Azure DevOps Engineer Expert (AZ 400). Eight Eleven Group provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, national origin, age, sex, citizenship, disability, genetic information, sexual orientation, gender identity, marital status, amnesty or status as a covered veteran in accordance with applicable federal, state, and local laws.",
        "url": "https://www.linkedin.com/jobs/view/3962986923"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3924412512,
        "company": "Adame Services LLC",
        "title": "ETL Pentaho Data Engineer",
        "created_on": 1720639051.4235802,
        "description": "Job Title: ETL Pentaho Data Engineer Location: Remote Jd Experience with relational databases, Data Warehouses and Data Integration is required. Experience with Pentaho highly preferred. 6+ Years’ experience working in a data integration capacity. Working knowledge of key concepts in data warehousing and data integration Experience and exposure to unit and user accepting testing. Demonstrated experience defining data management standards and principles. Strong working knowledge with SQL, such as SQL Server, or MySQL including physical table design, optimization, and development of stored procedures. Strong working knowledge with on-prem and Cloud integration/ETL tools like Talend and SSIS, other ETL tools like DataStage and Pentaho. Strong working knowledge with data modeling including logical and physical data models. Working knowledge with one or more data modeling tools like DBSchema or Erwin. Solid experience with creation of technical requirements, user documentation and operations guides. Knowledge of data virtualization concepts and technologies like Denodo are desirable. Good working knowledge of business processes as it relates to Data Management initiatives.",
        "url": "https://www.linkedin.com/jobs/view/3924412512"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Springdale, AR",
        "job_id": 3970279503,
        "company": "Tyson Foods",
        "title": "Data Engineer I",
        "created_on": 1720639053.5936997,
        "description": "Job Details: Job Summary: The entry-level position will be part of the team managing the Enterprise Asset Management business processes and strategies at Tyson Foods. The role involves supporting the industrial asset lifecycle from acquisition to disposition, including data structuring, recording acquisition, and decommission events, and transactional data related to the maintenance and operation phase. The position also involves assisting in platform configuration, system troubleshooting, user support, and the creation of educational documentation and resources. This role supports various departments to ensure effective property lifecycle management and optimal data usage in both SAP and other platforms, providing strategic analysis to align technology with business goals. Essential Duties and Responsibilities: Enter and maintain requisitions for construction projects in compliance with financial and data standard governance. Assist in managing financial aspects of projects and fixed assets using SAP PM, PS, MM, and FICO modules. Capture, structure, and record key data for acquisition and decommission events, and process transactions for maintenance operations. Analyze and control project costs to ensure adherence to GAAP and SOX requirements. Support financial aspects of construction projects from conception to commissioning, including budgeting, forecasting, and cost tracking. Develop and report on key financial metrics related to construction, operation, and decommissioning activities. Educate and support non-finance managers in maintaining budget and cost requirements. Assist in the preparation of financial reports and presentations for management review. Collaborate with IT to develop user-friendly financial systems and tools. Provide support for system configuration and integration decisions. Train team members in applications for property and project funding management. Provide analytics for property investment decisions to optimize procurement, maintenance, and decommissioning strategies. Cultivate strong business relationships and document necessary business cases. Apply best practices to data cleansing and extraction for migration. Requirements: Education/Certifications: Bachelor’s Degree in Business Management, Construction Management, Engineering, Accounting, CIS, or a related field preferred. Experience: Internships and school projects in related fields are considered valuable experience. 0-2 years’ experience or an equivalent combination of education and experience. Experience with SAP in food or manufacturing environments preferred, including configuration and data governance in PM, PS, FICO (focused on FA), and MM modules. Computer Skills: Basic computer skills; familiarity with SAP modules is a plus. Proficiency in Microsoft Office applications and willingness to learn new platforms. Basic skills in coding languages such as ABAP, Java, HTML, and SQL are a plus. Understanding of big data analytics concepts and experience with data visualization tools like Power BI and Tableau is highly valued. Experience with Power Automate, Power Pivots, Power Apps, and website design is a plus. Communication and Special Skills: Strong interpersonal and communication skills for effective collaboration. Above-average verbal and written business-communication skills. Experience with documentation and technical writing is a plus. Good communication, organizational, and time management skills. Self-driven, motivated, with a willingness to learn and strong problem-solving abilities. Ability to work well under pressure and meet deadlines. Supervisory: None Travel: 25% to 50% travel as needed. The successful candidate must be willing to relocate to the Northwest Arkansas area. This position does not offer sponsorship benefits. Candidate must be eligible to work in the US. Relocation Assistance Eligible: No Work Shift: 1ST SHIFT (United States of America) Hourly Applicants ONLY -You must complete the task after submitting your application to provide additional information to be considered for employment. Tyson is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will be considered without regard to race, national origin, color, religion, age, genetics, sex, sexual orientation, gender identity, disability or veteran status. We provide our team members and their families with paid time off; 401(k) plans; affordable health, life, dental, vision and prescription drug benefits; and more. CCPA Notice. If you are a California resident, and would like to learn more about what categories of personal information we collect when you apply for this job, and how we may use that information, please read our CCPA Job Applicant Notice at Collection, click here.",
        "url": "https://www.linkedin.com/jobs/view/3970279503"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Marysville, OH",
        "job_id": 3958491680,
        "company": "Tentek, Inc.",
        "title": "Data Engineer (JIH5JP00003017)",
        "created_on": 1720639055.3219614,
        "description": "The Data Engineer plays a crucial role in designing, developing, and maintaining scalable and reliable data platforms to support the organization's data needs. With a propensity of action over analysis, they are responsible for ensuring efficient data ingestion, storage, processing, and retrieval, as well as providing data integration solutions to enable effective data analysis and reporting. This role requires a strong understanding of data engineering concepts, data management, and programming languages to deliver innovative data solutions while respecting governance principles like responsible AI and data privacy. Duties Include Data Integration and Transformation: Develop and implement data integration solutions to enable seamless data movement across various systems and platforms. Implement efficient data workflows, data pipelines, and ETL processes to accommodate structured and unstructured data from various sources to ensure the timely delivery of high-quality data. Define data models and build data hierarchy structures to support AI/ML model integrations that are reliable and scalable. Transform and cleanse data to ensure accuracy, consistency, and integrity. Collaborate with data analysts and data scientists to understand data requirements and deliver tailored solutions. Troubleshoot and resolve data integration issues in a timely manner. Data Platform Development And Maintenance Design, develop, and maintain scalable data platforms that support data ingestion, storage, processing, and retrieval. Collaborate with cross-functional teams to ensure data platforms meet the organization's evolving data requirements. Regularly monitor the data platform's performance, identifying and resolving any issues or bottlenecks. Data Quality, Governance And Security Implement and enforce data quality and governance assurance policies, ensuring compliance with relevant data protection regulations and industry best practices. Develop and maintain data security measures, including access controls, encryption, and data anonymization techniques. Monitor data usage and access patterns, proactively identifying and mitigating potential security risks. Collaborate with the IT and cybersecurity teams to address data-related vulnerabilities and incidents. Perform data profiling, data validation, and data cleansing activities to ensure data accuracy and completeness. Collaborate with stakeholders to identify and resolve data quality issues. Define and monitor data quality metrics to measure and improve data quality over time. Conduct regular audits and reviews to ensure adherence to data quality standards. Ensure data governance and compliance standards, including responsible AI principles and data privacy, are adhered to during data integration and transformation processes. Performance Optimization Identify and implement performance optimization strategies for data platforms and processes. Optimize database design, data structures, and query performance to enhance data retrieval speed. Monitor and analyze data processing and query performance metrics, taking proactive actions to optimize their performance. Collaborate with infrastructure and network teams to ensure optimal data platform performance. Conduct regular performance testing and tuning activities and optimize data platforms for performance, reliability, and security. Documentation And Knowledge Sharing Document data platform architecture, data models, data flows, and technical specifications. Create and maintain comprehensive documentation of data engineering processes and workflows. Share knowledge and best practices with team members and stakeholders. Provide training and support to users on data engineering tools and technologies. Contribute to the development and enhancement of data engineering standards and guidelines. Continuously research, evaluate and implement emerging technologies and best practices in data engineering to drive innovation. Position Success Criteria BS in Technical discipline such as Computer Science, Information Systems, Computer Engineering or a related field. Proven experience as a Data Engineer, Database Developer, or relevant experience and certifications are welcome in lieu of a degree. 3-5 years working in cloud-based environments. Strong understanding of data engineering principles, data management, and data modeling concepts. Proficient in programming languages such as Python, Java, or Scala, with experience in database query languages (e.g., SQL). Experience with cloud-based data platforms (e.g., AWS, Azure, GCP) and associated services (e.g., S3, Redshift, BigQuery). Familiarity with data integration techniques, ETL frameworks (e.g., Apache Spark), and workflow management tools (e.g., Airflow). Experience with data streaming and real-time data processing frameworks (e.g., Kafka, Apache Flink, AWS Kinesis, etc). Familiarity with machine learning and AI techniques for data analysis and prediction. Understanding of data security, encryption, privacy, and compliance requirements. Excellent problem-solving and analytical skills, with the ability to optimize data processing pipelines for performance and efficiency. Strong communication skills, with the ability to effectively collaborate with cross-functional teams and explain complex technical concepts to non-technical stakeholders. Other Job Specific Skills Experience with data engineering tools and frameworks such as Apache Airflow, Apache NiFi, Talend, etc. Experience with Data science tools such as Open Data Hub (Seldon, Prometheus, Dataiku, IBM Watson Studio, etc) Deep learning - machine learning that is a neural network with three or more layers, which helps to “learn” from large amounts of data Cloud/big data tools (ex. blob storage, Redshift, Kafka, Hadoop, Spark, Hive etc.) Experience with containerization technologies such as Docker or Kubernetes.",
        "url": "https://www.linkedin.com/jobs/view/3958491680"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Charlotte, NC",
        "job_id": 3949735780,
        "company": "Steneral Consulting",
        "title": "Azure Data Engineer",
        "created_on": 1720639057.0460043,
        "description": "Azure Data Engineer Charlotte, NC - Hybrid - Local Candidates under commutable distance of 60-70 Min 2 interviews Must have valid LinkedIn Role Objectives These roles will be part of the Data Strategy team spanning across the Client securities teams, Client Americas Division’s broker-dealer and swap-dealer entities. These roles will be involved in the active development of the data platform in close coordination with the Client team, beginning with the establishment of a reference data system for securities and pricing data, and later moving to other data domains. The consulting team will need to follow internal developments standards to contribute to the overall agenda of the Data Strategy team. The implementation of this strategic platform on SMBC’s Azure Cloud Platform will require solutions and know-how as listed in the qualifications below. Qualifications And Skills Role Objectives These roles will be part of the Data Strategy team spanning across the client Capital Markets and Nikko securities teams, client Americas Division’s broker-dealer and swap-dealer entities. These roles will be involved in the active development of the data platform in close coordination with the client team, beginning with the establishment of a reference data system for securities and pricing data, and later moving to other data domains. The consulting team will need to follow internal developments standards to contribute to the overall agenda of the Data Strategy team. The implementation of this strategic platform on client Azure Cloud Platform will require solutions and know-how as listed in the qualifications below. Qualifications And Skills 10+ years of experience in software development using Python and its frameworks. Proven experience as a Data Engineer with experience in Azure cloud. Experience implementing solutions using - Azure cloud services Azure Data Factory Azure Lake Gen 2 Azure Databases Azure Data Fabric API Gateway management Functions Experience with Databricks and SQL Auto loader Strong SQL skills with RDMS or noSQL databases Familiarity with the DevOps lifecycle (git, Jenkins, etc.), CI/CD processes Good understanding of ETL/ELT processes Experience in financial services industry, financial instruments, asset classes and market data are a plus.",
        "url": "https://www.linkedin.com/jobs/view/3949735780"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3959342617,
        "company": "VARITE INC",
        "title": "Data Engineer",
        "created_on": 1720639060.598225,
        "description": "Title: Data Engineer Location: 100% Remote Duration: 6+ months contract with possible extension Client Location: Based In The USA Notes: This role needs to be supported in the USA EASTERN TIME ZONE THAT IS (07.00 PM - 04.00 AM) Indian STANDARD TIME. About Our client leads the real estate industry in technology and data innovation. Born of the need to help Agents better market homes and identify prospects we continue to evolve what is means to be an MLS and premiere real estate technology provider. Arming our subscribers with the highest quality, most transactable data and products is what we do. Data is at the foundation of our success and the talent within our Data Engineering team is the secret sauce to our winning strategy. Data Engineering builds data and analytics solutions for various use cases including API's, stream processing, reporting, product analytics, machine learning/AI, marketing optimization and financial reporting. By implementing cutting edge data solutions on our data mesh foundation, we deliver data structures, and insights that are the foundation for real estate transactions and decision-making for our client subscribers. Essential Job Functions To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. This position will work closely with all client business groups and customers. The requirements listed below are representative of the knowledge, skill, and/or ability required. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. Design and develop efficient and scalable data pipelines between enterprise transactional systems, third-party and analytics platforms Must be a crack coder in one of the following (go-lang, node.js or python) Must be proficient in writing and refactoring efficient SQL queries. Must be able to explain features of good data model design Build and maintain a data environment for speed, accuracy, consistency and 'up' time Support analytics and data science by building a world-class data mesh environment that empowers analysts to determine insights into revenue and power products across the organization Integrate third-party data sources and API's into the Bright data mesh ecosystem Work closely with Data Science team and participate in development of feature engineering pipelines Design and develop data products with modern AWS cloud technologies such as S3, Redshift, EMR, Hive, Presto, Flink and Spark Work with the machine learning engineering team to build a data eco system that supports AI products at scale Design and deploy an enterprise data warehouse that supports internal and market facing analytics products at scale Ensure data governance principles adopted, data quality checks and data lineage implemented in each hop of the data Partner with adjacent organizations to ensure proper integration and adherence to standards Be in tune with emerging trends in data management and cloud technologies and participate in evaluation of new technologies Ensure compliance through the adoption of enterprise standards and promotion of best practice / guiding principles aligned with organization standard Education BS or MS degree in Computer Science or Information Technology or equivalent experience Required Skills/Experience 8+ years of experience as data engineer at an innovative organization 4+ years of hands-on experience in implementing data lake systems using AWS cloud technologies such as S3, Redshift, EMR, Hive, Kafka and Spark Expert managing AWS services (EC2, S3, Route 53, ELB, VPC, CloudWatch, Lambda) in a multi account production environment Experience with development frameworks as well as data and integration technologies such as Informatica, Python, Scala Create new ETLs in AWS Glue with Python or Node.js as the scripting language Create AWS Lambdas using Python or Node.js as the scripting language Modify existing ETLs to fix issues where approach is appropriate Use Glue for ETLs inside of AWS to and from all AWS types of data sources Support the migration of data into S3, Redshift, DynamoDB, AWS RDS Experience With Machine Learning Libraries and Frameworks (TensorFlow, MLlib) is an added advantage Exposure to R, SparklyR, and Other R packages is a Plus Expert knowledge of Agile approaches to software development and able to put key Agile principles into practice to deliver solutions incrementally. Monitors industry trends and directions; develops and presents substantive technical recommendations to senior management Excellent analytical thinking, interpersonal, oral and written communication skills with strong ability to influence both IT and business partners Ability to prioritize and manage work to critical project timelines in a fast-paced environment Advanced knowledge for Microsoft SQL Server for future migration to an AWS Database Platform Preferred Experience Previous experience with cloud development (AWS, GCP) Previous experience in design and deployment of data lakes, data mesh, data warehouse and streaming platforms Previous experience with data quality projects and public records Previous experience with: AWS DynamoDB, AWS Elastic Map Reduce, AWS Lambda, AWS Step Functions, AWS Redshift, AWS RDS, Terraform or CloudFormation AWS Architect Certification is a plus .",
        "url": "https://www.linkedin.com/jobs/view/3959342617"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Charlotte, NC",
        "job_id": 3951199304,
        "company": "Steneral Consulting",
        "title": "Azure Data Engineer",
        "created_on": 1720639062.3239229,
        "description": "Azure Data Engineer - w2/1099 candidates only - No employer at all Charlotte, NC - Hybrid - Local Candidates under commutable distance of 60-70 Min 2 interviews Must have valid LinkedIn Role Objectives These roles will be part of the Data Strategy team spanning across the Client securities teams, Client Americas Division’s broker-dealer and swap-dealer entities. These roles will be involved in the active development of the data platform in close coordination with the Client team, beginning with the establishment of a reference data system for securities and pricing data, and later moving to other data domains. The consulting team will need to follow internal developments standards to contribute to the overall agenda of the Data Strategy team. The implementation of this strategic platform on SMBC’s Azure Cloud Platform will require solutions and know-how as listed in the qualifications below. Qualifications And Skills Role Objectives These roles will be part of the Data Strategy team spanning across the client Capital Markets and Nikko securities teams, client Americas Division’s broker-dealer and swap-dealer entities. These roles will be involved in the active development of the data platform in close coordination with the client team, beginning with the establishment of a reference data system for securities and pricing data, and later moving to other data domains. The consulting team will need to follow internal developments standards to contribute to the overall agenda of the Data Strategy team. The implementation of this strategic platform on client Azure Cloud Platform will require solutions and know-how as listed in the qualifications below. Qualifications And Skills 10+ years of experience in software development using Python and its frameworks. Proven experience as a Data Engineer with experience in Azure cloud. Experience implementing solutions using - Azure cloud services Azure Data Factory Azure Lake Gen 2 Azure Databases Azure Data Fabric API Gateway management Functions Experience with Databricks and SQL Auto loader Strong SQL skills with RDMS or noSQL databases Familiarity with the DevOps lifecycle (git, Jenkins, etc.), CI/CD processes Good understanding of ETL/ELT processes Experience in financial services industry, financial instruments, asset classes and market data are a plus.",
        "url": "https://www.linkedin.com/jobs/view/3951199304"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Colorado, United States",
        "job_id": 3844343171,
        "company": "Syntricate Technologies",
        "title": "Enterprise Data Engineer",
        "created_on": 1720639065.7505417,
        "description": "Job Summary The Data Engineer will be responsible for developing, optimizing, testing, and creating extracts, integrations, and migrations of data to new and existing databases to help users retrieve data effectively. This role involves working with technologies in the data field to support advanced analytics and data processing capabilities. The ideal candidate will have a strong background in software development, database management, and a solid understanding of data mapping and transformation, extraction, and loading. Development Essential Duties and Responsibilities: The Developer Candidate Must Have The Following Experience Proficient with Structured Query Language (SQL). Proficient in creating, modifying, and querying relational databases. Proficient in the creation of SQL queries to retrieve specific data from Needles by specifying various search criteria. At least 5 years of experience with interpreting database tables and data to understand correct extraction and mapping. Experience formatting, running totals, summaries, custom formulas, and custom Functions (SQL Queries). Experience with PLSQL Developer, SQL Plus, SQL Navigator, and AWS Glue. Strong ability to research and manage complex data requests. Sound oral and written communication skills. Migration experience a plus. High evaluative and interpretative skills. Qualifications Bachelor’s or Master’s degree in Computer Science, Information Technology, or a related field. Proven work experience as a Data Engineer , Database Developer, or similar role. Experience with database-backed web applications. Proficiency in SQL and experience with database management systems (e.g., MySQL, PostgreSQL, MongoDB). Familiarity with data manipulation languages and the principles of database design. Knowledge of software development and user interface web applications. Excellent organizational and analytical abilities. Outstanding problem solver. Preferences Background with both structured and unstructured data. Experience with both SQL and NoSQL databases. Familiarity with columnar databases. Experience with Salesforce, AWS Redshift, and Snowflake.",
        "url": "https://www.linkedin.com/jobs/view/3844343171"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Orange City, IA",
        "job_id": 3971265365,
        "company": "Staples",
        "title": "Sr Data Engineer I",
        "created_on": 1720639067.3474836,
        "description": "Staples is business to business. You’re what binds us together. While you may know Staples as the world’s leading office supply company, Staples Promotional Products – a division of Staples – is a national leader in the promotional products industry. At Staples Promotional Products, we help customers build love for their brands with customized merchandise solutions. Whatever story they want to tell, connection they want to make, or goal they need to deliver, Staples Promo makes it easy to design promo experiences that create lasting impact. Join our winning team! Our digital solutions team is more than a traditional IT organization. We are a team of passionate, collaborative, agile, inventive, customer-centric, results-oriented problem solvers. We are intellectually curious, love advancements in technology and seek to adapt technologies to drive Staples forward. We anticipate the needs of our customers and business partners and deliver reliable, customer-centric technology services. What You’ll Be Doing Design, build, test and implement new and maintain existing data pipelines that acquire data from new and existing sources and incorporate into business intelligence data models. Design, build, test and implement data analytics models that support business intelligence solutions. Design, build, test and implement new and maintain existing outbound data pipelines for various data consuming applications. Provide data expertise and query guidance to report developers. Maintain and administer overall data and business intelligence environment. Troubleshoot and repair data integrity and operational issues. Provide occasional SQL Server database operations support primarily including script review/execution. What You Bring To The Table Strong expertise in Data Engineering, Data Governance, and Data Analysis Excellent interpersonal and communication skills Basic Qualifications 5+ years of experience in software development using Microsoft Visual Studio with TFS or similar source control tools 7+ years of experience in writing complex SQL Queries, functions and procedures. 7+ years of experience in Data and Analytics Schema Modeling 7+ years of experience in using SSDT to create database schema, SSAS, and SSIS projects. 3+ years of interaction with report development tools and environments such as Power BI, Tableau, or MicroStrategy. Preferred Qualifications Software Engineering mindset Experience executing multiple concurrent projects Experience working projects of high technical complexity Experience bringing new technology and ideas to the table We Offer Inclusive culture with associate-led Business Resource Groups Flexible PTO (22 days) and Holiday Schedule (7 observed paid holidays) Online and Retail Discounts, Company Match 401(k), Physical and Mental Health Wellness programs, and more!",
        "url": "https://www.linkedin.com/jobs/view/3971265365"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Plano, TX",
        "job_id": 3942686667,
        "company": "Steneral Consulting",
        "title": "Data Engineer-locals",
        "created_on": 1720639068.9614906,
        "description": "do hybrid out of one of (Mclean, VA, Richmond, VA, Chicago, IL, Plano, TX) Candidates must have prior Capital One experience and be able to provide Cap One manager references. What You’ll Do Be a part of team designing and building Enterprise Level scalable, low-latency, fault-tolerant streaming data platform that provides meaningful and timely insights Build the next generation Distributed Streaming Data Pipelines and Analytics Data Stores using streaming frameworks (e.g. Flink, Spark Streaming, etc.) using programming languages like Java, Scala, Python Be part of a group of engineers building data pipelines using big data technologies (Spark, Flink, Kafka, Snowflake, AWS Big Data Services, Snowflake, Redshift) on medium to large scale datasets Work in a creative & collaborative environment driven by agile methodologies with focus on CI/CD, Application Resiliency Standards, and partnership with Cyber & Security teams Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance Basic Qualifications Bachelor’s Degree At least 4 years of experience in application development (Internship experience does not apply) At least 1 year of experience in big data technologies Preferred Qualifications 5+ years of experience in application development including Python, SQL, Scala, or Java 2+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud) 3+ years experience with Distributed data computing tools (Kafka, Spark, Flink etc) 2+ year experience working on real-time data and streaming applications 2+ years of experience with NoSQL implementation (DynamoDB, OpenSearch) 2+ years of data warehousing experience (Redshift or Snowflake) 3+ years of experience with UNIX/Linux including basic commands and shell scripting 2+ years of experience with Agile engineering practices",
        "url": "https://www.linkedin.com/jobs/view/3942686667"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3959666298,
        "company": "EarnIn",
        "title": "Staff Data Engineer",
        "created_on": 1720639070.984868,
        "description": "About EarnIn As one of the first pioneers of earned wage access, our passion at EarnIn is building products that deliver real-time financial flexibility for those with the unique needs of living paycheck to paycheck. Our community members access their earnings as they earn them, with options to spend, save, and grow their money without mandatory fees, interest rates, or credit checks. Since our founding, our app has been downloaded over 13M times and we have provided access to over $15 billion in earnings. We’re fortunate to have an incredibly experienced leadership team, combined with world-class funding partners like A16Z, Matrix Partners, DST, Ribbit Capital, and a very healthy core business with a tremendous runway. We’re growing fast and are excited to continue bringing world-class talent onboard to help shape the next chapter of our growth journey. Position Summary We are looking for an experienced Staff Data Engineer proficient in Data infrastructure operations, automation, and management, in addition to traditional data engineering responsibilities. As a Staff Data Engineer, you will work cross-functionally with various teams and contribute to helping build an enterprise-grade Data Platform. The US base salary range for this full-time position is $175,600 - $261,800 + equity + benefits. Our salary ranges are determined by role, level, and location. This is a remote friendly role with a preference for hybrid in our Palo Alto HQ. What You'll Do Lead the enhancement of internal processes, focusing on scaling infrastructure, streamlining data delivery, and implementing automation to replace manual operations. Design and implement advanced infrastructure for efficient data extraction, transformation, and loading (ETL) using cutting-edge AWS and SQL technologies. Develop sophisticated analytical tools that tap into the data pipeline, offering deep insights into crucial business metrics like customer growth and operational efficiency. Architect and manage extensive, sophisticated data sets to meet complex business needs and requirements. Engage closely with a wide array of stakeholders, including executive, product, data, and design teams, providing high-level support for data infrastructure challenges and advising on technical data issues. Make a meaningful impact in the lives of our community members. Collaborate and mentor other senior engineers while providing thoughtful guidance using code, design, and architecture reviews. Contribute to defining technical direction, planning the roadmap, escalating issues, and synthesizing feedback to ensure team success. Estimate and manage team project timelines and risks. Participate in hiring and onboarding for new team members. Lead cross-team engineering initiatives. Constantly learning about new technologies and industry standards in data engineering. What Sets Us Apart High-impact roles at a fast-moving company that’s aggressively growing our user base We are a collaborative team and genuinely enjoy working with each other. We believe in empowering our people to be successful We’re building a product that reimagines the way money moves to empower human potential What We're Looking For 6+ years in designing, building, and maintaining the Data infrastructure and the ability to lead complex projects and teams. Bachelor's, Master’s or PhD degree in computer science, computer engineering, or a related technical discipline or equivalent industry experience. Proficiency in programming languages like Python and Scala Strong knowledge of distributed computing frameworks such as Apache Hadoop, and Apache Spark with cloud platforms such as AWS, Azure, or GCP. Working experience with the Databricks Deep understanding of database design, SQL, and NoSQL databases. Experience in managing large datasets and optimizing database performance. Proficiency in Git and Terraform and experience in deploying continuous integration and continuous deployment (CI/CD) practices. Experience in managing event-driven systems, particularly with Kafka in cloud environments. Expertise in developing and implementing data governance frameworks, policies, and procedures to ensure data quality, compliance, and effective data management practices. Deep understanding of data security principles, including encryption, decryption, and secure data storage and transfer protocol At EarnIn, we believe that the best way to build a financial system that works for everyday people is by hiring a team that represents our diverse community. Our team is diverse not only in background and experience but also in perspective. We celebrate our diversity and strive to create a culture of belonging. EarnIn does not unlawfully discriminate based on race, color, religion, sex (including pregnancy, childbirth, breastfeeding, or related medical conditions), gender identity, gender expression, national origin, ancestry, citizenship, age, physical or mental disability, legally protected medical condition, family care status, military or veteran status, marital status, registered domestic partner status, sexual orientation, genetic information, or any other basis protected by local, state, or federal laws. EarnIn is an E-Verify participant. EarnIn does not accept unsolicited resumes from individual recruiters or third-party recruiting agencies in response to job postings. No fee will be paid to third parties who submit unsolicited candidates directly to our hiring managers or HR team.",
        "url": "https://www.linkedin.com/jobs/view/3959666298"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "New Jersey, United States",
        "job_id": 3961306539,
        "company": "UBS",
        "title": "Data Engineer",
        "created_on": 1720639075.1758144,
        "description": "Job Reference # 296427BR Job Type Full Time Your role Are you an analytical data expert with the technical skills and curiosity to solve complex problems? Do you blend mathematics, computer science, program management, and trend-spotting? Can you navigate both business and IT worlds, using technology to convert raw data into actionable insights? Are you interested in applying data science to AML (Anti-Money Laundering) in the banking/financial industry? We’re looking for a Data Engineer to: enable efficient compliance with new or changing regulations, develop and propose changes to reporting, data management (including metadata and data access technologies), processes, products, etc. handle business wide reporting - prepare, design, and develop reporting tools, targeting senior management audience with data gathering and modeling strategies function in many aspects of the development process including run-the-bank, change-the-bank, testing, development, and reporting conduct system wide audits to ensure a high level of scrutiny, accuracy, and efficiency to evaluate and mitigate business and sector risk introduce process improvement steps and controls throughout the reporting process, while maintaining the integrity of the data track progress against agreed remediation deadlines and actively manage key support areas to monitor and escalate issues manage all aspects of regulatory change projects, including project planning, reporting, execution, timing, quality, cost, risks, resources and overall project governance work closely with key stakeholders, SME's and clients to gather project requirements, specifications and limitations in order to develop state of the art reporting capabilities Your team You'll be joining our Business AML Organization (BAMLO) team, which focuses on driving strategy, implementation, and oversight for the AML program within the US Wealth Management line of business. This role is based out of Weehawken. We work collaboratively with our business partners, WM field leadership, and Second Line Compliance partners to ensure that our Financial Advisors serve their clients in accordance with regulatory requirements. Your expertise ideally 3 or more years of analytical data experience bachelor or international equivalent required; advanced degree a plus data preparation: the process of converting raw data into a more easily consumed format text analytics: the process of examining unstructured data to glean key business insights data visualization: analyze current and historical data and presents findings in easy-to-digest reports, dashboards, graphs, charts, and maps that can be shared statistical analysis and reporting, process reengineering develop forecasting models and other analytics program management, ownership, and entrepreneurism expert in software applications and programming languages including SQL, Python, Dataiku, Tableau, Alteryx, SharePoint, MS Office Suite (Excel, Word, PowerPoint, Visio) About Us UBS is the world’s largest and the only truly global wealth manager. We operate through four business divisions: Global Wealth Management, Personal & Corporate Banking, Asset Management and the Investment Bank. Our global reach and the breadth of our expertise set us apart from our competitors.. We have a presence in all major financial centers in more than 50 countries. Join us At UBS, we embrace flexible ways of working when the role permits. We offer different working arrangements like part-time, job-sharing and hybrid (office and home) working. Our purpose-led culture and global infrastructure help us connect, collaborate, and work together in agile ways to meet all our business needs. From gaining new experiences in different roles to acquiring fresh knowledge and skills, we know that great work is never done alone. We know that it's our people, with their unique backgrounds, skills, experience levels and interests, who drive our ongoing success. Together we’re more than ourselves. Ready to be part of #teamUBS and make an impact? Disclaimer / Policy Statements UBS is an Equal Opportunity Employer. We respect and seek to empower each individual and support the diverse cultures, perspectives, skills and experiences within our workforce.",
        "url": "https://www.linkedin.com/jobs/view/3961306539"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Litchfield Park, AZ",
        "job_id": 3953857005,
        "company": "JCO Workforce Solutions, LLC",
        "title": "Data Engineer",
        "created_on": 1720639076.845958,
        "description": "Job Summary: We are seeking a highly skilled and motivated Data Engineer with expertise in Oracle databases and SQL. The ideal candidate will be responsible for designing, developing, and optimizing our data pipelines and database systems to ensure efficient data flow and accessibility. This role requires a strong understanding of Oracle database architecture, SQL, and data engineering best practices. Key Responsibilities: Design and Development: Design, develop, and maintain scalable and efficient data pipelines and ETL processes using Oracle and SQL. Database Management: Manage and optimize Oracle databases, ensuring data integrity, security, and performance. Data Integration: Integrate data from various sources, ensuring data consistency and reliability. Performance Tuning: Monitor and optimize database performance, troubleshoot issues, and implement performance improvements. Collaboration: Work closely with data scientists, analysts, and other stakeholders to understand data requirements and provide necessary support. Data Quality: Implement data quality checks and validation processes to ensure data accuracy and completeness. Documentation: Create and maintain comprehensive documentation of data pipelines, database structures, and processes. Security: Ensure compliance with data security and privacy policies and best practices. Qualifications: Education: Bachelor’s degree in Computer Science, Information Technology, or a related field. Experience: Minimum of 5 years of experience in data engineering, with a strong focus on Oracle databases and SQL. Technical Skills: Proficiency in Oracle database management and development. Advanced SQL skills for querying, optimizing, and managing data. Experience with ETL tools and processes. Knowledge of data modeling, database design, and data warehousing. Familiarity with other database technologies and data integration tools is a plus. Soft Skills: Strong analytical and problem-solving skills. Excellent communication and collaboration abilities. Detail-oriented with a focus on data accuracy and quality. Ability to work independently and in a team environment. Preferred Qualifications: Experience with cloud-based data platforms such as AWS, Azure, or Google Cloud. Familiarity with big data technologies like Hadoop, Spark, or similar. Knowledge of scripting languages such as Python or Shell scripting. Benefits: Competitive salary and performance-based bonuses. Health, dental, and vision insurance. Retirement savings plan with company match. Paid time off and holidays. Professional development opportunities.",
        "url": "https://www.linkedin.com/jobs/view/3953857005"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3960183644,
        "company": "CryptoRecruit",
        "title": "Data Engineer",
        "created_on": 1720639078.7816176,
        "description": "Company The company is looking for a Senior Data Engineer to help build out our suite of analytics products. As a member of the Data Engineering Team, you’ll work closely with the world’s leading blockchain protocols, developing real-time data pipelines to ingest their data, and identify actionable insights that will help them grow. In this role, you will also play a leading part in continuing to build out our unique blockchain parsing technology, Chainwalkers. The engineering team has extensive expertise across data pipelining, distributed databases, at-scale web applications, large-scale front-end applications and data visualisations. They work relentlessly towards our goals, and care a great deal about building quality products with a talented, authentic team. Responsibilities Design, build and maintain real-time data pipelines that process blockchain transactions from dozens of different blockchain networks. Develop data models that translate complex, esoteric blockchain data into standardised formats that are analytics-ready. Design automated systems that evaluate and parse the results of smart contract calls. Work alongside the Data Science team to curate and prototype new data-sets to tackle emerging problems. Lead and scope large technical projects. Productionalise time-series metrics for our partners and product teams. Develop systems to monitor the integrity and uptime of data. Requirements You possess a strong technical background that includes 5+ years of experience working in a senior engineering position with data infrastructure/distributed systems. Strong familiarity with blockchain and cryptocurrencies You have a high bar for the quality of data, the quality of code and ultimately an attention to detail. You have experience writing, maintaining and debugging ETL jobs that leverage distributed data frameworks such as Spark, Kafka and Airflow. You are comfortable with the command line, and are not afraid to get your hands dirty with infrastructure and ops when required. You have extensive experience working with Spark. You have worked with languages such as Python, Scala and Go. You have extensive experience working with data warehouses/lakes such as AWS Redshift and Delta Lake. You are capable of gluing together different services and tools, even if you haven’t previously worked with them. You have worked in an agile sprint-based manner. You are relentless when tasked with solving hairy technical challenges. Remuneration And Benefits Better than market rate with equity plan Make sure to follow us here to get our most live jobs https://www.linkedin.com/company/cryptorecruit Cryptorecruit are the worlds leading specialist recruiter for the blockchain/Cryptocurrency industry. We recruit positions from CEO,CTO, Project Manager, Solidity developer, frontend and Backend Blockchain developers to marketing/sales and customer service roles. Please browse our website and at www.cryptorecruit.com to search all our job vacancies.",
        "url": "https://www.linkedin.com/jobs/view/3960183644"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Bethlehem, PA",
        "job_id": 3955649989,
        "company": "André",
        "title": "Data Engineer - Data & Analytics- Hybrid 3 days onsite",
        "created_on": 1720639082.395848,
        "description": "André Global Inc. is a global provider of information technology services. Since 2002, we have helped our clients in the APAC region achieve their business goals by leveraging the power of technology. In 2016, we entered the AMERICAS to re-create similar success here as well. Headquartered in New York City, we are now serving our global clients. We strive to deliver customer satisfaction through our service quality to all our clients! Our global and diverse team helps our customers achieve their project goals at a faster pace with high affordability but without any compromise on quality. We take pride in providing an outstanding quality of service to all our customers starting from startups, small businesses to Fortune 500. With more than a decade of experience servicing customers across geographies, we have stationed ourselves at a unique position in the global market. Our ability to assemble the best workforce in record time has been a great asset to our clients. This is a contract position with our Insurance Client. Location: Bethlehem, PA. The resource will be required to work onsite in our Bethlehem, PA office 3 days per week. We will not accept remote candidates. MUST Be extremely strong in SQL programming as there will be a live coding exercise in SQL programming during the interview Must have genuine experience working in Data Bricks Responsibilities Develop and maintain data marts using databricks to support business analytics and reporting use cases. Develop & Implement ETL/ELT processes to extract, transform and load data using an existing framework. Collaborate with business stakeholders to gather requirements and ensure the datamarts/data assets that are built meets the business needs. Create & maintain data dictionary/ETL mapping documents. Reverse engineer existing data prep processes and convert them into reusable governed and certified data assets. Implement data quality checks and ensure data integrity within the datamarts /data assets. Qualifications Proven experience as a Data Engineer, ETL developer or in a similar role Proficient in SQL and Python. Knowledge of Databricks. Familiarity with data warehousing concepts and best practices. Strong understanding of data modeling, and in particular dimension modeling. Excellent problem-solving skills and the ability to multitask, work independently and as part of a team. Good communication skills and the ability to collaborate effectively within and across the team.",
        "url": "https://www.linkedin.com/jobs/view/3955649989"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Richmond, VA",
        "job_id": 3953922301,
        "company": "Ampcus Inc",
        "title": "Data Engineer",
        "created_on": 1720639084.0621006,
        "description": "Qualifications Strong understanding of Data warehousing (Dimensional Modeling, ETL etc.) and RDBMS concepts Minimum 2 years working experience as data engineer Minimum 2 years working experience in SQL, Stored Procedures and Table Design Minimum 1 year working experience in Snowflake Cloud Data warehouse Experience with programming languages like R or Python a big plus Experience with machine learning and/or AI project is a big plus Experience with cloud technologies (Azure, AWS, GCP) is big plus",
        "url": "https://www.linkedin.com/jobs/view/3953922301"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Texas, United States",
        "job_id": 3966445784,
        "company": "...",
        "title": "Data Engineer",
        "created_on": 1720639085.616235,
        "description": "MUST HAVE: Snowflake (most important thing), ETL (Informatica or Snaplogic), AWS, Python, Control-M. Data Engineer The Expertise You Have: -Crafted and implemented operational data stores and or data warehouses in multi-site High Availability environments. Recent Snowflake experience is required. MDM experience is a plus. -Strong problem solving and proven data analytical skills -Proven expertise in Data Modeling, Data Profiling, Data Analysis, Data Quality, Data Governance and Data Lineage. -Experience migrating databases/warehouses from On Prem to AWS Cloud -Prior experience designing data processing pipelines and resilient messaging services -Data modeling skills and familiarity with BI reporting tools -SQL specialist; ETL background and experience with schedulers a must -Solid experience with AWS event streaming/Kafka -Ability to elicit and document process models and data models in support of defined use cases -Experience or certification in Cloud-based (AWS, Azure) Architecture, Security and Data certification a plus -Demonstrated ability to lead, mentor, influence and partner with architects, engineers, and product teams to deliver scalable robust application solutions -Ability to manage vendor relationships and drive issues to a timely resolution -Excellent interpersonal skills (written, oral, influencing) The Skills You Bring -A proven foundation in data engineering – bachelor’s degree + preferred, 8+ years’ experience -Hands-on experience with large Cloud-based implementations (AWS, Azure). -Experience with cloud-based NoSQL database and Relational databases and business intelligence tools -Strong background in data field and critical thinking skills to design end-to-end solutions -Experience and comfort working in an agile environment (Scrum and Kanban) -Excellent communication and influencing skills to facilitate collaboration across technology and business partners without having a direct authority over people -Demonstrated experience in mentoring and guiding software engineers in an agile environment The Value You Deliver: -Collaborating in an autonomous team, owning all aspects of delivery (Coding, Quality, DevSecOps) -Team player developing/deploying software in a fast-paced and highly flexible environment -Continued focus on improving test coverage, release velocity and production health -Working with global Agile teams and will contribute to enhancing engineering practices -Exercising operational expertise in a team, targeting Engineering and DevOps excellence -Forever learning and growing your skillsets to be an active contributor to a well-rounded team -Automating yourself out of today’s job so that you can move on to the next big challenge",
        "url": "https://www.linkedin.com/jobs/view/3966445784"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "North Liberty, NC",
        "job_id": 3947774366,
        "company": "Thomas & Moore Strategic Ventures, LLC",
        "title": "Data Engineer",
        "created_on": 1720639087.3779948,
        "description": "Description A Data Engineer has a deep understanding of performance optimization and data pipelining. In addition to the baseline skills of a data analyst, data engineers can make raw data more useful for the enterprise. Data engineers can create and integrate application programming interfaces (APIs). Their technical skills generally include multiple programming languages and a deep knowledge of SQL database design. The data engineer role requires a more in-depth knowledge in programming for integrating complex models and using advanced software library frameworks to distribute large, clustered data sets. Data engineers collect and arrange data in a form that is useful for analytics. A basic knowledge in machine learning is also required to build efficient and accurate data pipelines to meet the needs for downstream users such as data scientists to create the models and analytics that produce insight. The Data Engineer Shall Perform The Following Tasks Developing, maintaining, and testing infrastructures for data generation to transform data from various structured and unstructured data sources. Develop complex queries to ensure accessibility while optimizing the performance of NoSQL and or big data infrastructure. Create and maintain optimal data pipeline architecture. Build and maintain the infrastructure to support extraction, transformation, and loading (ETL) of data from a wide variety of data sources. Extract data from multiple data sources, relational SQL and NoSQL databases, and other platform APIs, for data ingestion and integration. Configure and manage data analytic frameworks and pipelines using databases and tools such as NoSQL, SQL, HDInsight, MongoDB, Cassandra, Neo4j, GraphDB, OrientDB, Spark, Hadoop, Kafka, Hive, and Pig. Apply distributed systems concepts and principles such as consistency and availability, liveness and safety, durability, reliability, fault-tolerance, consensus algorithms. Administrate cloud computing and CI/CD pipelines to include Azure, Google, and Amazon Web Service (AWS). Coordinate with stakeholders, including product, data and design teams to assist with data-related technical issues and support their data infrastructure needs Minimum Qualifications Minimum of 1-year experience is required Bachelor’s degree in a STEM field with preference towards Computer Science and Software Engineering. Verifiable work experience working with data structures, database management, distributed computing, and API driven architectures using SQL and No-SQL engines. A Certified Data Management Professional certification is preferred. Proficient in modeling frameworks like Universal Modeling Language (UML), Agile Development, and Git Operations. Clearance Requirements Active Top-Secret clearance eligible for SCI is required. Thomas & Moore Strategic Ventures, LLC (TMSV) offers Information Technology Services and Strategic Consulting Expertise to Federal, State and Local Governments as well as commercial entities. Come join our growing, dynamic company where mission first, people always is our culture. Benefits We offer: Health Care Plan (Medical, Dental & Vision) Retirement Plan (401k) Paid Time Off (Vacation, Sick & Federal Holidays) Training & Development If interested, please forward a cover letter and resume for consideration to careers@TMSVsolutions.com. Your cover letter shall indicate your background and how your experience relates and qualifies you for this position. TMSV provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or veteran status, or any other protected class.",
        "url": "https://www.linkedin.com/jobs/view/3947774366"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Roanoke, TX",
        "job_id": 3970506171,
        "company": "Randstad USA",
        "title": "SQL Data Engineer",
        "created_on": 1720639088.9302619,
        "description": "Job Summary Location: Westlake TX but will consider TX but will consider Merrimack NH or Smithfield RI Required Skills 6+ years experience in designing and delivering data lake, data warehouses and reporting platforms strong expertise in SQL is a must Experience with Extract, Transform & Load and ELT development is required Familiarity and good understanding of Data Models - relational and dimension models is required Experience with AWS cloud technology is required Experience with multiple database technologies required location: Westlake TX, Texas job type: Contract salary: $69 - 70 per hour work hours: 8am to 4pm education: Bachelors Responsibilities The Expertise You Have Bachelor's degree 6+ years experience in designing and delivering data lake, data warehouses and reporting platforms strong expertise in SQL is a must Experience with Extract, Transform & Load and ELT development is required Familiarity and good understanding of Data Models - relational and dimension models is required Experience with AWS cloud technology is required Experience with multiple database technologies required and Snowflake desired Experience with CI/ CD and software version control Experience with reporting tools such as Tableau, OBIEE, etc. is a plus Programming experience is preferred with Python, Java desired Qualifications Experience level: Experienced Minimum 6 years of experience Education: Bachelors Skills SQL Equal Opportunity Employer: Race, Color, Religion, Sex, Sexual Orientation, Gender Identity, National Origin, Age, Genetic Information, Disability, Protected Veteran Status, or any other legally protected group status. At Randstad Digital, we welcome people of all abilities and want to ensure that our hiring and interview process meets the needs of all applicants. If you require a reasonable accommodation to make your application or interview experience a great one, please contact HRsupport@randstadusa.com. Pay offered to a successful candidate will be based on several factors including the candidate's education, work experience, work location, specific job duties, certifications, etc. In addition, Randstad Digital offers a comprehensive benefits package, including health, an incentive and recognition program, and 401K contribution (all benefits are based on eligibility). This posting is open for thirty (30) days.",
        "url": "https://www.linkedin.com/jobs/view/3970506171"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Augusta, GA",
        "job_id": 3950190751,
        "company": "Zortech Solutions",
        "title": "ETL Data Engineer",
        "created_on": 1720639090.811713,
        "description": "Note for full time candidates: (Visa Independent Only for FTE) Role: ETL Data Engineer Location: Augusta GA (100% Onsite) Duration: C2C/Fulltime Job Description Strong hands-on coding experience with 6 to 8 years of experience in ETL Hands on exp. on SQL/PL SQL Strong hands-on Experience using Azure cloud Hands on exp. on Data cloud platforms like Snowflake Ability to plan and own the work packets and with minimal supervision or direction is highly desired",
        "url": "https://www.linkedin.com/jobs/view/3950190751"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Dallas, TX",
        "job_id": 3954269690,
        "company": "Insight Global",
        "title": "Data Engineer",
        "created_on": 1720639094.1897738,
        "description": "Title: ETL Developer / Data Engineer / Software Development Engineer of Big Data 6 month contract + extensions to hire Location: Remote - EST Hours Rate: 38-45HR Exact compensation may vary based on several factors, including skills, experience, and education. Benefit packages for this role will start on the 31st day of employment and include medical, dental, and vision insurance, as well as HSA, FSA, and DCFSA account options, and 401k retirement account access with employer matching. Employees in this role are also entitled to paid sick leave and/or other paid time off as provided by applicable law. The ideal candidate will be responsible for planning, coordinating, and implementing projects within the decided-upon budget, timeline, and scope. They will also effectively monitor and present project updates to relevant stakeholders, clients, or project team members. As a Senior Software Development Engineer Big Data you will design, implement, and maintain robust, scalable data pipelines using GCP services like Dataflow, Pub/Sub, and BigQuery. The role involves leveraging cloud-native tools and technologies to develop and maintain data pipelines, ensuring data is processed efficiently and securely. The Senior Software Development Engineer Big Data will integrate data from various sources ensuring data quality, consistency, and security. This position will be responsible for writing BigQuery, creating and enhancing reporting in Tableau. Required Qualifications 3+ years experience with development and optimizing Extract, Transform, Load (ETL) processes to handle large volumes of data 3+ years experience with big data technologies such as Apache Hadoop, Spark, Elastic Search, and Kafka in conjunction with GCP tools 3+ years experience with design and management of data storage solutions using BigQuery, Cloud Storage, and other GCP storage services 3+ years experience developing new reports and enhancing existing reports using Tableau and Power BI Preferred Qualifications 3+ years experience with optimizing data processing and storage solutions for performance and cost-efficiency Ability to work closely with data scientists, analysts, and other stakeholders to understand data requirements and deliver appropriate solutions Maintaining comprehensive documentation of data architecture, processes, and compliance with data governance and security standards Education Bachelor’s degree or, equivalent experience (HS diploma + 4 years relevant experience)",
        "url": "https://www.linkedin.com/jobs/view/3954269690"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Boston, MA",
        "job_id": 3957585866,
        "company": "Zendr",
        "title": "VP, Data Engineer",
        "created_on": 1720639095.8291466,
        "description": "Global Asset Manager seeking a Vice President of Data Engineering to build out and lead their team. This person must come from a Financial Services background, and posses excellent technical and people skills. This is a full-time, hybrid role based out of Boston, MA with the expectation being 3 days a week in-office. Our client is unable to transfer or sponsor visas. Requirements : 5+ years in a technical role within Financial Services Advanced SQL skills Experience with Python Experience with Data Visualization tools (Tableau, Power BI, etc) Background of leading projects that require multiple stakeholders, both technical and non-technical Excellent Communication Skills Knowledge of Alteryx and Airflow, a plus U.S. Citizen or Green Card Holder The anticipated base salary, as listed, is $150,000 - $175,000. The expected Total Compensation (with bonus) is to be greater than $200k+.",
        "url": "https://www.linkedin.com/jobs/view/3957585866"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Houston, TX",
        "job_id": 3962891710,
        "company": "Enzo Tech Group",
        "title": "Data Engineer / Machine Learning Engineer",
        "created_on": 1720639097.7147055,
        "description": "Data Engineer / Python Data Engineer Key Responsibilities: Python Artificial intelligence AI Google Cloud Platform Google Cloud Platform DevOps Kubernetes / Docker Experience: Python Artificial intelligence AI Google Cloud Platform Google Cloud Platform DevOps Kubernetes / Docker Design and Develop Data Pipelines Machine Learning Integration Data Modelling and Data Warehouse My client is the largest distributor of Electricity and Gas across Central America. They are searching for a Data Engineer with a particular focus on Google Cloud Platform to join their team. For more information, please apply directly.",
        "url": "https://www.linkedin.com/jobs/view/3962891710"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Greenville, SC",
        "job_id": 3962980515,
        "company": "Robert Half",
        "title": "Data Engineer",
        "created_on": 1720639099.2629137,
        "description": "Responsibilities: Design and develop scalable data pipelines using technologies like Python, SQL, and frameworks such as Spark and Airflow. Extract, transform, and load (ETL) data from various sources (databases, APIs, log files, etc.) ensuring data quality and consistency. Design and implement data warehousing and data lake solutions for efficient data storage and retrieval. Develop and maintain automated data pipelines for continuous data flow and integration. Monitor and optimize data pipelines for performance and scalability. Collaborate with data scientists, analysts, and other stakeholders to understand data needs and implement effective solutions. Stay up-to-date with the latest advancements in data engineering technologies and best practices. Requirements: 4+ years of experience in the data field (data analyst, database administrator, or similar) and a desire to grow Experience with SSIS, SSRS, and ETL pipelines Database scripting languages (e.g., T-SQL, PL/SQL) Cloud-based database management (AWS RDS, Azure SQL Database, etc.) High availability (HA) and disaster recovery (DR) solutions Cloud platforms (AWS, Azure, GCP) and their data services (S3, Azure Data Lake, etc.) Big data technologies (Hadoop, Spark, Kafka, etc.) Data version control and lineage management tools Docker and containerization technologies",
        "url": "https://www.linkedin.com/jobs/view/3962980515"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Austin, TX",
        "job_id": 3968432309,
        "company": "Augment Jobs",
        "title": "Data Engineer",
        "created_on": 1720639100.9266808,
        "description": "Position Overview: We are looking for a skilled and passionate Data Engineer to join our dynamic team. As a Data Engineer, you will be responsible for designing, developing, and maintaining our data pipelines and infrastructure to support data-driven initiatives. You will collaborate closely with data scientists, analysts, and other stakeholders to ensure efficient data flow and storage. The ideal candidate will have a strong background in data engineering, experience with big data technologies, and a drive to innovate and optimize data processes. Roles And Responsibilities . Data Pipeline Development: Design, construct, install, test, and maintain highly scalable data management systems and pipelines. Data Integration: Integrate data from various sources into our data warehouse and other systems, ensuring consistency and reliability. Data Modeling: Design and implement data models and schemas to support analytical and operational requirements. Performance Optimization: Identify and resolve performance bottlenecks in data pipelines and systems to ensure optimal performance. Data Quality Assurance: Develop and implement strategies for data validation, cleansing, and quality assurance. Collaboration: Work closely with data scientists and analysts to understand data requirements and provide technical solutions. Documentation and Monitoring: Document data pipelines, processes, and systems. Implement monitoring and alerting to ensure data availability and integrity. Security and Compliance: Ensure data security and compliance with regulatory requirements during all stages of data processing and storage. Skills And Qualifications Technical Skills: Proficiency in programming languages such as Python, Java, or Scala. Experience with big data technologies (e.g., Hadoop, Spark, Kafka) and cloud platforms (e.g., AWS, GCP, Azure). Database Knowledge: Strong understanding of relational and NoSQL databases, data warehousing concepts, and ETL processes. Data Engineering Experience: Proven experience in designing and implementing data pipelines and infrastructure. Familiarity with data modeling techniques and data integration patterns. Problem-Solving Ability: Strong analytical and problem-solving skills with the ability to troubleshoot complex data issues. Team Collaboration: Excellent communication skills with the ability to collaborate effectively with cross-functional teams. Adaptability: Ability to learn new technologies and stay updated with industry trends in data engineering and big data. Compensation Salary: Competitive salary based on experience and skills. Benefits: Comprehensive benefits package including health insurance, retirement plans, paid time off, and opportunities for professional development. Company Culture: We foster a culture of innovation, collaboration, and continuous learning. We value diversity and integrity in everything we do. Join our team and contribute to our mission. Application Process: To apply for the position of Data Engineer, please submit your resume and a cover letter highlighting your relevant experience and skills. We look forward to reviewing your application and discussing how you can contribute to our data-driven initiatives.",
        "url": "https://www.linkedin.com/jobs/view/3968432309"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Austin, TX",
        "job_id": 3949309493,
        "company": "Advanced Robotics Group, LLC",
        "title": "Data Engineer",
        "created_on": 1720639102.5556993,
        "description": "Job Description Advanced Robotics Group, LLC is looking for Data Engineers who are seriously good at what they do and want to join a small elite team defying conventional wisdom regarding what is possible with robotics . As a Data Engineer , you will be responsible for designing, developing, and maintaining the data infrastructure to support our industry-changing, data-driven services. This role is crucial for ensuring that our data is reliable, scalable, and accessible enabling our engineers, data scientists, and customers to access the over 20 terabytes of data processed per day ensuring security, reliability, and robustness. Advanced Robotics Group is a driven team of technology experts developing a portfolio of non-consumer robots to operate fully autonomously in some exceptionally demanding environments. Accompanying our robotics solutions, we are refining a pioneering proprietary analytics platform providing deep insights from more than a petabyte of data. Over the last 10 years, we have developed solutions to a range of autonomous robotics and analytics challenges still thought to be impossible by the outside and are expanding and refining our portfolio of solutions before emerging from stealth and going to market with a global footprint. We solve the most difficult challenges. We need additional talented, driven individuals to join our Team, and we recently expanded into a second technology development office in Houston giving us over 14,000 square feet of long-term space to help accommodate our growth. We compensate generously including stock options and provide full medical, disability, and vision coverage, plus fun lunches and a stocked kitchen. In this role you will: Design, build and maintain efficient and scalable data pipelines to process large volumes of data Develop and manage databases to store and organize large datasets, ensuring high performance and availability Design and implement ETL processes to ensure data quality and consistency Develop and manage data processing systems both on-premises and in the cloud Continuously monitor and optimize data systems for performance, reliability, and scalability Build rapid expertise in world-class technologies and grow as a technical leader and product owner Collaborate with a motivated interdisciplinary development team in a creative, organic start-up environment focused on delivering highly innovative solutions What we’re looking for: BS/MS in Computer Science, Information Technology, or a related field with 3+ years of applicable industry experience Knowledge of big data technologies (e.g., Spark, Hadoop) Experience with containerization and orchestration tools (e.g., Docker, Kubernetes) Strong programming skills in Python Familiarity with cloud platforms and their data services Proficiency in SQL and experience with relational databases Self-driven project owner, ready to embrace an exciting challenge with real-world impact Passion for growth and excellence in delivering high-quality, detail-oriented solutions Personal discipline to maintain strict confidentiality instinctively Location: Main development center in Houston, Texas Company Description Due to the confidential nature of our activities, we aren’t able to share specifics with you now but know that these robotics solutions will have a significant impact on both safety and the environment, thus making the world better. Due to the confidential nature of our activities, we aren’t able to share specifics with you now but know that these robotics solutions will have a significant impact on both safety and the environment, thus making the world better.",
        "url": "https://www.linkedin.com/jobs/view/3949309493"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Charleston, SC",
        "job_id": 3968501800,
        "company": "Maymont Homes",
        "title": "Data Engineer",
        "created_on": 1720639105.9926815,
        "description": "Location Charleston - 997 Morrison Drive, Suite 402 Business We are a leader in the single-family rental (SFR) Aggregation space with over 10,000 homes across the Southeast and Midwest. Maymont Homes was founded in 2011 to bring technology to the single-family rental space. Over the years we have become a full-service acquisition, renovation, and property management company growing throughout the South and Midwest. By the application of efficient processes enabled by advanced software, our company can provide clean, safe, affordable housing to thousands of people. We strive to offer better living opportunities for individual families, which ultimately improve the lives in the communities we serve! Job Description Primary Responsibilities: The Data Engineer is responsible for expanding and optimizing our data and data pipeline systems. The data engineer will support our development team, analysts, data science, and business units on data initiatives and processes. Skills & Competencies: Bachelor’s degree in data engineering or related field or equivalent work experience required. Advanced proficiency in writing complex SQL queries. Prior experience working with software development and business units to define and provide data and/or reporting needs. Excellent communication skills, both verbal and written. Good quantitative judgment, time management, and attention to detail. Thrive in high growth and high ambiguity environment. Working knowledge of Python is a plus. Working knowledge of data lake and/or data warehousing technologies is a plus. Detail-oriented, with the ability to produce high-quality work in a deadline-oriented environment and an ability to multi-task. Ability to prove critical thinking and problem-solving concepts. Ability to thrive in a high volume, data entry, and processing work environment, where applicable. Experience working in a work schedule environment, weekends and holidays, based on business needs. Appropriate means (reliable, wifi, uninterrupted workspace) to work from home Essential Job Functions: Create complex SQL queries to extract and transform data from various systems into useable formats for multiple stakeholders. Collaborate with internal stakeholders to identify data requirements for business needs and develop processes to collect and maintain that data. Ensure data integrity and quality in collaboration with the internal development team and third-party consultants/partners. Participate in the development and improvement of the entire data pipeline in conjunction with the development team and other stakeholders. Improve and maintain processes regarding data ingestion and storage. Assist in optimization of data retrieval via SQL query optimizations, stored procedures, etc., and guidance on database architecture. Work with multiple business departments and leaders to optimize and automate reporting and analysis processes. Support team members with creating ad hoc reports, queries, and analyses. Other duties, as assigned by supervisor or leadership team. Key Metrics & Responsibilities Data Transformation and Pipeline operations. Support metrics from multiple stakeholders and development team. Why work for Maymont Homes ? Our Mission – “We Positively Impact the Lives in the Communities We Serve”. We do this through the work we do and the volunteer efforts that the company sponsors. You can make a difference in your community while you work! Outstanding benefits package – our benefits are provided by Brookfield and offer immediate 5% match on the 401(k) plan, wellness credits that significantly reduce the employee cost for health care coverage, and up to 160 hours of PTO per year for full time employees. Huge parent company – support and backing from Brookfield Asset Management, one of the largest real estate asset management companies. Career growth – with our plans for growth and expansion into new markets, there are many opportunities to move up within the company. Equal Opportunity Employer: Minorities/Religion/Sex/Protected Veterans/Disability/Sexual Orientation/Gender Identity/Marital Status/Pregnancy/Age/National Origin/Genetic Information. #MYMT",
        "url": "https://www.linkedin.com/jobs/view/3968501800"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3961325545,
        "company": "KBRA",
        "title": "Data Engineer (remote)",
        "created_on": 1720639107.840237,
        "description": "Position Title: Data Engineer - (remote) Entity: KBRA Holdings, LLC Employment Type: Full-Time Location: Remote (Remote only in CA, CO, DC, FL, IL, MD, NJ, MA, NY, PA, SC, TX, VA) Summary/Overview KBRA (KBRA Holdings, LLC) is seeking an engaged and proactive Data Engineer to work with our financial data, along with supplemental data sets which can be utilized to enhance analysis and visibility into the scope and health of various financial institutions. We want someone who loves solving difficult problems, digs deeply to understand the domain in which they’re working, and excels at creating high-quality software in a collaborative environment. About The Team We believe that small, empowered teams can do amazing things. Across the engineering organization, we work hard to make the best systems for our customers using modern engineering practices. We are intentional in our investments in time and effort around creating a safe and successful workplace for our team members. We understand software engineering goes beyond the 1’s and 0’s and prioritize concrete value for our customers. Our engineering team is a highly collaborative unit that is responsible for creating web applications and REST APIs backing our financial products. Working with our product team, the team builds a high-quality user experience for our clients, both internal and external. About The Job This role involves joining an existing team with a growing product vision. This team operates collaboratively and there is an expectation to get involved in all aspects of design, delivery, and support of our systems, adding features to the product continuously and iteratively. The role requires finesse around legacy codebases, particularly with regards to their extension and maintenance, all with an eye for technical quality that continues to improve as the product matures. This role emphasizes collaboration with our technical and non-technical counterparts to learn our domain and its unique challenges, while delivering value to our customers. It also requires collaboration with our other engineering, design, product, and platform teams to develop, build, run, and support the system. About You You will be successful in this role if you: Able to work in a collaborative environment. Have experience collaborating and communicating with technical and non-technical partners. Has a sense of ownership and craftsmanship in legacy and green field codebases. Enjoy helping other developers grow and learn new technologies. Must Have Skills Hands-on Python Development Experience – Well-constructed, intelligently tested, easy to re-use and extendable packages. Developing REST APIs using Python frameworks (preferably Flask). Publishing Python packages, maintaining them, and building Python CLI tools. Deploying REST APIs in containerized environments (Docker swarm, Kubernetes, etc.). Working with other developers in the team to integrate those APIs with our web applications. Working with services from one of the major Cloud providers – Preferably AWS Services such as S3, Secrets Manager, SQS (Simple Queue Service), EKS, etc. Writing CI/CD pipelines -- Azure DevOps and/or GitLab preferred. SQL, MongoDB, and/or Snowflake using Python. Designing data models for effective data storage and retrieval (preferably SQL, MongoDB, Snowflake). Supporting legacy systems and responding to incidents. Troubleshooting ETL pipelines. Nice To Have Skills Writing CI/CD configuration (preferably GitLab). Configuring observability and alerting services (preferably Datadog and Opsgenie). Writing infrastructure as code (preferably Terraform). Integrating managed authentication services (preferably Auth0). Hands-on experience in designing, developing and deploying RESTful APIs and understanding of micro-services. You will have worked on at least one API that successfully met the acceptance tests of product stakeholders and delivered business value to your enterprise. Experience building docker images and deploying services and container-based applications with Docker swarm/Kubernetes. Understanding of Distributed and Event based systems (Kafka, SQS). C# experience. Salary Range The anticipated annual base salary range for this full-time position is $90,000 to $120,000. Offer amounts are determined by factors such as experience, skills, geography, and other job-related factors. Benefits Competitive benefits and paid time off Paid family and disability leave 401(k) plan, including employer match (100% vested) Educational and professional development financial assistance Employee referral bonus program Cell phone reimbursement About Us KBRA is a full-service credit rating agency registered in the U.S., the EU and the UK, and is designated to provide structured finance ratings in Canada. KBRA’s ratings can be used by investors for regulatory capital purposes in multiple jurisdictions. More Info KBRA encourages applications from all qualified individuals without regard to race, color, religion, gender, sexual orientation, gender identity or expression, age, national origin, marital status, citizenship, disability, and veteran status or any other basis prohibited by federal, state or local law.",
        "url": "https://www.linkedin.com/jobs/view/3961325545"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Augusta, GA",
        "job_id": 3943650941,
        "company": "Zortech Solutions",
        "title": "ETL Data Engineer",
        "created_on": 1720639109.5247498,
        "description": "Role: ETL Data Engineer Location: Augusta GA (100% Onsite) Duration: C2C/Fulltime Job Description Location: Augusta GA (100% Onsite) Need 100% commitment to work from Augusta GA Strong hands-on coding experience with 6 to 8 years of experience in ETL Hands on exp. on SQL/PL SQL Strong hands-on Experience using Azure Cloud Hands on exp. on Data cloud platforms like Snowflake Ability to plan and own the work packets and with minimal supervision or direction is highly desired",
        "url": "https://www.linkedin.com/jobs/view/3943650941"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Dallas, TX",
        "job_id": 3921600358,
        "company": "Syntricate Technologies",
        "title": "GCP Data Engineer",
        "created_on": 1720639111.337073,
        "description": "Title: GCP Data Engineer Location: Woonsocket, RI / Dallas, TX Fulltime only no C2C Technical / Functional Skills Strong skills in Python Programming Strong Experience in SQL Experience with data pipeline and workflow management tool: GCP Data Proc, Composser and GCS Strong experience in handling window function in SQL BigQuery Experience Roles And Responsibilities Support analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics. Support and monitor the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, and GCP 'big data' technologies. Work with data and analytics experts to strive for greater functionality in our data systems. Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Developing, supporting and monitoring data pipelines. Work closely with client to Client needs, collect appropriate data, and deliver valuable data products. Provide valuable insights from data sets quickly.",
        "url": "https://www.linkedin.com/jobs/view/3921600358"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Charlotte, NC",
        "job_id": 3947418444,
        "company": "NTT DATA North America",
        "title": "Data Engineer (Python/Pyspark)",
        "created_on": 1720639112.9734976,
        "description": "We are currently seeking a Data Engineer 2 to join our team in Charlotte, North Carolina (US-NC), United States (US). Job Description We are currently seeking a Mid-level Data Engineer with hands-on coding experience and a strong background in Python, PySpark, and Object-oriented programming. The ideal candidate will be responsible for designing, developing, and implementing new features to our existing framework using PySpark and Python. This position requires a deep understanding of data transformation and the ability to create standalone scripts based on given business logic. Responsibilities Design, develop and implement new features to our existing framework using PySpark and Python. Write efficient and effective standalone scripts in PySpark with transformations as per the defined business logic. Use your expertise in Python and Object-oriented concepts to solve complex problems and implement robust solutions. Work closely with the team to understand the requirements and develop solutions that align with the company's objectives. Test and debug code to ensure it produces the desired results. Document all programming tasks and procedures for future reference and troubleshooting. Qualifications: 3&plus; years of experience in Python, PySpark, and Object-oriented programming concepts Proficient in Python, PySpark, and Object-oriented programming concepts. Proven experience as a Mid-level Data Engineer or similar role. Strong problem-solving techniques with an ability to troubleshoot complex software issues. Experience with AWS is preferred, but not mandatory. Excellent communication skills, both written and verbal. Self-motivated and able to work independently with minimal supervision. About NTT DATA NTT DATA is a $30&plus; billion trusted global innovator of business and technology services. We serve 75% of the Fortune Global 100 and are committed to helping clients innovate, optimize, and transform for long-term success. We invest over $3.6 billion each year in R&D to help organizations and society move confidently and sustainably into the digital future. As a Global Top Employer, we have diverse experts in more than 50 countries and a robust partner ecosystem of established and start-up companies. Our services include business and technology consulting, data and artificial intelligence, industry solutions, as well as the development, implementation and management of applications, infrastructure, and connectivity. We are also one of the leading providers of digital and AI infrastructure in the world. NTT DATA is part of NTT Group and headquartered in Tokyo. Visit us at us.nttdata.com. NTT DATA is an equal opportunity employer and considers all applicants without regarding to race, color, religion, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other characteristic protected by law. We are committed to creating a diverse and inclusive environment for all employees. If you need assistance or an accommodation due to a disability, please inform your recruiter so that we may connect you with the appropriate team.",
        "url": "https://www.linkedin.com/jobs/view/3947418444"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Holmdel, NJ",
        "job_id": 3959381462,
        "company": "Top Stack",
        "title": "Data Engineer",
        "created_on": 1720639114.7148936,
        "description": "NOTE: Our client will sponsor/transfer visas! This is not CTC. Direct hire! Top Stack Group has been retained by a Fortune 250 company with offices in PA and NJ to identify a Data Engineer to join their Data Team. team. This is a full-time, salaried position with a competitive base salary, structured bonus and a comprehensive benefits package. This is a hybrid position (2 days per week in their Holmdel, NJ office and 3 days work from home) In this Data Engineer position, you will be working in the ETL Services Group. Your focus would be on Data Reliability and you would be responsible for assisting in the design, development, and implementation of enterprise level data products for business analytics and AI that provide views into our client’s customer base. You have: 10+ years of experience as a data reliability engineer or related data professional 3+ years of experience in modern data ingestion technology tools such as Fivetran for near real time data use. 2+ years of experience in Collibra technology tool and data reliability engineering. 4+ years of experience and deeper knowledge in data engineering using Python, Pyspark and Databricks Hands-on ETL tool Informatica PowerCenter, Informatica Cloud Data integration. Experience across multiple RDBMS and managed databases. 7+ years of experience strong UNIX scripting. Strong SQL, PL/SQL experience. Significant experience interfacing with both customers and management across business and IT and track record of collaborating with IT, Business, and vendor teams to provide technical solutions and improvements, delivering end-to-end products/processes on schedule and budget, as per business requirements and SDLC standards. Provide Development & Production Support for applications. Monitor production jobs, triage issues to appropriate team & resolve any issues in a timely manner. You will: Design, implement, and maintain robust data pipelines for ingesting, processing, and transforming large volumes of structured and unstructured data. Develop and implement data quality monitoring frameworks to detect anomalies, errors, and inconsistencies in the data pipeline. Collaborate with cross-functional teams to understand data requirements and design scalable solutions to meet business needs. Implement data governance policies and procedures to ensure compliance with regulatory requirements and industry best practices. Optimize data pipeline performance, scalability, and reliability through automation, monitoring, and tuning. Troubleshoot and resolve data pipeline issues in a timely manner to minimize disruptions to data workflows. Qualifications: Bachelor’s or Master’s degree in computer science, Engineering, or related field Proven experience in designing, building, and optimizing data pipelines for analytics Excellent problem-solving and troubleshooting skills with a focus on data quality and reliability.",
        "url": "https://www.linkedin.com/jobs/view/3959381462"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Newark, NJ",
        "job_id": 3843668538,
        "company": "Syntricate Technologies",
        "title": "AWS Data Engineer",
        "created_on": 1720639116.5155573,
        "description": "AWS Data Engineer Location: Newark, NJ Fulltime role AWS Glue, Lambda, Athena, Pyspark, Redshift, EMR, AWS RDB, AWS DynamoDB, AWS Glacier, Python. Minimum 5 years of professional experience in data integration and management. 3+ years of hands on development experience in AWS Glue, Lambda functions, AWS Athena, Redshift, Pyspark. Hands on working experience in AWS GLUE ETL tool and having good knowledge on any of the ETL tools(Informatica cloud,Talend etc.). Must have working experience with SQL, and databases (e.g.: Postgres, Redshift) implementing Data Models. Expertise with Python Language and Apache Spark. Have working experience in projects involving these. Strong hands on exposure to AWS Analytics services particulary Lambda,Step functions,EMR, Athena, Glue, Data-Pipelines. Understanding and technical knowledge on AWS service like EC2, S3. Should have used these technologies in previously executed projects. Strong AWS development experience for data ETL/pipeline/integration/automation work. Should have a deep understanding of BI & Analytics Solution development lifecycle. Full Flexed in AWS cloud development & Automation for Analytics. Should have a deep understanding of AWS Services (Redshift, Glue, Lambda, Athena, S3, EC2), Visual Studio, SQL Server, SSIS, Matillion; Python. Strong experience on real-time scalable workflow implementation. Responsibilities Developer will be part of an Agile Development team focusing on Sprint Agile implementation. Developer will be developing data ingestion pipelines from heterogenous sources like databases, Files, APIs, Kafka streaming etc using a variety of services. Developer may be assigned to deliver the data from AWS data platform for different consumption use cases and services. Developer may be engaged in peer reviews to ensure code accuracy and well architected architecture and implementation. Developer may be asked to do client demo or another project demo. Developer must be proactive in escalating impacting issues and be resourceful to look for alternative. Developer may have to work on tight timelines sometimes.",
        "url": "https://www.linkedin.com/jobs/view/3843668538"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Tampa, FL",
        "job_id": 3921398421,
        "company": "Amgen",
        "title": "Data Engineer",
        "created_on": 1720639118.2790482,
        "description": "HOW MIGHT YOU DEFY IMAGINATION? You’ve worked hard to become the professional you are today and are now ready to take the next step in your career. How will you put your skills, experience and passion to work toward your goals? At Amgen, our shared mission—to serve patients—drives all that we do. It is key to our becoming one of the world’s leading biotechnology companies, reaching over 10 million patients worldwide. Come do your best work alongside other innovative, driven professionals in this meaningful role. Data Engineer Live What You Will Do Let’s do this. Let’s change the world. In this vital role you will be part of the established technical/engineering team, develop web UI interface, plus data flow pipelines to extract, transform, and load data from various data sources in various data format to enterprise data lake and data warehouse system in three regions in AWS. Provide data analytics and predictive analysis to business users. Be a key team member assisting in design and development of the data pipeline for Global Data and Analytics team Collaborate with Data Architects, Business SME’s, and Data Scientists to design and develop end-to-end data pipeline to meet fast paced business need across geographic regions Serve as system admin to manage AWS and Databricks platform; Adhere to best practices for coding, testing and designing reusable code/component Able to explore new tools, technologies that will help to improve ETL platform performance and machine learning operations Participate in sprint planning meetings and provide estimations on technical implementation; Collaborate and communicate effectively with the product teams Win What We Expect Of You We are all different, yet we all use our unique contributions to serve patients. The professional we seek will have these qualifications. Basic Qualifications: Master’s Degree OR Bachelor’s degree with 2 years Data Engineering and/or and Software Engineering experience Or Associate’s degree 6 years of Data Engineering and/or Software Engineering experience Or High school diploma and 8 years of Data Engineering and/or Software Engineering experience Preferred Qualifications: Familiar with PySpark dataframe and data processing libraries, machine learning frameworks (like Tensorflow, Keras or PyTorch), and other machine learning libraries Familiar with machine learning operations Experience with software development (Java, Python preferred), end-to-end system design Experience with data modeling for both OLAP and OLTP databases, hands-on experience with SQL, preferred Oracle, PostgreSQL, and Hive SQL; SQL performance tuning Experience with web development, java script, html, CSS, any web framework or microservice architecture Experience with software DevOps CI/CD tools, such Git, Jenkins Experience on AWS, familiar with EC2, S3, Redshift/Spectrum, Glue, Athena, RDS, Lambda, DynamoDB, and API gateway Experience with docker container, Kubernetes container orchestration Experience with Apache Airflow and Apache Spark; Spark performance turning Experience with Tableau Dashboard and Tableau Server Experience with Pharmaceutical industry, commercial operations Ability to learn quickly, be organized and detail oriented Thrive What You Can Expect Of Us As we work to develop treatments that take care of others, we also work to care for our teammates’ professional and personal growth and well-being. The expected annual salary range for this role in the U.S. (excluding Puerto Rico) is posted. Actual salary will vary based on several factors including but not limited to, relevant skills, experience, and qualifications. Amgen offers a Total Rewards Plan comprising health and welfare plans for staff and eligible dependents, financial plans with opportunities to save towards retirement or other goals, work/life balance, and career development opportunities including: Comprehensive employee benefits package, including a Retirement and Savings Plan with generous company contributions, group medical, dental and vision coverage, life and disability insurance, and flexible spending accounts. A discretionary annual bonus program, or for field sales representatives, a sales-based incentive plan Stock-based long-term incentives Award-winning time-off plans and bi-annual company-wide shutdowns Flexible work models, including remote work arrangements, where possible Apply now for a career that defies imagination Objects in your future are closer than they appear. Join us. careers.amgen.com Application deadline Amgen does not have an application deadline for this position; we will continue accepting applications until we receive a sufficient number or select a candidate for the position. Amgen is an Equal Opportunity employer and will consider you without regard to your race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status. We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.",
        "url": "https://www.linkedin.com/jobs/view/3921398421"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3953815766,
        "company": "PrismHR",
        "title": "Data Engineer",
        "created_on": 1720639120.1469388,
        "description": "Do you have a passion for building data architectures that enable smooth and seamless product experiences? Are you an all-around data enthusiast with a knack for ETL? We're hiring Data Engineers to help build and optimize the foundational architecture of our product's data. We’ve built a strong data engineering team to date, but have a lot of work ahead of us, including: Migrating from relational databases to a streaming and big data architecture, including a complete overhaul of our data feeds Defining streaming event data feeds required for real-time analytics and reporting Leveling up our platform, including enhancing our automation, test coverage, observability, alerting, and performance As a Data Engineer, you will work with the development team to construct a data streaming platform and data warehouse that serves as the data foundations for our product. Help us scale our business to meet the needs of our growing customer base and develop new products on our platform. You'll be a critical part of our growing company, working on a cross-functional team to implement best practices in technology, architecture, and process. You’ll have the chance to work in an open and collaborative environment, receive hands-on mentorship and have ample opportunities to grow and accelerate your career! Responsibilities Build our next generation data warehouse Build our event stream platform Translate user requirements for reporting and analysis into actionable deliverables Enhance automation, operation, and expansion of real-time and batch data environment Manage numerous projects in an ever-changing work environment Extract, transform, and load complex data into the data warehouse using cutting-edge technologies Build processes for topnotch security, performance, reliability, and accuracy Provide mentorship and collaborate with fellow team members Qualifications: Bachelor’s or Master’s degree in Computer Science, Information Systems, Operations Research, or related field required 3+ years of experience building data pipelines 3+ years of experience building data frameworks for unit testing, data lineage tracking, and automation Fluency in Scala is required Working knowledge of Apache Spark Familiarity with streaming technologies (e.g., Kafka, Kinesis, Flink) Nice-to-Haves: Experience with Machine Learning Familiarity with Looker a plus Knowledge of additional server-side programming languages (e.g. Golang, C#, Ruby) Please note: This position can be remote/telecommute. Notice for candidates located in the following states: CA, CO, NJ, NY, WA: The base salary range for this position is between $110,000 - $130,000 (salary is dependent on location, experience, knowledge, and skills based on the responsibilities outlined in the job description). PrismHR is a fast-paced SaaS company which provides customers with a cloud-based payroll process software application. PrismHR also provides professional services including system implementation consulting, custom configurations, and training. Lastly, via the Company’s Marketplace platform customers and end users access other human resources and employee benefits applications from PrismHR’s Marketplace Partners. Diversity, Equity And Inclusion Program/Affirmative Action Plan We have transformed our company into an inclusive environment where individuals are valued for their talents and empowered to reach their fullest potential. At PrismHR, we strive to continually lead with our values and beliefs that enable our employees to develop their potential, bring their full self to work, and engage in a world of inclusion. Ensuring an inclusive environment for our employees is an integral part of the PrismHR culture. We aren't just checking a box, we are truly committed to creating a workplace that celebrates the diversity of our employees and fosters a sense of belonging for everyone. This is essential to our success. We are dedicated to building a diverse, inclusive, and authentic workplace, so if you’re excited about our roles but your past experience doesn’t align perfectly with every qualification in the job description, we encourage you to apply anyway. You may be just the right candidate for these open roles or other open roles. We particularly encourage applicants from traditionally under-represented groups as we seek to increase the diversity of our workforce and provide fair opportunities for all. As a proud Equal Opportunity and Affirmative Action Employer, PrismHR encourages talent from all backgrounds to join our team. Employment decisions are based on an individual’s qualifications as they relate to the job under consideration. The Company’s policy prohibits unlawful discrimination based on sex (which includes pregnancy, childbirth, breastfeeding, or related medical conditions, the actual sex of the individual, or the gender identity or gender expression), race, color, religion, including religious dress practices and religious grooming practices, sexual orientation, national origin, ancestry, citizenship, marital status, familial status, age, physical disability, mental disability, medical condition, genetic information, protected veteran or military status, or any other consideration made unlawful by federal, state or local laws, ordinances, or regulations. The Company is committed to complying with all applicable laws providing equal employment opportunities. This commitment applies to all persons involved in the operations of the Company and prohibits unlawful discrimination by any employee of the Company, including supervisors and co-workers. Privacy Policy: For information about how we collect and use your personal information, please see our privacy statement available at https://www.prismhr.com/about/privacy-policy. PrismHR provides reasonable accommodation for qualified individuals with disabilities and disabled veterans in job application procedures. If you have any difficulty using our online system and you need a reasonable accommodation due to a disability, you may use the following alternative email address to contact us about your interest in employment at PrismHR: taglobal@prismhr.com. Please indicate in the subject line of your email that you are requesting accommodation. Only candidates being considered for a position who require an accommodation will receive a follow-up response.",
        "url": "https://www.linkedin.com/jobs/view/3953815766"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Bethlehem, PA",
        "job_id": 3959044693,
        "company": "Perennial Resources International",
        "title": "Data Engineer",
        "created_on": 1720639121.8070009,
        "description": "***NO Third Party Candidates Please*** Contract Bethlehem, PA (Local Candidates Only) Requirements: -Bachelor's or master's degree with 5+ years of experience in Computer Science, Data Science, Engineering, or a related field. -4+ years of experience in working with Python, SQL, PySpark and bash scripts. -Proficient in software development lifecycle and software engineering practices. -2+ years of hands-on experience in using Databricks platform -3+ years of hands-on experience in operationalizing Machine Learning solutions which are used in live production processes. -2+ years of experience and proficiency in API development using FastAPI frameworks and familiarity with containerization technologies such as docker or Kubernetes. -3+ years of experience in developing and maintaining robust data pipelines data to be used by Data Scientists to build Client Models. -3+ years of experience working with Cloud Data Warehousing (Redshift, Snowflake, Databricks SQL or equivalent) platforms and experience in working with distributed framework like Spark. -Solid understanding of machine learning life cycle, data mining, and ETL techniques. -Experience with machine learning frameworks such as Keras or PyTorch and libraries such as scikit-learn, xgboost). -Hands-on experience in building and maintaining tools and libraries which have been used by multiple teams across organization. -Proficient in understanding and incorporating software engineering principles in design & development process. -Hands on experience with CI/CD tools (e.g., Jenkins or equivalent), version control (Github, Bitbucket), Orchestration (Airflow, Prefect or equivalent) -Excellent communication skills and ability to work and collaborate with cross functional teams across technology and business. Pluses only: -Familiarity with deep learning frameworks and deploying deep learning models for production use cases. -Familiarity in using GPU compute either for model training or inference. -Understanding of Large language models (LLM) and MLOps lifecycle for operationalizing LLM models.",
        "url": "https://www.linkedin.com/jobs/view/3959044693"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Greater Orlando",
        "job_id": 3958930055,
        "company": "Moorecroft Systems",
        "title": "Data Engineer",
        "created_on": 1720639123.5841577,
        "description": "Moorecroft is seeking to hire a Data Engineer in the Orlando area. Exciting and engaging consulting role in a fast-paced environment. Long-term engagement with potential to extend. Data Engineer Location: Orlando Duration: 6+ months This role communicates data engineering progress to the project leadership team, and actively participates in meetings and discussions. Technologies generally leveraged to fulfill the work include, but not limited to PostgreSQL, Snowflake, SQL, Python, Docker, and GitLab. 3-5 years of overall technical experience Experience with ELT/ETL data pipeline development and maintenance High Proficiency in SQL coding Proven experience and expertise using Python, Docker, Snowflake and/or Postgres Experience leveraging job scheduling software like Apache Airflow Prior experience gathering and refining data requirements and producing data design solutions Experience with developing in a multi environment (Dev, QA, Prod, etc.) and DevOps procedures for code deployment/promotion Strong understanding of database design and proficiency using various database platforms, such as PostgreSQL or Snowflake Experience managing and deploying code using a source control product such as GitLab/GitHub Able to optimally formulate solutions and communicate sophisticated technical concepts to non-technical team members Experience working with large datasets and big data technologies, preferably cloud-based, such as Snowflake or similar Knowledgeable on cloud architecture and product offerings, preferably AWS Experience using containerization technologies such as Docker or Kubernetes Education Bachelor’s degree (Computer Science, Mathematics, Software Engineering or related field, or equivalent experience) Master’s degree preferred (Computer Science, Mathematics, Engineering or related field preferred) W e do not work with 3rd party employers. Visa Sponsorship are NOT available ---------------------------------------------- ABOUT MOORECROFT A quality oriented national consulting firm. For more than two decades, providing expert IT professionals to our clients around the nation. Our core values are professionalism, honesty, and integrity. We are only interested in working with professionals that subscribe to the same values and are driven to truly serve clients.",
        "url": "https://www.linkedin.com/jobs/view/3958930055"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Manchester, NH",
        "job_id": 3948506957,
        "company": "Moore",
        "title": "Data Engineer",
        "created_on": 1720639128.8022532,
        "description": "Brief Description As the Data Engineer you will help design, develop, and maintain our data pipelines, databases, and analytical systems. This role collaborates with cross-functional teams to gather requirements, implement data solutions, and ensure the reliability and efficiency of our data infrastructure. Primarily, this role will partner with senior data engineers to design and implement scalable data pipelines for ingesting, transforming, and storing large volumes of structured and unstructured data. Moore is a data-driven constituent experience management (CXM) company achieving accelerated growth for clients through integrated supporter experiences across all platforms, channels and devices. We are an innovation-led company that is the largest marketing, data and fundraising company in North America serving the purpose-driven industry with clients across education, association, political and commercial sectors. Check out www.WeAreMoore.com for more information. Your impact: Build and maintain data warehouses, data pipelines, and other storage solutions to support business intelligence, reporting, and analytics needs. Design, implement, and maintain scalable data pipelines for extracting, transforming, and loading (ETL) data from various sources into our data warehouse. Work with data scientists and analysts to understand data requirements and implement solutions for data modeling, analysis, and visualization. Monitor data pipelines and systems performance, troubleshoot issues, and implement optimizations to improve reliability, scalability, and efficiency. Support leadership in identifying, designing, and implementing internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes. Stay up to date with emerging technologies and industry trends in data engineering and contribute ideas for continuous improvement of data infrastructure and processes. Your Profile: 2-5 years of experience in data engineering roles. Bachelor's degree in computer science, information systems, engineering, mathematics, or a related field. Experience Azure Data Factory for data integration, transformation, and validation processes. Proficient in T-SQL experience with Snowflake is a plus. Relevant certifications (optional but preferred) like Microsoft Azure Data Engineer Associate. Programming skills in Python, JavaScript, or .NET is a plus. Familiarity with distributed computing frameworks (e.g., Hadoop, Spark). Solid understanding of ETL processes and tools like SSIS. Understanding of data modeling, schema design, and data architecture. Basic understanding of cloud platforms such as AWS, Azure, or Google Cloud. Proven experience in building and maintaining data pipelines. Joining Moore, you will: Do meaningful, life-changing work every day by supporting the nation’s most beloved nonprofits and service organizations. Join the largest marketing and fundraising company in North America serving the nonprofit industry where we prioritize innovation and professional growth. Collaborate with industry subject matter experts among over 5,000 employees across the enterprise. Earn a competitive salary and have access to comprehensive health, wellness, and retirement benefits. Enjoy paid holidays and generous paid time off, ensuring you have the time and space to recharge and pursue your other passions. Moore’s commitment to candidates: Moore is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. Moore is also committed to compliance with all fair employment practices regarding citizenship and immigration status.",
        "url": "https://www.linkedin.com/jobs/view/3948506957"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Mentor, OH",
        "job_id": 3930973708,
        "company": "TelAdvisor Group",
        "title": "Data Engineer",
        "created_on": 1720639130.7678561,
        "description": "Description Race Winning Brands has an immediate opening for a Data Engineer to join our business intelligence team. The ideal candidate will have a strong background in data architecture, database design , data warehousing, data integration development, and ETL (Extract, Transform, Load) processes. The data engineer will collaborate with cross-functional teams to design and implement scalable data solutions that meet the needs of our growing business. This position is hybrid PM21 Requirements Responsibilities: Build and optimize data models for performance and efficiency. Design data warehouse solutions for storing and accessing large volumes of structured and unstructured data. Design, develop, and maintain data integrations and ETL processes to support data-driven applications and analytics. Implement data quality and validation procedures to ensure data accuracy and consistency. Collaborate with the business intelligence team and other stakeholders to understand data requirements and implement solutions. Monitor and troubleshoot data pipeline issues, performance bottlenecks, and system errors. Stay current with emerging technologies and best practices in data engineering and analytics. Qualifications Bachelor's degree in Computer Science/Information Systems, Engineering, or a related field. Proven experience as a data engineer or similar role, with a minimum of 5 years of experience. Strong understanding of database concepts and data modeling techniques. Proficiency in SQL and experience with relational and non-relational databases. Proficiency in the Microsoft Data Stack (SSAS, SSIS). Excellent problem-solving and analytical skills. Strong communication and collaboration skills with the ability to work effectively in a team environment. Preferred Qualifications Experience with data visualization tools (Power BI and SSRS). Experience with building/supporting an iPaaS solution (Jitterbit). Knowledge of AI/machine learning techniques and concepts. Experience with database administration Experience with cloud platforms (Azure). Knowledge of big data technologies",
        "url": "https://www.linkedin.com/jobs/view/3930973708"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3922692920,
        "company": "Career Renew",
        "title": "Data Engineer - Rust",
        "created_on": 1720639132.480566,
        "description": "Career Renew is recruiting for one of its clients a Data Engineer - Rust - candidates can be based in N or S America. We are looking for a Data Engineer with deep understanding of relational databases and modern data stacks, and a desire to code in Rust. A Rust Data Engineer will not just use Rust data tools, but build our general purpose blockchain data systems and also contribute to our dependencies. What You'll Be Doing Working within the team to improve Node's capabilities as a data processing and querying engine Familiarizing yourself with the large and complex Rust codebase Using subgraphs to understand both the specification implemented as well as the developer experience of the builders who are our users Develop an understanding of existing code and design choices, and then analyze how they can be experimented with and improved upon Setting up performance tests, doing quantitative assessments of any proposals and changes you make, and monitoring how the changes ultimately behave when rolled out Experimentally validating, and if necessary, falsifying your own ideas Reviewing pull requests of your colleagues and taking responsibility for the reviewed code as if it were your own Documenting important aspects of the software while understanding that we have a large community of independent operators that need to understand how to run, configure, and monitor our tool Requirements Based in North or South America Extensive experience with Rust (min two years professionally) Prior experience building data tools in Rust Web3 experience (or very interested in the space)",
        "url": "https://www.linkedin.com/jobs/view/3922692920"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Richardson, TX",
        "job_id": 3971398464,
        "company": "CVS Health",
        "title": "Associate Data Engineer ( SQL , ETL , RDB , Cloud )",
        "created_on": 1720639134.278065,
        "description": "Bring your heart to CVS Health. Every one of us at CVS Health shares a single, clear purpose: Bringing our heart to every moment of your health. This purpose guides our commitment to deliver enhanced human-centric health care for a rapidly changing world. Anchored in our brand — with heart at its center — our purpose sends a personal message that how we deliver our services is just as important as what we deliver. Our Heart At Work Behaviors™ support this purpose. We want everyone who works at CVS Health to feel empowered by the role they play in transforming our culture and accelerating our ability to innovate and deliver solutions to make health care more personal, convenient and affordable. Position Summary: As the Associate Data Engineer, you will be responsible for application development for the Pharmacy Benefits Management Enterprise Data Warehouse team. The position will be accountable for designing comprehensive solutions that meet business requirements and incrementally move the data warehouse to the future state. The position requires strong attention to detail, excellent communications skills and exceptional follow through. You will be working in a hybrid role (3 days a week onsite) in one of the CVS Health or Aetna offices in Richardson, TX; Irving, TX or Chicago, IL. Required Qualifications: 6+ months Data Engineering / Data Integration experience 6+ months SQL 6+ months ETL (i.e. Informatica/DataStage) experience 6+ months Relational Database experience (Teradata, SQL Server, Oracle) 6+ months Cloud computing (preferably GCP but open to AWS or Azure) Strong work ethic – evening and weekend hours may be required (for project release issues, approximately 2-3x/year) Excellent oral and written communications skills; ability to interact effectively with all levels within the organization Working knowledge of Agile methodology Excellent analytical and problem solving skills Ability to interact and work effectively with technical & non-technical levels within the organization Ability to drive clarity of purpose and goals during release and planning activities Excellent organizational skills including ability to prioritize tasks efficiently with high level of attention to detail Preferred Qualifications: 6+ months experience working in a cross-functional environment 6+ months Teradata BTEQ scripting experience Healthcare Industry experience Education: Bachelor's degree or equivalent Pay Range The typical pay range for this role is: $61,800.00 - $154,500.00 This pay range represents the base hourly rate or base annual full-time salary for all positions in the job grade within which this position falls. The actual base salary offer will depend on a variety of factors including experience, education, geography and other relevant factors. This position is eligible for a CVS Health bonus, commission or short-term incentive program in addition to the base pay range listed above. In addition to your compensation, enjoy the rewards of an organization that puts our heart into caring for our colleagues and our communities. The Company offers a full range of medical, dental, and vision benefits. Eligible employees may enroll in the Company’s 401(k) retirement savings plan, and an Employee Stock Purchase Plan is also available for eligible employees. The Company provides a fully-paid term life insurance plan to eligible employees, and short-term and long term disability benefits. CVS Health also offers numerous well-being programs, education assistance, free development courses, a CVS store discount, and discount programs with participating partners. As for time off, Company employees enjoy Paid Time Off (“PTO”) or vacation pay, as well as paid holidays throughout the calendar year. Number of paid holidays, sick time and other time off are provided consistent with relevant state law and Company policies. For more detailed information on available benefits, please visit jobs.CVSHealth.com/benefits We anticipate the application window for this opening will close on: 07/10/2024",
        "url": "https://www.linkedin.com/jobs/view/3971398464"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Fort Worth, TX",
        "job_id": 3971266443,
        "company": "Mindlance",
        "title": "Data Engineer",
        "created_on": 1720639136.485418,
        "description": "Bachelor's degree in Computer Science, Computer Engineering, Technology, Information Systems (CIS/MIS), Engineering or related technical discipline, or equivalent experience/training. 5 years software solution development using agile, DevOps, operating in a product model that includes designing, developing, and implementing large-scale applications or data engineering solutions 5 years data analytics experience using SQL 5 years of cloud development and data lake experience (prefer Microsoft Azure) including Azure EventHub, Azure Data Factory, Azure Databricks, Azure DevOps, Azure Blob Storage, Azure Data Lake, Azure Power Apps and Power BI. Combination of Development, Administration & Support experience in several of the following tools/platforms required: Scripting: Python, Spark, Unix, SQL Data Platforms: Teradata, Couchbase, MongoDB, Oracle, SQL Server, ADLS, Snowflake Azure Data Explorer. Administration skills a plus Azure Cloud Technologies: Azure Data Factory, Azure Databricks, Azure Blob Storage, Azure Data Lake, Azure Power Apps and Azure Functions CI/CD: GitHub, Jenkins, Azure DevOps, Terraform BI Analytics Tool Stack - Cognos, Tableau, Power BI, Alteryx, Denodo, and Grafana Data Warehousing: DataStage, Informatica “Mindlance is an Equal Opportunity Employer and does not discriminate in employment on the basis of – Minority/Gender/Disability/Religion/LGBTQI/Age/Veterans.”",
        "url": "https://www.linkedin.com/jobs/view/3971266443"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Atlanta, GA",
        "job_id": 3971534574,
        "company": "UPS",
        "title": "UPSC Data Engineer",
        "created_on": 1720639138.29062,
        "description": "Before you apply to a job, select your language preference from the options available at the top right of this page. Explore your next opportunity at a Fortune Global 500 organization. Envision innovative possibilities, experience our rewarding culture, and work with talented teams that help you become better every day. We know what it takes to lead UPS into tomorrow—people with a unique combination of skill + passion. If you have the qualities and drive to lead yourself or teams, there are roles ready to cultivate your skills and take you to the next level. Job Description: Job Summary This position develops batch and real-time data pipelines utilizing various data analytics processing frameworks in support of Data Science and Machine Learning practices. This position assists in the integration of data from various data sources, both internal and external. This position performs extract, transform, load (ETL) data conversions, and facilitates data cleansing and enrichment as well as performs full systems life cycle management activities, such as analysis, technical requirements, design, coding, testing, implementation of systems and applications software. This position assists in synthesizing disparate data sources to create reusable and reproducible data assets. This position assists the Data Science community working through analytical model feature tuning. Responsibilities Contributes to data engineering projects and builds solutions by leveraging a foundational knowledge in software/application development, literate in the programming languages used for statistical modeling and analysis, data warehousing and Cloud solutions, and building data pipelines. Collaborates effectively, produces data engineering documentation, gathers requirements, organizes data and defines the scope of a project. Performs data analysis and presents findings to the stakeholders to support business needs. Participates in the integration of data for data engineering projects. Understands and utilizes analytic reporting tools and technologies. Assists with data engineering maintenance and support. Assists in defining the data interconnections between organizations’ operational and business functions. Assists in backup and recovery and utilizes technology solutions to perform POC analysis. Qualifications Requirements: Understanding of database systems and data warehousing solutions. Understanding of data life cycle stages - data collection, transformation, analysis, storing the data securely, providing data accessibility Understanding of the data environment to ensure that it can scale for the following demands: Throughput of data, increasing data pipeline throughput, analyzing large amounts of data, real-time predictions, insights and customer feedback, data security, data regulations, and compliance. Contributes to the following: Building a data platform, ensuring data is secure in motion and at rest, automating data compliance and auditing, data warehousing solutions for scalable analytics. Familiarity with analytics reporting technologies and environments – (e.g., PBI, Looker, Qlik, etc.) Basic knowledge of algorithms and data structures to assist in understanding the big picture of the organization’s overall data function. Knowledge in data filtering and data optimization. Familiarity with a Cloud services platform (e.g., GCP, or AZURE, or AWS) and all the data life cycle stages. Understanding ETL tools capabilities. Ability to pull data from various sources, perform a load of the transformed data into a database or business intelligence platform. Familiarity with Machine learning algorithms which help data scientists make predictions based on current and historical data. Knowledge of algorithms and data structures with the ability to organize the data for reporting, analytics, and data mining and perform data filtering and data optimization. Ability to build data APIs to enable data scientists and business intelligence analysts to query the data. Ability to code using programming language used for statistical analysis and modeling such as Python/Java/Scala/C++. Understanding the basics of distributed systems. A Bachelor’s degree in MIS or mathematics, statistics, or computer science, international equivalent, or equivalent job experience. Employee Type: Permanent UPS is committed to providing a workplace free of discrimination, harassment, and retaliation. Other Criteria: UPS is an equal opportunity employer. UPS does not discriminate on the basis of race/color/religion/sex/national origin/veteran/disability/age/sexual orientation/gender identity or any other characteristic protected by law. Basic Qualifications: Must be a U.S. Citizen or National of the U.S., an alien lawfully admitted for permanent residence, or an alien authorized to work in the U.S. for this employer.",
        "url": "https://www.linkedin.com/jobs/view/3971534574"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3929064524,
        "company": "Steneral Consulting",
        "title": "Remote Work - Need Data Engineer",
        "created_on": 1720639140.0541077,
        "description": "100% remote Need valid LinkedIn with picture Resume must be under 5 pages Job Summary: As a Data Engineer you will play a critical role in designing, building, and maintaining our data infrastructure. You will work closely with data analysts, data scientists, and other stakeholders to ensure the seamless extraction, transformation, and loading (ETL) of data from various sources into our Snowflake data warehouse. Your expertise in Fivetran, dbt, and Snowflake will be pivotal in helping us create a scalable and efficient data ecosystem. Key Responsibilities ETL Development and Maintenance: Design, implement, and manage ETL processes using Fivetran to automate data extraction from various sources. Ensure data is accurately and efficiently transformed and loaded into the Snowflake data warehouse. Data Modeling and Transformation: Utilize dbt (data build tool) to develop and maintain data models that support business requirements. Write, test, and maintain SQL code for data transformations and ensure data quality and consistency. Data Warehouse Management: Maintain and optimize our Snowflake data warehouse, ensuring high performance and availability. Implement best practices for data warehousing, including schema design, indexing, and query optimization. Collaboration and Communication: Work closely with data analysts, data scientists, and other stakeholders to understand data needs and deliver solutions. Communicate complex technical concepts to non-technical stakeholders in a clear and concise manner. Monitoring and Troubleshooting: Monitor ETL pipelines and data workflows to ensure they are running smoothly and efficiently. Troubleshoot and resolve any issues that arise in the ETL processes or data warehouse. Qualifications Educational Background: Bachelor’s degree in Computer Science, Information Technology, Data Science, or a related field. Technical Skills: Proven experience with Fivetran for automated data extraction and loading. Strong proficiency in dbt for data modeling and transformation. Expertise in Snowflake, including data warehouse architecture, performance tuning, and security. Advanced SQL skills and experience with data manipulation and querying. Familiarity with Python or other programming languages for scripting and automation is a plus. Professional Experience: Minimum of 3-5 years of experience in a data engineering role or similar position. Experience with cloud platforms (e.g., AWS, GCP, Azure) and their data services. Soft Skills: Strong analytical and problem-solving skills. Excellent communication and teamwork abilities. Ability to work independently and manage multiple tasks in a fast-paced environment.",
        "url": "https://www.linkedin.com/jobs/view/3929064524"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Salt Lake City, UT",
        "job_id": 3946211233,
        "company": "CyberCoders",
        "title": "Data Engineer",
        "created_on": 1720639141.8323607,
        "description": "**Join Our Team in Salt Lake City!** About Us Our mission is to revolutionize data intelligence for tourist destinations. Based in Salt Lake City, Utah, we have over 30 years of experience in technology, tourism, and destination marketing. About The Role As an engineer on the core product team, you'll leverage advanced data processing techniques to provide valuable insights across the company, enhancing decision-making. You'll collaborate with top-tier engineers to develop essential data warehouse tables, supporting our mission of enabling data-driven products and insights. Develop scalable data infrastructure to handle diverse data types and sources. Build robust data pipelines for extracting, transforming, and loading data. Create key datasets for machine learning, growth analysis, and business forecasting initiatives. Requirements 2+ years experience in Data or Software Engineering BA/BS in quantitative or computer science Proficient in Python/SQL and programming GCP / Terraform Expert in manipulating large datasets Quick learner, adept at tackling new challenges Skilled in coding for real product development Bonus Points For Experience with data visualization Benefits Looking forward to receiving your resume through our website and going over the position with you. Clicking apply is the best way to apply, but you may also: Email Your Resume In Word To hanna.frauen@cybercoders.com Please do NOT change the email subject line in any way. You must keep the JobID: linkedin : HF1-1806576 -- in the email subject line for your application to be considered.*** Hanna Frauen - Executive Recruiter Applicants must be authorized to work in the U.S. CyberCoders is proud to be an Equal Opportunity Employer All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, sexual orientation, gender identity or expression, national origin, ancestry, citizenship, genetic information, registered domestic partner status, marital status, status as a crime victim, disability, protected veteran status, or any other characteristic protected by law. CyberCoders will consider qualified applicants with criminal histories in a manner consistent with the requirements of applicable law. CyberCoders is committed to working with and providing reasonable accommodation to individuals with physical and mental disabilities. If you need special assistance or an accommodation while seeking employment, please contact a member of our Human Resources team to make arrangements.",
        "url": "https://www.linkedin.com/jobs/view/3946211233"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Philadelphia, PA",
        "job_id": 3969268248,
        "company": "V-Soft Consulting Group, Inc.",
        "title": "Senior Data Engineer",
        "created_on": 1720639143.4853837,
        "description": "Hiring- Data Engineer III- Location: Philadelphia, PA; 3-4 days a week onsite-Only Locals-Client will not provide any sponsorship Data Engineer III Location: Philadelphia, PA; 3-4 days a week onsite. Contract can extend into 2025. Multiple Teams rounds for interviews and may include coding Team: • We have 18 team members split between US/India. • We do routing of critical infrastructure, automating graph DB representation of network topology. • Looking for someone very hands on with Databricks and Python. Skills: • Neo4j Graph database or Neptune. • Python – must have • Databricks – must have • Ingest through multiple sources • Lambda functions – must have • API’s – must have • AWS – certs are a big plus but must have Lambdas and development in AWS. • 5-7 years’ experience with the above technology Regards Priyanka pneelam@vsoftconsulting.com #LI-PN1",
        "url": "https://www.linkedin.com/jobs/view/3969268248"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Dallas, TX",
        "job_id": 3966159068,
        "company": "Amazon Web Services (AWS)",
        "title": "Data Engineer I, AWS Fintech",
        "created_on": 1720639145.2149138,
        "description": "Description Do you love building tools and data pipelines? Are you excited by the opportunity to create clear effective reports and data visualizations, and partner with stakeholders to answer key business questions? Do you want to be a part of a fast-paced environment and contribute to one of the most visited sites on the Internet? If this describes you, consider joining us as a Data Engineer. You will have the opportunity to impact the evolution of Amazon technology as well as lead mission critical projects early in your career. Your work will contribute to solving some of the most complex technical challenges in the company. Key job responsibilities Design, implement, and automate deployment of our distributed system for collecting and processing data from multiple upstream sources Design data schema and operate internal data warehouses and SQL/NoSQL database systems Own the design, development, and maintenance of ongoing metrics, reports, analyses, and dashboards to drive key business decisions Monitor and troubleshoot operational or data issues in the data pipelines Drive architectural plans and implementation for future data storage, reporting, and analytic solutions Work collaboratively with TPMs, Product managers, and other internal partners to identify opportunities/problems Provide assistance to the team with troubleshooting, researching the root cause, and thoroughly resolving defects in the event of a problem Basic Qualifications 1+ years of data engineering experience Bachelor's degree Experience with data modeling, warehousing and building ETL pipelines Experience with one or more query language (e.g., SQL, PL/SQL, DDL, MDX, HiveQL, SparkSQL, Scala) Experience with one or more scripting language (e.g., Python, KornShell) Experience with SQL Preferred Qualifications Experience with data visualization software (e.g., AWS QuickSight or Tableau) or open-source project Experience with big data technologies such as: Hadoop, Hive, Spark, EMR Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us. Company - Amazon.com Services LLC Job ID: A2650835",
        "url": "https://www.linkedin.com/jobs/view/3966159068"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Corvallis, OR",
        "job_id": 3932307189,
        "company": "Conch Technologies, Inc",
        "title": "Data Engineer/ Architect",
        "created_on": 1720639146.9655104,
        "description": "Hi, Fulltime or Long Term Contract Greetings from Conch Technologies Inc Data Engineer/ Architect Location: Corvallis, OR – Onsite Role Long Term Project OR Direct Fulltime Job Description The data engineering role is a team member that will help enhance and maintain the CSS Business Analytics and Instant Ink Business Intelligence system. You will drive work you're doing to completion with hands-on development responsibilities, and partner with the Data Engineering leaders to implement data engineering pipelines to build solution to help provide trusted and reliable data to customers. Responsibilities At least 15+ years’ experience in data engineering, AI ML and AWS/Azure Architect, Design and implement distributed data processing pipelines using Spark, Python, SQL and other tools and languages prevalent in the Big Data/Lakehouse ecosystem. Experience in AI and ML on AWS/AZURE Analyzes design and determines coding, programming, and integration activities required based on general objectives. Play the technical lead role representing deliverables from vendor team resources at onsite and offshore locations. Lead the technical co-ordination and Business Knowledge transition activities to offshore team. Reviews and evaluates designs and project activities for compliance with architecture, security and quality guidelines and standards. Writes and executes complete testing plans, protocols, and documentation for assigned portion of data system or component; identifies defects and creates solutions for issues with code and integration into data system architecture. Collaborates and communicates with project team regarding project progress and issue resolution. Works with the data engineering team for all phases of larger and more-complex development projects and engages with external users on business and technical requirements. Collaborates with peers, engineers, data scientists and project team. Typically interacts with high-level Individual Contributors, Managers and Program Teams on a daily/weekly basis. What You Bring Bachelor's or Master's degree in Computer Science, Information Systems, Engineering or equivalent. 6+ years of relevant experience with detailed knowledge of data technical architectures, infrastructure components, ETL/ ELT and reporting/analytic tools. 3+ years of experience with Cloud based DW such as Redshift, Snowflake etc. 3+ years’ experience in Big Data Distributed ecosystems (Hadoop, SPARK, Unity Catalog & Delta Lake) 3+ years experience in Workflow orchestration tools such as Airflow etc. 3+ years’ experience in Big Data Distributed systems such as Databricks, AWS EMR, AWS Glue etc. Leverage monitoring tools/frameworks, like Splunk, Grafana, CloudWatch etc. Experience with container management frameworks such as Docker, Kubernetes, ECR etc. 3+ year’s working with multiple Big Data file formats (Parquet, Delta Lake) Experience working on CI/CD processes such as Jenkins, Codeway etc. and source control tools such as GitHub, etc. Strong experience in coding languages like Python, Scala & Java Thank you & Regards V S Durga Prasad | I T Recruiter E: vprasad@conchtech.com | T: 901-466-4708 | 615-922-1491 Conch Technologies | www.conchtech.com",
        "url": "https://www.linkedin.com/jobs/view/3932307189"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Bellevue, WA",
        "job_id": 3955155862,
        "company": "IT Minds LLC",
        "title": "Data Engineer",
        "created_on": 1720639148.7379997,
        "description": "Data Engineer @ Bellevue, WA Qualifications And Skills 3-5 years of experience in large-scale software development (preferably Agile) with emphasis on data modeling and database development 3-5 years of experience with data modeling tools (Erwin, ER/Studio, PowerDesigner) 3-5 years of experience with relational DBMSs and SQL coding (SQL Server, Oracle, Teradata, Snowflake) Ability to communicate effectively (both orally and in writing) with business users, project team leaders and application developers Experience participating in Agile/Scrum projects in a highly collaborative, multi-discipline team environment Proficiency with ETL tools and techniques (SSIS, Attunity, Informatica) 2+ years of experience with AWS and related services (EC2, S3, DynamoDB, ElasticSearch, SQS, SNS, Lambda, Airflow, Snowflake, etc.) Experience with object function/object-oriented scripting (Python, Java, C++, Scala) Experience in R Programming Thanks & Regards Krishna | IT Minds LLC | Phone: (949)534-3939 Ext 406 Direct: 949-200-7533 | Email: krishna@itminds.net | : 9070 Irvine Centre DR, Suite 220 | Irvine, CA 92618 | 44075 Pipeline Plaza, Suite 305 | Ashburn, VA 20147| 102, Manjeera Trinity Corporate, Kukatpally, Hyderabad 500072| www.itminds.net",
        "url": "https://www.linkedin.com/jobs/view/3955155862"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Florida, United States",
        "job_id": 3970538526,
        "company": "The Hill",
        "title": "Data Engineer II",
        "created_on": 1720639150.5159087,
        "description": "At Blue Origin, we envision millions of people living and working in space for the benefit of Earth. We’re working to develop reusable, safe, and low-cost space vehicles and systems within a culture of safety, collaboration, and inclusion. Join our diverse team of problem solvers as we add new chapters to the history of spaceflight! We are a diverse team of collaborators, doers, and problem-solvers who are relentlessly committed to a culture of safety. This position will directly impact the history of space exploration! It will require your dedication and detailed attention towards safe and repeatable space flight. Join us in lowering the cost of access to space and enabling Blue Origin’s vision of millions of people living and working in space to benefit Earth. As part of a passionate and accomplished team of experts, you will play a pivotal role in ensuring the smooth operation of our data infrastructure and systems. You will be responsible for managing and optimizing data workflows, solve complex issues and driving continuous improvements to enhance system reliability and performance. This role offers an exciting opportunity to work with groundbreaking technologies and collaborate with talented individuals in a fast-paced environment. Responsibilities Include But Are Not Limited To Monitor, maintain and optimize data pipelines, databases and related infrastructure to ensure high availability, reliability and performance. Troubleshoot and resolve issues related to data ingestion, processing and storage in a timeline manner. Conduct root cause analysis for incidents and implement correction actions to prevent and mitigate system failures and performance bottlenecks. Be part of the on-call rotation and occasionally work weekends. Collaborate with multi-functional teams to design and implement scalable and automated data solutions around pipelines, databases and storage systems. Minimum Qualifications Bachelor's degree in Computer Science, Information Technology or related field; or equivalent work experience. Minimum of 3 years in data operations, development operation, or related role. Hands-on experience with cloud platforms (e.g., AWS) and containerization technologies(eg., Docker, Kubernetes). Solid Knowledge of data warehouse concepts, ETL processes, and data integration tools. Knowledge of DevOps practices and tools for infrastructure as code, continuous integration and deployment automation. Experience implementing CI/CD pipelines and testing automation. Must be a U.S citizen or national, U.S permanent resident (current Green Card holder), or lawfully admitted into the U.S as a refugee or granted asylum Inclusivity Statement Don’t meet all desired requirements? Studies have shown that some people are less likely to apply to jobs unless they meet every single desired qualification. At Blue Origin, we are dedicated to building a diverse, inclusive, and authentic workplace, so if you’re excited about this role but your past experience doesn’t align perfectly with every desired qualification in the job description, we encourage you to apply anyway. You may be just the right candidate for this or other roles. Export Control Regulations Applicants for employment at Blue Origin must be a U.S. citizen or national, U.S. permanent resident (i.e. current Green Card holder), or lawfully admitted into the U.S. as a refugee or granted asylum. Benefits Benefits include: Medical, dental, vision, basic and supplemental life insurance, paid parental leave, short and long-term disability, 401(k) with a company match of up to 5%, and an Education Support Program. Paid Time Off: Up to four (4) weeks per year based on weekly scheduled hours, and up to 14 company-paid holidays. Discretionary bonus: Bonuses are designed to reward individual contributions as well as allow employees to share in company results. Eligibility for benefits varies by role type, please check with your recruiter for a comprehensive list of the benefits available for this role. Equal Employment Opportunity Blue Origin is proud to be an Equal Opportunity/Affirmative Action Employer and is committed to attracting, retaining, and developing a highly qualified, diverse, and dedicated work force. Blue Origin hires and promotes people on the basis of their qualifications, performance, and abilities. We support the establishment and maintenance of a workplace that fosters trust, equality, and teamwork, in which all employees recognize and appreciate the diversity of individual team members. We provide all qualified applicants for employment and employees with equal opportunities for hire, promotion, and other terms and conditions of employment, regardless of their race, color, religion, gender, sexual orientation, gender identity, national origin/ethnicity, age, physical or mental disability, genetic factors, military/veteran status, or any other status or characteristic protected by federal, state, and/or local law. Blue Origin will consider for employment qualified applicants with criminal histories in a manner consistent with applicable federal, state, and local laws, including the Washington Fair Chance Act, the California Fair Chance Act, the Los Angeles Fair Chance in Hiring Ordinance, and other applicable laws. Affirmative Action and Disability Accommodation Applicants wishing to receive information on Blue Origin’s Affirmative Action Plans, or applicants requiring a reasonable accommodation in order to participate in the application and/or interview process, please contact us at EEOCompliance@blueorigin.com. California Applicant Privacy Notice If you are a California resident, please reference the CA Applicant Privacy Notice here. Apply",
        "url": "https://www.linkedin.com/jobs/view/3970538526"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "New Jersey, United States",
        "job_id": 3961439513,
        "company": "LHH",
        "title": "Data Engineer",
        "created_on": 1720639152.2463677,
        "description": "A top-tier computer hardware manufacturing company is looking for a Senior Data Engineer to primarily develop solutions for enterprise data warehousing, reporting, and business intelligence projects. This role involves collaborating with business users, functional teams, and development teams to gather, analyze, and document requirements. Responsibilities include designing, coding, testing, and scheduling ETL programs to support the enterprise and reporting data warehouse. The ideal candidate is well-versed in various data collection and visualization products and solutions and has hands-on experience with end-to-end development. Responsibilities: Understand technical requirements and create data models that are user-friendly and efficient for ETL processes. Design, develop, and organize ETL workflows from diverse data sources and create dashboards for scalable and refreshable analysis and reporting. Assist with impact analysis of changes in upstream processes on Data Warehouse and Reporting systems. Support analysts by providing data queries and developing dashboards for business decision-making and performance monitoring. Write technical documentation, including design specifications, data model diagrams, data flow diagrams, Configuration Management, and Production Support documents. Adhere to development standards, time tracking, policies, and change management processes. Monitor ETL processes in production and troubleshoot any issues that arise. Qualifications: A Bachelor’s degree in a related field is required, or an advanced degree with equivalent work experience. Over 2 years of relevant experience in software application development, information technology, or a related field. Minimum of 2 years of experience as a data warehouse or ETL developer. Understanding of database management systems, online analytical processing (OLAP), and ETL (Extract, Transform, Load) framework. Proficient in MS SQL queries and SSIS. Working knowledge of Microsoft Power BI and Tableau. Familiarity with at least one ERP system such as SAP, Syspro, Oracle, etc., is a plus but not required. Must be able to translate requirements into technical implementation. Strong verbal communication skills and team collaboration abilities are required. Analytical mindset with problem-solving aptitude. Ability to work independently and collaboratively on simultaneous tasks in a fast-paced environment, with an emphasis on accuracy and timeliness.",
        "url": "https://www.linkedin.com/jobs/view/3961439513"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Woonsocket, RI",
        "job_id": 3941878932,
        "company": "CVS Health",
        "title": "Data Engineer",
        "created_on": 1720639153.9866686,
        "description": "Bring your heart to CVS Health. Every one of us at CVS Health shares a single, clear purpose: Bringing our heart to every moment of your health. This purpose guides our commitment to deliver enhanced human-centric health care for a rapidly changing world. Anchored in our brand — with heart at its center — our purpose sends a personal message that how we deliver our services is just as important as what we deliver. Our Heart At Work Behaviors™ support this purpose. We want everyone who works at CVS Health to feel empowered by the role they play in transforming our culture and accelerating our ability to innovate and deliver solutions to make health care more personal, convenient and affordable. Position Summary Caremark LLC, a CVS Health company, is hiring for the following role in Woonsocket, RI: Data Engineer to design, build and manage large scale data structures, pipelines and efficient Extract/Load/Transform (ETL) workflows to support business applications. Duties include: develop large-scale data structures and pipelines to organize, collect and standardize data to generate insights and addresses reporting needs; write ETL (Extract/Transform/Load) processes, design database systems, and develop tools for real-time and offline analytic processing; collaborate with Data Science team to transform data and integrate algorithms and models into automated processes; leverage knowledge of Hadoop architecture, HDFS commands, and designing and optimizing queries to build data pipelines; utilize programming skills in Python, Java, or similar languages to build robust data pipelines and dynamic systems; build data marts and data models to support Data Science and other internal customers; integrate data from a variety of sources and ensure adherence to data quality and accessibility standards; analyze current information technology environments to identify and assess critical capabilities and recommend solutions; and experiment with available tools and advise on new tools to provide optimal solutions that meet the requirements dictated by the model/use case. Telecommuting available. Multiple positions. - Requirements: Master’s degree (or foreign equivalent) in Computer Science, Data Science, Statistics, Mathematics, Analytics, Information Systems, Information Technology Management, or a related field, and two (2) years of experience in a related position. Requires two (2) years of experience in each of the following: Hadoop; Hive; Agile methodologies; SAFe Software development principles; HDFS commands; unit testing, quality assurance, and troubleshooting; writing clean, high-quality, high-performing, scalable code; analyzing large data sets from multiple data sources for retail and/or healthcare industries; data analytics, data engineering, and data visualization. Telecommuting available. Pay Range: $109,283.00/year to $150,000.00/year. This pay range represents the base hourly rate or base annual full-time salary for all positions in the job grade within which this position falls. The actual base salary offer will depend on a variety of factors including experience, education, geography and other relevant factors. In addition to your compensation, enjoy the rewards of an organization that puts our heart into caring for our colleagues and our communities. The Company offers a full range of medical, dental, and vision benefits. Eligible employees may enroll in the Company’s 401(k) retirement savings plan, and an Employee Stock Purchase Plan is also available for eligible employees. The Company provides a fully-paid term life insurance plan to eligible employees, and short-term and long term disability benefits. CVS Health also offers numerous well-being programs, education assistance, free development courses, a CVS store discount, and discount programs with participating partners. As for time off, Company employees enjoy Paid Time Off (“PTO”) or vacation pay, as well as paid holidays throughout the calendar year. Number of paid holidays, sick time and other time off are provided consistent with relevant state law and Company policies. For more detailed information on available benefits, please visit jobs.CVSHealth.com/benefits This job does not have an application deadline, as CVS Health accepts applications on an ongoing basis.",
        "url": "https://www.linkedin.com/jobs/view/3941878932"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Los Angeles, CA",
        "job_id": 3964676375,
        "company": "Blueprint Test Prep",
        "title": "Data Engineer II",
        "created_on": 1720639155.7732177,
        "description": "Blueprint Test Preparation is seeking a highly perceptive data engineer to develop and maintain Blueprint's data warehouse and the analytics initiatives that help drive Blueprint's success. What You'll Do Design innovative solutions that push the boundaries of the education technology space Understand that quality data is the differentiator for our learners Generate models and visualizations that explain stories and allow for data-driven solutions Understand the KPIs that move the needle on the business side and that quality insights can be a huge differentiator for our learners Resolve complex problems, break down complex data and propose creative solutions Be a beacon of trust for everyone at Blueprint and provide analytical and logical solutions to problems Who You Are Experience in identifying, designing and implementing infrastructure for greater scalability, optimizing data delivery, and automating manual processes Experience in backend databases and surrounding technologies such as Redshift, DynamoDB, Glue and S3 Experience in building BI models and visualizations such as Looker, Tableau or Power BI Ability to create visualizations of complex data, experience with Looker preferred Knowledge of modeling including proficiency in acquiring, organizing, and analyzing large amounts of data Strong attention to detail and data accuracy, and the ability to think holistically Some experience with the analysis of AI algorithms Life at Blueprint We're Blueprint! We live at the intersection of education and technology. We use technology and powerful data to create personalized and innovative learning experiences. Our team is passionate about education and its ability to improve lives. Our learners hold us to a high standard, and we do the same with each other. We thrive on change, we are passionate about improvement, we practice trust-based vulnerability, and we are committed to a culture of freedom and responsibility. We get a lot done, and we have a lot of fun doing it. Meaning. Do meaningful work that has a positive impact on society. Influence. Good ideas win, and we value contributions from everyone. Innovation. It's central to who we are and everything we do. Growth. We are always learning and you will sharpen your skills. Performance. We set aspirational goals and make them happen. Low ego. High energy! That's our recipe for success. Autonomy. We thrive with freedom and responsibility. Flexibility. We trust our people to do phenomenal work without unnecessary rules. Remote first. And not going back. Benefits include: Competitive salary at a growing company Remote-first work environment allowing for flexibility Unlimited PTO, comprehensive health benefits and 401(k) match Additional Wellness Days to support a balanced schedule Monthly remote-work and annual home office equipment stipends Flexible Spending Account Parental leave benefits Volunteer opportunities Continual learning and mentoring opportunities Free access to test prep programs for team members, and family discounts Diversity, Equity, Accessibility, and Inclusion at Blueprint We believe diversity, equity, accessibility and inclusion are essential to our excellence and are the key to innovation. We hire based solely on your strengths and qualifications, and the way in which those strengths can directly contribute to your success in your new position. The Blueprint community values differences in the pursuit of inquiry and knowledge, mutual understanding, respect, trust, transparency, and partnership. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. In our innovative and inclusive workplace, we prohibit discrimination and harassment of any kind. We are committed to creating a welcoming workplace that reflects the diversity of the communities we serve and includes individuals with a diverse set of backgrounds and experiences. Individuals of color, women, LGBTQIA+, veterans and persons with disabilities are encouraged to apply. We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, perform crucial job functions, and receive other benefits and privileges of employment. Please contact us to request accommodation. Blueprint participates in E-Verify for U.S. Employees. California residents, please review Blueprint's privacy policy here . Application Deadline: Friday, July 12th *updated* Final compensation is based on candidate experience and relevant certifications/qualifications. US - Remote, Salary $100,000—$120,000 USD",
        "url": "https://www.linkedin.com/jobs/view/3964676375"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Warren, NJ",
        "job_id": 3961549731,
        "company": "Accord Technologies Inc",
        "title": "Urgent: AWS Data engineer",
        "created_on": 1720639159.3838444,
        "description": "Title: AWS Data Engineer Location: Warren, NJ Duration: 6 months Position type: W2 contract. Required Skills & Experience AWS (the dataset from the Cloud Engineer and Snowflake data lake), Snowflake, SQL, Python and PowerBI Responsibilities They will be designing, developing, and deploying a portfolio of Data & Analytics technology assets and platforms leveraging cloud-based tools and capabilities to capture, explore, transform, and deliver data. Additionally, they will analyze, document and develop solutions based on business needs and opportunities to deliver the intended outcomes in a timely manner. Key skills in AWS (the dataset from the Cloud Engineer and Snowflake data lake), Snowflake, SQL, Python and PowerBI willenable you to excel in this role. Put simply - We need a hands on Data Engineer that is really good with PowerBI and have experience working with AWS Data & Analytics Capabilities. They will like working with our business teams to rapidly build new datasets and dashboards.",
        "url": "https://www.linkedin.com/jobs/view/3961549731"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3961739801,
        "company": "Vibrant Emotional Health",
        "title": "Data Engineer II",
        "created_on": 1720639161.1336873,
        "description": "Position Title: Data Engineer II Salary Range: $92,000-$105,800 Department: Information Technology Reports to: Data Engineering Manager Location: Remote Schedule: (Overnight and Weekend) Formerly the Mental Health Association of New York City (MHA-NYC), Vibrant Emotional Health’s groundbreaking solutions have delivered high quality services and support, when, where and how people need it for over 50 years. Through our state-of-the-art technology-enabled services, community wellness programs, and advocacy and education work, we are building a society in which emotional wellness can be a reality for everyone. Position Summary: The Data Engineer II is responsible for the development of data pipelines, the orchestration and planning of data transformations and the development and support of data automations. This role interfaces with the Analytics and Research team around the design of data points, measures, and the implementation of models; with Data Governance roles around the alignment of policy and definitions with their implementation; and with Dev Ops, Security and Software Development teams around the broader organization’s use of data infrastructure. This role focuses on providing weekend and overnight support to key stakeholders. Duties/Responsibilities: Develop patterns for data ingestion using Fivetran, AWS Lambda, and related technologies. Orchestrate data transformations using DBT, database functions, materialized views and similar patterns. Write tests, integrity checks, conduct performance monitoring and tuning of data systems. Write anomaly detection routines to support alerting and monitoring. Design and develop secure, high performance, API’s to support data requests. Establish patterns for automation of forecasts, scoring, and support of ML/AI implementations. Establish patterns for publishing and distributing data sets. Establish and maintain archiving and retention schedules. Ensure data systems conform with data governance and data security best practices. Support automated reporting functions. Deploy and support reporting platform. Required Skills/Abilities: Extensive experience building and supporting data infrastructure. Direct experience working with Snowflake, DBT and Fivetran. Strong experience with AWS services such as Data Migration Services, RDS, Lambda, API Gateway, S3, etc. Expert knowledge of SQL / PSQL. Ability to code in Python and R (additional languages a +) Strong track record of managing high availability systems. Initiative to solve complex problems; takes an outside in perspective to identify innovative solutions. We value candidates who have demonstrated commitment to the goal of working with people to achieve mental and emotional wellbeing with dignity and respect. Required Qualifications: Bachelors’ or Masters’ Degree in an analytics or engineering focused discipline or equivalent experience and knowledge. 1+ years of data engineering experience developing and maintaining high availability systems. Excellent comprehensive benefits, including medical, dental, vision, supplemental income insurance, pre-tax transit/parking, pre-tax FSA for medical and dependent care, and 401K available. 4 weeks’ vacation, plum benefits, etc. Studies have shown that women and people of color are less likely to apply for jobs unless they believe they are able to perform every task in the job description. We are most interested in finding the best candidate for the job, and that candidate may be one who come from a less traditional background. Vibrant will consider any equivalent combination of knowledge, skills, education and experience to meet minimum qualifications. If you are interested in applying, we encourage you to think broadly about your background and skill set for the role. Vibrant Emotional Health is an equal opportunity employer. Applicants are considered for positions without regard to veteran status, uniformed service member status, race, creed, color, religion, gender, gender identity, sex, sexual orientation, citizenship status, national origin, marital status, age, physical or mental disability, genetic information, caregiver status or any other category protected by applicable federal, state or local laws.",
        "url": "https://www.linkedin.com/jobs/view/3961739801"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "St Louis, MO",
        "job_id": 3960270596,
        "company": "Cynet Systems",
        "title": "Data Engineer",
        "created_on": 1720639162.9391162,
        "description": "Job Description: Responsibilities: Write secure, production-quality Python code. Implement cloud native solutions in AWS designed by our solutions and data architects for end-to-end data processing pipelines and data operations tools. Perform code review on peers’ code. Help document processes. Deploy code to production servers using GitLab CI/CD pipelines. Participate in agile development team. Qualifications: Experience developing and maintaining data processing pipelines, including extraction, transformation, and loading of data (ETL processes). Experience with open-source programming languages, including. Python. SQL. PHP. Combination of the following. Experience with Git or other source control for code management. Experience with Amazon Web Services (AWS) such as Lambdas, Step Functions, ECS, SNS, SQS, and EventBridge. Experience writing production-ready Terraform code. Experience with Docker or containerized application development. Experience with Agile methodologies. Can meet commitments and deliver quality work in a fast paced environment.",
        "url": "https://www.linkedin.com/jobs/view/3960270596"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Irving, TX",
        "job_id": 3923156641,
        "company": "OscarMike",
        "title": "Sr Data Engineer- Big Data/Python/Databricks",
        "created_on": 1720639164.649306,
        "description": "HYBRID* Sr Data Engineer- Big Data/Python/Databricks Direct Hire 1099 or W2 only. The client is not offering subcontracting for this position. This role is onsite in Irving, TX. Hybrid schedule, 2 days a week required onsite. [10824]-HOLD We are hiring highly enthusiastic Python/Databricks engineers to create, tune, maintain and build reporting for the Big Data platforms that drive their retail business. This role is key to [the Company's] continued market dominance, and requires people that are passionate about data and seeking perfection in their development. Requirements: KEY DUTIES AND RESPONSIBILITIES: Performing systems integration design and development in cloud architecture design (Azure) Building sizing and cost models for Cloud Services (Azure) Review enhancement requests and recommend best solution to stakeholders as SME. Containerizing ingestion processes using Docker and Kubernetes services. Understand business requirements to design , build, develop and test data integration pipelines. Plan, direct DevOps implementation and system hardware/software updates for better performance, stability. Work on the BI dashboards to support retail stores and the Azure AI platform that runs the personalized consumer experiences used by millions each day. Education And Experience ﻿ EDUCATION: Bachelors/4 Yr Degree YEARS OF RELEVANT WORK EXPERIENCE: 7+Years Specific Knowledge And Skills 5+ years or demonstrated mastery of development with programming languages - Python, SQL, Unix. 2+ years experience with Apache Spark and Databricks cloud tool sets (AWS or Azure). 1+ years experience with GCP/Azure Cloud Big Data tools (PowerBI/DataLake/ Azure Databricks). Self-motivated with excellent analytical, problem solving, verbal and communication skills. Proficiency in API security frameworks, token management and UAC including OAuth, JWT, etc. Basic knowledge of using Devops, Github, JIRA tools and ability to work in agile environment. Bachelors degree - Computer Science or equivalent. Open Source Development experience is a bonus",
        "url": "https://www.linkedin.com/jobs/view/3923156641"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3939444116,
        "company": "Marathon TS",
        "title": "Data Engineer II",
        "created_on": 1720639168.3813126,
        "description": "Responsibilities Data Engineer II is responsible for supporting our enterprise data warehouse for our Supply Chain Operations. As a Data Engineer II, you will be part of a dynamic team with engineers of all experience levels who help each other build and grow technical and leadership skills while creating, deploying, and supporting production applications. In addition, Data Engineer IIs may be involved in configuration, security, resilience, performance tuning and production monitoring.Key Responsibilities: 60% Delivery and Execution - Collaborates and pairs with other product team members (UX, engineering, and product management) to create secure, reliable, scalable software solutions; Documents, reviews and ensures that all quality and change control standards are met; Works with Product Team to ensure user stories that are developer-ready, easy to understand, and testable; Writes custom code or scripts to automate infrastructure, monitoring services, and test cases; Writes custom code or scripts to do destructive testing to ensure adequate resiliency in production; Program configuration/modification and setup activities on large projects using HD approved methodology; Configures commercial off the shelf solutions to align with evolving business needs Creates meaningful dashboards, logging, alerting, and responses to ensure that issues are captured and addressed proactively 20% Learning - Actively seeks ways to grow and be challenged using both formal and informal development channels; Learns through successful and failed experiment when tackling new problems 20% Plans and Aligns - Collaborates with other team members in agile processes; Assists in creating new and better ways for the team to be successful; Relates openly and comfortably with diverse groups of people; Builds partnerships and works collaboratively with others to meet shared objectives Marathon TS is committed to the development of a creative, diverse and inclusive work environment. In order to provide equal employment and advancement opportunities to all individuals, employment decisions at Marathon TS will be based on merit, qualifications, and abilities. Marathon TS does not discriminate against any person because of race, color, creed, religion, sex, national origin, disability, age or any other characteristic protected by law (referred to as \"protected status\").",
        "url": "https://www.linkedin.com/jobs/view/3939444116"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Plano, TX",
        "job_id": 3960203667,
        "company": "Trident Consulting",
        "title": "Data Engineer",
        "created_on": 1720639170.15165,
        "description": "Trident Consulting is seeking a \"Data Engineer\" for one of our client in Plano TX (Hybrid) . A global leader in business and technology services. Job Title: Data Engineer Location: Hybrid in Plano TX Job Type: Contract Required Skills : Unix,Python,PySpark,SQL,AWS Roles & Responsibilities Analyze business processes to identify areas for improvement and optimization. Develop and implement data-driven solutions using Python, PySpark, and SQL. Collaborate with cross-functional teams to gather and document business requirements. Utilize AWS services to manage and analyze large datasets effectively. Create detailed reports and dashboards to present findings and recommendations to stakeholders Conduct data analysis to support decision-making and strategic planning. Oversee the integration of new technologies and tools to streamline business operations. Provide technical support and guidance to team members on Python, PySpark, AWS, SQL, and Unix. Ensure data integrity and security by following best practices and company policies. Monitor and evaluate the performance of implemented solutions to ensure they meet business objectives. Stay updated with industry trends and emerging technologies to continuously improve business processes. Facilitate training sessions and workshops to educate team members on new tools and methodologies. Work closely with IT and development teams to ensure seamless implementation of solutions. About Trident Trident Consulting is an award-winning IT/engineering staffing company founded in 2005 and headquartered in San Ramon, CA. We specialize in placing high-quality vetted technology and engineering professionals in contract and full-time roles. Trident's commitment is to deliver the best and brightest individuals in the industry for our clients' toughest requirements. Some Of Our Recent Awards Include 2022, 2021, 2020 Inc. 5000 fastest-growing private companies in America 2022, 2021 SF Business Times 100 fastest-growing private companies in Bay Area",
        "url": "https://www.linkedin.com/jobs/view/3960203667"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Irving, TX",
        "job_id": 3963361291,
        "company": "GM Financial",
        "title": "Data Engineer",
        "created_on": 1720639172.0115647,
        "description": "Overview Why GMF Technology? GM Financial is set to change the auto finance industry and is leading the path of embarking on tech modernization - we have a startup mindset, and preserve our small company culture, in a public company environment with financial stability and intense growth over a decade-plus history. We are data junkies and trust in data and insights to advance our business objectives. We take our goal of zero emission, zero collision, zero congestion, and zero friction very seriously. We believe as an auto finance market leader we are in the driver's seat to lead us in the GM EV mission to change the world. We are building global platforms, in LATAM, Europe, China, U.S. and Canada - and we are looking to grow our high-performing team. GMF is comprised of over 10,000 team members globally. Join our fintech culture within a Blue-Chip company where we are changing the way we use technology to support our customers, dealers and business. Responsibilities About the role: We are expanding our efforts into complementary data technologies for decision support in areas of ingesting and processing large data sets including data commonly referred to as semi-structured or unstructured data. Our interests are in enabling data science and search-based applications on large and low latent data sets in both a batch and streaming context for processing. To that end, this role will engage with team counterparts in exploring and deploying technologies for creating data sets using a combination of batch and streaming transformation processes. These data sets support both off-line and in-line machine learning training and model execution. Other data sets support search engine-based analytics. Exploration and deployment of technologies activities include identifying opportunities that impact business strategy, collaborating on the selection of data solutions software, and contributing to the identification of hardware requirements based on business requirements. Responsibility also includes coding, testing, and documentation of new or modified scalable analytic data systems including automation for deployment and monitoring. This role participates along with team counterparts to develop solutions in an end-to-end framework on a group of core data technologies. Job Duties Work with internal business partners to identify, collect, and format data from external sources, internal systems and the data warehouse and lakehouse to extract features of interest Contribute to evaluation, research, and experimentation efforts with batch and streaming data engineering technologies in a lab to keep pace with industry innovation Work with data engineering related groups to inform on and showcase capabilities of emerging technologies and to enable the adoption of these new technologies and associated techniques Coordinate with Privacy Compliance to ensure proper data collection and handling Create and implement business rules and functional enhancements for data schemas and processes Coordinate with Privacy Compliance to ensure proper data collection and handling Perform data load monitoring and resolution Work with internal business clients to problem solve data availability and activation issues Qualifications What makes you a dream candidate? Experience with processing large data sets using Hadoop, HDFS, Spark, Kafka, Flume or similar distributed systems Experience with ingesting various source data formats such as JSON, Parquet, SequenceFile, Cloud Databases, MQ, Relational Databases such as Oracle Experience with Cloud technologies (such as Azure, AWS, GCP) and native toolsets such as Azure ARM Templates, Hashicorp Terraform, AWS Cloud Formation Understanding of cloud computing technologies, business drivers and emerging computing trends Thorough understanding of Hybrid Cloud Computing: virtualization technologies, Infrastructure as a Service, Platform as a Service and Software as a Service Cloud delivery models and the current competitive landscape Working knowledge of Object Storage technologies to include but not limited to Data Lake Storage Gen2, S3, Minio, Ceph, ADLS etc Experience with containerization to include but not limited to Dockers, Kubernetes, Spark on Kubernetes, Spark Operator Working knowledge of Agile development /SAFe, Scrum and Application Lifecycle Management Strong background with source control management systems (GIT or Subversion); Build Systems (Maven, Gradle, Webpack); Code Quality (Sonar); Artifact Repository Managers (Artifactory), Continuous Integration/ Continuous Deployment (Azure DevOps) Experience with NoSQL data stores such as CosmosDB, MongoDB, Cassandra, Redis, Riak or other technologies that embed NoSQL with search such as MarkLogic or Lily Enterprise Creating and maintaining ETL processes Knowledgeable of best practices in information technology governance and privacy compliance Experience with REST APIs Working knowledge of Databricks platform and associated features including workflows, unity catalog, delta live tables, time travel, SQL statement execution API, etc Understanding of Databricks medallion architecture Proficient with programming concepts and languages including SQL and Python/PySpark Additional Skills Troubleshoot complex problems and works across teams to meet commitments Excellent computer skills and proficiency in digital data collection Ability to work in an Agile/Scrum team environment Strong interpersonal, verbal, and writing skills Digital technology solutions (DMPs, CDPs, Tag Management Platforms, Cross-Device Tracking, SDKs, etc) Knowledge of Real Time-CDP and Journey Analytics solutions Understanding of big data platforms and architectures, data stream processing pipeline/platform, data lake and data lake houses SQL experience: querying data and sharing what insights can be derived Understanding of cloud solutions such as Google Cloud Platform, Microsoft Azure & Amazon AWS cloud architecture & services Understanding of GDPR, privacy & security topics Experience And Education 2-4 years of hands-on experience with data engineering required 2-4 years of hands-on experience with processing large data sets required 2-4 years of hands-on experience with SQL, data modeling, relational databases and/or no SQL databases required Bachelor’s Degree in related field or equivalent work experience required What We Offer: Generous benefits package available on day one to include: 401K matching, bonding leave for new parents (12 weeks, 100% paid), tuition assistance, training, GM employee auto discount, community service pay and nine company holidays. Our Culture: Our team members define and shape our culture — an environment that welcomes innovative ideas, fosters integrity, and creates a sense of community and belonging. Here we do more than work — we thrive. Compensation: Competitive pay and bonus eligibility Work Life Balance: Flexible hybrid work environment, 2-days a week in office in Irving, TX Salary The base salary range for this role is: USD $88,000.00 to $162,900.00. At GM Financial, we strive for transparency in all aspects of our business, including pay equity. This is the GM Financial pay range for this role and job level. The exact salary and compensation will vary based on factors like knowledge, skills, experience, and education. This role is eligible to participate in a performance-based incentive plan. Full time employees are eligible to participate in health benefits on day one of employment.",
        "url": "https://www.linkedin.com/jobs/view/3963361291"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Baltimore, MD",
        "job_id": 3941361199,
        "company": "Lumen Solutions Group Inc.",
        "title": "Data Intelligence - Engineer, Data Ld",
        "created_on": 1720639173.8229723,
        "description": "About us: Lumen Solutions Group Inc., a dynamic small and minority-owned, Disadvantaged Business Enterprise headquartered in Florida, USA. As a leading consulting services and solutions provider, we focus on IT Staffing, Business/IT Strategy, Business Process Blueprints, Enterprise Architecture, Enterprise Transformation for our clients. Our client base includes Fortune 500, Government, non-profit and emerging growth companies. Job Description: PURPOSE: The Lead Data Engineer is responsible for orchestrating, deploying, maintaining and scaling cloud OR on-premise infrastructure targeting big data and platform data management (Relational and NoSQL, distributed and converged) with emphasis on reliability, automation and performance. This role will focus on leading the development of solutions and helping transform the company's platforms deliver data-driven, meaningful insights and value to company. ESSENTIAL FUNCTIONS: 20% Lead the team to design, configure, implement, monitor, and manage all aspects of Data Integration Framework. Defines and develop the Data Integration best practices for the data management environment of optimal performance and reliability. 20% Develops and maintains infrastructure systems (e.g., data warehouses, data lakes) including data access APIs. Prepares and manipulates data using Hadoop or equivalent MapReduce platform. 15% Provides detailed guidance and performs work related to Modeling Data Warehouse solutions in the cloud OR on-premise. Understands Dimensional Modeling, De-normalized Data Structures, OLAP, and Data Warehousing concepts. 15% Oversees the delivery of engineering data initiatives and projects. Supports long term data initiatives as well as Ad-Hoc analysis and ELT/ETL activities. Creates data collection frameworks for structured and unstructured data. Applies data extraction, transformation and loading techniques in order to connect large data sets from a variety of sources. 15% Enforces the implementation of best practices for data auditing, scalability, reliability and application performance. Develop and apply data extraction, transformation and loading techniques in order to connect large data sets from a variety of sources. 10% Interprets data, analyzes results using statistical techniques, and provides ongoing reports. Executes quantitative analyses that translate data into actionable insights. Provides analytical and data-driven decision-making support for key projects. Designs, manages, and conducts quality control procedures for data sets using data from multiple systems. 5% Improves data delivery engineering job knowledge by attending educational workshops; reviewing professional publications; establishing personal networks; benchmarking state-of-the-art practices; participating in professional societies. Qualifications To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements listed below are representative of the knowledge, skill, and/or ability required. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. Education Level: Bachelor's Degree Education Details: Computer Science, Information Technology or Engineering or related field Experience: 8 years Experience in leading data engineering and cross functional team to implement scalable and fine tuned ETL/ELT solutions for optimal performance. Experience developing and updating ETL/ELT scripts. Hands-on experience with application development, relational database layout, development, data modeling. In Lieu of Education In lieu of a Bachelor's degree, an additional 4 years of relevant work experience is required in addition to the required work experience. Preferred Qualifications Knowledge, Skills and Abilities (KSAs) ETL Design and Development experience using AbInitio ., Expert Data Integration project experience on Hadoop Platform, preferably Cloudera., at least one project some AWS cloud experience and exposure to MongoDB is a big plus Knowledge and understanding of at least one programming language (i.e., SQL, NoSQL, Python)., Expert Knowledge and understanding of database design and implementation concepts. , Expert Knowledge and understanding of data exchange formats., Expert Knowledge and understanding of data movement concepts, Expert Strong technical and analytical and problem solving skills to troubleshoot to solve a variety of problems., Expert Requires strong organizational and communication skills, written and verbal, with the ability to handle multiple priorities., Expert Able to effectively provide direction to and lead technical teams., Expert The incumbent is required to immediately disclose any debarment, exclusion, or other event that makes them ineligible to perform work directly or indirectly on Federal health care programs. Must be able to effectively work in a fast-paced environment with frequently changing priorities, deadlines, and workloads that can be variable for long periods of time. Must be able to meet established deadlines and handle multiple customer service demands from internal and external customers, within set expectations for service excellence. Must be able to effectively communicate and provide positive customer service to every internal and external customer, including customers who may be demanding or otherwise challenging. ***Lumen and / or its clients will not provide equipment (Laptop, monitor, etc.) to the selected contractor. The contractor must have their own equipment. Access to a virtual desktop set up (software) will be provided by Lumen’s client, allowing the user access to the required systems and technology. *** Lumen Solutions Group Inc. is an Equal Opportunity Employer and does not discriminate in employment on the basis of Minority/Gender/Disability/Religion/LGBTQI/Age/Veterans.",
        "url": "https://www.linkedin.com/jobs/view/3941361199"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Bloomfield, CT",
        "job_id": 3954737630,
        "company": "CGI",
        "title": "Data Engineer (Snowflake and AWS)",
        "created_on": 1720639175.5945458,
        "description": "Position Description CGI is looking for an experienced Data Engineer with solid hands-on experience in Snowflake and AWS. If you are looking for a new challenge and want to make a difference in the Healthcare Industry, this role is for you. This role must be performed from Bloomfield, CT in a hybrid model (3 days per week on client site). Your future duties and responsibilities As a skilled Snowflake expert, you’ll play a crucial role in enhancing data management and performance within a migration project. Proactively design and engineer data solutions, collaborating with onshore and offshore teams Develop Snow Pipe and complex data transformations using Snow SQL. Optimize performance through Snowflake external tables, staging, and scheduler. Understand Snowflake Time travel concepts and zero-copy cloning. Work with AWS, Gitlab, Docker, and Kubernetes. Communicate effectively with business analysts and technical stakeholders. Perform maintenance support in a production environment. Required Qualifications To Be Successful In This Role Strong development hands-on background in creating Snow pipe, and complex data transformations and manipulations using Snow Pipe, Snow SQL Hands-on experience with Snowflake external tables concepts, Staging, Snow scheduler & performance tuning. Good understanding of Snowflake Time travel concepts and zero-copy cloning, Network policies, clustering, and tasks 5+ year experience working in snowflake, SAS, AWS skills Hands-on experience in development, deployment and operation of data technologies and platforms such as: Integration using APIs, micro-services and ETL patterns Low latency/Streaming, batch and micro batch processing Data platforms such as Hadoop, Hive, Redshift or Snowflake Cloud Services such as AWS Cloud query services such as Athena DevOps Platforms such as Gitlab Containerization technologies such as Docker and Kubernetes Orchestration solutions such as Airflow Deep knowledge of key non-functional requirements such as availability, scalability, operability, and maintainability Deep knowledge of SQL OS knowledge particularly Linux Responsible for planning, highlighting, and implementing possible improvements for existing and new applications. Good to have: Relevant professional qualification such as AWS Certifications, SnowPro Core certification, etc. Migration experience to Snowflake Hands on experience with Oracle RDBMS Exposure to Streamsets, DBT or other ETL tool Education: Bachelors in Computer Science, Engineering, or related discipline. #DICE #CSG-TMC-F24 CGI is required by law in some jurisdictions to include a reasonable estimate of the compensation range for this role. The determination of this range includes various factors not limited to: skill set level; experience and training; and licensure and certifications. CGI typically does not hire individuals at or near the top of the range for their role. Compensation decisions are dependent on the facts and circumstances of each case. A reasonable estimate of the current range is $60,300- $165,200. At CGI we call our professionals “members” to reinforce that all who join our team are, as owners, empowered to participate in the challenges and rewards that come from building a world-class company. CGI’s benefits include: Competitive base salaries Eligibility to participate in an attractive Share Purchase Plan (SPP) in which the company matches dollar-for-dollar contributions made by eligible employees, up to a maximum, for their job category 401(k) Plan and Profit Participation for eligible members Generous holidays, vacation, and sick leave plans Comprehensive insurance plans that include, among other benefits, medical, dental, vision, life, disability, out-of-county emergency coverage in all countries of employment; Back-up child care, Pet insurance, a Member Assistance Program, a 529 college savings program, a personal financial management tool, lifestyle management programs and more. Together, as owners, let’s turn meaningful insights into action. Life at CGI is rooted in ownership, teamwork, respect and belonging. Here, you’ll reach your full potential because… You are invited to be an owner from day 1 as we work together to bring our Dream to life. That’s why we call ourselves CGI Partners rather than employees. We benefit from our collective success and actively shape our company’s strategy and direction. Your work creates value. You’ll develop innovative solutions and build relationships with teammates and clients while accessing global capabilities to scale your ideas, embrace new opportunities, and benefit from expansive industry and technology expertise. You’ll shape your career by joining a company built to grow and last. You’ll be supported by leaders who care about your health and well-being and provide you with opportunities to deepen your skills and broaden your horizons. Come join our team—one of the largest IT and business consulting services firms in the world. Qualified applicants will receive consideration for employment without regard to their race, ethnicity, ancestry, color, sex, religion, creed, age, national origin, citizenship status, disability, pregnancy, medical condition, military and veteran status, marital status, sexual orientation or perceived sexual orientation, gender, gender identity, and gender expression, familial status, political affiliation, genetic information, height, weight, or any other legally protected status or characteristics. CGI provides reasonable accommodations to qualified individuals with disabilities. If you need an accommodation to apply for a job in the U.S., please email the CGI U.S. Employment Compliance mailbox at US_Employment_Compliance@cgi.com. You will need to reference the Position ID of the position in which you are interested. Your message will be routed to the appropriate recruiter who will assist you. Please note, this email address is only to be used for those individuals who need an accommodation to apply for a job. Emails for any other reason or those that do not include a Position ID will not be returned. We make it easy to translate military experience and skills! Click here to be directed to our site that is dedicated to veterans and transitioning service members. All CGI offers of employment in the U.S. are contingent upon the ability to successfully complete a background investigation. Background investigation components can vary dependent upon specific assignment and/or level of US government security clearance held. Dependent upon role and/or federal government security clearance requirements, and in accordance with applicable laws, some background investigations may include a credit check. CGI will consider for employment qualified applicants with arrests and conviction records in accordance with all local regulations and ordinances. CGI will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with CGI’s legal duty to furnish information.",
        "url": "https://www.linkedin.com/jobs/view/3954737630"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Houston, TX",
        "job_id": 3959318631,
        "company": "ClearpointCo",
        "title": "Data Engineer",
        "created_on": 1720639177.4805064,
        "description": "n TITLE: Data Engineer u0026 n LOCATION: Hybrid (4 Days Work from Home, 1 day in office) u0026 n TYPE: Direct hire u0026 n SALARY: $130,000 - $150,000 plus bonusu0026 n SUMMARY: n The Data Engineer will work in the IT data team to build clean data sets u0026amp; solutions for the business using technologies available within Microsoft Fabric. In this role, the Python Developer will actively collaborate with business stakeholders, BI analysts, u0026amp; others to design, implement, u0026amp; maintain data systems and solutions. u0026 n DUTIES: n Build, maintain, u0026amp; optimize ETL data pipelines n Build, maintain, u0026amp; optimize corporate semantic models n Address IT tickets u0026amp; troubleshoot system performance n Perform administrative duties to maintain the current data environment n Assist, as needed, with the development of reports and other solutions n Work with stakeholders throughout solution lifecycle n Improve platform stability, data value u0026amp; quality, and promote a data culture n Actively collaborate with teammates to share workloads, develop skills, u0026amp; strengthen the team u0026 n REQUIREMENTS: n 3+ years sustaining an enterprise data architecture, such as a common data model or data warehouse n 3+ years of owning full lifecycle of BI projects/solutions n 3+ years of hands-on enterprise data engineering/ETL experience n 3+ years of hands-on enterprise data modeling experience n 3+ years of hands-on experience with Microsoft Business Intelligence stack n Work within an enterprise data environment- version control, change u0026amp; integration management, etc. n Strong skills in data engineering with a willingness to support on the visualization layer n Strong skills in troubleshooting concepts n Strong skills in Python, SQL, M, DAX n Experience with SQL Server Management Studio, DAX Studio, Tabular Editor, u0026amp; ALM Toolkit a plus u0026 u0026 n EDUCATION: n Bacheloru0026rsquo;s degree in computer science, data science, engineering, or comparable field, or equivalent experience in field n Microsoft Fabric Analytics Engineer Certification preferred n",
        "url": "https://www.linkedin.com/jobs/view/3959318631"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Norfolk, VA",
        "job_id": 3964519893,
        "company": "Life Protect 24/7",
        "title": "Looker / Data Engineer",
        "created_on": 1720639179.1059701,
        "description": "Overview Life Protect 24/7 is one of North America’s leading medical alert companies. We unite a group of over 500 hardworking professionals that are consistently working to find better solutions for our customers and clients in the medical alert and insurance industries. We utilize modern web technologies to keep our company on the cutting edge of these industries and we’re looking for smart, dedicated people to help us continue to grow. Job Description Data Engineer will be responsible for working closely with software engineers, database administrators, and key business personnel to create a large variety of reports using Looker and Google BigQuery (Standard SQL). Benefits and Compensation Competitive starting salary 401K with Company Match Paid Time Off Medical, Dental, Vision, AD&D, and Life Insurance HSA Options Fully stocked kitchen with snacks and coffee On site company Cafe Bistro Convenient access to walking trails and Norfolk Premium Outlets Responsibilities Rapidly deliver on concepts through prototypes that can be presented for feedback· Work closely with engineers and business users to troubleshoot and resolve data quality issues and maintain high level of data accuracy Assist with the selection, implementation, and integration of new data tools Promote a data-driven culture in the company Think about how you can make end users more productive through self-serve capabilities Basic Qualifications Looker experience 2+ years of relevant experience in analytics/data engineering Proven experience in taking the requirements and developing business dashboard Proficient with SQL Ability to solve ambiguous problems independently, write clear, and concise documentation Strong communication skills and ability to gather requirements and translate them to specs and design Preferred Qualifications Proficiency with Git Knowledge of data warehouse principles and methodologies Experience in data science, advanced analytics, machine learning Experience with general-purpose programming (e.g. Python, Java, Go), dealing with a variety of data structures, algorithms, and serialization formats Experience GCP cloud services and data warehouse stores like BigQuery Self-driven, highly motivated and able to learn quickly",
        "url": "https://www.linkedin.com/jobs/view/3964519893"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Houston, TX",
        "job_id": 3967945138,
        "company": "Macquarie Group",
        "title": "Data Engineer",
        "created_on": 1720639180.7331562,
        "description": "The CGM Data Analytics team within COG Technology is building a leading data and analytics platform. We partner and build batch and real-time analytics solutions for traders, quants, desk analysts and central data teams across the Commodities and Global Markets business. At Macquarie, our advantage is bringing together diverse people and empowering them to shape all kinds of possibilities. We are a global financial services group operating in 34 markets and with 55 years of unbroken profitability. You’ll be part of a friendly and supportive team where everyone - no matter what role - contributes ideas and drives outcomes. What role will you play? You will work directly with our business, developing data analytics capabilities and solutions, such as the data catalog, data science workbench, visualisation tools, and real-time analytics. You’ll be working with our public-cloud based data lake, data pipelines for internal operational and external fundamental and alternative market data. You will join us on our journey to continually improve the robustness of our solutions and the speed with which we can respond to our business’s needs. What You Offer Interest in Data and Automation Ability to code and debug in Python Knowledge of GIT and CI/CD Awareness of Cloud eg. AWS We love hearing from anyone inspired to build a better future with us, if you're excited about the role or working at Macquarie we encourage you to apply. About Technology Technology enables every aspect of our business, for our people, our customers and our communities. Bring your unique perspective and join a global team who is passionate about accelerating the digital enterprise, connecting people and data, building platforms and applications and designing tomorrow’s technology solutions. Benefits Macquarie employees can access a wide range of benefits which, depending on eligibility criteria, include: Hybrid and flexible working arrangements One wellbeing leave day per year and minimum 25 days of annual leave Primary caregivers are eligible for 20 weeks paid leave along with 12 days of transition leave upon return to work and 6 weeks paid leave for secondary caregivers Paid volunteer leave and donation matching Range of benefits to support your physical, psychological and financial wellbeing Employee Assistance Program, a robust behavioural health network with counselling and coaching services Recognition and service awards Our commitment to diversity, equity and inclusion We are committed to providing a working environment that embraces diversity, equity and inclusion. As an inclusive employer, Macquarie does not discriminate on the grounds of age, disability, sex, sexual orientation, gender identity or expression, marriage, civil partnership, pregnancy, maternity, race (including color and ethnic or national origins), religion or belief.",
        "url": "https://www.linkedin.com/jobs/view/3967945138"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Indianapolis, IN",
        "job_id": 3955274431,
        "company": "Eli Lilly and Company",
        "title": "Data Engineer",
        "created_on": 1720639182.5129216,
        "description": "At Lilly, we unite caring with discovery to make life better for people around the world. We are a global healthcare leader headquartered in Indianapolis, Indiana. Our employees around the world work to discover and bring life-changing medicines to those who need them, improve the understanding and management of disease, and give back to our communities through philanthropy and volunteerism. We give our best effort to our work, and we put people first. We’re looking for people who are determined to make life better for people around the world. Note: Lilly does not anticipate providing sponsorship for employment visa status (e.g., H-1B or TN status) for this employment position now or in the future. Do you enjoy learning new knowledge domains and answering engaging questions? The Global Services Tech at Lilly team is actively seeking a data engineer in Indianapolis, IN to partner with internal business and Tech at Lilly partners to accelerate the delivery of data solutions for analytics and business purposes. What You’ll Be Doing A data engineer onsite in Indianapolis, IN is responsible for designing, developing, and maintaining the data solutions that ensure the availability and quality of data for analysis and/or business transactions. They design and implement efficient data storage, processing and retrieval solutions for datasets and build data pipelines, optimize database designs, and work closely with data scientists, architects, and analysts to ensure data quality and accessibility. Data engineers require strong skillsets in data integration, acquisition, cleansing, harmonization, and transforming data. They play a crucial role in transforming raw data into datasets designed for analysis which enable organizations to unlock valuable insights for decision making. How You’ll Succeed Engage and partner with cross-functional tech teams across Global Services (finance), third party solution delivery providers, and Data architects to understand the business problem and enhance/develop the appropriate data solution leveraging the modern tech stack Design and implement highly performant data ingestion/processing pipelines from multiple sources Develop technical solutions which combine disparate information to create meaningful insights for business partners Operate with a quality mindset always considering the impact of design decisions on the long-term support and maintenance of data pipelines/jobs Ensure data integrity, security, and privacy requirements are met Stay abreast of tools and technologies to influence our strategy so that it provides best usage opportunities for business What You Should Bring A foundational set of knowledge in: communication, leadership, teamwork, problem solving skills, solution / blueprint definition, business acumen, architectural processes (e.g. blueprinting, reference architecture, governance, etc.), technical standards, project delivery, and industry knowledge. Strong skillsets in data integration, acquisition, cleansing, harmonization, and transforming data Experience designing large scale data models for functional, operational, and analytical environments (Conceptual, Logical, Physical & Dimensional) Experience in several of the following disciplines: statistical methods, data modeling, ontology development, semantic graph construction and linked data, relational schema design. Demonstrated SQL/PLSQL and data modeling proficiency. Experience with data modeling tools such as, ER*Studio and Erwin or TOAD Experience in AWS or Azure techstack Experience in building/integrating APIs Experience in creating data products using APIs Experience with security models and development on large data sets Experience with multiple database technologies Experience with multiple database solutions (e.g. Postgres, Redshift, Aurora, Athena) and formal database designs (3NF, Dimensional Models) Experience with Agile Development, CI/CD, Jenkins, Github, Automation platforms Experience in implementing effective data loading strategy (CDC, incremental loads) Demonstrated ability to analyze large, complex data domains and craft practical solutions for subsequent data exploitation via analytics. Demonstrated ability to communicate with a geographically dispersed group of business and technical colleagues Ability to review and provide practical recommendations on design patterns, performance considerations & optimization, database versions, and database deployment strategies Knowledgeable in data functions such as Data Governance, Master Data Management, Business Intelligence Basic Qualifications Bachelors Degree in computer science, information technology, or management information systems. Or High School Diploma with at least four years of Data Engineer experience Experience in data integration, acquisition, cleansing, harmonization, and transforming data Demonstrated ability to analyze large, complex data domains and craft practical solutions for subsequent data exploitation via analytics. Qualified candidates must be legally authorized to be employed in the United States. Lilly does not anticipate providing sponsorship for employment visa status (e.g., H-1B or TN status) for this employment position Other Considerations Lilly does not anticipate providing sponsorship for employment visa status (e.g., H-1B or TN status) for this employment position. Must be willing to relocate to the Indianapolis area Hybrid work schedule 0-10% travel Additional Preferences CDMP Certification AWS Solution Architect Understanding of a highly regulated environment that involves data privacy and confidential information requirements About The Organization Technology at Lilly builds and maintains capabilities using cutting edge technologies like most prominent tech companies. What differentiates our team is that we create new possibilities through tech to advance our purpose – creating medicines that make life better for people around the world, like data driven drug discovery and connected clinical trials. We hire the best technology professionals from a variety of backgrounds, so they can bring an assortment of knowledge, skills, and diverse thinking to deliver innovative solutions in every area of our business. The Global Services IT team leads digitalization across organizations spanning the enterprise with Finance, Legal, Ethics & Compliance, Corporate Affairs and Human Resources as well delivers best-in-class ERP Solutions.   This team leverages technology and analytics to enable transformations across the enterprise. Eli Lilly and Company, Lilly USA, LLC and our wholly owned subsidiaries (collectively “Lilly”) are committed to help individuals with disabilities to participate in the workforce and ensure equal opportunity to compete for jobs. If you require an accommodation to submit a resume for positions at Lilly, please email Lilly Human Resources ( Lilly_Recruiting_Compliance@lists.lilly.com ) for further assistance. Please note This email address is intended for use only to request an accommodation as part of the application process. Any other correspondence will not receive a response. Lilly is an EEO/Affirmative Action Employer and does not discriminate on the basis of age, race, color, religion, gender, sexual orientation, gender identity, gender expression, national origin, protected veteran status, disability or any other legally protected status. Our employee resource groups (ERGs) offer strong support networks for their members and help our company develop talented individuals for future leadership roles. Our current groups include: Africa, Middle East, Central Asia Network, African American Network, Chinese Culture Network, Early Career Professionals, Japanese International Leadership Network (JILN), Lilly India Network, Organization of Latinos at Lilly, PRIDE (LGBTQ + Allies), Veterans Leadership Network, Women’s Network, Working and Living with Disabilities. Learn more about all of our groups. #WeAreLilly",
        "url": "https://www.linkedin.com/jobs/view/3955274431"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Richmond, VA",
        "job_id": 3965662226,
        "company": "Compunnel Inc.",
        "title": "Data Engineer",
        "created_on": 1720639184.3185332,
        "description": "Description Responsibilities: Be a part of team designing and building Enterprise Level scalable, low-latency, fault-tolerant data Pipelines that provides meaningful and timely insights Build the next generation Distributed Data Pipelines using programming languages like Python Be part of a group of engineers building data pipelines using big data technologies (Spark, Snowflake, AWS Big Data Services like Glue) on medium to large scale datasets Work in a creative & collaborative environment driven by agile methodologies with focus on CI/CD, Application Resiliency Standards, and partnership with Cyber & Security teams Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies Preferred Skills SQL Snowflake Data Bricks Python Spark Education: Bachelors Degree",
        "url": "https://www.linkedin.com/jobs/view/3965662226"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Dallas-Fort Worth Metroplex",
        "job_id": 3959698356,
        "company": "Apex Systems",
        "title": "Data Engineer",
        "created_on": 1720639186.0618753,
        "description": "Apex Systems is looking for Junior Data Engineers to join an exciting team at one of our clients, a leader in insurance and financial services for our armed forces and their families. If you have just graduated or still in your junior years as a Data Engineer, please apply today! Client Industry: Financial Services & Insurance Location: Plano, TX (4 days a week onsite, 1 day remote) Pay Range: $35 - $40/HR Contract Type: W2 (No C2C at this time) Contract Length: 6 Months Contract-to-hire Junior Data Engineer Someone with a data background. It can be Python, ELT, or ETL. Needs someone who is motivated, driven and willing to come in and work hard and learn. Experience in Banking/AML/Financial industry is a plus. 1-2 years of experience or comparable university degree! Must have: SQL Snowflake ETL concepts ELT concepts Unix understand of how the environment works Data Warehousing concepts Nice to have: Snowflake Hadoop Kafka Python",
        "url": "https://www.linkedin.com/jobs/view/3959698356"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Ridgefield Park, NJ",
        "job_id": 3970169689,
        "company": "RSC Solutions",
        "title": "Data Engineer: 24-00791",
        "created_on": 1720639187.7056372,
        "description": "Job Title: Data Engineer Location: Ridgefield Park, NJ Contract Job Summary: The main function of the Data Engineer is to develop, evaluate, test, and maintain architectures and data solutions within the organization. The Data Engineer will execute plans, policies, and practices that control, protect, deliver, and enhance the value of the organization's data assets. Qualifications 3+ years of code-based ETL development using Python and SQL 3+ years of experience writing complex SQL queries 3+ years of Python development experience 2+ years of experience on GCP services such as Bigquery, Kubernetes, and Composer 2+ years of working experience in Apache Airflow Experience in developing high-performance, reliable, and maintainable code Analytical and problem-solving skills applied to Big Data domain Experience and understanding of Big Data engineering concepts End-to-End exposure and understanding of Data engineering projects Experience with Spark and Dataproc is a plus Proven understanding and hands-on experience with GitHub, development IDEs such as VS Code B.S. or M.S. in Computer Science or Engineering Bachelor's or Master's in Computer Engineering 4 years of experience in Big Data Solutions on GCP Expertise in Python, Bigquery, Kubernetes, and Airflow is a must-have Responsibilities Design, construct, install, test, and maintain highly scalable data management systems Ensure systems meet business requirements and industry practices Design, implement, automate, and maintain large-scale enterprise data ETL processes Build high-performance algorithms, prototypes, predictive models, and proof of concepts Ability to work as part of a team, as well as work independently or with minimal direction Excellent written, presentation, and verbal communication skills Collaborate with data architects, modelers, and IT team members on project goals Strong PC skills including knowledge of Microsoft SharePoint",
        "url": "https://www.linkedin.com/jobs/view/3970169689"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Florida, United States",
        "job_id": 3963972040,
        "company": "LHH",
        "title": "Data Engineer",
        "created_on": 1720639191.9248464,
        "description": "LHH Recruitment Solutions is seeking a Data Engineer (AWS), for a direct hire remote opportunity with a rapidly growing industry leader. **must be located on the East Coast** Responsibilities: Improving internal workflows by upgrading infrastructure, optimizing data delivery, and automating tasks. Setting up roles, databases, schemas, and ETL tools using Snowflake and cloud technologies. Measuring and tuning SQL performance. Utilizing SQL and various cloud technologies. Implementing access control models for systems and data. Handling data masking, encryption, tokenization, and managing data pipelines. Configuring AWS S3, EC2, external stages, and SQS/SNS. Integrating data with MSK Kafka connect and Delta Lake (Databricks). Developing systems for data extraction, transformation, and loading using AWS and SQL. Creating tools to analyze data and provide business insights. Qualifications: 3+ years of experience as a Data Engineer SQL: For database and query management. Snowflake: For data modeling and cloud data warehousing. Python and Java AWS: Including S3, EC2, SQS, and SNS for cloud services. ETL Tools: For data extraction, transformation, and loading. Data Integration Tools: Such as MSK Kafka connect and Delta Lake (Databricks). Education: Bachelor's Degree in Computer Science and/or related field Compensation: Salary will be commensurate with experience and qualifications. Benefits: 401(k) matching, Dental insurance, Health insurance, Health savings account, Life insurance, Paid time off, Vision insurance. Bonus of up to 10%. Type of Hire: Full-time, permanent position.",
        "url": "https://www.linkedin.com/jobs/view/3963972040"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Richmond, VA",
        "job_id": 3964642262,
        "company": "Open Systems Technologies",
        "title": "Databricks Cloud Data Engineer",
        "created_on": 1720639193.5950272,
        "description": "A financial firm is looking for a Databricks Cloud Data Engineer to join their team in Richmond, VA or Remote. Pay: $85-90/hr Responsibilities Understand technology vision and strategic direction of business needs Understand our current data model and infrastructure, proactively identify gaps, areas for improvement, and prescribe architectural recommendations with a focus on performance and accessibility. Partner across engineering teams to design, build, and support the next generation of our analytics systems. Partner with business and analytics teams to understand specific requirements for data systems to support both development and deployment of data workloads ranging from Tableau reports to ad hoc analyses. Own and develop architecture supporting the translation of analytical questions into effective reports that drive business action. Automate and optimize existing data processing workloads by recognizing patterns of data and technology usage and implementing solutions. Solid grasp of the intersection between analytics and engineering while maintaining a proactive approach to assure solutions demonstrate high levels of performance, privacy, security, scalability, and reliability upon deployment. Provide guidance to partners on effective use of the database management systems (DBMS) platform through collaboration, documentation, and associated standard methodologies. Design and build end to end automation to support and maintain software currency Create automation services for builds using Terraform, Python, and OS shell scripts. Develop validation and certification process through automation tools Design integrated solutions in alignment with design patterns, blueprints, guidelines, and standard methodologies for products Participate in developing solutions by incorporating cloud native and 3rd party vendor products Participate in research and perform POCs (proofs of concept) with emerging technologies and adopt industry best practices in the data space for advancing the cloud data platform. Develop data streaming, migration and replication solutions Demonstrate leadership, collaboration, exceptional communication, negotiation, strategic and influencing skills to gain consensus and produce the best solutions. Engage with Senior leadership, business leaders at Client and the Board to share the business value. Qualifications Demonstrates mutual respect, embraces diversity, and acts with authenticity Bachelor's degree in Computer Science, Management Information Systems, Computer Engineering, or related field or equivalent work experience; advance degree preferred Seven or more years of experience in designing and building large-scale solutions in an enterprise setting in both Three years in designing and building solutions in the cloud Expertise in building and managing Cloud databases such as AWS RDS, DynamoDB, DocumentDB or analogous architectures Expertise in building Cloud Database Management Systems in Databricks Lakehouse or analogous architectures Expertise in Cloud Data Warehouses in Redshift, BigQuery or analogous architectures a plus Deep SQL expertise, data modeling, and experience with data governance in relational databases Experience with the practical application of data warehousing concepts, methodologies, and frameworks using traditional (Vertica, Teradata, etc.) and current (SparkSQL, Hadoop, Kafka) distributed technologies Refined skills using one or more scripting languages (e.g., Python, bash, etc.) Experience using ETL/ELT tools and technologies such as Talend, Informatica a plus Embrace data platform thinking, design and develop data pipelines keeping security, scale, uptime and reliability in mind Expertise in relational and dimensional data modeling UNIX admin and general server administration experience required Presto, Hive, SparkSQL, Cassandra, or Solr other Big Data query and transformation experience a plus Experience using Spark, Kafka, Hadoop, or similar distributed data technologies a plus Able to expertly express the benefits and constraints of technology solutions to technology partners, business partners, and team members Experience with leveraging CI/CD pipelines Experience with Agile methodologies and able to work in an Agile manner is preferred One or more cloud certifications 23-06256",
        "url": "https://www.linkedin.com/jobs/view/3964642262"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Tulsa Metropolitan Area",
        "job_id": 3931635536,
        "company": "ONEOK",
        "title": "Data Analytics Engineer",
        "created_on": 1720639195.2403877,
        "description": "#WeAreONEOK – Fortune 500 company. 100+ years in business. Leading midstream service provider. Safety first. Sustainable operations. Environmentally responsible. Employee focused. Job Summary We are seeking a skilled Data Analytics Engineer to join our enterprise Advanced Analytics and Data Science team where we are solving challenging and impactful business problems by leveraging AI, ML, and other advanced algorithms. The Data Analytics Engineer will play a pivotal role in blending analytical rigor, advanced algorithms, and domain knowledge to drive decision-making and business strategy across the enterprise. This is a hybrid role. In this role, you would work in the Tulsa office on Mon/Tue/Thu and at home on Wed/Fri each week. Key Responsibilities: Design and develop sophisticated data models and simulations with python. Collaborate with Data Analysts, Business Analysts, Data Engineers, and Data Scientists to ensure seamless integration and alignment of analytics solutions with business objectives. Create robust data pipelines in cloud environments to support scalable analytics solutions. Engage with stakeholders across the business to translate complex datasets into actionable insights and clear, impactful business solutions. Continually monitor and refine analytics models and frameworks to improve accuracy, efficiency, and functionality. Required Skills and Qualifications: Bachelor's Degree Quantitative field (Finance, Economics, Math, Engineering, Science), Computer Science, Computer Engineering, Information Technology, Systems Analysis, or a related study preferred Minimum 3 Years Related Work Experience Expert proficiency in SQL and Python, with a solid foundation in statistical methods and data analysis techniques. Strong experience in deploying and managing analytics solutions on cloud and hybrid environments. Excellent problem-solving skills and the ability to work collaboratively in a dynamic, cross-functional team environment. Superior communication skills, capable of explaining complex analytical concepts to non-technical stakeholders. Proficiency in BI tools like Spotfire and Power BI. Job Profile Summary Architecting, building, creating, managing, and optimizing data pipelines and then moving these data pipelines effectively into production for use by Data Analysts, Data Modelers, and Data Scientists. Data Engineers also need to guarantee compliance with data governance and data security requirements while creating and improving these integrated and reusable data pipelines. Will require both training, promoting, and collaborative working with all IT departments and business units. Consistent use and/or application of advanced principles/theories/concepts/techniques in area of expertise. Often has unique knowledge in the area of specialty plus solid knowledge of other related fields. Organizes, examines, and analyzes information to provide resolution to a diverse range of complex problems/issues where analysis requires evaluation of variable factors and solutions often require creativity and a broad knowledge base to provide resolutions. Work is completed without considerable direction and acts independently to determine methods/procedures to obtain results. Generally, only special assignments may be reviewed to ensure objectives have been met. Failure to obtain results or erroneous judgment/recommendations may result in the inability to reach crucial department/organizational goals and may have serious prolonged effects or impacts. Essential Functions And Responsibilities Build data pipelines: Manage a series of stages through which data flows from data sources or endpoints of acquisition to integration to consumption for specific use cases. Innovate data pipelines: Drive innovative use of modern tools, techniques, and architectures to partially or completely automate the most common, repeatable, and tedious data preparation and integration tasks. Training counterparts such as Data Scientists, Data Modelers, and Data Analysts in using data pipelines and preparation techniques, which make it easier for them to integrate and consume the data they need for their own use cases. Promotion of the available data and analytics capabilities to end users and business unit leadership. Ensure Data Scientists, Data Modelers, and Data Analysts use the data provisioned to them responsibly through data governance and compliance initiatives. Education Bachelor's Degree Computer Science, Computer Engineering, Information Technology, Systems Analysis Work Experience 6 - 10 years related work experience preferred Knowledge, Skills And Abilities Ability to: communicate effectively orally and in writing in English Ability to: communicate effectively with supervisors, co-workers, internal and external customers Ability to: work under time pressure, tight deadlines and interruptions Ability to: use a personal computer with the Windows® operating system to complete time sheets, send and receive e-mail, and access information posted on the Company’s intranet Ability to: follow instructions or standard procedures Ability to: compile and/or examine information and select the best action from defined alternatives Ability to: organize and/or analyze information and identify solutions from a range of alternatives Ability to: deal with complex issues which require substantial analysis or independent judgment Licenses and Certifications None required Strength Factor Rating - Physical Demands/Requirements Sedentary Work - Exerting up to 10 pounds of force occasionally (Occasionally: activity or condition exists up to 1/3 of the time) and/or a negligible amount of force frequently (Frequently: activity or condition exists from 1/3 to 2/3 of the time) to lift, carry, push, pull, or otherwise move objects, including the human body. Sedentary work involves sitting most of the time, but may involve walking or standing for brief periods of time. Jobs are sedentary if walking and standing are required only occasionally and all other sedentary criteria are met. Strength Factor Description - Physical Demands/Requirements Standing: Remaining on one's feet in an upright position at a work station without moving about (Occasionally) Walking: Moving about on foot (Frequently) Sitting: Remaining in a seated position (Constantly) Lifting: Raising or lowering an object from one level to another (includes upward pulling) (Occasionally) Carrying: Transporting an object, usually holding it in the hands or arms, or on the shoulder (Occasionally) Pushing: Exerting force upon an object so that the object moves away from the force (Occasionally) Pulling: Exerting force upon an object so that the object moves toward the force (includes jerking) (Occasionally) Climbing: Ladders, Stairs (Occasionally) Balancing: Maintaining body equilibrium to prevent falling (Occasionally) Stooping: Bending the body downward and forward by bending the spine at the waist (Occasionally) Kneeling: Bending the legs at the knees to come to rest on the knee or knees (Occasionally) Crouching: Bending the body downward and forward by bending the legs and spine (Occasionally) Crawling: Moving about on the hands and arms in any direction (Occasionally) Reaching: Extending hands and arms in any direction (Constantly) Handling: Seizing, holding, grasping, turning or otherwise working with the hand or hands (Manual Dexterity) (Constantly) Fingering: Picking, pinching or otherwise working with the fingers primarily (Finger Dexterity) (Constantly) Feeling: Perceiving such attributes of objects/materials as size, shape, temperature, texture, movement or pulsation by receptors in the skin, particularly those of the finger tips (Constantly) Talking: Expressing or exchanging ideas/information by means of the spoken word (Frequently) Hearing: Perceiving the nature of sound by the ear (Frequently) Tasting/Smelling: (Occasionally) Near Vision: Clarity of vision at 20 inches or less (Constantly) Far Vision: Clarity of vision at 20 feet for more (Frequently) Depth Perception: Three-dimensional vision; ability to judge distances and spatial relationships so as to see objects where and as they actually are (Frequently) Vision: Color - The ability to identify and distinguish colors (Constantly) Working Conditions/Environment Employee is subject to inside environmental conditions Working Conditions Well lighted, climate controlled areas (Constantly) Frequent repetitive motion (Constantly) CRT (Computer Monitor(s)) (Constantly) Travel Less than 10% Driving Based on assigned tasks, employee may be assigned a company vehicle requiring the applicable driver's license ONEOK is an equal opportunity employer committed to diversity and inclusion. All qualified applicants will receive consideration for employment without regard to race, color, sex, pregnancy, sexual orientation, age, religion, creed, national origin, gender identity, disability, military/veteran status, genetic information or any other categories protected by applicable law. The job description is not intended to be a complete list of all responsibilities, duties or skills required for the job and is subject to review and change at any time, with or without notice, in accordance with the needs of ONEOK. ONEOK is committed to making our workplace accessible to individuals with disabilities and will provide reasonable accommodations, upon request, for individuals to participate in the application and hiring process. To request an accommodation email HRSolutions@ONEOK.com or call 1-855-663-6547 . Expected Salary Range $0.00 - $0.00",
        "url": "https://www.linkedin.com/jobs/view/3931635536"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Scottsdale, AZ",
        "job_id": 3946613732,
        "company": "Zortech Solutions",
        "title": "Data Engineer (Paze)/ Fulltime, Contract- US",
        "created_on": 1720639196.897229,
        "description": "Role: Data Engineer (Paze) Location: Scottsdale AZ (Day 1 onsite) Duration: FTE & C2C Job Description Must have : Skillset: Java, Scala, S3, Glue, aws , Redshift. 6-8 years of IT experience focusing on enterprise data architecture and management. Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling Experience with Databricks & on Prem, Structured Streaming, Delta Lake concepts, and Delta Live Tables required Experience with Spark Scala and Java programming Data Lake concepts such as time travel and schema evolution and optimization Structured Streaming and Delta Live Tables with Databricks as a bonus Experience leading and architecting enterprise-wide initiatives specifically system integration, data migration, transformation, data warehouse build, data mart build, and data lakes implementation/support Advanced level understanding of streaming data pipelines and how they differ from batch systems Formalize concepts of how to handle late data, defining windows, and data freshness Advanced understanding of ETL and ELT and ETL/ELT tools such as Data Migration Service etc Understanding of concepts and implementation strategies for different incremental data loads such as tumbling window, sliding window, high watermarks, etc. Familiarity and/or expertise with Great Expectations or other data quality/data validation frameworks a bonus Familiarity with concepts such as late data, defining windows, and how window definitions impact data freshness Advanced level SQL experience (Joins, Aggregation, Windowing functions, Common Table Expressions, RDBMS schema design performance optimization) Indexing and partitioning strategy experience Debug, troubleshoot, design, and implement solutions to complex technical issues Experience with large-scale, high-performance enterprise big data application deployment and solution Architecture experience in AWS environment a bonus Familiarity working with Lambda specifically with how to push and pull data, how to use AWS tools to view data for processing massive data at scale a bonus Experience with Gitlabs and CloudWatch and ability to write and maintain Gitlabs for supporting CI/CD pipelines Experience working with AWS Lambdas for configuration and optimization and experience with S3 Familiarity with Schema Registry, and message formats such as Avro, ORC, etc. Ability to thrive in a team-based environment Experience briefing the benefits and constraints of technology solutions to technology partners, stakeholders, team members, and senior-level of management",
        "url": "https://www.linkedin.com/jobs/view/3946613732"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3945738250,
        "company": "Chelsoft Solutions Co.",
        "title": "Python/Java Data Engineer_W2_McLean, VA",
        "created_on": 1720639198.7319248,
        "description": "Required Skills Python, will accept Java but Python is preferred Spark, PySpark AWS, Lambda, S3 SQL ETL PREVIOUS CAPITAL ONE EXPERIENCE WILL BE PREFERED!!! Additional Skills Job Description Job Title: Python/Java Data Engineer Duration of project: 6 months Location: McLean – Hybrid 2-3 days ﻿ Must Haves Python, will accept Java but Python is preferred. Spark, PySpark AWS ,lambda, S3 SQL ETL ﻿ PREVIOUS CAPITAL ONE EXPERIENCE WILL BE PREFERED!!! ﻿ Org/Team 2 teams under HM 7 engineers within core team ﻿ Project Details/Day2Day ETL data transformation project Self-service data transformation – onboard their use cases and data transformations Maintaining existing application Feature developments – here and there API’s – data brick clusters New developments on the Python side ﻿",
        "url": "https://www.linkedin.com/jobs/view/3945738250"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Wayne, PA",
        "job_id": 3967449494,
        "company": "ICONMA",
        "title": "Agile Data Engineer",
        "created_on": 1720639200.3835766,
        "description": "Our Financial client is looking for Agile Data Engineer in Wayne, PA/Des Moines, IA Description Responsible for maintaining production applications within the Data Management Domain of client including the reporting and Datawarehouse environments, both cloud and on-premise. Responsible for Data DEVOPS including development and implementation to ensure production applications are available as defined by the SLA’s. Responsible for project delivery work related to applications within the Business Intelligence Domain of client. Duties And Responsibilities Ensure Data DEVOPS is followed using Agile methodology, providing full system lifecycle for data components. Create traceability from fix recommendation generation through to Test Case execution. Ensure that analysis is compliant with IT standards and requirements Testing activities validate quality and alignment to requirements Identify opportunities for software improvement Ensure delivery of deliverables are on time, on budget and ‘on quality’ Ensure that all deliverables comply with the boundaries set by enterprise architecture and by security and compliance. Ensure applicable audit actions are managed and fulfilled in a timely manner Support and when necessary, execute Link and SIT testing activities Ensure the development and introduction of changes to existing systems, is in line with functional requirement. Ensure the execution of changes, is in line with change requests or because of technical necessity. Ensure the execution of the technical deployment activities is in line with defined process. Skills Required Possess a broad range of knowledge in support of multiple applications within a production support environment including: Azure, Azure Data Factory, SQLServer, Oracle SQL. Technical Skills Including Some Of The Following Azure Data Factory SQL Server Database script generation and formalization Database design, modifications, and improvements for transactional systems Data normalization/denormalization Design and modification of Advanced queries, Triggers, Stored procedures (Oracle & SQLServer) Performance Testing (Database Performance tuning for real-time service calls, tuning of Conversion process) Experience working with cloud base data repositories and managing data pipelines ETL Knowledge, mapping, troubleshooting. Azure Data Factory and Microsoft SQL Server Integration Services (SSIS) preferred. Knowledge of other Object-Relational Mapping (ORM) Frameworks is acceptable Knowledge of alternative SQL, general understanding and the ability to write queries. Experience in a role that supports complex production applications. Solid understanding of the SDLC and various development efforts such as debugging and unit testing Knowledge in SQL tuning and performance monitoring Knowledge of or experience with Star/Snowflake Schema and working with Data Warehouses/marts Experience writing technical design documents, or documenting implementations Strong Interpersonal skills in areas such as teamwork, facilitation, communication, and negotiation. Excellent written and verbal communication skills, especially within the IT community. Excellent planning and organizational skills. Quality driven and results oriented and ready to go above and beyond to deliver results Education And Other Credentials Minimum proven bachelor’s degree or equivalent work experience Physical Requirements and Working Conditions Substantial Experience In The Defined Required Technology Area. Excellent customer Service skills Must have the ability to communicate, both orally and written, with other members of the team and with clients / prospective clients. Able to work in a very fast-paced open environment Must be able to demonstrate a high degree of attention and quality, details, correctness, and deadlines. Must have the proven ability to handle and organize multiple projects and deadlines. Fluent speaking and writing in English Skills - Nice To Have Experience with Delta Lake, DataVault2.0 and other modern data architectures Experience with Data Streaming technologies Azure DevOps (ADO) or Jira for Devops processes. Azure Function Apps As an equal opportunity employer, ICONMA provides an employment environment that supports and encourages the abilities of all persons without regard to race, color, religion, gender, sexual orientation, gender identity or express, ethnicity, national origin, age, disability status, political affiliation, genetics, marital status, protected veteran status, or any other characteristic protected by federal, state, or local laws",
        "url": "https://www.linkedin.com/jobs/view/3967449494"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Maryland Heights, MO",
        "job_id": 3930172625,
        "company": "Accroid Inc",
        "title": "Hybrid Data Engineer",
        "created_on": 1720639201.9644902,
        "description": "Location - Maryland Heights MO (hybrid) Need Local.. MUST HAVE Skillsets 5+ years of experience developing data pipelines and API calls using SQL, Scala, Spark, and Java Experience utilizing Hadoop for Big Data Scripting experience using Shell Scripting for automation. Cloud experience in AWS for migrating on prem to the cloud and managing data in AWS. Experience working with NoSQL Databases (ex: MongoDB, Redis, DynamoDB, Apache Cassandra, Amazon Neptune.",
        "url": "https://www.linkedin.com/jobs/view/3930172625"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3943398611,
        "company": "Programmers.io",
        "title": "Data Engineer",
        "created_on": 1720639203.6748252,
        "description": "Programmers.io India Pvt. Ltd./ One of our client is looking for Data Engineer to join our awesome team and deliver a streamlined user experience. we are seeking an Data Engineer. Here you can go through job description. Role: Data Engineer Location- Remote Responsibilities Advanced Collaboration: Define and lead projects in collaboration with Data Scientists and Engineers to enhance data workflows, implementing cutting-edge solutions to meet complex data challenge Build data pipelines: Managed data pipelines consist of a series of stages through which data flows (for example, from data sources or endpoints of acquisition to integration to consumption for specific use cases). Data pipelines must be created, maintained, and optimized as workloads move from development to production for specific use cases. Architecting, creating, and maintaining data pipelines will be the primary responsibility of the data engineer. Data Governance: Spearhead the development and enforcement of data management practices, ensuring the highest quality of data in our data lake and compliance with data privacy standards Collaborate with Business Intelligence and Analytics team to share knowledge and improve data quality. Drive Automation through effective metadata management: The Data Engineer will be responsible for using innovative and modern tools, techniques, and architectures to automate the most-common, repeatable and tedious data preparation and integration tasks to minimize manual and error-prone processes and improve productivity. Participate in ensuring compliance and governance during data use:. Data engineer should work with data governance teams (and information stewards within these teams) and participate in vetting and promoting content created in the agency for governance and compliance. Qualifications A Bachelor's degree in Computer Science or Information Systems 3-5 years of IT experience Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema construction, required Hands-on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, required Knowledge of Microsoft SQL Servers, Azure Synapse or Azure SQL instance Experience with one or more of the following C# (Programming Language); Java (Programming Language); Python (Programming Language); SQL (Programming Language) required for sophisticated data processing and API development Advanced proficiency in relational database modeling, Data Mart design, SQL development, and performance tuning. Must be expert at SQL coding and troubleshooting. Preferably working knowledge of Agile Methods and enterprise Cloud Integration Experience with Other diagnosing system issues, engaging in data validation, and providing quality assurance testing Excellent communication skills (both written and verbal) Excellent interpersonal and technical support skills Excellent organizational skills, accuracy and attention to detail Strong analytical and problem-solving skills Ability to work in a collaborative team environment and ability to complete work within demanding time-lines Commitment to effectively handle confidential information Our Commitment to Equity and Justice About Us, # We are honored to be recognized and ranked 59th on this year 2022 SMU Cox School of Business Dallas 100 list of the fastest-growing companies in Dallas-Ft. Worth. # We ranked #895 for the year 2022 in inc5000 # We ranked #1052ndfor the year 2023 in inc5000 #We are Great Place to Work Certified in 2023 Programmers.io is an IBM registered business partner with Microsoft Gold and ISO/IEC 27001 certification, listed on Inc. 5000 2022, We are experts in both legacy (IBMi/AS400) and modern platforms.",
        "url": "https://www.linkedin.com/jobs/view/3943398611"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Davenport, IA",
        "job_id": 3950542854,
        "company": "Palmer College of Chiropractic",
        "title": "Data Engineer",
        "created_on": 1720639205.3825321,
        "description": "Palmer College of Chiropractic is looking for an on-site Data Engineer at our Davenport, Iowa campus! The Data Engineer's role is to plan, build, and manage data pipelines, systems, and applications that help the college achieve its data-driven objectives and targets. The Data Engineer collaborates with the Data Governance team, data analysts, software engineers, and other relevant parties to guarantee the quality, dependability, and accessibility of data throughout the organization. The Data Engineer also uses the most advanced technologies and methods to develop data solutions that are scalable, secure, and effective, and that suit the college's requirements and standards. ORGANIZATIONAL RELATIONSHIPS Responsible to the Director of Applications and has a support responsibility to all other departments and college personnel as necessary. Specific Duties And Responsibilities Use SQL, NoSQL, Python, Snowflake, and other tools and technologies to build and maintain data pipelines, systems, and applications that collect, transform, and integrate data from various sources. Ensure the data solutions meet the standards and policies for data quality, data governance, and data management, and are performant, reliable, secure, and compliant. Work with the data engineer, data analysts, software engineers, and other stakeholders to understand data needs, design data models, and provide data solutions that support the college's data-driven goals and initiatives. Research and evaluate new data technologies, tools, and methodologies, and provide recommendations and guidance for data innovation and improvement. Monitor, troubleshoot, and resolve data issues, and provide technical support and documentation for data solutions. Create and maintain documentation related to Data configuration, mapping, processes, and solutions. Develop and implement policies, procedures, and training plans for Data usage and Data governance. Ensure compliance and regulatory requirements (e.g., PCI, HIPAA, FERPA, etc.) are adhered to for all technology implemented and supported by Palmer College. Perform all responsibilities in a way that fully complies with Palmer’s Equal Employment Opportunity/Affirmative Action policy. Perform other duties as assigned. Skills ADDITIONAL SKILLS AND QUALIFICATIONS A keen interest in data and in finding data solutions for complex data challenges. A flexible and creative method that can deliver data solutions at different levels of accuracy and complexity. Strong skills in technical documentation and communication. Highly self-driven and focused. Highly skilled in analysis, organization, and time management to work in a cooperative, team-based environment. Well-developed skills in public speaking and conflict resolution. Strong orientation towards customer service. Ability To Quickly learn the organization's goals and objectives. Design, build, and maintain data pipelines, systems, and applications that meet the college's data requirements and standards. Manage storage, optimize data schemas, and ensure data quality and integrity. Effectively prioritize and complete tasks in a stressful environment and perform multiple assigned duties with time pressures and frequent interruptions. Education And Experience Minimum Qualifications: Bachelor's degree or equivalent combination of relevant work experience and education. Excellent customer service skills and effective communication abilities with individuals at all levels, including external vendors and support. Proven team-player with strong organizational and time management skills. Must demonstrate organization, accuracy, thoroughness, and the ability to ensure work quality. Regular and reliable attendance is required for success in this position. Availability to work evenings and weekends as required. Desired Qualifications 3 years of direct experience in data engineering, data warehousing, data integration, or related field. Evidence of knowledge and skills in, and experience with: The principles and practices of data engineering, data warehousing, data integration, and data analytics. The methodologies and standards of data modeling, data quality, data governance, and data management. SQL, NoSQL, Python, Snowflake, and other data tools and technologies. Data analytics, cloud computing, and big data platforms and tools. Machine learning, data mining, and statistical analysis techniques and tools. Proficiency in programming languages, such as Python and Java, for creating ETL tools, processes, and automation. ETL process, creating pipelines for data transformation, data cleaning, aggregation, and enrichment for data analysis. Integrations, API, and external data connections to enable data ingestion, extraction, and exchange. Project management and the relevant data privacy laws and practices. Physical Requirements And Work Environment Perform sedentary to light work in a ventilated, lighted, and temperature-controlled office setting. Frequent need to stand, stoop, walk, sit, lift light objects (up to 10 pounds) and perform other similar actions during the workday.",
        "url": "https://www.linkedin.com/jobs/view/3950542854"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "South Carolina, United States",
        "job_id": 3953109239,
        "company": "Capgemini",
        "title": "Snowflake Data Engineer - AWS",
        "created_on": 1720639206.9864767,
        "description": "Title - Snowflake Data Engineer - AWS Location – Fort Mills, SC (hybrid) Duration – Fulltime Key Responsibilities Design develop and maintain data pipelines and ETL processes using Snowflake AWS services Python and DBT Collaborate with data scientists and analysts to understand data requirements and implement solutions Optimize data workflows for performance scalability and reliability Troubleshoot and resolve data related issues in a timely manner Stay updated on the latest technologies and best practices in data engineering Proven experience in data engineering roles with a focus on Snowflake AWS services Python and DBT Strong analytical and problem solving skills Excellent communication and teamwork abilities Required Skills The candidate should have a deep understanding of Snowflake data warehousing platform and be proficient in using Snowpark for data processing and analytics AWS services Airflow. The candidate should have hands on experience with AWS services particularly Apache Airflow for orchestrating complex data workflows and pipelines. AWS services Lambda Proficiency in AWS Lambda for serverless computing and event driven architecture is essential for this role. AWS services Glue, the candidate should be well versed in AWS Glue for ETL Extract Transform Load processes and data integration. Python Strong programming skills in Python are required for developing data pipelines data transformations and automation tasks. DBT Experience with DBT Data Build Tool for modeling data and creating data transformation pipelines is a plus. Mandatory Skills Exp in AWS (lamda, glue airflow), Snowflake, DBT & Python If your Interested. Please share us your resume on sajid.k.shaikh@capgemini.com Life At Capgemini Capgemini supports all aspects of your well-being throughout the changing stages of your life and career. For eligible employees, we offer: Flexible work Healthcare including dental, vision, mental health, and well-being programs Financial well-being programs such as 401(k) and Employee Share Ownership Plan Paid time off and paid holidays Paid parental leave Family building benefits like adoption assistance, surrogacy, and cryopreservation Social well-being benefits like subsidized back-up child/elder care and tutoring Mentoring, coaching and learning programs Employee Resource Groups Disaster Relief About Capgemini Capgemini is a global business and technology transformation partner, helping organizations to accelerate their dual transition to a digital and sustainable world, while creating tangible impact for enterprises and society. It is a responsible and diverse group of 340,000 team members in more than 50 countries. With its strong over 55-year heritage, Capgemini is trusted by its clients to unlock the value of technology to address the entire breadth of their business needs. It delivers end-to-end services and solutions leveraging strengths from strategy and design to engineering, all fueled by its market leading capabilities in AI, cloud and data, combined with its deep industry expertise and partner ecosystem. The Group reported 2023 global revenues of €22.5 billion. Get The Future You Want | www.capgemini.com Disclaimer Capgemini is an Equal Opportunity Employer encouraging diversity in the workplace. All qualified applicants will receive consideration for employment without regard to race, national origin, gender identity/expression, age, religion, disability, sexual orientation, genetics, veteran status, marital status or any other characteristic protected by law. This is a general description of the Duties, Responsibilities and Qualifications required for this position. Physical, mental, sensory or environmental demands may be referenced in an attempt to communicate the manner in which this position traditionally is performed. Whenever necessary to provide individuals with disabilities an equal employment opportunity, Capgemini will consider reasonable accommodations that might involve varying job requirements and/or changing the way this job is performed, provided that such accommodations do not pose an undue hardship. Capgemini is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to your recruiting contact. Click the following link for more information on your rights as an Applicant http://www.capgemini.com/resources/equal-employment-opportunity-is-the-law",
        "url": "https://www.linkedin.com/jobs/view/3953109239"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Milwaukee, WI",
        "job_id": 3950761757,
        "company": "Codeworks IT Careers",
        "title": "Data & Analytics Engineer",
        "created_on": 1720639208.7916527,
        "description": "Codeworks is an IT Services firm in SE Wisconsin, known for our strong commitment to quality and for our direct client relationships. Who We’re Looking For!... A Data Analytics Engineer for a 6 month contract-to-hire role with a financial services company with a reputation for being ranked as one of the best companies to work for. The is role requires hybrid model onsite in Milwaukee. Key Skills: Data modeling, Data analytics, and Azure or Snowflake (a plus!) Our client is looking for a Data & Analytics Engineer for their growing data team. This is a key role on our IT Data Team requiring a broad range of skills and the ability to step into different roles depending on the size and scope of the business need. The self-motivated candidate will have proven experience architecting successful data solutions on key projects in a collaborative environment. Success will come from being able to prioritize, deliver value incrementally, problem solve, and manage changing priorities. You will work closely with our business partners and interface with both technical and non-technical colleagues. As a Data & Analytics Engineer, you will: Data Architecture Specialize in data modeling, both 3NF and dimensional, with experience in conceptual, logical, physical, and industry data modeling. Strong knowledge and experience with data architecture methodologies. Apply the appropriate level of modeling theory, pattern recognition, and abstractions to architect and design a pragmatic solution that functionally meets the business and technical requirements. Partner with internal business units to define information requirements and translate them into appropriate data solutions. Collaborate with IT and business partners to lead data discovery, profiling, analysis, and quality assessments in order to obtain clear information requirements. Develop and validate source to target mappings and transformation logic required to support business needs. Understand the importance of capturing data lineage. Architect, implement and verify end-to-end data solutions. Develop test plans needed to ensure a quality deliverable. Participate in validation testing, coordinate user acceptance testing and training to ensure the final implementation enables the user to solve their business problem. General Data Management Play a critical role in architecting our data and analytics solution landscape Demonstrate competence, experience, knowledge, understanding, and advocacy of data management concepts, data warehousing, BI, and analytics. Demonstrate ability to perform appropriate level of strategic thinking by viewing initiatives both within the immediate project context as well as the overall architectural vision. Participate and/or Lead in data architectural design and strategy discussions. Data Delivery Work with the business users to conduct data discovery engagements and can quickly identify, and prototype, a solution that brings together multiple data sources into one coherent concept and understanding. (data blending) Leverage existing tools to create data visualizations and mentors the business to be self-sufficient. Collaborate – build relationships! Identify and communicate project risks and impediments and proactively work with other members of the Analytics team to complete high-value deliverables as identified by business partners and team leadership. Partner with Analytics team members to translate business and functional requirements into technical designs Strive to understand the data consumption needs of the business community, as well as the problems faced by business users involving the access and use of data Help Analytics teams develop solutions that enable businesses to capitalize on business insights and drive toward gaining a competitive advantage What makes this opportunity great: Information technology is a core part of the business strategy and plays a critical role in the growth and transformation of the firm. On Computerworld’s ‘Best Company to Work For’ list for five consecutive years with a collaborative culture that values diverse backgrounds and perspectives while emphasizing teamwork and a strong sense of partnership. Support and flexibility to grow and be your best at work, at home, and in the community. What we look for: Minimum of 3-5 years of experience in Data Solution delivery in a complex environment working collaboratively in a team setting Proficient in Data Solution tools and concepts such as: Business Intelligence tools: Microsoft tools (SQL Server Management Studio, SSRS, SSAS, Power Pivot, Power Query, PowerBI), Alteryx Database: SQL Server Data Query tools: SQL, T-SQL Data Management and Quality: data mapping, data profiling, metadata repository, relational data modeling, master data management Data Modeling: ER/Studio Data Architect, 3NF and dimensional modeling Data Warehousing concepts: Inmon, Kimball, Data Lake Data Integration concepts and strategies: EII, ETL, EL-T and EAI About Our Client— Our client is a multinational independent investment bank and financial services company with a reputation for being ranked as one of the best companies to work for. Local candidates to Milwaukee area will be given priority. If you feel that you meet the qualifications listed above and are open to working in the Milwaukee area, please forward your resume in Word format to kristy.harmann@codeworks-inc.com. About CODEWORKS: Headquartered in Milwaukee, WI with an office in Madison, WI—Codeworks has over 23+ years of experience serving Fortune 1000 companies in Wisconsin as well as our client's national locations. Our recruiting team is extremely good at evaluating, advising, and connecting IT professionals with new opportunities that will satisfy their expectations both in salary and opportunity for growth. For more information, please visit our website at: www.codeworks-inc.com. For priority career/job posting updates, please follow us on Twitter: @CodeworksIT At Codeworks, we're committed to diversity, equity, and inclusion in our workforce and beyond. We believe in equal opportunities and value the unique perspectives that every individual brings to our team. Join us in creating an inclusive, innovative, and collaborative workplace where your talents can thrive.",
        "url": "https://www.linkedin.com/jobs/view/3950761757"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Santa Clara, CA",
        "job_id": 3964477092,
        "company": "TechStar Consulting Services Inc. - Salem NH",
        "title": "Data Engineer",
        "created_on": 1720639210.4477499,
        "description": "Job Title: Data Engineer Location: Silicon Valley Scheduling: On-site 5 days/week, potentially hybrid for local candidates Pay Rate: $73/hr on W2 Key Responsibilities: - Develop electric vehicle user model or surrogate models using telematic data - Conduct descriptive analytics on collected data - Analyze, summarize, and present research findings and results - Qualifications and Skills Required: - Graduate degree in Data Analytics, Data Science, Computer Science, or related technical subject area - Minimum 2-3 years experience with data analysis, machine learning and statistical modeling - Expertise in Python or MatLab - Hands-on experience with machine learning frameworks like Tensor Flow, and data science packages such as Pandas, Scipy, Numpy - Experience with Big Data and cloud environments like AWS - Attention to detail, flexibility, creativity, and excellent communication and teamwork skills - Passion for empirical research and data-driven answers - Additional Notes: - Master's degree with a specialization in data analytics or data science strongly desired - Role involves handling large databases from EV's and performing data cleaning and statistical work - Experience with data processing, Big Data, AWS required - Knowledge of cloud and Snowflake necessary - Python skills required, MatLab not needed.",
        "url": "https://www.linkedin.com/jobs/view/3964477092"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Houston, TX",
        "job_id": 3965989280,
        "company": "Conduit Power",
        "title": "Data Engineer",
        "created_on": 1720639212.1550243,
        "description": "About the Position: Conduit Power is on a path to supplement our Nation's power generation system with reliable and dispatchable generation in ways that reduces carbon footprint and accelerates the transition to clean, renewable energy. We are seeking an experienced Data Engineer who will report directly to the Manager of Data Engineer and who will lead the data engineering and data architecture efforts of the Firm. The position will manage existing AWS and Python-based data pipelines and develop additional pipelines and ETL processes as Conduit expands beyond Texas. These pipelines will download, process, and sanitize hundreds of millions of natural gas and power prices per day across U.S. markets. You will also help Conduit develop and deliver operational data pipelines extracted from deployed generation equipment, market insight dashboards on power and gas market prices and significant operational KPIs. You Will: Improve, deploy, quality control, and update data systems, both internal and external, working under the guidance of the Manager of Data Engineer and alongside existing technical team members in a distributed environment Manage the Firm's existing AWS-based data infrastructure (EC2, S3, Athena, Lambda, Glue, SNS) scraping ISO and RTO power pricing data over billions of points on a real-time, day ahead, and forward basis, ensuring data pipelines can be used intelligently by commercial and operational users Improve upon existing Python-based ETL and querying processes and Athena/S3 data lake architecture to reduce latency and improve storage efficiency Expand AWS data pipeline framework to additional strategic end markets and to include operational data from deployed generation equipment Work with the Firm's commercial customers to architect reporting dashboards and manage analytical queries regarding operational uptime, efficiency, and power market pricing fundamentals Assist in building and scaling a best-in-class data science and engineering team capable of managing real-time power decision-making and curating customer and site lists for long-term value What You Bring to the Team: Bachelor's degree in technical discipline 2+ years of professional experience working as a data engineer or web developer focused on backend systems architecture Excellent analytical and problem-solving skills Experience deploying data architecture solutions across all phases of a data engineering pipeline, including ETL, data warehouse/data lake architecture, statistical and machine learning modeling, and creating meaningful commercial outputs via dashboards or web applications Expert working knowledge of Python, experienced with Linux, AWS, SQL/NoSQL databases, and UI solutions such as Tableau Strong writing, editing, analytical, research, communication, presentation, and organizational skills Demonstrated success prioritizing and managing competing expectations effectively while working on multiple projects simultaneously Experience managing high-volume data solutions at scale (threading, vectorization, Spark/Hadoop, etc.) What Conduit Brings to You: Opportunity to play a significant role in transforming the energy industry Growth, development, learning, and advancement opportunities A flexible environment where your voice, thoughts, and ideas are valued and heard Excellent benefits (company-paid and voluntary): Health / Dental / Vision / Life / AD&D / STD / LTD / AFLAC / Pet / Legal / Commuter / 401(k) with Company Match / Paid Time Off and more! Location: Houston, Texas Target Comp Range: $120,000 to $130,000 About Conduit Power: Conduit Power builds and operates long-term contracted electrical infrastructure allowing customers to increase reliability, reduce power costs, and minimize carbon emissions. Conduit is a portfolio company of Grey Rock Investment Partners. Grey Rock Investment Partners is a private equity firm with more than $1.3 billion in asset value across its private equity fund platform. The firm invests across the energy value chain with private equity funds focusing on investments in natural resources, carbon capture, industrial electrification, and power optimization.",
        "url": "https://www.linkedin.com/jobs/view/3965989280"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Boston, MA",
        "job_id": 3951192954,
        "company": "Meet",
        "title": "Data Engineer",
        "created_on": 1720639213.923174,
        "description": "About Us: Join a pioneering biotech company revolutionizing protein profiling! They combine cutting-edge DNA nanotechnology, high-dimensional flow cytometry, laboratory automation, and machine learning to create the world’s highest throughput proteomic platform. About the Role: My client are seeking a talented Data Engineer to join their innovative team. In this role, you will: Design, build, and scale data pipelines for large-scale analysis. Support the development of our data platform architecture. Create internal tools to enhance R&D and Lab Operations. Play a critical role in advancing the future of biological measurement and research. Why Join them? Be at the forefront of biotech innovation. Collaborate with a team of top-tier scientists and engineers. Make a significant impact on the future of proteomics and biological research. Please apply below.",
        "url": "https://www.linkedin.com/jobs/view/3951192954"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Minneapolis, MN",
        "job_id": 3828573565,
        "company": "Syntricate Technologies",
        "title": "Data Engineer (Epic, Cerner, Databricks, Azure) | TELECOMMUTE",
        "created_on": 1720639215.5859804,
        "description": "100% telecommute. Hours: 9am-5pm EST additional overtime and weekends on as needed basis Team: Architects and 2 Data engineers along with India Team members supporting JMH Client is seeking a Data Engineer with health care experience to implement and manage cross-domain, modular, flexible, scalable, secure, reliable and quality data solutions that transform data to support analytics and insight generation for our clients. The Data Engineer will implement, test, deploy, monitor, and maintain the delivery of data in a systematic method and will support a wide variety of analytical needs for our customers. The Director of Data Engineering will also partner with the broader OAS Analytics organization to harness the power of client data to facilitate analytical insight and will be responsible for building quality and efficiency into every project. Responsibilities Support the full data engineering lifecycle including research, proof of concepts, design, development, testing, deployment, and maintenance of data management solutions Utilize knowledge of various data management technologies to drive data engineering projects Lead data acquisition efforts to gather data from various structured or semi-structured source systems of record to hydrate client data warehouse and power analytics across numerous health care domains Leverage combination of ETL/ELT methodologies to pull complex relational and dimensional data to support loading DataMarts and reporting aggregates. Eliminate unwarranted complexity and unneeded interdependencies Detect data quality issues, identify root causes, implement fixes, and manage data audits to mitigate data challenges Implement, modify, and maintain data integration efforts that improve data efficiency, reliability, and value Leverage and facilitate the evolution of best practices for data acquisition, transformation, storage, and aggregation that solve current challenges and reduce the risk of future challenges Effectively create data transformations that address business requirements and other constraints Partner with the broader analytics organization to make recommendations for changes to data systems and the architecture of data platforms Support the implementation of a modern data framework that facilitates business intelligence reporting and advanced analytics Prepare high level design documents and detailed technical design documents with best practices to enable efficient data ingestion, transformation and data movement. Leverage DevOps tools to enable code versioning and code deployment. Leverage data pipeline monitoring tools to detect data integrity issues before they result into user visible outages or data quality issues Leverage processes and diagnostics tools to troubleshoot, maintain and optimize solutions and respond to customer and production issues Continuously support technical debt reduction, process transformation, and overall optimization Leverage and contribute to the evolution of standards for high quality documentation of data definitions, transformations, and processes to ensure data transparency, governance, and security Ensure that all solutions meet the business needs and requirements for security, scalability, and reliability Ideal Background: Healthcare background including Azure ADF and Databricks background. Required Bachelors Degree (preferably in information technology, engineering, math, computer science, analytics, engineering or other related field) Minimum of 5+ years of combined experience in data engineering, ingestion, normalization, transformation, aggregation, structuring, and storage Minimum of 5+ years of combined experience working with industry standard relational, dimensional or non-relational data storage systems Minimum of 5+ years of experience in designing ETL/ELT solutions using tools like Informatica, DataStage, SSIS , PL/SQL, T-SQL, etc. Minimum of 5+ years of experience in managing data assets using SQL, Python, Scala, VB.NET or other similar querying/coding language Minimum of 3+ years of experience working with healthcare data or data to support healthcare organizations Preferred 5+ years of experience in creating Source to Target Mappings and ETL design for integration of new/modified data streams into the data warehouse/data marts Minimum of 2+ years of experience with Epic Clarity and/or Caboodle data models or with Cerner Millennium / HealthEintent and experience using Cerner CCL 2+ years of experience working with Health Catalyst product offerings, including data warehousing solutions, knowledgebase, and analytics solutions Epic certifications in one or more of the following modules: Caboodle, EpicCare, Grand Central, Healthy Planet, HIM, Prelude, Resolute, Tapestry, or Reporting Workbench Experience in Unix or Powershell or other batch scripting languages Depth of experience and proven track record creating and maintaining sophisticated data frameworks for healthcare organizations Experience supporting data pipelines that power analytical content within common reporting and business intelligence platforms (e.g. Power BI, Qlik, Tableau, MicroStrategy, etc.) Experience supporting analytical capabilities inclusive of reporting, dashboards, extracts, BI tools, analytical web applications and other similar products Exposure to Azure, AWS, or google cloud ecosystems Exposure to Amazon Redshift, Amazon S3, Hadoop HDFS, Azure Blob, or similar big data storage and management components Desire to continuously learn and seek new options and approaches to business challenges A willingness to leverage best practices, share knowledge, and improve the collective work of the team Experience contributing to cross-functional efforts with proven success in creating healthcare insights Ability to effectively communicate concepts verbally and in writing Experience and credibility interacting with analytics and technology leadership teams Willingness to support limited travel up to 10%",
        "url": "https://www.linkedin.com/jobs/view/3828573565"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Greater Cleveland",
        "job_id": 3967177780,
        "company": "Apex Systems",
        "title": "Data Engineer",
        "created_on": 1720639217.453274,
        "description": "Role: Data Engineer Location: Hybrid - Cleveland, OH or Columbus, OH (4 days onsite) Project Details: Migrating on-prem servers to the cloud Building Enterprise data platform from the ground up Working with membership, specialty, and subsidiary data Day to day: Working with businesses to get their data needs& getting data loaded into new platform Building data pipelines and data movement Prepping data and responsible for pipeline quality Nothing to do with machine learning. Developing ingestion pipeline and making data more ingestible Required Experience: 5+ years of Data Engineering experience Data Bricks (Comparable tools) Azure Python & SQL (language) Data Modeling Understanding of syntax Preferred Experience: Insurance industry experience Data Warehousing Data Brick or AWS Comparable tools Building and moving Data pipelines",
        "url": "https://www.linkedin.com/jobs/view/3967177780"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Missouri City, MO",
        "job_id": 3967834756,
        "company": "Mavinsys",
        "title": "Data Engineer",
        "created_on": 1720639219.0897155,
        "description": "Hello, We are from Mavinsys Talent Acquisition team based on One World Trade Centre, New York. We are specializing in IT services and staffing majorly in lateral hiring/contract. Below is one of our requirement to fill immediately, if you're interested, please share your candidature to joinus@mavinsys.com Job Title: Data Engineer Location: Missouri, MO Duration: 12months Job Description; Proficient IT professional experience with 8+ years of expertise as a Data Engineer, ETL Developer and Software Engineer includes designing, developing, & implementing data models for enterprise-level applications. Worked on ETL Migration services by creating & deploying AWS Lambda functions to provide a serverless data pipeline that can be written to Glue Catalog & queried from Athena. Experience in Analytics & cloud migration from on-premises to AWS Cloud with AWS EMR, S3, & DynamoDB. Experience in creating & managing reporting & analytics infrastructure for internal business clients using AWS services including Athena, Redshift, Spectrum, EMR, & Quick Sight. Extensive expertise with Amazon Web Services such as Amazon EC2, S3, RDS, IAM, Auto Scaling, CloudWatch, SNS, Athena, Glue, Kinesis, Lambda, EMR, Redshift, & DynamoDB. Proficiency in setting up the CI/CD pipelines using Jenkins, GitHub, Chef, Terraform & AWS. Created an Azure SQL database, monitored it, & restored it. Migrated Microsoft SQL server to Azure SQL database. Experience with Azure Cloud, Azure Data Factory, Azure Data Lake Storage, Azure Synapse Analytics, Azure Analytical services, Big Data Technologies (Apache Spark), & Data Bricks. Developed ETL pipelines in & out of the data warehouse using a mix of Python & Snowflakes, SnowSQL Writing SQL queries against Snowflake. Extensive experience developing & implementing cloud architecture on Microsoft Azure. Excellent understanding of connecting Azure Data Factory V2 with a range of data sources & processing the data utilizing pipelines, pipeline parameters, activities, activity parameters, & manually/window-based/event-based task scheduling. Created a connection from Azure to an on-premises data center using the Azure Express Route for Single & Multi-Subscription. Working knowledge in Python programming with a variety of packages such as NumPy, Matplotlib, SciPy, & Pandas. Integrated Jenkins with Docker container using Cloud bees Docker pipeline plugin & provisioned the EC2 instance using Amazon EC2 plugin Extensive experience creating Web Services with the Python programming language, including implementation of JSON-based RESTful & XML- based SOAP web services. Experienced in writing complex Python scripts with Object-Oriented principles such as class creation, constructors, overloading, & modules. Experience establishing & maintaining multi-node development & production Hadoop clusters. Worked with Spark to improve the speed & optimization of current Hadoop algorithms utilizing SparkContext, Spark-SQL, Data Frame, Pair RDD, & Spark YARN. Worked with the Map Reduce programming paradigm & the Hadoop Distributed File System.",
        "url": "https://www.linkedin.com/jobs/view/3967834756"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Brooklyn, OH",
        "job_id": 3960764247,
        "company": "Medical Mutual",
        "title": "Data Engineer",
        "created_on": 1720639220.8692563,
        "description": "There’s a reason we’ve been selected as a Top Workplace in Northeast Ohio for 14 consecutive years. At Medical Mutual, we are committed to caring for our most valuable asset – our dedicated employees. Founded in 1934, Medical Mutual is the oldest and one of the largest health insurance companies based in Ohio. We provide peace of mind to more than 1.6 million Ohioans through our high-quality health, life, disability, dental, vision and indemnity plans. We offer fully insured and self-funded group coverage, including stop loss, as well as Medicare Advantage, Medicare Supplement, and individual plans. Medical Mutual’ s status as a mutual company means we are owned by our policyholders, not stockholders, so we don’t answer to Wall Street analysts or pay dividends to investors. Instead, we focus on developing products and services that allow us to better serve our customers and the communities around us and help our members achieve their best possible health and quality of life. This position is a hybrid role with 1 day/week on site ( Brooklyn, Ohio ). Data engineer responsible for the design and development of data integration pipelines. Build data solutions for business problems and support the applied use of data to enable business insights and action. Job duties involve profiling data, developing data transformation pipelines, measuring data quality, designing data models, and supporting applied use of data to solve problems (exploratory analysis, business intelligence/dashboards, reporting, alerts). About The Role: Develop data pipelines to integrate and transform data for analysis Support other data users and enable applied use of data to solve business problems Business meetings to understand data and business needs, socialize knowledge of data products and insights, and generate consensus Data profiling, data documentation, measuring and enforcing data quality Developing data models to represent information Qualifications This position is a hybrid role with 1 day/week on site ( Brooklyn, Ohio ). Data Engineer II Education And Experience: Bachelors degree in Computer Science, Computer Information Systems, business degree with focus on data transformation or analysis, or equivalent work experience Two or more years professional experience developing software to solve business problems Technical Skills And Knowledge: Experience with data engineering system design and software implementation Experience with all parts of the systems development life cycle Experience with database technologies and query semantics Experience with cloud database technology (DataBricks, Azure Synapse, BigQuery) Experience programming in ETL or ELT technology Understanding of data warehouse data model theory and techniques Experience with Python programming language Data Engineer III Education And Experience: Bachelors degree in Computer Science, Computer Information Systems, business degree with focus on data transformation or analysis, or equivalent work experience Four or more years professional experience developing software to solve business problems Three or more years experience with data engineering system design and software implementation Technical Skills And Knowledge: Experience with public cloud database technology (DataBricks, Azure Synapse, BigQuery, SnowFlake ) Experience programming in ETL or ELT technology Experience with all parts of the systems development life cycle Experience with database technologies and query semantics Understanding of data warehouse data model theory and techniques Experience with Python programming language Data Engineer IV Education And Experience: Bachelors degree in Computer Science, Computer Information Systems, business degree with focus on data transformation or analysis, or equivalent work experience Seven or more years professional experience developing software to solve business problems Five or more years experience with data engineering system design and software implementation Technical Skills And Knowledge: Experience with cloud database technology (DataBricks, Azure, Synapse, BigQuery, SnowFlake ) Experience programming in ETL or ELT technology Proficient with all parts of the systems development life cycle Experience with database technologies and query semantics Experience leading and coordinating technical initiatives Understanding of data warehouse data model theory and techniques Experience with Python programming language Data Engineer V Education And Experience: Bachelors degree in Computer Science, Computer Information Systems, business degree with focus on data transformation or analysis, or equivalent work experience Seven or more years professional experience developing software to solve business problems Five or more years experience and demonstrated excellence with data engineering system design and software implementation Technical Skills And Knowledge: Experience with cloud database technology (DataBricks, Azure Synapse, BigQuery, SnowFlake ) Experience programming in ETL or ELT technology Demonstrated excellence with all parts of the systems development life cycle Expert in database technologies and query semantics Demonstrated excellence leading and coordinating technical initiatives Expert understanding of data warehouse data model theory and techniques Experience with Python programming language Medical Mutual Is Looking To Grow Our Team! We Truly Value And Respect The Talents And Abilities Of All Of Our Employees. That's Why We Offer An Exceptional Package That Includes: A Great Place to Work We will provide the equipment you need for this role, including a laptop, monitors, keyboard, mouse, and headset. Whether you are working remotely or in the office, employees have access to on-site fitness centers at many locations, or a gym membership reimbursement when there is no Medical Mutual facility available. Enjoy the use of weights, cardio machines, locker rooms, classes and more. On-site cafeteria, serving hot breakfast and lunch, at our Brooklyn, OH location. Convenience stores at many locations. Discounts at many places in and around town, just for being a Medical Mutual team member. The opportunity to earn cash rewards for shopping with our customers. Business casual attire, including jeans. Excellent Benefits Employee bonus program 401(k) with company match up to 4% and an additional company contribution. Health Savings Account with a company matching contribution. Excellent medical, dental, vision, life, and disability insurance — insurance is what we do best, and we make affordable coverage for our team a priority. Access to an Employee Assistance Program, which includes professional counseling, personal and professional coaching, self-help resources and assistance with work/life benefits. Company holidays and up to 16PTO days during the first year of employment with options to carry over unused PTO time. After one year of service, parental leave for eligible employees who become parents through maternity, paternity, or adoption. An Investment in You Career development programs and classes. Mentoring and coaching to help you advance in your career. Tuition reimbursement up to $5,250 per year, theIRS maximum. Diverse, inclusive, and welcoming culture with Business Resource Groups. About Medical Mutual Medical Mutual’s status as a mutual company means we are owned by our policyholders, not stockholders, so we do not answer Wall Street analysts or pay dividends to investors. Instead, we focus on developing products and services that allow us to better serve our customers and the communities around us. There is a good chance you already know many of our Medical Mutual customers. As the official insurer of everything you love, we are trusted by businesses and nonprofit organizations throughout Ohio to provide high-quality health, life, disability, dental, vision and indemnity plans. We offer fully insured and self-funded group coverage, including stop loss, as well as Medicare Advantage, Medicare Supplement, and individual plans. Our plans provide peace of mind to more than 1.5 million Ohioans. We are not just one of the largest health insurance companies based in Ohio, we are also the longest running. Founded in 1934, we are proud of our rich history with the communities where we live and work. At Medical Mutual and its family of companies we celebrate differences and are mutually invested in our employees and our community. We are proud to be an Equal Employment Opportunity and Affirmative Action Employer. Qualified applicants will receive consideration for employment regardless of race, color, religion, sex, sexual orientation, gender perception or identity, national origin, age, marital status, veteran status, or disability status. We maintain a drug-free workplace and perform pre-employment substance abuse and nicotine testing. Primary Location US-OH-Brooklyn Other Locations United States Work Locations Brooklyn Job 7 - General Staff Organization IT Infrastructure Schedule Regular Shift Standard Employee Status Individual Contributor Job Type Full-time Job Level Day Job Travel No Job Posting Jun 7, 2024, 1:49:47 PM",
        "url": "https://www.linkedin.com/jobs/view/3960764247"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Plymouth, MN",
        "job_id": 3969212074,
        "company": "Horizontal Talent",
        "title": "Data Engineer",
        "created_on": 1720639222.7248025,
        "description": "Do you have Data Engineering experience, and are you seeking a new contract in Plymouth? Horizontal Talent is helping a healthcare company find a Data Engineer to join their collaborative team, and the role comes with an attractive pay rate. As a Data Engineer, you will be responsible for the development of complex data sources and pipelines into the company's data platform (i.e. Snowflake) along with other data applications, such as Azure, Terraform, etc., automation and innovation. You will also create and maintain data pipelines using Azure and Snowflake as primary tools. During your first few weeks in this Data Engineer role, you can expect to begin work on some of the following: Create SQL stored procs and functions to perform complex transformations Understand data requirements and design optimal pipelines to fulfill the use cases Create logical & physical data models to ensure data integrity is maintained Design and build best-in-class processes to clean and standardize data. To apply for this Data Engineer role, your soft skills, expertise, and experience should include: Excellent communication skills – verbal and written Excellent knowledge of SQL Azure Services such as Blobs, Functions, Azure Data Factory, Service Principal, Containers, Key Vault, etc Knowledge of Snowflake - Architecture, Features, Best Practices Data warehousing principles, architecture, and its implementation in large environments You'll receive an excellent pay rate in return for your knowledge, expertise, and flexibility. To apply for this contract Data Engineer job in Plymouth, please reach out to Horizontal Talent today. We'd love to help you get your next role and enable you to fulfill your professional ambitions.",
        "url": "https://www.linkedin.com/jobs/view/3969212074"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3958514248,
        "company": "TekWissen ®",
        "title": "Data Engineer III",
        "created_on": 1720639224.5137362,
        "description": "Overview TekWissen Group is a workforce management provider throughout the USA and many other countries in the world. Our client is a company operating a marketplace for consumers, sellers, and content creators. It offers merchandise and content purchased for resale from vendors and those offered by thirdparty sellers. Job Title: Data Engineer III Location: Seattle, WA Duration: 11 Months Job Type: Contract Work Type: Remote Responsibilities Job Description: As a Data Engineer you will be working in one of the world's largest and most complex data warehouse environments. You will be developing and supporting the analytic technologies that give our customers timely, flexible and structured access to their data. You will be responsible for designing and implementing a platform using third-party and in-house reporting tools, modeling metadata, building reports and dashboards in Oracle BI Enterprise Edition (OBIEE). You will work with business customers in understanding the business requirements and implementing solutions to support analytical and reporting needs. Required Skills & Experience 3-6 years of related experience. Very Strong development experience with notable BI reporting tools (Oracle BI Enterprise Edition (OBIEE)). Should have experience developing complex and a variety of reports. A good candidate has strong analytical skills and enjoys working with large complex data sets. Good knowledge of SQL A good candidate can partner with business owners directly to understand their requirements and provide data which can help them observe patterns and spot anomalies. Preferred Strong OBIEE reporting experience SQL skills Typical Day In The Role Mon- Fri. 9am -6pm depending on the timezone; OT only during on-call Mainly working on warehouse Migration from Informatica to AWS 14 teams members and 2 more joining there will be a total of 18 people, but they would work with totally with 6 or 7 people Work from home environment Can flex hours; role is 100% remote Work max of 11 months- no extension or conversion Master’s degree- the contractor does not need prior experience; however, if bachelor’s degree only it requires 4-5 years of experience Best vs. average: The role can be ambiguous and the contractor would need to know how to understand the issue and troubleshoot Performance indicators: The performance will be based on how the contractor is able to troubleshoot issues Top Skills SQL Informatica Power Center Informatica Data modelling Must-have Technology: Informatica, SQL, Python Skills: Data Engineering, Data Warehousing Nice-to-have Technology: AWS Skills: Software Development, CDKs, etc TekWissen® Group is an equal opportunity employer supporting workforce diversity. TekWissen is an emerging global human capital, recruitment and IT services organization. Operating since 2009, we draw upon more than a decade of staffing experience to deliver critical talent acquisition solutions and IT engagements for our clients. We’re founded on a culture that is passionate about delivering tailored solutions, that create lasting partnerships. Our global footprint covers six countries: United States, Canada, Australia, India, United Kingdom and the Philippines. This allows us to work in close partnership with organizations and manage everything from global talent needs with demanding resourcing strategies, to single sites with lower recruitment volumes. TekWissen® is an equal opportunity employer supporting workplace diversity.",
        "url": "https://www.linkedin.com/jobs/view/3958514248"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3930920994,
        "company": "Eastport Analytics",
        "title": "Data Engineer",
        "created_on": 1720639226.5562613,
        "description": "Data Engineer (remote is an option) Eastport Analytics is hiring for a newly created Data Engineer role. You will have a critical role in bringing a wide range of external data sets to bear on financial crime analytics. You will be working with structured and unstructured data; account and transactional data; internal and external databases; relational and other architectures. You will architect, design, and implement approaches for bringing second- and first-party data together to address financial compliance, investigative, and enforcement efforts. We are looking for a data engineer experienced in data architecture and database design, ETL, data provisioning, query design and management of data engineering. About You Strong database and data warehouse experience including developing, constructing, testing, and maintaining data architectures, aligning data architecture with business requirements, data acquisition, developing data set processes, using programming language and tools, and identifying ways to improve data reliability, efficiency, and quality. Designed solutions for structured and unstructured data with 2nd party data. Created and managed data architecture, data pipelines, and processing systems for the extraction, transformation, and loading (ETL) of data from a variety of data sources. Developed robust and scalable solutions that transform data into a useful format for analysis, enhance data flow, and enable end users to consume and analyze data faster and easier. Designed, built, implemented, and maintained infrastructure necessary for storing and processing large volumes of data. Created the systems and tools used to collect and analyze information. Writes complex SQL queries to support analytics needs. Evaluates and recommends tools and technologies for data infrastructure and processing. Collaborates with engineers, data scientists, data analysts, product teams, and other stakeholders to translate business requirements to technical specifications and coded data pipelines. Works with structured and unstructured data from a variety of data stores, such as data lakes, relational database management systems, and/or data warehouses. Experience with Microsoft SQL Server and Sybase, PostgreSQL database systems. Experience developing and operating large-scale data structures for business intelligence analytics using: ETL/ELT processes, OLAP technologies, data modeling, SQL. Good communication skills with the ability to communicate clearly with all audiences about data architectures and implementation. Industry experience as a Data Engineer or related specialty (e.g. Backend Engineer, Systems Administrator/Database Administrator) with a track record of manipulating, processing, and extracting value from large datasets. Completed a Bachelor’s degree in quantitative or data intensive field or equivalent related. US Citizen or LPR status with at least 3 consecutive years of living in the US since obtaining your LPR status. Unfortunately, this is not an option. Must be able to successfully pass a government background investigation and obtain a government public trust clearance. Flexible to work and collaborate during core business hours Eastern standard time. As an ideal candidate, you will have: Solid understanding of data modeling in a NoSQL database, with experience building data pipelines to ingest and transform data, maintaining schemas, and developing APIs to leverage data in software applications. Strong experience with structured databases & newer tools for network graph database, text analysis and big data. Government contracting experience or industry experience in Fintech, financial provisioning, financial data interaction. Coding proficiency in at least one modern programming language (Python, Ruby, Java, etc.). Experience building /operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets. Active Public Trust clearance or above Local residency in MD/DC/VA area. About Eastport Federal law enforcement and compliance professionals face constantly evolving challenges and are inundated with data about these challenges. At Eastport, we work closely with clients to deliver the insights needed to understand these behaviors, so our clients can focus on driving compliance and building better cases. Our talented technical staff of analysts, engineers, data scientists, architects, and consultants work closely with clients to define the analytics required; improve the breadth, depth, and fidelity of information available; deliver actual insights; and build insight delivery into the client business processes. As a small thriving company who has been around for 25 plus years, Eastport Analytics offers a full suite of benefits including a balanced work life culture, robust leave program and radical flex time, employer funded health reimbursement account (HRA) to cover deductibles for health insurance, employer funded 401K, professional development, and an inclusive professional work environment, fun quarterly events, and more. To learn more about Eastport Analytics, please visit us at https://www.eastportanalytics.com/ and come join our team! Eastport Analytics, Inc. is an equal opportunity employer and committed to the full inclusion of all qualified individuals. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, disability, age, sexual orientation, gender identity, national origin, hair texture or protected hairstyle, veteran status, or genetic information. Eastport Analytics, Inc. is also committed to providing equal opportunity and access to individuals with disabilities by ensuring reasonable accommodations are provided to participants in the job application or interview process. To request a reasonable accommodation, contact hr@eastportanalytics.com or 703-600-1900.",
        "url": "https://www.linkedin.com/jobs/view/3930920994"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Bentonville, AR",
        "job_id": 3953822629,
        "company": "Walmart",
        "title": "(USA) Data Engineer III",
        "created_on": 1720639228.1741679,
        "description": "Position Summary... What you'll do... As a Data Engineer, candidate will be responsible for developing and maintaining data pipeline, implementing ETL processes, and ensuring the accuracy and availability of data. About Team Focusing on customer, associate and business needs, this team works with Walmart International, which includes more than 5,200 retail units, operating in 23 countries such as Canada, Central America, Chile, China, India, Mexico and South Africa to name a few. What You'll Do Design and develop data pipeline solutions using MSSQL, SSIS, and other ETL tools. Implement efficient and scalable ETL processes to extract, transform, and load data from various sources into our data warehouse. Proficient in distributed computing ex: Spark with Java/Scala. Preferred to have Java over Scala Proficient in SQL Proficient in Kafka and Spark streaming Experience in Workflow management using Airflow or similar tool Experience in dealing with large volumes of data (Flat files, APIs and streaming) Google Cloud Platform with BigQuery and DataProc is a plus Knowledge in data modeling What You'll Bring Strong experience in MSSQL, SSIS and other ETL tools. Good data mapping knowledge. Proficiency in SQL Data pipeline creations Good knowledge on data modeling. About Walmart Global Tech Imagine working in an environment where one line of code can make life easier for hundreds of millions of people. That's what we do at Walmart Global Tech. We're a team of software engineers, data scientists, cybersecurity expert's and service professionals within the world's leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered. We train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail. Flexible, Hybrid Work We use a hybrid way of working that is primarily in office coupled with virtual when not onsite. Our campuses serve as a hub to enhance collaboration, bring us together for purpose and deliver on business needs. This approach helps us make quicker decisions, remove location barriers across our global team and be more flexible in our personal lives. Benefits Benefits: Beyond our great compensation package, you can receive incentive awards for your performance. Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and much more. Equal Opportunity Employer Walmart, Inc. is an Equal Opportunity Employer - By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing diversity- unique styles, experiences, identities, ideas and opinions - while being inclusive of all people. The above information has been designed to indicate the general nature and level of work performed in the role. It is not designed to contain or be interpreted as a comprehensive inventory of all responsibilities and qualifications required of employees assigned to this job. The full Job Description can be made available as part of the hiring process. At Walmart, we offer competitive pay as well as performance-based bonus awards and other great benefits for a happier mind, body, and wallet. Health benefits include medical, vision and dental coverage. Financial benefits include 401(k), stock purchase and company-paid life insurance. Paid time off benefits include PTO (including sick leave), parental leave, family care leave, bereavement, jury duty, and voting. Other benefits include short-term and long-term disability, company discounts, Military Leave Pay, adoption and surrogacy expense reimbursement, and more. ‎ ‎ ‎ You will also receive PTO and/or PPTO that can be used for vacation, sick leave, holidays, or other purposes. The amount you receive depends on your job classification and length of employment. It will meet or exceed the requirements of paid sick leave laws, where applicable. ‎ For information about PTO, see https://one.walmart.com/notices . ‎ ‎ Live Better U is a Walmart-paid education benefit program for full-time and part-time associates in Walmart and Sam's Club facilities. Programs range from high school completion to bachelor's degrees, including English Language Learning and short-form certificates. Tuition, books, and fees are completely paid for by Walmart. ‎ Eligibility requirements apply to some benefits and may depend on your job classification and length of employment. Benefits are subject to change and may be subject to a specific plan or program terms. ‎ For Information About Benefits And Eligibility, See One.Walmart . ‎ The annual salary range for this position is $90,000.00-$180,000.00 ‎ Additional Compensation Includes Annual Or Quarterly Performance Bonuses. ‎ Additional Compensation For Certain Positions May Also Include ‎ ‎ Stock ‎ ‎ Minimum Qualifications... Outlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications. Option 1: Bachelor's degree in Computer Science and 2 years' experience in software engineering or related field. Option 2: 4 years' experience in software engineering or related field. Option 3: Master's degree in Computer Science. Preferred Qualifications... Outlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications. Data engineering, database engineering, business intelligence, or business analytics, Master's degree in Computer Science or related field and 2 years' experience in software engineering or related field, We value candidates with a background in creating inclusive digital experiences, demonstrating knowledge in implementing Web Content Accessibility Guidelines (WCAG) 2.2 AA standards, assistive technologies, and integrating digital accessibility seamlessly. The ideal candidate would have knowledge of accessibility best practices and join us as we continue to create accessible products and services following Walmart's accessibility standards and guidelines for supporting an inclusive culture. Primary Location... 2501 Se J St, Ste A, Bentonville, AR 72716-3724, United States of America",
        "url": "https://www.linkedin.com/jobs/view/3953822629"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Bellevue, WA",
        "job_id": 3967793107,
        "company": "Apex Systems",
        "title": "Senior Data Engineer",
        "created_on": 1720639231.5297778,
        "description": "*Apex Systems is hiring a hybrid Data Engineer II for a client in Bellevue, WA!* Job Summary The client is looking for a Data Engineer to join the Infrastructure Automation team. The automation team is responsible for delivering the software that powers the client's infrastructure. Responsibilities As a Data Engineer you will be working in one of the world's largest and most complex data transformation environments in AWS. You will be developing and improving data tables (through API, SNS topics, S3) that give our customers timely, flexible and structured access to their data. You will be responsible for designing and implementing metrics within a platform using third-party and in-house development tools, modeling metadata, building reports and dashboards. You will work with business customers in understanding the business requirements and implementing solutions to support analytical and reporting needs. Able to build scalable solutions along with adherence to privacy/security standards Required Skills & Experience: Bachelor's degree in STEM 7-10 years of related experience. 5+ years’ experience with AWS services (Lambda, Gateway, SNS, Firehose, etc.) Experience in at least one modern scripting or programming language, such as Python, Java, Scala, or NodeJS Good knowledge of SQL and other reporting solutions like Quicksight or Tableau Should have experience developing complex data models and implementation across large scale systems. A good candidate has strong analytical skills and enjoys working with large complex data sets. A good candidate can partner with business owners directly to understand their requirements and provide data which can help them observe patterns and spot anomalies. 10 years AWS experience preferred Prior e-commerce experience preferred",
        "url": "https://www.linkedin.com/jobs/view/3967793107"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Washington, United States",
        "job_id": 3966689094,
        "company": "hackajob",
        "title": "Data Engineer",
        "created_on": 1720639233.3388207,
        "description": "hackajob has partnered with a global leader in Microsoft Technologies looking for a talented Data Engineer with expertise in SQL databases to join our dynamic team. In this role, you will be responsible for designing, developing, and maintaining our data architecture, with a focus on SQL database management The role: Data Engineer Location: Hybrid in Washington, DC Offer: up to $170k yr. (depending on experience) + benefits package Qualifications: Active Security Clearance required Bachelor’s degree in Computer Science, Engineering, Mathematics, or a related field; or equivalent work experience. Proven experience as a Data Engineer or similar role with a strong focus on SQL database management. In-depth knowledge of SQL and experience with relational database management systems (e.g., MySQL, PostgreSQL, SQL Server). Experience with ETL tools and techniques. Proficiency in scripting languages such as Python, Shell scripting, or similar. Strong understanding of data warehousing concepts and methodologies. Familiarity with cloud platforms (e.g., AWS, Azure, GCP) and database services. Excellent problem-solving and analytical skills. Ability to work effectively both independently and as part of a team. Strong communication skills and the ability to collaborate with stakeholders at all levels. If you're interested in finding out more about this fantastic opportunity, please get your application in and we can arrange a call. hackajob is a recruitment platform that will match you with relevant roles based on your preferences and in order to be matched with the roles you need to create an account with us. *This role requires you to be based in the US*",
        "url": "https://www.linkedin.com/jobs/view/3966689094"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Atlanta Metropolitan Area",
        "job_id": 3931575985,
        "company": "WebMD",
        "title": "Data Engineer",
        "created_on": 1720639234.912916,
        "description": "WebMD Ignite is a full service growth engine for health systems and health-centric organizations. Our comprehensive experience, datasets and breadth of capabilities maximize our partners' marketing investments resulting in an enhanced reputation, deeper loyalty and profitable growth. Position Summary The Web Data Engineer is responsible for developing and maintaining a robust data management framework supporting the implementation of our health system client web products. Key responsibilities include building and managing database structures, building and supporting API based data integrations, articulating data nuances to a team of cross functional front end developers, project managers and clients.The ideal candidate will have strong database development and management experience, proficiency in APIs, data migrations and integrations, and a solid understanding and data transformation techniques. Primary Responsibilities & Essential Functions Essential Functions: Collaborate with healthcare clients and internal stakeholders to understand data requirements and develop solutions for web data extraction, transformation, and loading. Design and implement robust and scalable data pipelines to collect and integrate web data from healthcare websites, portals, APIs, and other sources. Ensure the accuracy, completeness, and quality of healthcare data collected through web scraping and other techniques, adhering to data governance and compliance requirements. Stay abreast of regulatory changes and industry trends in healthcare data management and privacy (e.g., HIPAA, GDPR) and incorporate best practices into data engineering processes. Monitor data pipeline/integration performance, troubleshoot issues, and optimize processes for efficiency and scalability. Document data pipelines/integrations, processes, and methodologies, and provide support to internal teams and clients as needed. Assist in defining the data management strategy across data domains and products. Oversee data transformation, normalization, cleansing, aggregation, workflow management, and business rule application. Create and maintain documentation to support developed processes and applications. Perform quality assurance checks on data integrity and governance processes. Load, process, and migrate incoming data feeds, as well as create outgoing data extracts. Minimum Required Knowledge, Skills, Abilities and Qualifications Qualifications Bachelor's degree in Computer Science, Information Technology, or related field. Proven experience as a Data Engineer or similar role in product development. SQL competence (query performance tuning) and a grasp of database structure is required. Knowledge of at least one ETL tool (Informatica, SSIS, Talend, Kofax RPA, etc.) Experience with ETL tools, data pipelines, data modeling, and data integration techniques. Knowledge of data quality management, data governance, and performance monitoring. Familiarity with cloud platforms (e.g., AWS, Azure) and related services for data processing and storage. Proficiency in programming languages commonly used in web scraping and data engineering, such as Python, R, or Java, Experience with Drupal 10 is a strong plus Experience with Kofax Kapow Robotic Process Automation (RPA) is a very strong plus Attributes Strong attention to detail to ensure accuracy and reliability of data solutions, data integration processes, and data quality management. Effective problem-solving and analytical skills to troubleshoot data issues, optimize data processes, and address challenges during product development and client implementations. Ability to collaborate effectively with cross-functional teams, including project managers, engineers, implementation teams, and clients, to achieve successful outcomes. Customer-centric mindset with a focus on delivering value to clients through effective data solutions, data integration, and technical support during implementations. Commitment to continuous learning and professional development in the field of data engineering, staying updated with industry trends, technologies, and best practices.",
        "url": "https://www.linkedin.com/jobs/view/3931575985"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "New York City Metropolitan Area",
        "job_id": 3969262276,
        "company": "Brigit",
        "title": "Senior Data Engineer",
        "created_on": 1720639236.549662,
        "description": "Hi, we're Brigit! A holistic financial health company helping every American build a brighter financial future. With a business model that is aligned with our customers, we create transparent, fair, and simple financial products that put money back in the hands of our members, help them spend wisely, avoid unfair fees and build their credit quickly. If autonomy, ownership, and having meaningful input at the company you work for is important to you, come join our growing team! Brigit is doing innovative and exciting work, but don’t just take our word for it, our work is being recognized by others: Built In’s 2023 & 2024 Best Startups to Work For In New York City Built In’s 2024 Best Startups to Work For In the U.S. Fast Company’s Most Innovative Companies of 2022 Forbes Fintech 50 2022 Business Insider’s Most Promising Consumer Startups 2022 The Role We’re hiring a Senior Data Engineer to join our team responsible for building Brigit’s data platforms. You'll work closely with our Analytics Engineering, Engineers, Analysts, Data Scientists, and Product Managers as a thought partner, technical leader, and resource. This is a unique opportunity to build the data strategy and architecture at a data-driven startup that’s changing the world of FinTech. Today, our stack consists of Fivetran, Snowflake, dbt, Metaplane and Mode. Over the next 12-18 months you’ll help us architect and implement the data strategy for new products and new partners while maturing our current platform. You’ll be working closely with the Head of Data and Analytics to launch our new BI tool, with the Analytics Engineer to mature our instance of dbt and several Engineering and Product leads to help us think through the data flows in new and enhanced product offerings. What you’ll do: Data Infrastructure Development: Design, develop, and maintain data pipelines and data models to provide valuable business insights Optimize data processing, storage, and retrieval capabilities with appropriate technologies, tools, and frameworks. Proactively monitor, maintain and optimize our data platform to ensure data systems' reliability, performance, and security. Collaboration and Project Management: Drive cross-functional projects to improve our data capture techniques and know-how. Liaise between data and engineering teams to align needs and priorities. Work with technical and non-technical stakeholders to explain complex technical concepts, present project updates, and gather feedback. Process Improvement and Best Practices: Shape a holistic data quality strategy that ensures accuracy, consistency, and reliability across our organization's data landscape. Establish and promote data engineering best practices and standards. Identify opportunities for process improvement and automation. Who you are: You have 6+ years of experience as a data engineer coding in Python and using SQL, preferably in a fast-paced consumer finance or fintech startup. Experience with building off-the-shelf and custom data pipelines on cloud infrastructure. Proficiency with data warehouse administration, operations, and optimization. Fluency with dbt and analytics engineering. Familiarity with a variety of data sources such as relational databases, APIs, SFTP, and customer data platforms (CDPs). Excellent problem-solving skills and ability to work in a fast-paced, dynamic environment The anticipated annual base salary for this position is $155,000 - $180,000. This range does not include any other compensation components or other benefits for which an individual may be eligible. The actual base salary offered depends on a variety of factors, which may include as applicable, the qualifications of the individual applicant for the position, years of relevant experience, specific and unique skills, level of education attained, certifications or other professional licenses held, and the location in which the applicant lives and/or from which they will be performing the job. Our Benefits & Team Medical, dental, and vision insurance Equity participation Flexible PTO Policy 401k plan Paid Parental Leave Physical and mental wellbeing benefits including Wellhub for access to virtual workouts and discounted gym memberships, and Headspace for covered virtual therapy sessions and unlimited on demand health support Monthly reimbursements to use against wifi and cell phone bills Annual reimbursement for Learning & Development Help hard working Americans build a brighter financial future High-growth company at an early stage A dynamic, flexible and collaborative start-up work environment with a highly talented team Brigit is committed to providing equal employment opportunities for all applicants and employees without regard to race, religion, color, sex, pregnancy (including breast feeding and related medical conditions), national origin, citizenship status, uniform service member status, age, genetic information, disability, or any other protected status in accordance with all applicable federal, state and local laws. We are proud to be an equal opportunity workplace.",
        "url": "https://www.linkedin.com/jobs/view/3969262276"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Atlanta, GA",
        "job_id": 3931938784,
        "company": "Medscape",
        "title": "Data Engineer",
        "created_on": 1720639238.5586052,
        "description": "Description Position at WebMD WebMD Ignite is a full service growth engine for health systems and health-centric organizations. Our comprehensive experience, datasets and breadth of capabilities maximize our partners' marketing investments resulting in an enhanced reputation, deeper loyalty and profitable growth. Position Summary The Web Data Engineer is responsible for developing and maintaining a robust data management framework supporting the implementation of our health system client web products. Key responsibilities include building and managing database structures, building and supporting API based data integrations, articulating data nuances to a team of cross functional front end developers, project managers and clients.The ideal candidate will have strong database development and management experience, proficiency in APIs, data migrations and integrations, and a solid understanding and data transformation techniques. Essential Functions Primary Responsibilities & Essential Functions Collaborate with healthcare clients and internal stakeholders to understand data requirements and develop solutions for web data extraction, transformation, and loading. Design and implement robust and scalable data pipelines to collect and integrate web data from healthcare websites, portals, APIs, and other sources. Ensure the accuracy, completeness, and quality of healthcare data collected through web scraping and other techniques, adhering to data governance and compliance requirements. Stay abreast of regulatory changes and industry trends in healthcare data management and privacy (e.g., HIPAA, GDPR) and incorporate best practices into data engineering processes. Monitor data pipeline/integration performance, troubleshoot issues, and optimize processes for efficiency and scalability. Document data pipelines/integrations, processes, and methodologies, and provide support to internal teams and clients as needed. Assist in defining the data management strategy across data domains and products. Oversee data transformation, normalization, cleansing, aggregation, workflow management, and business rule application. Create and maintain documentation to support developed processes and applications. Perform quality assurance checks on data integrity and governance processes. Load, process, and migrate incoming data feeds, as well as create outgoing data extracts. Qualifications Minimum Required Knowledge, Skills, Abilities and Qualifications Bachelor's degree in Computer Science, Information Technology, or related field. Proven experience as a Data Engineer or similar role in product development. SQL competence (query performance tuning) and a grasp of database structure is required. Knowledge of at least one ETL tool (Informatica, SSIS, Talend, Kofax RPA, etc.) Experience with ETL tools, data pipelines, data modeling, and data integration techniques. Knowledge of data quality management, data governance, and performance monitoring. Familiarity with cloud platforms (e.g., AWS, Azure) and related services for data processing and storage. Proficiency in programming languages commonly used in web scraping and data engineering, such as Python, R, or Java, Experience with Drupal 10 is a strong plus Experience with Kofax Kapow Robotic Process Automation (RPA) is a very strong plus Attributes Strong attention to detail to ensure accuracy and reliability of data solutions, data integration processes, and data quality management. Effective problem-solving and analytical skills to troubleshoot data issues, optimize data processes, and address challenges during product development and client implementations. Ability to collaborate effectively with cross-functional teams, including project managers, engineers, implementation teams, and clients, to achieve successful outcomes. Customer-centric mindset with a focus on delivering value to clients through effective data solutions, data integration, and technical support during implementations. Commitment to continuous learning and professional development in the field of data engineering, staying updated with industry trends, technologies, and best practices.",
        "url": "https://www.linkedin.com/jobs/view/3931938784"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Bentonville, AR",
        "job_id": 3950764417,
        "company": "Walmart",
        "title": "Data Engineer III",
        "created_on": 1720639240.318104,
        "description": "Position Summary... What you'll do... We are seeking a highly motivated and talented Data Engineer to join our dynamic team. As a Data Engineer, you will play a critical role in designing, developing, and implementing data pipelines and data integration solutions using Spark, Scala, and Google Cloud Platform (GCP). You will be responsible for building scalable and efficient data processing systems, optimizing data workflows, and ensuring data quality and integrity. About Team Everyone has data, but the sheer volume of data at Walmart can be limitless. In the Data Engineering team, we help Walmart manage this data by building pipelines and data lakes to prepare big data for analysis and unlocking actionable insights in real-time. We also use cross-departmental data and machine learning to build a holistic view of true profitability, saving millions of dollars across item categories and geographies while assisting our leadership in making better decisions faster. What You'll Do Collaborate with cross-functional teams to understand data requirements and design data solutions that meet business needs Develop and maintain data pipelines and ETL processes using Spark and Scala Design, build, and optimize data models and data architecture for efficient data processing and storage Implement data integration and data transformation workflows to ensure data quality and consistency Monitor and troubleshoot data pipelines to ensure data availability and reliability Conduct performance tuning and optimization of data processing systems for improved efficiency and scalability Work closely with data scientists and analysts to provide them with the necessary data sets and tools for analysis and reporting Stay up-to-date with the latest industry trends and technologies in data engineering and apply them to enhance the data infrastructure What You'll Bring Proven working experience as a Data Engineer with a minimum of 3 years in the field. Strong programming skills in Scala and experience with Spark for data processing and analytics Familiarity with Google Cloud Platform (GCP) services such as BigQuery, GCS, Dataproc, Pub/Sub, etc. Experience with data modeling, data integration, and ETL processes Strong knowledge of SQL and database systems Understanding of data warehousing concepts and best practices Proficiency in working with large-scale data sets and distributed computing frameworks Strong problem-solving and analytical skills Excellent communication and teamwork abilities About Walmart Global Tech Imagine working in an environment where one line of code can make life easier for hundreds of millions of people. That's what we do at Walmart Global Tech. We're a team of software engineers, data scientists, cybersecurity expert's and service professionals within the world's leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered. We train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail. Flexible, Hybrid Work We use a hybrid way of working that is primarily in office coupled with virtual when not onsite. Our campuses serve as a hub to enhance collaboration, bring us together for purpose and deliver on business needs. This approach helps us make quicker decisions, remove location barriers across our global team and be more flexible in our personal lives. Benefits Benefits: Beyond our great compensation package, you can receive incentive awards for your performance. Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and much more. Equal Opportunity Employer Walmart, Inc. is an Equal Opportunity Employer - By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing diversity- unique styles, experiences, identities, ideas and opinions - while being inclusive of all people. The above information has been designed to indicate the general nature and level of work performed in the role. It is not designed to contain or be interpreted as a comprehensive inventory of all responsibilities and qualifications required of employees assigned to this job. The full Job Description can be made available as part of the hiring process. At Walmart, we offer competitive pay as well as performance-based bonus awards and other great benefits for a happier mind, body, and wallet. Health benefits include medical, vision and dental coverage. Financial benefits include 401(k), stock purchase and company-paid life insurance. Paid time off benefits include PTO (including sick leave), parental leave, family care leave, bereavement, jury duty, and voting. Other benefits include short-term and long-term disability, company discounts, Military Leave Pay, adoption and surrogacy expense reimbursement, and more. ‎ ‎ ‎ You will also receive PTO and/or PPTO that can be used for vacation, sick leave, holidays, or other purposes. The amount you receive depends on your job classification and length of employment. It will meet or exceed the requirements of paid sick leave laws, where applicable. ‎ For information about PTO, see https://one.walmart.com/notices . ‎ ‎ Live Better U is a Walmart-paid education benefit program for full-time and part-time associates in Walmart and Sam's Club facilities. Programs range from high school completion to bachelor's degrees, including English Language Learning and short-form certificates. Tuition, books, and fees are completely paid for by Walmart. ‎ Eligibility requirements apply to some benefits and may depend on your job classification and length of employment. Benefits are subject to change and may be subject to a specific plan or program terms. ‎ For Information About Benefits And Eligibility, See One.Walmart . ‎ The annual salary range for this position is $90,000.00-$180,000.00 ‎ Additional Compensation Includes Annual Or Quarterly Performance Bonuses. ‎ Additional Compensation For Certain Positions May Also Include ‎ ‎ Stock ‎ ‎ Minimum Qualifications... Outlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications. Option 1: Bachelor's degree in Computer Science and 2 years' experience in software engineering or related field. Option 2: 4 years' experience in software engineering or related field. Option 3: Master's degree in Computer Science. Preferred Qualifications... Outlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications. Data engineering, database engineering, business intelligence, or business analytics, Master's degree in Computer Science or related field and 2 years' experience in software engineering or related field, We value candidates with a background in creating inclusive digital experiences, demonstrating knowledge in implementing Web Content Accessibility Guidelines (WCAG) 2.2 AA standards, assistive technologies, and integrating digital accessibility seamlessly. The ideal candidate would have knowledge of accessibility best practices and join us as we continue to create accessible products and services following Walmart's accessibility standards and guidelines for supporting an inclusive culture. Primary Location... 2501 Se J St, Ste A, Bentonville, AR 72716-3724, United States of America",
        "url": "https://www.linkedin.com/jobs/view/3950764417"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Seattle, WA",
        "job_id": 3843670312,
        "company": "Intelliswift Software",
        "title": "Data Engineer - Remote",
        "created_on": 1720639242.1836207,
        "description": "Job Title: Data Engineer Location: Remote Duration: 10 months Contract Type: W2 only Pay Rate Range: $45-$50.72/Hour Must Have SQL, Python, Cloud experience(google). Need to be complete hands-on with writing code Experience/Qualifications 2-4 years of professional experience in software/data engineering Reasonable understanding of entity relationships Working grasp of data modelling Good working knowledge of SQL and at least one core programming language (Python, Scala, Java, C#, R, etc.) General understanding of batch and streaming data patterns and technologies Good grasp of TDD and data validation/QA Knowledge of CI/CD practices Exposure to Big Data scenarios and distributed computing tools/frameworks, experience in our tool set a plus Exposure to PaaS offerings in public cloud environments.",
        "url": "https://www.linkedin.com/jobs/view/3843670312"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3959391359,
        "company": "CitiusTech",
        "title": "Lead Data Engineer",
        "created_on": 1720639244.0442688,
        "description": "Lead Data Engineer Location: Remote Skills: Lead a Team, Snowflake, SQL, ETL/DWH, DevOps (CICD, Jenkins) Summary: This role is about developing, debugging, and testing applications, transforming data, and solving complex business problems. It requires a bachelor’s degree or equivalent experience, strong SQL knowledge, and experience with various technologies. The role also involves adherence to departmental and corporate policies, regular status reporting, and may include on-call duties. Responsibilities: Lead, Mentor and develop offshore Data Engineers in adopting best practices and deliver data products. Programming, debugging, and testing applications Transforming data Consulting support for IT and Business partners Developing efficient and maintainable code Solving complex business problems Preparing system test plans and documentation Qualifications: Bachelor’s degree in a relevant field or equivalent experience 4+ years of experience in database development Strong SQL knowledge Good communication skills Knowledge of data architecture patterns",
        "url": "https://www.linkedin.com/jobs/view/3959391359"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Nashville, TN",
        "job_id": 3942683735,
        "company": "TailorCare",
        "title": "Data Engineer II",
        "created_on": 1720639245.777794,
        "description": "About TailorCare TailorCare is transforming the experience of specialty care. Our comprehensive care program takes a deeply personal, evidence-based approach to improving patient outcomes for joint, back, and muscle conditions. By combining a careful assessment of patients' symptoms, health histories, preferences, and goals with predictive data and latest evidence-based guidelines, we help patients choose—and navigate—the most effective treatment pathway for them, every step of the way. TailorCare values the experiences and perspectives of individuals from all backgrounds. We are a highly collaborative, curious, and determined team passionate about scaling a high-growth start-up to improve the lives of those in pain. TailorCare is a remote-first company with a hybrid office in Nashville. About the Role: We are seeking a talented Data Engineer to join our Data team at TailorCare. In this crucial role, you will be integral to the creation, maintenance, and ongoing enhancement of our data platform and infrastructure. You will develop and optimize our cloud infrastructure and data pipelines, empowering various stakeholders, including analytics and reporting, data science, clinical operations, and product and engineering teams. Primary Responsibilities: Develop and maintain data pipelines to consolidate information from diverse source systems, ensuring seamless data integration. Utilize a diverse set of programming languages, including Scala, Python, and SQL, to write traditional code and serverless functions tailored to specific tasks, ensuring code quality, maintainability, and scalability. Build APIs and data microservices to share our data across and outside of the organization, and write interfaces to public data sets to enrich our analytics data stores Implement robust data quality checks to ensure the accuracy, consistency, and reliability of the data. Work closely with cross-functional teams, including data scientists, analysts, and product managers, to understand their data needs and provide appropriate solutions. Follow agile principles to enhance collaboration, automation, and continuous improvement in the development process. Qualifications: Bachelor's degree in Computer Science, Computer Engineering, Information Systems, or equivalent experience required 2+ years' experience building scalable, real-time, and high-performance cloud data lake solutions Skills: Strong experience with relational SQL and programming languages such as Python, Scala, or Java Experience with source control tools such as GitHub and CI/CD processes Understands distributed computing technologies such as Hadoop/Spark Built pipelines using Databricks and well versed with their APIs Experience working with Big Data streaming services such as Kinesis, Kafka, etc. Familiar with AWS cloud infrastructure and security Experience with IaaC tools like Terraform Able to work in a fast-paced environment and multi-task effectively Strong work ethic, good judgment, and decision-making skills Excellent organizational and prioritization skills Understands healthcare coding standards, including CPT, HCPCS, ICD10 Procedure, ICD10 Diagnostic, DRG, MS-DRG, APR-DRG, etc. Propensity for building infrastructure and designing processes rather than relying on those already in place Openness to stepping in and helping other teams/ functional areas when needed Ability to take on ambiguous projects, determine requirements, and successfully complete them What's In It For You Meaningful work each day, we care deeply about our mission, our patients, and each-other. Work from anywhere in the US that best fits your lifestyle, or, for those that enjoy an in-person environment, join teammates in our hybrid hub Nashville. Rich PTO and holiday plans to ensure you have time away to rest and recharge We offer paid parental leave, support a healthy work-life integration, and offer work flexibility – we love to talk about our pets and families. Medical, dental, vision, life, disability, wellness resources, and an employer HSA contribution all from Day 1. We are committed to fair and equitable pay for all employees, and we help you achieve your future goals with an employer match 401k. An inclusive workplace where you can lean on your teammates, offer candid feedback, and bring your true self to work each day. TailorCare seeks to recruit and retain staff from diverse backgrounds and encourages qualified candidates to apply. TailorCare is an equal opportunity employer and does not discriminate on the basis of age, sex, gender identity/expression, sexual orientation, color, race, creed, national origin, ancestry, religion, marital status, political belief, physical or mental disability, pregnancy, military, or veteran status.",
        "url": "https://www.linkedin.com/jobs/view/3942683735"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Cambridge, MA",
        "job_id": 3928604100,
        "company": "GSK",
        "title": "Data Engineer",
        "created_on": 1720639247.4780982,
        "description": "Site Name: USA - Massachusetts - Cambridge Posted Date: Jun 13 2024 Hybrid role at Cambridge Park Drive, MA - requires 2-3 days on-site per week. We are seeking a Software Engineer/Data Engineer who is passionate about improving patients' quality of life. The successful candidate will join our team to create solutions for collecting, storing, automating processes, and analyzing extensive sets of small molecule-protein interaction data. The primary focus will be on selecting optimal methods for these tasks, implementing, maintaining, and monitoring these solutions throughout their lifecycle, and integrating these methods with the company-wide information architecture. The role also involves supporting legacy systems, maintaining the current user interface, and helping to define and develop next-generation solutions on the Cloud platform. This role offers an exciting opportunity to revolutionize our data collection and storage infrastructure. You will be an integral part of the Encoded Technology (ET) Scientific Computing team, collaborating with biologists, chemists, and data scientists who are dedicated to advancing ET technology. You will also have the chance to interface and collaborate with analysts and engineers in the GSK AI/ML groups, R&D infrastructure, and computational chemistry/biology teams. Key Responsibilities: Select and integrate in-house software packages with Big Data tools and frameworks. Create, document, and maintain custom software solutions for the analysis of affinity selection results. Implement Extract Transform Load (ETL) processes for Encoded Technology assay data. Monitor pipeline performance and advise on any necessary infrastructure changes. Maintain the existing user interface and help define and implement the next generation user interface that enables scientists to track data and perform simple analysis. Why you? Basic Qualifications: We are looking for professionals with these required skills to achieve our goals: BS/MS in Computer Science, Data Engineering, or a similar discipline. 2+ years of professional software engineering experience. Experience with Linux and Python programming. Experience with Cloud platform. Preferred Qualifications: If you have the following characteristics, it would be a plus: Ability to work independently. Experience implementing, documenting, and maintaining data and automated analytic pipelines. Demonstrated ability to translate requirements into technical specifications. Experience working with large data sets, and performance and query optimization. Experience with database design & data modeling. Strong interpersonal skills and the ability to communicate complex concepts to chemists, biologists, and data scientists. Skills: Python3, C++, Shell Script, SQL, Google Cloud Platform using Big Query, Cloud Run, Cloud Function, Dataflow and Dataform, Nextflow, High Power Computing (HPC), Slurm, Github, Docker. Please visit  GSK US Benefits Summary to learn more about the comprehensive benefits program GSK offers US employees. Why GSK? Uniting science, technology and talent to get ahead of disease together. GSK is a global biopharma company with a special purpose – to unite science, technology and talent to get ahead of disease together – so we can positively impact the health of billions of people and deliver stronger, more sustainable shareholder returns – as an organisation where people can thrive. We prevent and treat disease with vaccines, specialty and general medicines. We focus on the science of the immune system and the use of new platform and data technologies, investing in four core therapeutic areas (infectious diseases, HIV, respiratory/ immunology and oncology). Our success absolutely depends on our people. While getting ahead of disease together is about our ambition for patients and shareholders, it’s also about making GSK a place where people can thrive. We want GSK to be a place where people feel inspired, encouraged and challenged to be the best they can be. A place where they can be themselves – feeling welcome, valued, and included. Where they can keep growing and look after their wellbeing. So, if you share our ambition, join us at this exciting moment in our journey to get Ahead Together. If you require an accommodation or other assistance to apply for a job at GSK, please contact the GSK Service Centre at 1-877-694-7547 (US Toll Free) or +1 801 567 5155 (outside US). GSK is an Equal Opportunity Employer and, in the US, we adhere to Affirmative Action principles. This ensures that all qualified applicants will receive equal consideration for employment without regard to race, color, national origin, religion, sex, pregnancy, marital status, sexual orientation, gender identity/expression, age, disability, genetic information, military service, covered/protected veteran status or any other federal, state or local protected class. Important notice to Employment businesses/ Agencies GSK does not accept referrals from employment businesses and/or employment agencies in respect of the vacancies posted on this site. All employment businesses/agencies are required to contact GSK's commercial and general procurement/human resources department to obtain prior written authorization before referring any candidates to GSK. The obtaining of prior written authorization is a condition precedent to any agreement (verbal or written) between the employment business/ agency and GSK. In the absence of such written authorization being obtained any actions undertaken by the employment business/agency shall be deemed to have been performed without the consent or contractual agreement of GSK. GSK shall therefore not be liable for any fees arising from such actions or any fees arising from any referrals by employment businesses/agencies in respect of the vacancies posted on this site. Please note that if you are a US Licensed Healthcare Professional or Healthcare Professional as defined by the laws of the state issuing your license, GSK may be required to capture and report expenses GSK incurs, on your behalf, in the event you are afforded an interview for employment. This capture of applicable transfers of value is necessary to ensure GSK’s compliance to all federal and state US Transparency requirements. For more information, please visit GSK’s Transparency Reporting For the Record site.",
        "url": "https://www.linkedin.com/jobs/view/3928604100"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Boston, MA",
        "job_id": 3950341152,
        "company": "Quintrix, by Mindlance",
        "title": "Junior AWS Data engineer",
        "created_on": 1720639249.1118069,
        "description": "Job Title: AWS Data Engineer Duration: Full-time Locations: Boston, MA (Onsite) Responsibilities: Assemble large, complex data sets that meet functional / non-functional business requirements. Create and maintain optimal data pipeline architecture. Devise and execute continual improvement initiatives in all Data Management Service delivery and technology, with a focus on delivery velocity and quality. Partner with business leaders to determine and prioritize delivery initiatives. Devise and execute in software development life cycle including requirements gathering, development, testing, release management, and maintenance. Design and develop enterprise data / data architecture solutions using AWS Glue, Lambda and other data technologies like Spark, Scala, Java, Python, SQL etc. Design and develop machine learning algorithms and AI models for business requirements. Run machine learning tests and experiments, and document findings and results. Train, retrain, and monitor machine learning systems and models as needed. Engage with business partners to report (formally and informally) on technology strengths, weaknesses, successes, and challenges on a regular basis. Will be responsible for design and delivery of data solutions to empower data migration initiatives, BI initiatives, dashboards development etc. Qualifications: 0-2 years of experience in the areas of Data Management, BI Architecture, Product Development, RDBMS and non-RDBMS platforms. 0-2 years of experience in Python & Spark, AWS Glue, Java, Scala, Lambda, etc. Knowledge of SQL and database concepts Knowledge of python Libraries like Pandas, NumPy, etc. Bachelor’s Degree in computer science or related field Should have excellent analytical skills, able to recognize data patterns and troubleshoot the data. Highly organized and analytic, capable of solving business problems using technology Excellent analytical and problem-solving skills 3.0 GPA or higher Good written and oral communication skills Benefits Pre-employment online training {2 weeks (15 hrs./week)} - Training Stipend of $500 Relocation Reimbursement of up to $1,500 2 weeks of Paid Vacation Health Insurance including Vision and Dental Employee Assistance Program Dependent Care FSA Commuter Benefits Voluntary Life Insurance Who is Quintrix? Quintrix is on a mission to help individuals like you develop your technology talent. We have helped hundreds of candidates kick-start their career in tech. You will be “paid-to-learn”, qualifying you for a high-paying tech job with one of our top employers EEO: “Mindlance is an Equal Opportunity Employer and does not discriminate in employment on the basis of – Minority/Gender/Disability/Religion/LGBTQI/Age/Veterans.”",
        "url": "https://www.linkedin.com/jobs/view/3950341152"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3932083364,
        "company": "Colibri Group",
        "title": "Data Analytics Engineer",
        "created_on": 1720639251.1305509,
        "description": "At Colibri, culture is a critical part of our collective success, and we live our values everyday: Love, Joy, Boldness, Teamwork and Curiosity. These values guide our interactions with each other, our customers, and the community as a whole. We have a rich and storied history. Colibri is one of the pioneers of online professional education, introducing some of the first web-based professional education courses in 2001. Today, the company’s family of brands are the leading online professional education platforms in their respective end-markets. We proudly to serve >1 million customers annually and employ more than 1,000 mission-aligned professionals. To learn more, please visit: www.colibrigroup.com Position Overview The Data Analytics Engineer will be a part of the Data Engineering team whose primary mission is to build trusted Data Ingestion Pipelines to seamlessly move and transform data from SaaS and in house transactional databases to a Data Warehouse to enable actionable business insights. The data in the Warehouse will serve as a foundation for Reporting, Machine Learning Analysis and Generative AI. The ideal candidate will be a strong technical resource with effective communication and leadership skills to be a member of data engineering team. The role requires collaboration with cross functional teams and ability to play multiple roles depending upon the situation. The ideal candidate should have strong bias for action, able to dig into details when requirements are fuzzy and have a passion to push forward to progress. If you are an ambitious technologist who is looking to grow, and who is passionate about building world-class products and technology, we encourage you to apply for this exciting opportunity. What You'll Do Implementation of data ingestion pipelines following the guidelines and best practices defined. Implement data mapping, data transformation, automated testing and data validation Contribute and implement standards and best practices within the Data Engineering Team Write detailed stories, sprint grooming and actively contribute in the Agile software development life cycle. Accountable for Delivery and 100% uptime of the Environments Collaborate with cross-functional teams, including ecosystem leadership and data insights teams, engineering, sales, and marketing to ensure all stakeholders are aligned Partner with scrum master daily to provide updates Partner with Data Insights team on a daily basis to stay aligned on priorities Stay up to date with the latest industry trends and emerging technologies to identify opportunities for innovation and growth Establish and maintain effective communication channels with internal and external stakeholders What You'll Need to Succeed BS in Engineering, Business Administration, Statistics, Business Analytics, or applicable experience in related field Minimum of (3) three years of SDLC experience in ETL/ELT environments with tools/technologies such as Fivetran, dbt, SQL, AWS Redshift, Boomi, PowerBI , Git Ability to work on-site at least two days a week in St. Louis, MO Minimum of (2) years of experience in building out schemas for Analytical purposes Minimum (1) years of hands on experience AWS Cloud and Services such as AWS Glue, AWS Lambda, Python Familiar with ERP, CRM products such as Salesforce, NetSuite, Hubspot and Eloqua Strong development background and expert level skills with modern SDLC best practices Proven track record of delivering successful data initiatives Excellent communication and interpersonal skills, with the ability to build strong relationships with internal and external stakeholders Strong analytical and problem-solving skills, with the ability to make data-driven decisions Experience working in a fast-paced, agile environment Ability to prioritize and manage multiple projects simultaneously Colibri Group welcomes applicants from all backgrounds and experiences, and we understand that not every candidate will meet every requirement listed in the job description. Research has shown that women and people of color may be less likely to apply to jobs unless they feel they meet every qualification, and we want to actively combat this bias in our hiring process. If you're excited about the role and believe you have the skills and experience to contribute to our team, we encourage you to apply, even if your background doesn't align perfectly with every qualification listed. We are committed to building a diverse and inclusive workplace, and we believe that diversity of perspectives and experiences is essential to our success. You may be just the right candidate for this role or another position within our organization. Don't hesitate to take the leap and apply today! Colibri Group is an equal opportunity employer that is committed to diversity and inclusion in the workplace. Colibri Group prohibits discrimination and harassment of any kind based on race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, disability, genetic information, or any other status protected under federal, state, or local law.",
        "url": "https://www.linkedin.com/jobs/view/3932083364"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Atlanta, GA",
        "job_id": 3945906032,
        "company": "Tata Consultancy Services",
        "title": "Data Engineer",
        "created_on": 1720639252.670236,
        "description": "The ETL Engineer performs design, development and implementation of integration processes for both the Enterprise Data lake, Data Warehouse and Applications Analyzes requirements and existing resources to create efficient database and integration designs that meet company IT standards. Works with project and business analyst leads to develop and clarify in-depth technical requirements. Participates in all phases of the integration development lifecycle, including unit testing, quality assurance(QA) and ongoing support. Helps with Production support as needed Excellent communication skills (both verbal and written) Proven ability to provide strong problem solving skills. Must be self-motivated and know when to seek guidance Must be flexible, be able to change priorities quickly, and handle multiple tasks concurrently Individual must be a self-starter and capable of working independently as well as part of a team Capable of learning new tools and technologies",
        "url": "https://www.linkedin.com/jobs/view/3945906032"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3943518881,
        "company": "Steneral Consulting",
        "title": "Remote Work - Need Data Analytics Engineer",
        "created_on": 1720639254.35989,
        "description": "100% Remote Must be W2 or 1099 Qualifications Experience with cloud platform AWS and their respective data services. AWS experience: Data Storage Tools: Redshift, S3 Automation Tools: Batch, EventBridge, Lambda, Step Functions, Glue, etc. MadPlotLib for Python Sagemaker Experience working closely with a Data Science team Key Responsibilities Design, develop, and maintain robust analytics solutions to support data-driven decision-making across the organization. Collaborate with data scientists, analysts, and business stakeholders to understand analytical requirements and translate them into technical specifications. Develop and maintain ETL (Extract, Transform, Load) processes to ingest and process large volumes of structured and unstructured data from various sources. Implement and manage data models and architecture to ensure the integrity, security, and performance of analytics systems. Optimize and troubleshoot analytics workflows to ensure data quality and system reliability. Perform data wrangling and cleansing to prepare data for analysis and reporting. Implement and manage data governance practices to ensure data accuracy, consistency, and compliance with regulatory requirements. Develop and maintain dashboards, reports, and visualizations to communicate insights to business stakeholders. Monitor and maintain analytics infrastructure, including databases, data warehouses, and cloud-based storage solutions.",
        "url": "https://www.linkedin.com/jobs/view/3943518881"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Seattle, WA",
        "job_id": 3967744534,
        "company": "MokshaaLLC",
        "title": "Senior Data Engineer - Seattle, ONSITE Day 1",
        "created_on": 1720639260.3251827,
        "description": "JOB TITLE: SENIOR DATA ENGINEER ONSITE LOCATION: SEATTLE, WA LONG TERM CONTRACT(1YR+) ENGAGEMENT TYPE: W2/1099/C2C Core skills: Advanced SQL, AWS Redshift, Apache Spark, Data deep dive, Effective communication with stakeholders, autonomous project execution. Profiles with FAANG (Facebook/Meta, Amazon, Apple, Netflix, Google/Alphabet) style company experience preferred. We're looking for experienced consultants with at least 10 years of real work experience. We only want genuine profiles, so please make sure your LinkedIn profile reflects your relevant experience. Also, please avoid submitting lengthy resumes that are common in the industry with 8 and 9 years of typical bench marketed resumes. As a Data Engineer, you should be an expert with data warehousing technical components (e.g. Data Modeling, ETL and Reporting), infrastructure (e.g. hardware and software) and their integration. You should have deep understanding of the architecture for enterprise level data warehouse solutions using multiple platforms (RDBMS, Columnar, Cloud) . You should be an expert in the design, creation, management, and business use of large datasets. You should have excellent business and communication skills to be able to work with business owners to develop and define key business questions, and to build data sets that answer those questions. Our ideal candidate thrives in a fast-paced environment, relishes working with large transactional volumes and big data, enjoys the challenge of highly complex business contexts (that are typically being defined in real-time). Key job responsibilities * Design, implement, and support a platform providing secured access to large datasets. * Own the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions. * Recognize and adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation. * Tune application and query performance using profiling tools and SQL. * Analyze and solve problems at their root, stepping back to understand the broader context. * Learn and understand a broad range of Amazon’s data resources and know when, how, and which to use and which not to use. * Keep up to date with advances in big data technologies and run pilots to design the data architecture to scale with the increased data volume using AWS. * Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for datasets. * Triage many possible courses of action in a high-ambiguity environment, making use of both quantitative analysis and business judgment. Core skills: Advanced SQL, AWS Redshift, Apache Spark, Data deep dive, Effective communication with stakeholders, autonomous project execution. We're looking for experienced consultants with at least 10 years of real work experience. We only want genuine profiles, so please make sure your LinkedIn profile reflects your relevant experience. Also, please avoid submitting lengthy resumes that are common in the industry with 8 and 9 years of typical bench marketed resumes.",
        "url": "https://www.linkedin.com/jobs/view/3967744534"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Rocky Hill, CT",
        "job_id": 3951226686,
        "company": "Infowave Systems, Inc",
        "title": "Data Engineer with PySpark",
        "created_on": 1720639262.1113896,
        "description": "Job Title: Data Engineer with PySpark -W2 only Location: Rancho Cucamonga,CA/ (Onsite Role) We are open for Visa Sponsorship As a Data Engineer, you will play a key role in designing and implementing robust data pipelines and infrastructure to support our data-driven initiatives. Responsibilities Key Responsibilities: Design, develop, and maintain scalable data pipelines using PySpark. Collaborate with data scientists and analysts to understand data requirements and implement solutions. Parse JSON files to SQL Tables using PySpark. Build automation pipelines for Continuous Integration/Continuous Deployment (CI/CD) using Azure DevOps or similar tools. Optimize data workflows for performance, reliability, and efficiency. Implement data quality checks and monitoring processes to ensure data integrity. Troubleshoot and resolve issues related to data pipeline performance and reliability. Document technical designs, processes, and procedures. Requirements Requirements: Bachelor's degree in Computer Science, Engineering, or related field. Proven experience working as a Data Engineer in a professional setting. Hands-on experience with PySpark for data processing and analysis. Strong proficiency in SQL and relational databases (e.g., PostgreSQL, MySQL). Experience building automation pipelines for CI/CD using Azure DevOps or similar tools (e.g., Jenkins, GitLab CI). Experience Parsing JSON files to SQL Tables suing Python/PySpark. Familiarity with cloud platforms such as Azure, AWS, or Google Cloud Platform. Excellent problem-solving skills and attention to detail. Strong communication and collaboration skills. Nice To Have Experience with containerization technologies (e.g., Docker, Kubernetes). Knowledge of big data technologies (e.g., Hadoop, Spark). Familiarity with data warehousing concepts and tools (e.g., Snowflake, Redshift). Understanding of machine learning concepts and algorithms. Why Join Us Opportunity to work on cutting-edge data technologies and projects. Collaborative and supportive team environment. Competitive salary and benefits package. Career growth and development opportunities. If you are passionate about leveraging data to drive business impact and thrive in a fast-paced environment, we would love to hear from you! Apply now to join our team as a Data Engineer and contribute to our mission of transforming data into insights. Infowave Systems is an equal opportunity employer that is committed to diversity and inclusion in the workplace.",
        "url": "https://www.linkedin.com/jobs/view/3951226686"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "United States",
        "job_id": 3951341717,
        "company": "GoodParty.org",
        "title": "Data Engineer",
        "created_on": 1720639265.965518,
        "description": "Data Engineer (Remote) GoodParty.org is looking for a Data Engineer with JavaScript or Python and experience building ETL pipelines who’s based in the U.S. to join us in our mission of making people matter more than money in our democracy. About GoodParty.org : GoodParty/org is a tech platform and movement powering the mission to make people matter more than money in our democracy. GoodParty.org is founded and fully-funded by serial entrepreneur, Farhad Mohit, with over $1B in exits — including Flipagram (now TikTok), Shopzilla, and BizRate. Why? Americans – especially young people – are losing faith in democracy. The rigged, two party system has turned “Independent” into a dirty word. Voters are trapped between choosing the lesser of two evils or sitting out elections altogether. A majority of eligible voters (over 130M Americans), including more than half of Millennials and Gen Z, say that neither Republicans, nor Democrats represent them. But they keep getting re-elected! Whether you’re concerned about the environment, healthcare costs, wars, inequality, or our individual freedoms, many great solutions are held back by the dark doom-loop of dysfunctional partisan politics. While mainstream media and the two major parties focus on federal elections (e.g. Congress and President), GoodParty.org is building from the bottom up. With over 500,000 elected offices in the United States, 70% of which are uncontested, we are powering a grassroots revolution to take back our communities from career politicians by helping real people run and win local and state level campaigns all across the country. How? GoodParty.org’s solution is to make it simple to run, win and serve. We offer free AI powered tools to help real people run and win elections, independent of partisan support or big money. We train people to run for office and support candidates who take our pledge so they don’t have to sell out to the Democrats or Republicans, raise big money, or cow-tow to special interests. Our free AI Campaign Manager and team with 50+ years of campaign experience make running, winning and serving as an independent, people-powered anti-corruption candidate possible. We’re also building a movement of bright, caring Americans who are ready to end this country's two-party political dysfunction. Our community’s collective power is being harnessed to elect the most promising independents around the country. By leveraging social media and creators, we’re making our mission to make people matter more than money mainstream. What you’ll do: Build systems that collect, manage, and convert raw data into usable information that powers the product our candidates are using to run for office as well as inform GoodParty.org’s team to help candidates win. Implement methods to improve data reliability, security, and quality. Combine raw information from different sources to create consistent and machine-readable formats. Develop and test architectures that enable data extraction and transformation for predictive or prescriptive modeling. Create unique data infrastructure, run tests, and keep systems updated using big data technologies and databases. Collaborate with our lean, agile team using CI/CD and Kanban methodologies. Work directly with Product, Design, and QA on your projects. Also partner closely with Data Analytics Lead on backend for data projects that power team KPIs and product performance. Report to the CTO, join a team of senior developers as the first Data Engineer hire. Who we’re looking for: You’re up for the audacious challenge of re-coding our democracy for Good. You’ve got strong scripting skills with JavaScript or Python and you can do data exploration in SQL. You have experience with data modeling, warehousing, and building ETL pipelines that process millions of records. You’re a proven A-player with 8+ years of relevant full-time work experience, ideally within a fast-paced 0-to-1 environment. We care more about talent and trajectory than years of experience. You’ve got experience with big data technologies such as Hadoop, Hive, Spark, or EMR You’re actively interested in using AI, LLMs, and automation of tasks through AI and have done previous work or personal projects in this area. You have advanced technical and analytical skills to diagnose, isolate, and resolve problems.. You’re able to work independently and collaborate with others when needed to iteratively solve problems in real-time. Our solution is desperately needed and the issues of our time are all bottlenecked by our corrupted democracy. Bonus if: You have experience studying, working, or volunteering in politics, political activism, or democracy reform. You’re sick of sitting on the sidelines watching our democracy fail to meet its potential. Why you should work at GoodParty.org : Join us in solving the biggest problem of our time – fixing our democracy – so all other problems can be solved We’re a Public Benefit Corporation that prioritizes social impact over profit Work where and how you like on a fully remote team spread across the country Work when you work best with flexible work hours. We aim to limit meetings and ensure they’re useful. Join a passionate team with diverse political views focused on one mission 100% coverage of health, dental, and vision benefits for you and your dependents We encourage you to take time off to recharge and have an unlimited PTO (sick and vacation) policy. This is a marathon, not a sprint. We believe a work-life balance is needed to get there. We’ll make sure you’ve got what you need to work remotely through our workspace setup stipend If you work from home, we give $50/month to help with your internet and other expenses. If you prefer co-working, that’s an option we can discuss too. Fully funded for our mission. No stressing over whether the next round of funding will come through. Focus on making an impact. That’s what matters here. The salary range is $150,000 to $200,000 for this remote, full-time position with excellent benefits. Title will also be commensurate with experience. This range reflects the range of possible compensation for this role at the time of this posting. We may ultimately pay more or less than the posted range. An employee’s position within the salary range will be based on several factors including, but not limited to, relevant experience, skills, seniority, education, qualifications, certifications, and organizational needs. We reserve the right to modify this pay range at any time. Ready to learn more or apply? This is an exciting opportunity to join a growing organization dedicated to making people matter more than money in our democracy. If you are excited about GoodParty.org, we hope you'll apply! Stay connected with our mission by signing up atGoodParty.org. If you know anyone else who may be interested, please share this job posting – thanks!",
        "url": "https://www.linkedin.com/jobs/view/3951341717"
    },
    {
        "task_id": "abd8fe5b07d14899a01e4e1078754fee",
        "keyword": "Data Engineer",
        "location": "Spring, TX",
        "job_id": 3956043395,
        "company": "Western Midstream",
        "title": "Data Engineer",
        "created_on": 1720639270.162315,
        "description": "Job Summary Western Midstream is seeking an early career data engineer to join our new Data Center of Excellence team in The Woodlands, TX. As a data engineer, you will be responsible for deploying and maintaining innovative solutions utilizing best in class platforms, cloud services, and opensource technologies. You will use your skills in data and development to improve and streamline our operations and help our users succeed. Responsibilities: Design, build, and manage data pipelines to ingest, transform, and distribute data between on-premises and cloud destinations Utilize technologies from Aveva PI, Qlik, OpenText, SAP Datasphere, and Azure Cloud Services, including Fabric, Data Factory, Synapse, and more to enable custom applications and BI efforts Employ programming languages like C#, Java, or Python for automation and API integrations tasks Work closely with developers and engineers to provide automation and data necessary for custom applications, visualizations, and analytic models Conduct thorough functional testing, troubleshooting, and performance tuning to ensure the reliability and efficiency of developed solutions Integrate and maintain new data management technologies, processes, and monitoring tools Requirements: 1-2 years of experience as Data Engineer or Developer (Python preferred) Basic of Azure cloud services, relational database design principles, performance tuning, and optimization Exhibit analytic skills related to working with both structured and unstructured datasets. Have proficiency in SQL, ETL/ELT processes/tooling and data modeling Must have excellent communication skills with the ability to interact with developers supporting requirements gathering. Education: Bachelor's Degree in Information Science, Computer Science, or equivalent experience Travel Requirements: Travel will be required up to 5%. Work Schedule : This position will work 9 hours Monday – Thursday and 4 hours on Friday. The position will require Monday, Tuesday, and Thursday in The Woodlands office.",
        "url": "https://www.linkedin.com/jobs/view/3956043395"
    }
]